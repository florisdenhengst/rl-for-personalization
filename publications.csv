Title,Abstract,Keywords,Domain,Domain other,Database,How many learners are used?,Where are traits of individuals included?,Is training performed online or in batch?,Is training performed on a simulator?,Is training performed on static real life data?,"Is training performed live, in a real-life setting?",Is an evaluation performed on a simulator?,Is an evaluation performed on static real life data?,Is a live evaluation performed in a real life setting?,Is a comparison with 'no personalization' performed?,Is there a comparison between RL/CB and other personalization techniques?,Is suitability of actions explicitly defined by users?,Is suitability of actions implicitly derived from observations on user behavior?,Is safety mentioned as a concern in the application?,Are models of user responses to system behavior available?,Are data on human responses to system behavior available?,Can new interactions with users be sampled with ease?,Is privacy mentioned as a concern in the application?,Can all information to base personalization on be measured directly?,algorithms,Source Title,Authors,Authors with affiliations,Citation Count,DOI,Type,Year,Bibtex_key,Bibtex
A contextual-bandit approach to personalized news article recommendation,"Personalized web services strive to adapt their services (advertisements, news articles, etc.) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation. In this work, we model personalized recommendation of news articles as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks. The contributions of this work are three-fold. First, we propose a new, general contextual bandit algorithm that is computationally efficient and well motivated from learning theory. Second, we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic. Finally, using this offline evaluation method, we successfully applied our new algorithm to a Yahoo Front Page Today Module dataset containing over 33 million events. Results showed a 12.5% click lift compared to a standard context-free bandit algorithm, and the advantage becomes even greater when data gets more scarce. © 2010 International World Wide Web Conference Committee (IW3C2).",contextual bandit; exploration/exploitation dilemma; personalization; recommender systems; web service,Entertainment,,SCOPUS,1,state representation,online,n,y,n,n,y,n,y,n,n,y,n,n,y,y,y,n,LinUCB,"Proceedings of the 19th International Conference on World Wide Web, WWW '10","Li L., Chu W., Langford J., Schapire R.E.","Li, L., Yahoo Labs., United States; Chu, W., Yahoo Labs., United States; Langford, J., Yahoo Labs., United States; Schapire, R.E., Dept. of Computer Science, Princeton University, United States",350.0,10.1145/1772690.1772758,Conference Paper,2010-01-01,Li_2010," @article{Li_2010, title={A contextual-bandit approach to personalized news article recommendation}, ISBN={9781605587998}, url={http://dx.doi.org/10.1145/1772690.1772758}, DOI={10.1145/1772690.1772758}, journal={Proceedings of the 19th international conference on World wide web - WWW  ’10}, publisher={ACM Press}, author={Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E.}, year={2010}}
"
Reinforcement learning design for cancer clinical trials,"We develop reinforcement learning trials for discovering individualized treatment regimens for lifethreatening diseases such as cancer. A temporal-difference learning method called Q-learning is utilized that involves learning an optimal policy from a single training set of finite longitudinal patient trajectories. Approximating the Q-function with time-indexed parameters can be achieved by using support vector regression or extremely randomized trees. Within this framework, we demonstrate that the procedure can extract optimal strategies directly from clinical data without relying on the identification of any accurate mathematical models, unlike approaches based on adaptive design. We show that reinforcement learning has tremendous potential in clinical research because it can select actions that improve outcomes by taking into account delayed effects even when the relationship between actions and outcomes is not fully known. To support our claims, the methodology's practical utility is illustrated in a simulation analysis. In the immediate future, we will apply this general strategy to studying and identifying new treatments for advanced metastatic stage IIIB/IV non-small cell lung cancer, which usually includes multiple lines of chemotherapy treatment. Moreover, there is significant potential of the proposed methodology for developing personalized treatment strategies in other cancers, in cystic fibrosis, and in other life-threatening diseases. Copyright © 2009 John Wiley & Sons, Ltd.",Adaptive design; Clinical trials; Dynamic treatment regime; Extremely randomized trees; Multistage decision problems; Non-small cell lung cancer; Optimal policy; Reinforcement learning; Support vector regression,Health,,SCOPUS,1,state representation,batch,y,n,n,y,n,n,n,n,n,y,n,n,n,n,n,y,Q-Learning,Statistics in Medicine,"Zhao Y., Kosorok M.R., Zeng D.","Zhao, Y., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States; Kosorok, M.R., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States; Zeng, D., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States",68.0,10.1002/sim.3720,Article,2009-01-01,Zhao_2009," @article{Zhao_2009, title={Reinforcement learning design for cancer clinical trials}, volume={28}, ISSN={1097-0258}, url={http://dx.doi.org/10.1002/sim.3720}, DOI={10.1002/sim.3720}, number={26}, journal={Statistics in Medicine}, publisher={Wiley}, author={Zhao, Yufan and Kosorok, Michael R. and Zeng, Donglin}, year={2009}, month={Nov}, pages={3294–3315}}
"
Reinforcement learning agent for personalized information filtering,"This paper describes a method for learning user's interests in the Web-based personalized information filtering system called WAIR. The proposed method analyzes user's reactions to the presented documents and learns from them the profiles for the individual users. Reinforcement learning is used to adapt the term weights in the user profile so that user's preferences are best represented. In contrast to conventional relevance feedback methods which require explicit user feedbacks, our approach learns user preferences implicitly from direct observations of user behaviors during interaction. Field tests have been made which involved 7 users reading a total of 7,700 HTML documents during 4 weeks. The proposed method showed superior performance in personalized information filtering compared to the existing relevance feedback methods.",,Other,Information filtering,,1/person,state representation,online,n,n,y,n,n,y,y,y,n,y,n,n,y,y,n,y,Q-Learning,"International Conference on Intelligent User Interfaces, Proceedings IUI","Seo Young-Woo, Zhang Byoung-Tak","Seo, Young-Woo, Seoul Natl Univ, Seoul, South Korea; Zhang, Byoung-Tak, Seoul Natl Univ, Seoul, South Korea",61.0,,Conference Paper,2000-01-01,seo2000reinforcement,"@inproceedings{seo2000reinforcement,
  title={A reinforcement learning agent for personalized information filtering},
  author={Seo, Young-Woo and Zhang, Byoung-Tak},
  booktitle={Proceedings of the 5th international conference on Intelligent user interfaces},
  pages={248--251},
  year={2000},
  organization={ACM}
}
"
Reinforcement learning strategies for clinical trials in nonsmall cell lung cancer,"Typical regimens for advanced metastatic stage IIIB/IV nonsmall cell lung cancer (NSCLC) consist of multiple lines of treatment. We present an adaptive reinforcement learning approach to discover optimal individualized treatment regimens from a specially designed clinical trial (a ""clinical reinforcement trial"") of an experimental treatment for patients with advanced NSCLC who have not been treated previously with systemic therapy. In addition to the complexity of the problem of selecting optimal compounds for first- and second-line treatments based on prognostic factors, another primary goal is to determine the optimal time to initiate second-line therapy, either immediately or delayed after induction therapy, yielding the longest overall survival time. A reinforcement learning method calledQ-learning is utilized, which involves learning an optimal regimen from patient data generated from the clinical reinforcement trial. Approximating theQ-function with time-indexed parameters can be achieved by using a modification of support vector regression that can utilize censored data. Within this framework, a simulation study shows that the procedure can extract optimal regimens for two lines of treatment directly from clinical data without prior knowledge of the treatment effect mechanism. In addition, we demonstrate that the design reliably selects the best initial time for second-line therapy while taking into account the heterogeneity of NSCLC across patients. © 2011, The International Biometric Society.",Adaptive design; Dynamic treatment regime; Individualized therapy; Multistage decision problems; Nonsmall cell lung cancer; Personalized medicine; Q-learning; Reinforcement learning; Support vector regression,Health,,SCOPUS,1/group,state representation,batch,y,n,n,y,y,n,y,n,n,y,n,n,n,n,n,y,Q-Learning,Biometrics,"Zhao Y., Zeng D., Socinski M.A., Kosorok M.R.","Zhao, Y., Global Biostatistics and Epidemiology, Amgen Inc., One Amgen Center Drive, Thousand Oaks, CA 91320, United States; Zeng, D., Department of Biostatistics, University of North Carolina at Chapel Hill, 3101 McGavran-Greenberg, CB 7420, Chapel Hill, NC 27599, United States; Socinski, M.A., Department of Medicine, University of North Carolina at Chapel Hill, Physicians Office Building, 170 Manning Drive, Chapel Hill, NC 27599, United States; Kosorok, M.R., Department of Biostatistics, University of North Carolina at Chapel Hill, 3101 McGavran-Greenberg, CB 7420, Chapel Hill, NC 27599, United States",50.0,10.1111/j.1541-0420.2011.01572.x,Article,2011-01-01,Zhao_2011," @article{Zhao_2011, title={Reinforcement Learning Strategies for Clinical Trials in Nonsmall Cell Lung Cancer}, volume={67}, ISSN={0006-341X}, url={http://dx.doi.org/10.1111/j.1541-0420.2011.01572.x}, DOI={10.1111/j.1541-0420.2011.01572.x}, number={4}, journal={Biometrics}, publisher={Wiley}, author={Zhao, Yufan and Zeng, Donglin and Socinski, Mark A. and Kosorok, Michael R.}, year={2011}, month={Mar}, pages={1422–1433}}
"
Learning user's preferences by analyzing Web-browsing behaviors,"This paper describes a method for an information filtering agent to learn user's preferences. The proposed method observes user's reactions to the filtered documents and learns from them the profiles for the individual users. Reinforcement learning is used to adapt the most significant terms that best represent user's interests. In contrast to conventional relevance feedback methods which require explicit user feedbacks, our approach learns user preferences implicitly from direct observations of browsing behaviors during interaction. Field tests have been made which involved 10 users reading a total of 18,750 HTML documents during 45 days. The proposed method showed superior performance in personalized information filtering compared to the existing relevance feedback methods.",,Entertainment,,SCOPUS,1/person,state representation,online,n,y,n,n,y,n,y,y,n,y,n,n,n,y,n,y,Q-Learning,Proceedings of the International Conference on Autonomous Agents,"Seo Young-Woo, Zhang Byoung-Tak","Seo, Young-Woo, Seoul Natl Univ, Seoul, South Korea; Zhang, Byoung-Tak, Seoul Natl Univ, Seoul, South Korea",47.0,,Conference Paper,2000-01-01,seo2000learning,"@inproceedings{seo2000learning,
  title={Learning user's preferences by analyzing Web-browsing behaviors},
  author={Seo, Young-Woo and Zhang, Byoung-Tak},
  booktitle={Proceedings of the fourth international conference on Autonomous agents},
  pages={381--387},
  year={2000},
  organization={ACM}
}
"
Personalized web-document filtering using reinforcement learning,"Document filtering is increasingly deployed in Web environments to reduce information overload of users. We formulate online information filtering as a reinforcement learning problem, i.e., TD(0). The goal is to learn user profiles that best represent information needs and thus maximize the expected value of user relevance feedback. A method is then presented that acquires reinforcement signals automatically by estimating user's implicit feedback from direct observations of browsing behaviors. This ""learning by observation"" approach is contrasted with conventional relevance feedback methods which require explicit user feedbacks. Field tests have been performed that involved 10 users reading a total of 18,750 HTML documents during 45 days. Compared to the existing document filtering techniques, the proposed learning method showed superior performance in information quality and adaptation speed to user preferences in online filtering.",,Entertainment,,SCOPUS,1/person,state representation,other,n,y,y,n,y,y,y,y,n,y,n,n,y,y,n,n,TD-Learning,Applied Artificial Intelligence,"Zhang B.-T., Seo Y.-W.","Zhang, B.-T., Biointelligence Lab., Sch. of Comp. Sci. and Engineering, Seoul National University, Seoul, South Korea, Biointelligence Lab., Sch. of Comp. Sci. and Engineering, Seoul National University, Seoul 151-742, South Korea; Seo, Y.-W., Biointelligence Lab., Sch. of Comp. Sci. and Engineering, Seoul National University, Seoul, South Korea",41.0,10.1080/088395101750363993,Article,2001-01-01,Zhang_2001," @article{Zhang_2001, title={Personalized web-document filtering using reinforcement learning}, volume={15}, ISSN={1087-6545}, url={http://dx.doi.org/10.1080/088395101750363993}, DOI={10.1080/088395101750363993}, number={7}, journal={Applied Artificial Intelligence}, publisher={Informa UK Limited}, author={Zhang, Byoung-Tak and Seo, Young-Woo}, year={2001}, month={Aug}, pages={665–685}}
"
Optimistic bayesian sampling in contextual-bandit problems,"In sequential decision problems in an unknown environment, the decision maker often faces a dilemma over whether to explore to discover more about the environment, or to exploit current knowledge. We address the exploration-exploitation dilemma in a general setting encompassing both standard and contextualised bandit problems. The contextual bandit problem has recently resurfaced in attempts to maximise click-through rates in web based applications, a task with significant commercial interest. In this article we consider an approach of Thompson (1933) which makes use of samples from the posterior distributions for the instantaneous value of each action. We extend the approach by introducing a new algorithm, Optimistic Bayesian Sampling (OBS), in which the probability of playing an action increases with the uncertainty in the estimate of the action value. This results in better directed exploratory behaviour. We prove that, under unrestrictive assumptions, both approaches result in optimal behaviour with respect to the average reward criterion of Yang and Zhu (2002). We implement OBS and measure its performance in simulated Bernoulli bandit and linear regression domains, and also when tested with the task of personalised news article recommendation on a Yahoo! Front Page Today Module data set. We find that OBS performs competitively when compared to recently proposed benchmark algorithms and outperforms Thompson's method throughout. © 2012 Benedict C. May, Nathan Korda, Anthony Lee and David S. Leslie.",Contextual bandits; Exploration-exploitation; Multi-armed bandits; Sequential allocation; Thompson sampling,Commerce,,SCOPUS,1/person,state representation,batch,n,y,n,n,y,n,n,y,n,n,n,n,y,y,n,y,"Optimistic Bayesian Sampling (OBS),Thompson Sampling",Journal of Machine Learning Research,"May B.C., Korda N., Lee A., Leslie D.S.","May, B.C., School of Mathematics, University of Bristol, Bristol, BS8 1TW, United Kingdom; Korda, N., Oxford-Man Institute, University of Oxford, Walton Well Road, Oxford, OX2 6ED, United Kingdom; Lee, A., Oxford-Man Institute, University of Oxford, Walton Well Road, Oxford, OX2 6ED, United Kingdom; Leslie, D.S., School of Mathematics, University of Bristol, Bristol, BS8 1TW, United Kingdom",40.0,,Article,2012-01-01,may2012optimistic,"@article{may2012optimistic,
  title={Optimistic Bayesian sampling in contextual-bandit problems},
  author={May, Benedict C and Korda, Nathan and Lee, Anthony and Leslie, David S},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={Jun},
  pages={2069--2106},
  year={2012}
}
"
A personalized and integrative comparison-shopping engine and its applications,"Agents are the catalysts for commerce on the Web today. For example, comparison-shopping agents mediate the interactions between consumers and suppliers in order to yield markets that are more efficient. However, today's shopping agents are price-dominated, unreflective of the nature of supplier/consumer differentiation or the changing course of differentiation over time. This paper aims to tackle this dilemma and advances shopping agents into a stage where both kinds of differentiation are taken into account for enhanced understanding of the realities. We call them personalized and integrative shopping agents. These agents can leverage the interactive power of the Web for a more accurate understanding of consumer's preferences. This paper then presents a comparison-shopping engine that can be easily instantiated to become personalized and integrative shopping agents. This engine comprises of a product/merchant information collector, a consumer behavior extractor, a user profile manager, and an on-line learning personalized ranking module. We have built this engine and instantiated a comparison-shopping system for collecting preliminary evaluation results. The results show that this system is quite promising in overcoming the reality challenges of comparison shopping. In order to strengthen the contributions of this engine, we also gave a fielded application of this engine for personalized travel information discovery and explained the great potentials of this engine for a variety of comparison-shopping tasks. © 2002 Elsevier Science B.V. All rights reserved.",(Multi-) agent systems; Comparison shopping; Consumer valuation models; Neural networks; Reinforcement learning,Commerce,,SCOPUS,1,not used,online,n,n,y,n,n,y,n,n,y,n,n,y,y,y,n,y,Q-Learning,Decision Support Systems,Yuan S.-T.,"Yuan, S.-T., Information Management Department, Fu-Jen University, 242 Taipei, Taiwan",38.0,10.1016/S0167-9236(02)00077-5,Conference Paper,2003-01-01,Yuan_2003," @article{Yuan_2003, title={A personalized and integrative comparison-shopping engine and its applications}, volume={34}, ISSN={0167-9236}, url={http://dx.doi.org/10.1016/S0167-9236(02)00077-5}, DOI={10.1016/s0167-9236(02)00077-5}, number={2}, journal={Decision Support Systems}, publisher={Elsevier BV}, author={Yuan, Soe-Tsyr}, year={2003}, month={Jan}, pages={139–156}}
"
Usage-based web recommendations: A reinforcement learning approach,"Information overload is no longer news; the explosive growth of the Internet has made this issue increasingly serious for Web users. Users are very often overwhelmed by the huge amount of information and are faced with a big challenge to find the most relevant information in the right time. Recommender systems aim at pruning this information space and directing users toward the items that best meet their needs and interests. Web Recommendation has been an active application area in Web Mining and Machine Learning research. In this paper we propose a novel machine learning perspective toward the problem, based on reinforcement learning. Unlike other recommender systems, our system does not use the static patterns discovered from web usage data, instead it learns to make recommendations as the actions it performs in each situation. We model the problem as Q-Learning while employing concepts and techniques commonly applied in the web usage mining domain. We propose that the reinforcement learning paradigm provides an appropriate model for the recommendation problem, as well as a framework in which the system constantly interacts with the user and learns from her behavior. Our experimental evaluations support our claims and demonstrate how this approach can improve the quality of web recommendations. Copyright 2007 ACM.",Machine learning; Personalization; Recommender systems; Reinforcement learning; Web usage mining,Commerce,,SCOPUS,1,not used,batch,y,y,n,y,y,n,n,y,n,y,n,n,n,y,n,y,Q-Learning,RecSys'07: Proceedings of the 2007 ACM Conference on Recommender Systems,"Taghipour N., Kardan A., Ghidary S.S.","Taghipour, N., Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran; Kardan, A., Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran; Ghidary, S.S., Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran",31.0,10.1145/1297231.1297250,Conference Paper,2007-01-01,Taghipour_2007," @article{Taghipour_2007, title={Usage-based web recommendations}, ISBN={9781595937308}, url={http://dx.doi.org/10.1145/1297231.1297250}, DOI={10.1145/1297231.1297250}, journal={Proceedings of the 2007 ACM conference on Recommender systems  - RecSys  ’07}, publisher={ACM Press}, author={Taghipour, Nima and Kardan, Ahmad and Ghidary, Saeed Shiry}, year={2007}}
"
Design and Evaluation of a Self-Learning HTTP Adaptive Video Streaming Client,"HTTP Adaptive Streaming (HAS) is becoming the de facto standard for Over-The-Top (OTT)-based video streaming services such as YouTube and Netflix. By splitting a video into multiple segments of a couple of seconds and encoding each of these at multiple quality levels, HAS allows a video client to dynamically adapt the requested quality during the playout to react to network changes. However, state-of-the-art quality selection heuristics are deterministic and tailored to specific network configurations. Therefore, they are unable to cope with a vast range of highly dynamic network settings. In this letter, a novel Reinforcement Learning (RL)-based HAS client is presented and evaluated. The self-learning HAS client dynamically adapts its behaviour by interacting with the environment to optimize the Quality of Experience (QoE), the quality as perceived by the end-user. The proposed client has been thoroughly evaluated using a network-based simulator and is shown to outperform traditional HAS clients by up to 13% in a mobile network environment.",Streaming media;intelligent agent;learning systems;quality of service,Entertainment,,IEEE Explore,1,not used,unknown,y,n,n,y,n,n,y,n,n,n,n,n,n,n,n,y,Q-Learning,IEEE Communications Letters,M. Claeys; S. Latre; J. Famaey; F. De Turck,,27.0,10.1109/LCOMM.2014.020414.132649,IEEE Journals & Magazines,2014-01-01,Claeys_2014," @article{Claeys_2014, title={Design and Evaluation of a Self-Learning HTTP Adaptive Video Streaming Client}, volume={18}, ISSN={1089-7798}, url={http://dx.doi.org/10.1109/LCOMM.2014.020414.132649}, DOI={10.1109/lcomm.2014.020414.132649}, number={4}, journal={IEEE Communications Letters}, publisher={Institute of Electrical and Electronics Engineers (IEEE)}, author={Claeys, Maxim and Latre, Steven and Famaey, Jeroen and De Turck, Filip}, year={2014}, month={Apr}, pages={716–719}}
"
Constructing evidence-based treatment strategies using methods from computer science,"This paper details a new methodology, instance-based reinforcement learning, for constructing adaptive treatment strategies from randomized trials. Adaptive treatment strategies are operationalized clinical guidelines which recommend the next best treatment for an individual based on his/her personal characteristics and response to earlier treatments. The instance-based reinforcement learning methodology comes from the computer science literature, where it was developed to optimize sequences of actions in an evolving, time varying system. When applied in the context of treatment design, this method provides the means to evaluate both the therapeutic and diagnostic effects of treatments in constructing an adaptive treatment strategy. The methodology is illustrated with data from the STAR*D trial, a multi-step randomized study of treatment alternatives for individuals with treatment-resistant major depressive disorder. © 2007 Elsevier Ireland Ltd. All rights reserved.",Clinical decision-making; Learning; Methodology; Sequential decisions; Treatment,Health,,SCOPUS,1/group,state representation,other,n,y,n,n,y,n,n,n,n,y,n,n,y,n,n,y,instance-based reinforcement learning,Drug and Alcohol Dependence,"Pineau J., Bellemare M.G., Rush A.J., Ghizaru A., Murphy S.A.","Pineau, J., McGill University, School of Computer Science, 3480 University St., Montreal, Que. H3A 2A7, Canada; Bellemare, M.G., McGill University, School of Computer Science, 3480 University St., Montreal, Que. H3A 2A7, Canada; Rush, A.J., University of Texas, Southwestern Medical Center, 323 Harry Hines Blvd, Dallas, TX 75390, United States; Ghizaru, A., McGill University, School of Computer Science, 3480 University St., Montreal, Que. H3A 2A7, Canada; Murphy, S.A., University of Michigan, Institute for Social Research, 426 Thompson St, Ann Arbor, MI 48106, United States",25.0,10.1016/j.drugalcdep.2007.01.005,Article,2007-01-01,Pineau_2007," @article{Pineau_2007, title={Constructing evidence-based treatment strategies using methods from computer science}, volume={88}, ISSN={0376-8716}, url={http://dx.doi.org/10.1016/j.drugalcdep.2007.01.005}, DOI={10.1016/j.drugalcdep.2007.01.005}, journal={Drug and Alcohol Dependence}, publisher={Elsevier BV}, author={Pineau, Joelle and Bellemare, Marc G. and Rush, A. John and Ghizaru, Adrian and Murphy, Susan A.}, year={2007}, month={May}, pages={S52–S60}}
"
Learning and adaptivity in interactive recommender systems,"Recommender systems are intelligent E-commerce applications that assist users in a decision-making process by offering personalized product recommendations during an interaction session. Quite recently, conversational approaches have been introduced in order to support more interactive recommendation sessions. Notwithstanding the increased interactivity offered by these approaches, the system employs an interaction strategy that is specified apriori (at design time) and followed quite rigidly during the interaction. In this paper, we present a new type of recommender system which is capable of learning autonomously an adaptive interaction strategy for assisting the users in acquiring their interaction goals. We view the recommendation process as a sequential decision problem and we model it as a Markov Decision Process (MDP). We learn a model of the user behavior, and use it to acquire the adaptive strategy using Reinforcement Learning (RL) techniques. In this context, the system learns the optimal strategy by observing the consequences of its actions on the users and also on the final outcome of the recommendation session. We apply our approach within an existing travel recommender system which uses a rigid, non-adaptive support strategy for advising a user in refining a query to a travel product catalogue. The initial results demonstrate the value of our approach and show that our system is able to improve the non-adaptive strategy in order to learn an optimal (adaptive) recommendation strategy. Copyright 2007 ACM.",Adaptivity; Conversational recommender systems; Markov decision process; Reinforcement learning,Commerce,,,1/person,state representation,online,y,n,n,y,n,n,n,n,n,y,n,y,n,y,n,y,Policy Iteration,ACM International Conference Proceeding Series,"Mahmood T., Ricci F.","Mahmood, T., University of Trento, Trento, Italy; Ricci, F., Free University of Bozen-Bolzano, Bolzano, Italy",25.0,10.1145/1282100.1282114,Conference Paper,2007-01-01,Mahmood_2007," @article{Mahmood_2007, title={Learning and adaptivity in interactive recommender systems}, ISBN={9781595937001}, url={http://dx.doi.org/10.1145/1282100.1282114}, DOI={10.1145/1282100.1282114}, journal={Proceedings of the ninth international conference on Electronic commerce - ICEC  ’07}, publisher={ACM Press}, author={Mahmood, Tariq and Ricci, Francesco}, year={2007}}
"
Q-learning with censored data,"We develop methodology for a multistage decision problem with flexible number of stages in which the rewards are survival times that are subject to censoring. We present a novel Q-learning algorithm that is adjusted for censored data and allows a flexible number of stages. We provide finite sample bounds on the generalization error of the policy learned by the algorithm, and show that when the optimal Q-function belongs to the approximation space, the expected survival time for policies obtained by the algorithm converges to that of the optimal policy. We simulate a multistage clinical trial with flexible number of stages and apply the proposed censored-Q-learning algorithm to find individualized treatment regimens. The methodology presented in this paper has implications in the design of personalized medicine trials in cancer and in other life-threatening diseases. © Institute of Mathematical Statistics, 2012.",Generalization error; Q-learning; Reinforcement learning; Survival analysis,Health,,SCOPUS,1/person,other,batch,y,n,n,y,n,n,n,n,y,n,n,y,y,y,y,y,Q-Learning,Annals of Statistics,"Goldberg Y., Kosorok M.R.","Goldberg, Y., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States; Kosorok, M.R., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States",24.0,10.1214/12-AOS968,Article,2012-01-01,Goldberg_2012," @article{Goldberg_2012, title={Q-learning with censored data}, volume={40}, ISSN={0090-5364}, url={http://dx.doi.org/10.1214/12-AOS968}, DOI={10.1214/12-aos968}, number={1}, journal={The Annals of Statistics}, publisher={Institute of Mathematical Statistics}, author={Goldberg, Yair and Kosorok, Michael R.}, year={2012}, month={Feb}, pages={529–560}}
"
Dynamic treatment regimes,"A dynamic treatment regime consists of a sequence of decision rules, one per stage of intervention, that dictate how to individualize treatments to patients, based on evolving treatment and covariate history. These regimes are particularly useful for managing chronic disorDers and fit well into the larger paradigm of personalized medicine. They provide one way to operationalize a clinical decision support system. Statistics plays a key role in the construction of evidence-based dynamic treatment regimes-informing the best study design as well as efficient estimation and valid inference. Owing to the many novel methodological challenges this area offers, it has been growing in popularity among statisticians in recent years. In this article, we review the key developments in this exciting field of research. In particular, we discuss the sequential multiple assignment randomized trial designs, estimation techniques like Q-learning and marginal structural models, and several inference techniques designed to address the associated nonstandard asymptotics. We reference software whenever available. We also outline some important future directions. © 2014 by Annual Reviews.",Dynamic treatment regime; Nonregularity; Q-learning; Reinforcement learning; Sequential randomization,Health,,y,1,not used,unknown,n,n,n,n,n,n,n,n,n,n,n,n,n,n,n,n,Q-Learning,Annual Review of Statistics and Its Application,"Chakraborty B., Murphy S.A.","Chakraborty, B., Duke-NUS Graduate Medical School, National University of Singapore, Singapore, Singapore; Murphy, S.A., Department of Statistics and Institute for Social Research, University of Michigan, Ann Arbor, MI 48109, United States",24.0,10.1146/annurev-statistics-022513-115553,Article,2014-01-01,Chakraborty_2014," @article{Chakraborty_2014, title={Dynamic Treatment Regimes}, volume={1}, ISSN={2326-831X}, url={http://dx.doi.org/10.1146/annurev-statistics-022513-115553}, DOI={10.1146/annurev-statistics-022513-115553}, number={1}, journal={Annual Review of Statistics and Its Application}, publisher={Annual Reviews}, author={Chakraborty, Bibhas and Murphy, Susan A.}, year={2014}, month={Jan}, pages={447–464}}
"
Automatic ad format selection via contextual bandits,"Visual design plays an important role in online display advertising: changing the layout of an online ad can increase or decrease its effectiveness, measured in terms of click-through rate (CTR) or total revenue. The decision of which layout to use for an ad involves a trade-off: using a layout provides feedback about its effectiveness (exploration), but collecting that feedback requires sacrificing the immediate reward of using a layout we already know is effective (exploitation). To balance exploration with exploitation, we pose automatic layout selection as a contextual bandit problem. There are many bandit algorithms, each generating a policy which must be evaluated. It is impractical to test each policy on live traffic. However, we have found that offline replay (a.k.a. exploration scavenging) can be adapted to provide an accurate estimator for the performance of ad layout policies at Linkedin, using only historical data about the effectiveness of layouts. We describe the development of our offline replayer, and benchmark a number of common bandit algorithms. Copyright 2013 ACM.",Bandit algorithms; Exploration/exploitation; Layout; Offline evaluation; Online advertising; Personalization; Recommender systems,Commerce,,,1,state representation,batch,n,y,y,n,y,y,y,y,n,y,n,n,y,y,n,y,Contextual Bandits,"International Conference on Information and Knowledge Management, Proceedings","Tang L., Rosales R., Singh A.P., Agarwal D.","Tang, L., School of Computer Science, Florida International Univ., 11200 S.W. 8th St., Miami, FL 33199, United States; Rosales, R., Applied Relevance Science LinkedIn, 2029 Stierlin Ct., Mountain View, CA 94043, United States; Singh, A.P., Applied Relevance Science LinkedIn, 2029 Stierlin Ct., Mountain View, CA 94043, United States; Agarwal, D., School of Computer Science, Florida International Univ., 11200 S.W. 8th St., Miami, FL 33199, United States",20.0,10.1145/2505515.2514700,Conference Paper,2013-01-01,Tang_2013," @article{Tang_2013, title={Automatic ad format selection via contextual bandits}, ISBN={9781450322638}, url={http://dx.doi.org/10.1145/2505515.2514700}, DOI={10.1145/2505515.2514700}, journal={Proceedings of the 22nd ACM international conference on Conference on information & knowledge management - CIKM  ’13}, publisher={ACM Press}, author={Tang, Liang and Rosales, Romer and Singh, Ajit and Agarwal, Deepak}, year={2013}}
"
A reinforcement learning approach for individualizing erythropoietin dosages in hemodialysis patients,"This paper presents a reinforcement learning (RL) approach for anemia management in patients undergoing chronic renal failure. Erythropoietin (EPO) is the treatment of choice for this kind of anemia but it is an expensive drug and with some dangerous side-effects that should be considered especially for patients who do not respond to the treatment. Therefore, an individualized treatment appears to be necessary. RL is a suitable approach to tackle this problem. Moreover, resulting policies are similar to medical protocols, and hence, they can easily be transferred to daily practice. A cohort of 64 patients are included in the study. An implementation of the Q-learning algorithm based on a state-aggregation table and another implementation using the multi-layer perceptron as a function approximator (Q-MLP) are compared with the protocols followed in the Nephrology Unit. The policy obtained by the Q-MLP approach outperforms the hospital policy in terms of the ratio of patients that are within the targeted range of hemoglobin (11.5-12.5 g/dl) at the end of the analyzed period, since an increase of 25% is observed. It ensures an improvement in patients' quality-of-life and considerable economic savings for the health care system due to both the expensiveness of EPO treatment and the costs incurred by the health care system in order to alleviate problems related to EPO over-dosing. It should be pointed out that the approach presented here is completely general, and therefore, it can be applied to any problem of drug dosage optimization. © 2009 Elsevier Ltd. All rights reserved.",Anemia; Chronic renal failure; Clinical pharmacokinetics; Erythropoietin; Reinforcement learning,Health,,SCOPUS,1,state representation,batch,n,y,n,n,y,n,y,n,n,y,n,y,n,n,n,y,Q-Learning,Expert Systems with Applications,"Martín-Guerrero J.D., Gomez F., Soria-Olivas E., Schmidhuber J., Climente-Martí M., Jiménez-Torres N.V.","Martín-Guerrero, J.D., Department of Electronic Engineering, University of Valencia, CL. Dr. Moliner, 50, 46100 Burjassot, Valencia, Spain; Gomez, F., Istituto Dalle Molle di Studi sull'Intelligenza Artificiale (IDSIA), Lugano, Switzerland; Soria-Olivas, E., Department of Electronic Engineering, University of Valencia, CL. Dr. Moliner, 50, 46100 Burjassot, Valencia, Spain; Schmidhuber, J., Istituto Dalle Molle di Studi sull'Intelligenza Artificiale (IDSIA), Lugano, Switzerland; Climente-Martí, M., Pharmacy Unit, University Hospital, Dr. Peset, Valencia, Spain; Jiménez-Torres, N.V., Pharmacy Unit, University Hospital, Dr. Peset, Valencia, Spain, Pharmacy and Pharmaceutical Technology Department, University of Valencia, Spain",17.0,10.1016/j.eswa.2009.02.041,Article,2009-01-01,Mart_n_Guerrero_2009," @article{Mart_n_Guerrero_2009, title={A reinforcement learning approach for individualizing erythropoietin dosages in hemodialysis patients}, volume={36}, ISSN={0957-4174}, url={http://dx.doi.org/10.1016/j.eswa.2009.02.041}, DOI={10.1016/j.eswa.2009.02.041}, number={6}, journal={Expert Systems with Applications}, publisher={Elsevier BV}, author={Martín-Guerrero, José D. and Gomez, Faustino and Soria-Olivas, Emilio and Schmidhuber, Jürgen and Climente-Martí, Mónica and Jiménez-Torres, N. Víctor}, year={2009}, month={Aug}, pages={9737–9742}}
"
Ensemble contextual bandits for personalized recommendation,"The cold-start problem has attracted extensive attention among various online services that provide personalized recommendation. Many online vendors employ contextual bandit strategies to tackle the so-called exploration/exploitation dilemma rooted from the cold-start problem. However, due to high-dimensional user/item features and the underlying characteristics of bandit policies, it is often difficult for service providers to obtain and deploy an appropriate algorithm to achieve acceptable and robust economic profit. In this paper, we explore ensemble strategies of contextual bandit algorithms to obtain robust predicted click-through rate (CTR) of web objects. The ensemble is acquired by aggregating different pulling policies of bandit algorithms, rather than forcing the agreement of prediction results or learning a unified predictive model. To this end, we employ a meta-bandit paradigm that places a hyper bandit over the base bandits, to explicitly explore/exploit the relative importance of base bandits based on user feedbacks. Extensive empirical experiments on two real-world data sets (news recommendation and online advertising) demonstrate the effectiveness of our proposed approach in terms of CTR. Copyright © 2014 ACM.",Contextual bandit; CTR prediction; Ensemble recommendation; Meta learning; Personalized recommendation,Domain Independent,,,1,state representation,online,y,y,n,y,y,n,n,n,n,y,n,y,y,y,n,y,Contextual Bandits,RecSys 2014 - Proceedings of the 8th ACM Conference on Recommender Systems,"Tang L., Jiang Y., Li L., Li T.","Tang, L., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Jiang, Y., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Li, L., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Li, T., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States",17.0,10.1145/2645710.2645732,Conference Paper,2014-01-01,Tang_2014," @article{Tang_2014, title={Ensemble contextual bandits for personalized recommendation}, ISBN={9781450326681}, url={http://dx.doi.org/10.1145/2645710.2645732}, DOI={10.1145/2645710.2645732}, journal={Proceedings of the 8th ACM Conference on Recommender systems - RecSys  ’14}, publisher={ACM Press}, author={Tang, Liang and Jiang, Yexi and Li, Lei and Li, Tao}, year={2014}}
"
AgentX: Using Reinforcement Learning to Improve the Effectiveness of Intelligent Tutoring Systems,"Reinforcement Learning (RL) can be used to train an agent to comply with the needs of a student using an intelligent tutoring system. In this paper, we introduce a method of increasing efficiency by way of customization of the hints provided by a tutoring system, by applying techniques from RL to gain knowledge about the usefulness of hints leading to the exclusion or introduction of other helpful hints. Students are clustered into learning levels and can influence the agents method of selecting actions in each state in their cluster of affect. In addition, students can change learning levels based on their performance within the tutoring system and continue to affect the entire student population. The RL agent, AgentX, then uses the cluster information to create one optimal policy for all students in the cluster and begin to customize the help given to the cluster based on that optimal policy. © Springer-Verlag 2004 References.",,Education,,,1/group,state representation,batch,y,n,n,y,n,n,y,y,n,y,n,y,n,y,n,y,Policy Iteration,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"Martin K.N., Arroyo I.","Martin, K.N., Department of Computer Science, University of Massachusetts, 140 Governors Drive, Amherst, MA 01003, United States; Arroyo, I., Department of Computer Science, University of Massachusetts, 140 Governors Drive, Amherst, MA 01003, United States",17.0,,Article,2004-01-01,martin2004agentx,"@inproceedings{martin2004agentx,
  title={AgentX: Using reinforcement learning to improve the effectiveness of intelligent tutoring systems},
  author={Martin, Kimberly N and Arroyo, Ivon},
  booktitle={International Conference on Intelligent Tutoring Systems},
  pages={564--572},
  year={2004},
  organization={Springer}
}
"
Affective personalization of a social robot tutor for children's second language skills,"Though substantial research has been dedicated towards using technology to improve education, no current methods are as effective as one-on-one tutoring. A critical, though relatively understudied, aspect of effective tutoring is modulating the student's affective state throughout the tutoring session in order to maximize long-term learning gains. We developed an integrated experimental paradigm in which children play a second-language learning game on a tablet, in collaboration with a fully autonomous social robotic learning companion. As part of the system, we measured children's valence and engagement via an automatic facial expression analysis system. These signals were combined into a reward signal that fed into the robot's affective reinforcement learning algorithm. Over several sessions, the robot played the game and personalized its motivational strategies (using verbal and non-verbal actions) to each student. We evaluated this system with 34 children in preschool classrooms for a duration of two months. We saw that (1) children learned new words from the repeated tutoring sessions, (2) the affective policy personalized to students over the duration of the study, and (3) students who interacted with a robot that personalized its affective feedback strategy showed a significant increase in valence, as compared to students who interacted with a nonpersonalizing robot. This integrated system of tablet-based educational content, affective sensing, affective policy learning, and an autonomous social robot holds great promise for a more comprehensive approach to personalized tutoring. © Copyright 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,Education,,SCOPUS,1/person,other,other,n,n,y,n,n,y,y,n,n,y,n,n,y,n,n,y,SARSA,"30th AAAI Conference on Artificial Intelligence, AAAI 2016","Gordon G., Spaulding S., Korywestlund J., Lee J.J., Plummer L., Martinez M., Das M., Breazeal C.","Gordon, G., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States, Industrial Engineering Department, Curiosity Lab, Tel-Aviv Univerisity, Israel; Spaulding, S., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Korywestlund, J., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Lee, J.J., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Plummer, L., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Martinez, M., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Das, M., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Breazeal, C., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States",16.0,,Conference Paper,2016-01-01,gordon2016affective,"@inproceedings{gordon2016affective,
  title={Affective personalization of a social robot tutor for children’s second language skills},
  author={Gordon, Goren and Spaulding, Samuel and Westlund, Jacqueline Kory and Lee, Jin Joo and Plummer, Luke and Martinez, Marayna and Das, Madhurima and Breazeal, Cynthia},
  booktitle={Thirtieth AAAI Conference on Artificial Intelligence},
  year={2016}
}
"
Exploration in interactive personalized music recommendation: A reinforcement learning approach,"Current music recommender systems typically act in a greedymanner by recommending songs with the highest user ratings. Greedy recommendation, however, is suboptimal over the long term: it does not actively gather information on user preferences and fails to recommend novel songs that are potentially interesting. A successful recommender system must balance the needs to explore user preferences and to exploit this information for recommendation. This article presents a new approach to music recommendation by formulating this exploration-exploitation trade-off as a reinforcement learning task. To learn user preferences, it uses Bayesian model that accounts for both audio content and the novelty of recommendations. A piecewise-linear approximation to the model and a variational inference algorithm help to speed up Bayesian inference. One additional benefit of our approach is a single unified model for both music recommendation and playlist generation. We demonstrate the strong potential of the proposed approach with simulation results and a user study. © 2014 ACM.",Application; Machine learning; Model; Music; Recommender systems,Entertainment,,SCOPUS,1,not used,online,y,y,y,y,y,y,y,y,y,n,n,n,y,y,y,n,"LinUCB,UCB","ACM Transactions on Multimedia Computing, Communications and Applications","Wang Y., Wang X., Wang Y., Hsu D.","Wang, Y., Department of Computer Science, National University of Singapore, Singapore 117417, Singapore; Wang, X., Department of Computer Science, National University of Singapore, Singapore 117417, Singapore; Wang, Y., Computing Science Department, Institute of High Performance Computing, A STAR, Singapore 138632, Singapore; Hsu, D., Department of Computer Science, National University of Singapore, Singapore 117417, Singapore",15.0,10.1145/2623372,Article,2014-01-01,Wang_2014," @article{Wang_2014, title={Exploration in Interactive Personalized Music Recommendation}, volume={11}, ISSN={1551-6857}, url={http://dx.doi.org/10.1145/2623372}, DOI={10.1145/2623372}, number={1}, journal={ACM Transactions on Multimedia Computing, Communications, and Applications}, publisher={Association for Computing Machinery (ACM)}, author={Wang, Xinxi and Wang, Yi and Hsu, David and Wang, Ye}, year={2014}, month={Sep}, pages={1–22}}
"
The route not taken: Driver-centric estimation of electric vehicle range,"This paper addresses the challenge of efficiently and accurately predicting an electric vehicle's attainable range. Specifically, our approach accounts for a driver's generalised route preferences to provide up-to-date, personalised information based on estimates of the energy required to reach every possible destination in a map. We frame this task in the context of sequential decision making and show that energy consumption in reaching a particular destination can be formulated as policy evaluation in a Markov Decision Process. In particular, we exploit the properties of the model adopted for predicting likely energy consumption to every possible destination in a realistically sized map in real-time. The policy to be evaluated is learned and, over time, refined using Inverse Reinforcement Learning to provide for a life-long adaptive system. Our approach is evaluated using a publicly available dataset providing real trajectory data of 50 individuals spanning approximately 10,000 miles of travel. We show that by accounting for driver specific route preferences our system significantly reduces the relative error in energy prediction compared to more common, driver-agnostic heuristics such as shortest-path or shortest-time routes. Copyright © 2014, Association for the Advancement of Artificial Intelligence.",,Transport,,,1/person,not used,online,n,y,n,n,y,n,y,y,n,y,n,n,n,y,n,y,Maximum Entropy IRL,"Proceedings International Conference on Automated Planning and Scheduling, ICAPS","Ondrúška P., Posner I.","Ondrúška, P., Mobile Robotics Group, University of Oxford, United Kingdom; Posner, I., Mobile Robotics Group, University of Oxford, United Kingdom",11.0,,Conference Paper,2014-01-01,ondruska2014route,"@inproceedings{ondruska2014route,
  title={The route not taken: Driver-centric estimation of electric vehicle range},
  author={Ondruska, Peter and Posner, Ingmar},
  booktitle={Twenty-Fourth International Conference on Automated Planning and Scheduling},
  year={2014}
}
"
Hybrid-ε-greedy for mobile context-aware recommender system,"The wide development of mobile applications provides a considerable amount of data of all types. In this sense, Mobile Context-aware Recommender Systems (MCRS) suggest the user suitable information depending on her/his situation and interests. Our work consists in applying machine learning techniques and reasoning process in order to adapt dynamically the MCRS to the evolution of the user's interest. To achieve this goal, we propose to combine bandit algorithm and case-based reasoning in order to define a contextual recommendation process based on different context dimensions (social, temporal and location). This paper describes our ongoing work on the implementation of a MCRS based on a hybrid-ε-greedy algorithm. It also presents preliminary results by comparing the hybrid-ε-greedy and the standard ε-greedy algorithm. © 2012 Springer-Verlag.",contextual bandit; exploration/exploitation dilemma; Machine learning; personalization; recommender systems,Commerce,,,1/person,other,online,n,y,n,n,y,n,n,n,n,y,n,n,y,y,n,y,Case based reasoning combined with a bandit algorithm. Epsilon greedy iand a hybrid epsilon strategy (developed by the authors) are used.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"Bouneffouf D., Bouzeghoub A., Gançarski A.L.","Bouneffouf, D., Department of Computer Science, Télécom SudParis, UMR CNRS Samovar, 91011 Evry Cedex, France; Bouzeghoub, A., Department of Computer Science, Télécom SudParis, UMR CNRS Samovar, 91011 Evry Cedex, France; Gançarski, A.L., Department of Computer Science, Télécom SudParis, UMR CNRS Samovar, 91011 Evry Cedex, France",11.0,10.1007/978-3-642-30217-6_39,Conference Paper,2012-01-01,Bouneffouf_2012," @article{Bouneffouf_2012, title={Hybrid-ε-greedy for Mobile Context-Aware Recommender System}, ISBN={9783642302176}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-642-30217-6_39}, DOI={10.1007/978-3-642-30217-6_39}, journal={Lecture Notes in Computer Science}, publisher={Springer Berlin Heidelberg}, author={Bouneffouf, Djallel and Bouzeghoub, Amel and Gançarski, Alda Lopes}, year={2012}, pages={468–479}}
"
A hybrid web recommender system based on Q-learning,"Different efforts have been made to address the problem of information overload on the Internet. Recommender systems aim at directing users through this information space, toward the resources that best meet their needs and interests. Web Content Recommendation has been an active application area for Information Filtering, Web Mining and Machine Learning research. Recent studies show that combining the conceptual and usage information can improve the quality of web recommendations. In this paper we exploit this idea to enhance a reinforcement learning framework, primarily devised for web recommendations based on web usage data. A hybrid web recommendation method is proposed by making use of the conceptual relationships among web resources to derive a novel model of the problem, enriched with semantic knowledge about the usage behavior. With our hybrid model for the web page recommendation problem we show the apt and flexibility of the reinforcement learning framework in the web recommendation domain, and demonstrate how it can be extended in order to incorporate various sources of information. We evaluate our method under different settings and show how this method can improve the overall quality of web recommendations. Copyright 2008 ACM.",Machine learning; Personalization; Recommender systems; Reinforcement learning; Web mining,Entertainment,,SCOPUS,1,not used,batch,y,n,n,y,n,n,n,y,n,y,n,n,y,y,n,n,Q-Learning,Proceedings of the ACM Symposium on Applied Computing,"Taghipour N., Kardan A.","Taghipour, N., Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran; Kardan, A., Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran",11.0,10.1145/1363686.1363954,Conference Paper,2008-01-01,Taghipour_2008," @article{Taghipour_2008, title={A hybrid web recommender system based on Q-learning}, ISBN={9781595937537}, url={http://dx.doi.org/10.1145/1363686.1363954}, DOI={10.1145/1363686.1363954}, journal={Proceedings of the 2008 ACM symposium on Applied computing  - SAC  ’08}, publisher={ACM Press}, author={Taghipour, Nima and Kardan, Ahmad}, year={2008}}
"
A Bayesian reinforcement learning approach for customizing human-robot interfaces,"Personal robots are becoming increasingly prevalent, which raises a number of interesting issues regarding the design and customization of interfaces to such platforms. The particular problem addressed by this paper is the use of learning methods to improve the quality and effectiveness of human-machine interaction onboard a robotic wheelchair. In support of this, we present a method for learning and adapting probabilistic models with the aid of a human operator. We use a Bayesian reinforcement learning framework, that allows us to mix learning and execution, as well as take advantage of prior information about the world. We address the problems of learning, handling a partially observable environment, and limiting the number of action requests. We demonstrate empirical feasibility of our approach on an interface for an autonomous wheelchair. Copyright 2009 ACM.",Activity & plan recognition; Intelligent assistants; Intelligent interfaces for ubiquitous computing,Health,,SCOPUS,1/person,not used,online,n,n,y,n,n,y,y,n,y,n,n,n,y,y,n,y,"RL, not further specified","International Conference on Intelligent User Interfaces, Proceedings IUI","Atrash A., Pineau J.","Atrash, A., School of Computer Science, McGill University, Montreal, QC H3A 2A7, Canada; Pineau, J., School of Computer Science, McGill University, Montreal, QC H3A 2A7, Canada",10.0,10.1145/1502650.1502700,Conference Paper,2009-01-01,Atrash_2008," @article{Atrash_2008, title={A bayesian reinforcement learning approach for customizing human-robot interfaces}, ISBN={9781605581682}, url={http://dx.doi.org/10.1145/1502650.1502700}, DOI={10.1145/1502650.1502700}, journal={Proceedingsc of the 13th international conference on Intelligent user interfaces - IUI  ’09}, publisher={ACM Press}, author={Atrash, Amin and Pineau, Joelle}, year={2008}}
"
Inducing effective pedagogical strategies using learning context features,"Effective pedagogical strategies are important for e-learning environments. While it is assumed that an effective learning environment should craft and adapt its actions to the user's needs, it is often not clear how to do so. In this paper, we used a Natural Language Tutoring System named Cordillera and applied Reinforcement Learning (RL) to induce pedagogical strategies directly from pre-existing human user interaction corpora. 50 features were explored to model the learning context. Of these features, domain-oriented and system performance features were the most influential while user performance and background features were rarely selected. The induced pedagogical strategies were then evaluated on real users and results were compared with pre-existing human user interaction corpora. Overall, our results show that RL is a feasible approach to induce effective, adaptive pedagogical strategies by using a relatively small training corpus. Moreover, we believe that our approach can be used to develop other adaptive and personalized learning environments. © 2010 Springer-Verlag.",,Education,,SCOPUS,1,not used,batch,n,y,n,n,n,y,n,y,n,y,n,n,y,n,n,y,"RL, not further specified",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"Chi M., Vanlehn K., Litman D., Jordan P.","Chi, M., Machine Learning Department, Carnegie Mellon University, PA 15213, United States; Vanlehn, K., School of Computing and Informatics, Arizona State University, AZ 85287, United States; Litman, D., Department of Computer Science, University of Pittsburgh, PA 15260, United States; Jordan, P., Department of Biomedical Informatics, University of Pittsburgh, Pittsburgh, PA 15260, United States",10.0,10.1007/978-3-642-13470-8_15,Conference Paper,2010-01-01,Chi_2010_0," @article{Chi_2010_0, title={Inducing Effective Pedagogical Strategies Using Learning Context Features}, ISBN={9783642134708}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-642-13470-8_15}, DOI={10.1007/978-3-642-13470-8_15}, journal={Lecture Notes in Computer Science}, publisher={Springer Berlin Heidelberg}, author={Chi, Min and VanLehn, Kurt and Litman, Diane and Jordan, Pamela}, year={2010}, pages={147–158}}
"
Automatic web content personalization through reinforcement learning,"This paper deals with the automatic adaptation of Web contents. It is recognized that quite often users need some personalized adaptations to access Web contents. This is more evident when we focus on people with some accessibility needs. Based on the user profile, it is possible to transcode or modify contents (e.g., adapt text fonts) so as to meet the user preferences. The problem is that applying such a kind of transformations to the whole content might significantly alter Web pages that might become unreadable, hence making matters worse. We present a system that employs Web intelligence to perform automatic adaptations on single elements composing a Web page. A reinforcement learning algorithm is utilized to manage user profiles. We evaluate our system through simulation and a real assessment where elderly users where asked to use for a time period our system prototype. Results confirm the feasibility of the proposal. © 2016 Elsevier Inc.",Reinforcement learning; User profiling; Web personalization,Entertainment,,SCOPUS,1/person,state representation,batch,y,n,y,y,n,y,n,n,y,y,n,y,y,y,n,y,Q-Learning,Journal of Systems and Software,"Ferretti S., Mirri S., Prandi C., Salomoni P.","Ferretti, S., Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy; Mirri, S., Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy; Prandi, C., Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy; Salomoni, P., Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy",9.0,10.1016/j.jss.2016.02.008,Article,2016-01-01,Ferretti_2016_0," @article{Ferretti_2016_0, title={Automatic web content personalization through reinforcement learning}, volume={121}, ISSN={0164-1212}, url={http://dx.doi.org/10.1016/j.jss.2016.02.008}, DOI={10.1016/j.jss.2016.02.008}, journal={Journal of Systems and Software}, publisher={Elsevier BV}, author={Ferretti, Stefano and Mirri, Silvia and Prandi, Catia and Salomoni, Paola}, year={2016}, month={Nov}, pages={157–169}}
"
Contextual recommender problems,"The contextual recommender task is the problem of making useful offers, e.g., placing ads or related links on a web page, based on the context information, e.g., contents of the page and information about the user visiting, and information on the available alternatives, i.e., the advertisements or relevant links. In the case of ads for example, the goal is to select ads that result in high click rates, where the (ad) click rate is some unknown function of the attributes of the context and ad. We describe the task and make connections to related problems including recommender and multi-armed bandit problems. Copyright 2005 ACM.",data mining; exploration-exploitation; multi-armed bandit; personalization; recommenders; regression; reinforcement learning; utility,Commerce,,,1/group,state representation,unknown,n,n,n,n,n,n,n,n,n,y,n,n,n,y,n,y,Contextual Bandits,"Proceedings of the 1st International Workshop on Utility-Based Data Mining, UBDM '05","Madani O., DeCoste D.","Madani, O., Yahoo Research, 74 N. Pasadena Ave, Pasadena, CA 91103, United States; DeCoste, D., Yahoo Research, 74 N. Pasadena Ave, Pasadena, CA 91103, United States",9.0,10.1145/1089827.1089838,Conference Paper,2005-01-01,Madani_2005," @article{Madani_2005, title={Contextual recommender problems [extended abstract]}, ISBN={1595932089}, url={http://dx.doi.org/10.1145/1089827.1089838}, DOI={10.1145/1089827.1089838}, journal={Proceedings of the 1st international workshop on Utility-based data mining  - UBDM  ’05}, publisher={ACM Press}, author={Madani, Omid and DeCoste, Dennis}, year={2005}}
"
Dynamic personalization in conversational recommender systems,"Conversational recommender systems are E-Commerce applications which interactively assist online users to acquire their interaction goals during their sessions. In our previous work, we have proposed and validated a methodology for conversational systems which autonomously learns the particular web page to display to the user, at each step of the session. We employed reinforcement learning to learn an optimal strategy, i.e., one that is personalized for a real user population. In this paper, we extend our methodology by allowing it to autonomously learn and update the optimal strategy dynamically (at run-time), and individually for each user. This learning occurs perpetually after every session, as long as the user continues her interaction with the system. We evaluate our approach in an off-line simulation with four simulated users, as well as in an online evaluation with thirteen real users. The results show that an optimal strategy is learnt and updated for each real and simulated user. For each simulated user, the optimal behavior is reasonably adapted to this user’s characteristics, but converges after several hundred sessions. For each real user, the optimal behavior converges only in several sessions. It provides assistance only in certain situations, allowing many users to buy several products together in shorter time and with more page-views and lesser number of query executions. We prove that our approach is novel and show how its current limitations can catered. © Springer-Verlag Berlin Heidelberg 2013.",Conversational recommender systems; Dynamic personalization; Individual user; Off-line simulation; On-line experiment; Optimal strategy; Real users; Reinforcement learning,Commerce,,SCOPUS,1/person,not used,online,y,n,y,y,n,y,n,n,y,y,y,n,n,y,n,n,Q-Learning,Information Systems and e-Business Management,"Mahmood T., Mujtaba G., Venturini A.","Mahmood, T., Department of Computer Science, National University of Computer and Emerging Sciences (NUCES), Shah Lateef Town, National Highway, Karachi, Pakistan; Mujtaba, G., Sukkur Institute of Business Administration, Airport Road, Sukkur, Sindh, Pakistan; Venturini, A., ECTRL Solutions SRL, Via Solteri 38, Trento, Italy",9.0,10.1007/s10257-013-0222-3,Article,2014-01-01,Mahmood_2013," @article{Mahmood_2013, title={Dynamic personalization in conversational recommender systems}, volume={12}, ISSN={1617-9854}, url={http://dx.doi.org/10.1007/s10257-013-0222-3}, DOI={10.1007/s10257-013-0222-3}, number={2}, journal={Information Systems and e-Business Management}, publisher={Springer Nature}, author={Mahmood, Tariq and Mujtaba, Ghulam and Venturini, Adriano}, year={2013}, month={Apr}, pages={213–238}}
"
Personalized recommendation via parameter-free contextual bandits,"Personalized recommendation services have gained increasing popularity and attention in recent years as most useful information can be accessed online in real-time. Most online recommender systems try to address the information needs of users by virtue of both user and content information. Despite extensive recent advances, the problem of personalized recommendation remains challenging for at least two reasons. First, the user and item repositories undergo frequent changes, which makes traditional recommendation algorithms ineffective. Second, the so-called cold-start problem is difficult to address, as the information for learning a recommendation model is limited for new items or new users. Both challenges are formed by the dilemma of exploration and exploitation. In this paper, we formulate personalized recommendation as a contextual bandit problem to solve the exploration/exploitation dilemma. Specifically in our work, we propose a parameter-free bandit strategy, which employs a principled resampling approach called online bootstrap, to derive the distribution of estimated models in an online manner. Under the paradigm of probability matching, the proposed algorithm randomly samples a model from the derived distribution for every recommendation. Extensive empirical experiments on two real-world collections of web data (including online advertising and news recommendation) demonstrate the effectiveness of the proposed algorithm in terms of the click-through rate. The experimental results also show that this proposed algorithm is robust in the cold-start situation, in which there is no sufficient data or knowledge to tune the parameters. © 2015 ACM.",Bootstrapping; Contextual bandit; Personalization; Probability matching; Recommender systems,Entertainment,,SCOPUS,1,other,online,y,y,n,y,y,n,y,n,n,y,n,n,y,y,n,n,"Contextual Bandits,LinUCB,Thompson Sampling",SIGIR 2015 - Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,"Tang L., Jiang Y., Li L., Zeng C., Li T.","Tang, L., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Jiang, Y., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Li, L., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Zeng, C., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Li, T., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States",8.0,10.1145/2766462.2767707,Conference Paper,2015-01-01,Tang_2015," @article{Tang_2015, title={Personalized Recommendation via Parameter-Free Contextual Bandits}, ISBN={9781450336215}, url={http://dx.doi.org/10.1145/2766462.2767707}, DOI={10.1145/2766462.2767707}, journal={Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval - SIGIR  ’15}, publisher={ACM Press}, author={Tang, Liang and Jiang, Yexi and Li, Lei and Zeng, Chunqiu and Li, Tao}, year={2015}}
"
Online context-aware recommendation with time varying multi-armed bandit,"Contextual multi-armed bandit problems have gained increasing popularity and attention in recent years due to their capability of leveraging contextual information to deliver online personalized recommendation services (e.g., online advertising and news article selection). To predict the reward of each arm given a particular context, existing relevant research studies for contextual multi-armed bandit problems often assume the existence of a fixed yet unknown reward mapping function. However, this assumption rarely holds in practice, since real-world problems often involve underlying processes that are dynamically evolving over time. In this paper, we study the time varying contextual multi-armed problem where the reward mapping function changes over time. In particular, we propose a dynamical context drift model based on particle learning. In the proposed model, the drift on the reward mapping function is explicitly modeled as a set of random walk particles, where good fitted particles are selected to learn the mapping dynamically. Taking advantage of the fully adaptive inference strategy of particle learning, our model is able to effectively capture the context change and learn the latent parameters. In addition, those learnt parameters can be naturally integrated into existing multi-arm selection strategies such as LinUCB and Thompson sampling. Empirical studies on two real-world applications, including online personalized advertising and news recommendation, demonstrate the effectiveness of our proposed approach. The experimental results also show that our algorithm can dynamically track the changing reward over time and consequently improve the click-through rate. © 2016 ACM.",Particle learning; Personalization; Probability matching; Recommender system; Time varying contextual bandit,Commerce,,SCOPUS,1,other,batch,n,y,n,y,n,n,y,n,n,y,n,n,n,n,n,y,Contextual Bandits,Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,"Zeng C., Wang Q., Mokhtari S., Li T.","Zeng, C., School of Computing and Information Science, Florida International University, Miami, United States; Wang, Q., School of Computing and Information Science, Florida International University, Miami, United States; Mokhtari, S., School of Computing and Information Science, Florida International University, Miami, United States; Li, T., School of Computing and Information Science, Florida International University, Miami, United States",8.0,10.1145/2939672.2939878,Conference Paper,2016-01-01,Zeng_2016," @article{Zeng_2016, title={Online Context-Aware Recommendation with Time Varying Multi-Armed Bandit}, ISBN={9781450342322}, url={http://dx.doi.org/10.1145/2939672.2939878}, DOI={10.1145/2939672.2939878}, journal={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD  ’16}, publisher={ACM Press}, author={Zeng, Chunqiu and Wang, Qing and Mokhtari, Shekoofeh and Li, Tao}, year={2016}}
"
Hierarchical exploration for accelerating contextual bandits,"Contextual bandit learning is an increasingly popular approach to optimizing recommender systems via user feedback, but can be slow to converge in practice due to the need for exploring a large feature space. In this paper, we propose a coarse-to-fine hierarchical approach for encoding prior knowledge that drastically reduces the amount of exploration required. Intuitively, user preferences can be reasonably embedded in a coarse low-dimensional feature space that can be explored efficiently, requiring exploration in the high-dimensional space only as necessary. We introduce a bandit algorithm that explores within this coarse-to-fine spectrum, and prove performance guarantees that depend on how well the coarse space captures the user's preferences. We demonstrate substantial improvement over conventional bandit algorithms through extensive simulation as well as a live user study in the setting of personalized news recommendation. Copyright 2012 by the author(s)/owner(s).",,Commerce,,SCOPUS,1/person,state representation,online,y,n,y,y,n,y,y,y,n,n,n,n,y,y,n,y,"LinUCB,CoFineUCB","Proceedings of the 29th International Conference on Machine Learning, ICML 2012","Yue Y., Hong S.A., Guestrin C.","Yue, Y., iLab, H. John Heinz III College, Carnegie Mellon University, Pittsburgh, PA 15213, United States; Hong, S.A., School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States; Guestrin, C., School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States",8.0,,Conference Paper,2012-01-01,yue2012hierarchical,"@inproceedings{yue2012hierarchical,
  title={Hierarchical exploration for accelerating contextual bandits},
  author={Yue, Yisong and Hong, Sue Ann and Guestrin, Carlos},
  booktitle={Proceedings of the 29th International Coference on International Conference on Machine Learning},
  pages={979--986},
  year={2012},
  organization={Omnipress}
}
"
Satisfaction based Q-learning for integrated lighting and blind control,"Various lighting and blind control methods have been presented to improve user comfort and reduce energy consumption simultaneously. However, there are opportunities to improve control performances by introducing more recent information and machine learning technologies which allow more comprehensive consideration of the balance between user comfort and system energy consumption. To be more specific, in terms of user comfort, unified set-point may not be desirable since different people may have different comfort preferences. In terms of energy consumption, the excessive cooling load of HVAC system should be considered in summer when utilizing solar incidence to reduce the lighting electricity consumption. The setting of the blind slat angle still has great room to improve instead of the cut-off angle. Moreover, users' demands are not fully met, so sometimes they still want to override the automated control. Thus, a closed-loop satisfaction based system is developed in this paper, specifically we introduce an improved reinforcement learning controller to obtain an optimal control strategy of blinds and lights. It could provide a personalized service via introducing subjects perceptions of surroundings gathered by a novel interface as the feedback signal. The proposed system was implemented on a practical test-bed in an energy-efficient building. Compared with the traditional control, it can provide a more acceptable and energy-efficient luminous environment. © 2016 Elsevier B.V.",Blinds; Day-light; Energy saving; Integrated control; Q-learning,Smart Home,,,1/person,not used,online,n,n,y,n,n,y,y,n,y,y,n,n,n,y,n,y,Q-Learning,Energy and Buildings,"Cheng Z., Zhao Q., Wang F., Jiang Y., Xia L., Ding J.","Cheng, Z., Center for Intelligent and Networked System, Department of Automation and TNList, Tsinghua University, Beijing, China; Zhao, Q., Center for Intelligent and Networked System, Department of Automation and TNList, Tsinghua University, Beijing, China; Wang, F., Department of Building Science, Tsinghua University, Beijing, China; Jiang, Y., Department of Building Science, Tsinghua University, Beijing, China; Xia, L., Center for Intelligent and Networked System, Department of Automation and TNList, Tsinghua University, Beijing, China; Ding, J., United Technologies Research Center (China) Ltd., Shanghai, China",7.0,10.1016/j.enbuild.2016.05.067,Article,2016-01-01,Cheng_2016," @article{Cheng_2016, title={Satisfaction based Q-learning for integrated lighting and blind control}, volume={127}, ISSN={0378-7788}, url={http://dx.doi.org/10.1016/j.enbuild.2016.05.067}, DOI={10.1016/j.enbuild.2016.05.067}, journal={Energy and Buildings}, publisher={Elsevier BV}, author={Cheng, Zhijin and Zhao, Qianchuan and Wang, Fulin and Jiang, Yi and Xia, Li and Ding, Jinlei}, year={2016}, month={Sep}, pages={43–55}}
"
Personalized galaxies of information,"The Personalized Galaxies of Information demonstration presents a new interface approach for visualizing, navigating and accessing information objects in a large body of unstructured information, such as on-line new stories, photographs and video clips available via Clarinews; electronic mail; and World Wide Web documents. The system provides mechanisms to analyze the relationships between information objects and builds a representation of the underlying structure of the entire body of information. This relational structure is used to construct a visual information space with which the user interacts to explore the contents of the information base. The system also uses a learning algorithm to adaptively customize the presentation of information to a particular user's interests. This dynamic, personalized structuring of information helps users perform directed searches while simultaneously affording general browsing in a fluid and seamless environment.","Information visualization, abstracted information spaces, 3D interactive graphics, user interest models, reinforcement learning",Education,,SCOPUS,1/person,state representation,batch,n,y,n,n,y,n,n,n,n,n,n,n,n,n,n,y,"RL, not further specified",Conference on Human Factors in Computing Systems - Proceedings,Rennison Earl,"Rennison, Earl, MIT Media Lab, Cambridge, United States",7.0,,Conference Paper,1995-01-01,rennison1995personalized,"@inproceedings{rennison1995personalized,
  title={Personalized Galaxies of Infomation},
  author={Rennison, E},
  booktitle={Companion of the ACM Conference on Human Factors in Computing Systems (CHI'95)},
  year={1995}
}
"
Personalized ad recommendation systems for life-time value optimization with guarantees,"In this paper, we propose a framework for using reinforcement learning (RL) algorithms to learn good policies for personalized ad recommendation (PAR) systems. The RL algorithms take into account the long-term effect of an action, and thus, could be more suitable than myopic techniques like supervised learning and contextual bandit, for modern PAR systems in which the number of returning visitors is rapidly growing. However, while myopic techniques have been well-studied in PAR systems, the RL approach is still in its infancy, mainly due to two fundamental challenges: how to compute a good RL strategy and how to evaluate a solution using historical data to ensure its ""safety"" before deployment. In this paper, we propose to use a family of off-policy evaluation techniques with statistical guarantees to tackle both these challenges. We apply these methods to a real PAR problem, both for evaluating the final performance and for optimizing the parameters of the RL algorithm. Our results show that a RL algorithm equipped with these offpolicy evaluation techniques outperforms the myopic approaches. Our results also give fundamental insights on the difference between the click through rate (CTR) and life-time value (LTV) metrics for evaluating the performance of a PAR algorithm.",,Commerce,,y,1,not used,batch,n,y,n,n,y,n,n,n,n,y,y,n,y,y,n,y,"High confidence off-policy evaluation
(HCOPE)",IJCAI International Joint Conference on Artificial Intelligence,"Theocharous G., Thomas P.S., Ghavamzadeh M.","Theocharous, G., Adobe Research, United States; Thomas, P.S., Adobe Research, United States, UMassAmherst, United States; Ghavamzadeh, M., Adobe Research, United States, INRIA, France",7.0,,Conference Paper,2015-01-01,theocharous2015personalized,"@inproceedings{theocharous2015personalized,
  title={Personalized ad recommendation systems for life-time value optimization with guarantees},
  author={Theocharous, Georgios and Thomas, Philip S and Ghavamzadeh, Mohammad},
  booktitle={Twenty-Fourth International Joint Conference on Artificial Intelligence},
  year={2015}
}
"
A dialogue game framework with personalized training using reinforcement learning for computer-assisted language learning,"We propose a framework for computer-assisted language learning as a pedagogical dialogue game. The goal is to offer personalized learning sentences on-line for each individual learner considering the learner's learning status, in order to strike a balance between more practice on poorly-pronounced units and complete practice on the whole set of pronunciation units. This objective is achieved using a Markov decision process (MDP) trained with reinforcement learning using simulated learners generated from real learner data. Preliminary experimental results on a subset of the example dialogue script show the effectiveness of the framework.",Computer-Assisted Language Learning;Dialogue Game;Markov Decision Process;Reinforcement Learning,Education,,IEEE Explore,1,state representation,batch,y,y,n,y,n,n,n,y,n,y,n,n,n,y,n,y,MDP,"2013 IEEE International Conference on Acoustics, Speech and Signal Processing",P. h. Su; Y. B. Wang; T. h. Yu; L. s. Lee,,6.0,10.1109/ICASSP.2013.6639266,IEEE Conferences,2013-01-01,Su_2013," @article{Su_2013, title={A dialogue game framework with personalized training using reinforcement learning for computer-assisted language learning}, ISBN={9781479903566}, url={http://dx.doi.org/10.1109/ICASSP.2013.6639266}, DOI={10.1109/icassp.2013.6639266}, journal={2013 IEEE International Conference on Acoustics, Speech and Signal Processing}, publisher={IEEE}, author={Su, Pei-hao and Wang, Yow-Bang and Yu, Tien-han and Lee, Lin-shan}, year={2013}, month={May}}
"
Reinforcement learning of context models for a ubiquitous personal assistant,"Ubiquitous environments may become a reality in a foreseeable future and research is aimed on making them more and more adapted and comfortable for users. Our work consists on applying reinforcement learning techniques in order to adapt services provided by a ubiquitous assistant to the user. The learning produces a context model, associating actions to perceived situations of the user. Associations are based on feedback given by the user as a reaction to the behavior of the assistant. Our method brings a solution to some of the problems encountered when applying reinforcement learning to systems where the user is in the loop. For instance, the behavior of the system is completely incoherent at the be-ginning and needs time to converge. The user does not accept to wait that long to train the system. The user's habits may change over time and the assistant needs to integrate these changes quickly. We study methods to accelerate the reinforced learning process. © 2009 Springer-Verlag Berlin Heidelberg.",,Domain Independent,,SCOPUS,1/person,state representation,batch,y,n,n,y,n,n,n,n,y,y,n,y,y,y,n,n,DYNA-Q,Advances in Soft Computing,"Zaidenberg S., Reignier P., Crowley J.L.","Zaidenberg, S., Laboratoire LIG, 681 rue de la Passerelle, St-Martin d'Hères 38402, France; Reignier, P., Laboratoire LIG, 681 rue de la Passerelle, St-Martin d'Hères 38402, France; Crowley, J.L., Laboratoire LIG, 681 rue de la Passerelle, St-Martin d'Hères 38402, France",6.0,10.1007/978-3-540-85867-6_30,Conference Paper,2009-01-01,Zaidenberg," @article{Zaidenberg, title={Reinforcement Learning of Context Models for a Ubiquitous Personal Assistant}, ISBN={9783540858676}, ISSN={1860-0794}, url={http://dx.doi.org/10.1007/978-3-540-85867-6_30}, DOI={10.1007/978-3-540-85867-6_30}, journal={3rd Symposium of Ubiquitous Computing and Ambient Intelligence 2008}, publisher={Springer Berlin Heidelberg}, author={Zaidenberg, Sofia and Reignier, Patrick and Crowley, James L.}, pages={254–264}}
"
Optimal radio channel recommendations with explicit and implicit feedback,"The very large majority of recommender systems are running as server-side applications, and they are controlled by the content provider, i.e., who provides the recommended items. This paper focuses on a different scenario: the user is supposed to be able to access content from multiple providers, in our application they offer radio channels, and it is up to a personal recommender installed on the clients' side to decide which channel to select and recommend to the user. We exploit the implicit feedback derived from the user's listening behavior, and we model channel recommendation as a sequential decision making problem. We have implemented a personal RS that integrates reinforcement learning techniques to decide what channel to play every time the user asks for a new music track or the current track finishes playing. In a live user study we show that the proposed system can sequentially select the next channel to play such that the users listen to the streamed tracks for a larger fraction, and for more time, compared to a baseline system not exploiting implicit feedback. Copyright © 2012 by the Association for Computing Machinery, Inc. (ACM).",Implicit feedback; Reinforcement learning; Sequential music recommendations,Entertainment,,SCOPUS,1,not used,online,n,y,n,n,n,y,n,n,n,y,n,n,y,n,n,y,Q-Learning,RecSys'12 - Proceedings of the 6th ACM Conference on Recommender Systems,"Moling O., Baltrunas L., Ricci F.","Moling, O., Free University of Bozen-Bolzano, Piazza Domenicani 3, Bolzano, Italy; Baltrunas, L., Telefonica Research, Plaza se E. Lluchi Martin 5, Barcelona, Spain; Ricci, F., Free University of Bozen-Bolzano, Piazza Domenicani 3, Bolzano, Italy",6.0,10.1145/2365952.2365971,Conference Paper,2012-01-01,Moling_2012," @article{Moling_2012, title={Optimal radio channel recommendations with explicit and implicit feedback}, ISBN={9781450312707}, url={http://dx.doi.org/10.1145/2365952.2365971}, DOI={10.1145/2365952.2365971}, journal={Proceedings of the sixth ACM conference on Recommender systems - RecSys  ’12}, publisher={ACM Press}, author={Moling, Omar and Baltrunas, Linas and Ricci, Francesco}, year={2012}}
"
A multimodal adaptive session manager for physical rehabilitation exercising,"Physical exercising is an essential part of any rehabilitation plan. The subject must be committed to a daily exercising routine, as well as to a frequent contact with the therapist. Rehabilitation plans can be quite expensive and time-consuming. On the other hand, tele-rehabilitation systems can be really helpful and efficient for both subjects and therapists. In this paper, we present ReAdapt, an adaptive module for a tele-rehabilitation system that takes into consideration the progress and performance of the exercising utilizing multisensing data and adjusts the session difficulty resulting to a personalized session. Multimodal data such as speech, facial expressions and body motion are being collected during the exercising and feed the system to decide on the exercise and session difficulty. We formulate the problem as a Markov Decision Process and apply a Reinforcement Learning algorithm to train and evaluate the system on simulated data. © 2015 ACM.",Markov Decision Process; Multimodal adaptive systems; Personalized rehabilitation systems; Reinforcement Learning,Health,,SCOPUS,1,state representation,online,y,n,n,y,n,n,n,n,y,y,y,y,n,y,n,y,DYNA-Q,"8th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2015 - Proceedings","Tsiakas K., Huber M., Makedon F.","Tsiakas, K., HERACLEIA Lab, Computer Science and Engineering Department, University of Texas, Arlington, United States; Huber, M., Computer Science and Engineering Department, University of Texas, Arlington, United States; Makedon, F., HERACLEIA Lab, Computer Science and Engineering Department, University of Texas, Arlington, United States",6.0,10.1145/2769493.2769507,Conference Paper,2015-01-01,Tsiakas_2015," @article{Tsiakas_2015, title={A multimodal adaptive session manager for physical rehabilitation exercising}, ISBN={9781450334525}, url={http://dx.doi.org/10.1145/2769493.2769507}, DOI={10.1145/2769493.2769507}, journal={Proceedings of the 8th ACM International Conference on PErvasive Technologies Related to Assistive Environments - PETRA  ’15}, publisher={ACM Press}, author={Tsiakas, Konstantinos and Huber, Manfred and Makedon, Fillia}, year={2015}}
"
An Actor-Critic based controller for glucose regulation in type 1 diabetes,"A novel adaptive approach for glucose control in individuals with type 1 diabetes under sensor-augmented pump therapy is proposed. The controller, is based on Actor-Critic (AC) learning and is inspired by the principles of reinforcement learning and optimal control theory. The main characteristics of the proposed controller are (i) simultaneous adjustment of both the insulin basal rate and the bolus dose, (ii) initialization based on clinical procedures, and (iii) real-time personalization. The effectiveness of the proposed algorithm in terms of glycemic control has been investigated in silico in adults, adolescents and children under open-loop and closed-loop approaches, using announced meals with uncertainties in the order of ±25% in the estimation of carbohydrates.The results show that glucose regulation is efficient in all three groups of patients, even with uncertainties in the level of carbohydrates in the meal. The percentages in the A. +. B zones of the Control Variability Grid Analysis (CVGA) were 100% for adults, and 93% for both adolescents and children.The AC based controller seems to be a promising approach for the automatic adjustment of insulin infusion in order to improve glycemic control. After optimization of the algorithm, the controller will be tested in a clinical trial. © 2012 Elsevier Ireland Ltd.",Actor-Critic; Closed-loop control; Glucose control; Reinforcement learning,Health,,y,1/group,not used,unknown,y,n,n,y,n,n,n,n,n,y,n,n,n,y,n,y,Actor-Critic,Computer Methods and Programs in Biomedicine,"Daskalaki E., Diem P., Mougiakakou S.G.","Daskalaki, E., ARTORG Center for Biomedical Engineering Research, Diabetes Technology Research Group, University of Bern, Murtenstrasse 50, 3010 Bern, Switzerland; Diem, P., Division of Endocrinology, Diabetes and Clinical Nutrition, University Hospital, Inselspital, University of Bern, 3010 Bern, Switzerland; Mougiakakou, S.G., ARTORG Center for Biomedical Engineering Research, Diabetes Technology Research Group, University of Bern, Murtenstrasse 50, 3010 Bern, Switzerland",6.0,10.1016/j.cmpb.2012.03.002,Article,2013-01-01,Daskalaki_2013_0," @article{Daskalaki_2013_0, title={An Actor–Critic based controller for glucose regulation in type 1 diabetes}, volume={109}, ISSN={0169-2607}, url={http://dx.doi.org/10.1016/j.cmpb.2012.03.002}, DOI={10.1016/j.cmpb.2012.03.002}, number={2}, journal={Computer Methods and Programs in Biomedicine}, publisher={Elsevier BV}, author={Daskalaki, Elena and Diem, Peter and Mougiakakou, Stavroula G.}, year={2013}, month={Feb}, pages={116–125}}
"
A personalized QoE-aware handover decision based on distributed reinforcement learning,"Recent developments in heterogeneous mobile networks and growing demands for variety of real-time and multimedia applications have emphasized the necessity of more intelligent handover decisions. Addressing the context knowledge of mobile devices, users, applications, and networks is the subject of context-aware handoff decision as a recent effort to this aim. However, user perception has not been attended adequately in the area of context-aware handover decision making. Mobile users may have different judgments about the Quality of Service (QoS) depending on their environmental conditions, and personal and psychological characteristics. This reality has been exploited in this paper to introduce a personalized user-centric handoff decision method to decide about the time and target of handover based on User Perceived Quality (UPQ) feedbacks. The UPQ degradations are mainly for the sake of (1) exiting the coverage of the serving Point of Attachment (PoA) or (2) QoS degradation of serving access network. Using UPQ metric, the proposed method obviates the necessity of being aware about rapidly varying network QoS parameters and overcomes the complexity and overhead of gathering and managing some other context information. Moreover, considering the underlying network and geographical map, the proposed method is able to inherently exploit the trajectory information of mobile users for handover decision. UPQ degradation is not only due to the user behaviour, but also due to the behaviours of others users. As such, multi-agent reinforcement learning paradigm has been considered for target PoA selection. The employed decision algorithm is based on WoLF-PHC learning method where UPQ is used as a delayed reward for training. The proposed handoff decision has been implemented under IEEE 802.21 framework using NS2 network simulator. The results have shown better performance of the proposed method comparing to conventional methods assuming regular movement of mobile users. © 2013 Springer Science+Business Media New York.",Context-aware handover; Distributed reinforcement learning; QoE-aware handover; User perceived quality,Entertainment,,SCOPUS,1,not used,online,y,n,n,y,n,n,y,n,n,y,n,n,n,n,n,y,"Q-Learning,Multi-agent Reinforcement Learning",Wireless Networks,"Ghahfarokhi B.S., Movahhedinia N.","Ghahfarokhi, B.S., Department of Information Technology Engineering, University of Isfahan, Isfahan, Iran; Movahhedinia, N., Department of Computer Engineering, University of Isfahan, Isfahan, Iran",5.0,10.1007/s11276-013-0572-2,Article,2013-01-01,Ghahfarokhi_2013," @article{Ghahfarokhi_2013, title={A personalized QoE-aware handover decision based on distributed reinforcement learning}, volume={19}, ISSN={1572-8196}, url={http://dx.doi.org/10.1007/s11276-013-0572-2}, DOI={10.1007/s11276-013-0572-2}, number={8}, journal={Wireless Networks}, publisher={Springer Nature}, author={Ghahfarokhi, Behrouz Shahgholi and Movahhedinia, Naser}, year={2013}, month={Mar}, pages={1807–1828}}
"
A self-taught artificial agent for multi-physics computational model personalization,"Personalization is the process of fitting a model to patient data, a critical step towards application of multi-physics computational models in clinical practice. Designing robust personalization algorithms is often a tedious, time-consuming, model- and data-specific process. We propose to use artificial intelligence concepts to learn this task, inspired by how human experts manually perform it. The problem is reformulated in terms of reinforcement learning. In an off-line phase, Vito, our self-taught artificial agent, learns a representative decision process model through exploration of the computational model: it learns how the model behaves under change of parameters. The agent then automatically learns an optimal strategy for on-line personalization. The algorithm is model-independent; applying it to a new model requires only adjusting few hyper-parameters of the agent and defining the observations to match. The full knowledge of the model itself is not required. Vito was tested in a synthetic scenario, showing that it could learn how to optimize cost functions generically. Then Vito was applied to the inverse problem of cardiac electrophysiology and the personalization of a whole-body circulation model. The obtained results suggested that Vito could achieve equivalent, if not better goodness of fit than standard methods, while being more robust (up to 11% higher success rates) and with faster (up to seven times) convergence rate. Our artificial intelligence approach could thus make personalization algorithms generalizable and self-adaptable to any patient and any model. © 2016",Artificial intelligence; Computational modeling; Model personalization; Reinforcement learning,Health,,SCOPUS,1,state representation,batch,n,y,n,n,y,n,n,y,n,y,n,y,n,n,n,y,model based RL,Medical Image Analysis,"Neumann D., Mansi T., Itu L., Georgescu B., Kayvanpour E., Sedaghat-Hamedani F., Amr A., Haas J., Katus H., Meder B., Steidl S., Hornegger J., Comaniciu D.","Neumann, D., Medical Imaging Technologies, Siemens Healthcare GmbH, Erlangen, Germany, Pattern Recognition Lab, FAU Erlangen-Nürnberg, Erlangen, Germany; Mansi, T., Medical Imaging Technologies, Siemens Healthcare, Princeton, United States; Itu, L., Siemens Corporate Technology, Siemens SRL, Brasov, Romania, Transilvania University of Brasov, Brasov, Romania; Georgescu, B., Medical Imaging Technologies, Siemens Healthcare, Princeton, United States; Kayvanpour, E., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Sedaghat-Hamedani, F., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Amr, A., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Haas, J., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Katus, H., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Meder, B., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Steidl, S., Pattern Recognition Lab, FAU Erlangen-Nürnberg, Erlangen, Germany; Hornegger, J., Pattern Recognition Lab, FAU Erlangen-Nürnberg, Erlangen, Germany; Comaniciu, D., Medical Imaging Technologies, Siemens Healthcare, Princeton, United States",5.0,10.1016/j.media.2016.04.003,Article,2016-01-01,Neumann_2016," @article{Neumann_2016, title={A self-taught artificial agent for multi-physics computational model personalization}, volume={34}, ISSN={1361-8415}, url={http://dx.doi.org/10.1016/j.media.2016.04.003}, DOI={10.1016/j.media.2016.04.003}, journal={Medical Image Analysis}, publisher={Elsevier BV}, author={Neumann, Dominik and Mansi, Tommaso and Itu, Lucian and Georgescu, Bogdan and Kayvanpour, Elham and Sedaghat-Hamedani, Farbod and Amr, Ali and Haas, Jan and Katus, Hugo and Meder, Benjamin and et al.}, year={2016}, month={Dec}, pages={52–64}}
"
Knowledge transfer between speakers for personalised dialogue management,"Model-free reinforcement learning has been shown to be a promising data driven approach for automatic dialogue policy optimization, but a relatively large amount of dialogue interactions is needed before the system reaches reasonable performance. Recently, Gaussian process based reinforcement learning methods have been shown to reduce the number of dialogues needed to reach optimal performance, and pre-training the policy with data gathered from different dialogue systems has further reduced this amount. Following this idea, a dialogue system designed for a single speaker can be initialised with data from other speakers, but if the dynamics of the speakers are very different the model will have a poor performance. When data gathered from different speakers is available, selecting the data from the most similar ones might improve the performance. We propose a method which automatically selects the data to transfer by defining a similarity measure between speakers, and uses this measure to weight the influence of the data from each speaker in the policy model. The methods are tested by simulating users with different severities of dysarthria interacting with a voice enabled environmental control system.. © 2015 Association for Computational Linguistics.",,Domain Independent,,SCOPUS,1/person,other,online,y,n,n,y,n,n,n,n,n,y,n,n,y,y,n,n,GP-RL,"SIGDIAL 2015 - 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Proceedings of the Conference","Casanueva I., Hain T., Christensen H., Marxer R., Green P.","Casanueva, I., Department of Computer Science, University of Sheffield, United Kingdom; Hain, T., Department of Computer Science, University of Sheffield, United Kingdom; Christensen, H., Department of Computer Science, University of Sheffield, United Kingdom; Marxer, R., Department of Computer Science, University of Sheffield, United Kingdom; Green, P., Department of Computer Science, University of Sheffield, United Kingdom",5.0,,Conference Paper,2015-01-01,casanueva2015knowledge,"@inproceedings{casanueva2015knowledge,
  title={Knowledge transfer between speakers for personalised dialogue management},
  author={Casanueva, Inigo and Hain, Thomas and Christensen, Heidi and Marxer, Ricard and Green, Phil},
  booktitle={Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
  pages={12--21},
  year={2015}
}
"
Personalized intelligent tutoring system using reinforcement learning,"In this paper, we present a Personalized Intelligent Tutoring System that uses Reinforcement Learning techniques to implicitly learn teaching rules and provide instructions to students based on their needs. The system works on coarsely labeled data with minimum expert knowledge to ease extension to newer domains. Copyright © 2011, Association for the Advancement of Artificial Intelligence. All rights reserved.",,Education,,y,1,not used,batch,n,y,n,n,y,n,n,n,n,y,n,n,n,n,n,n,Actor-Critic,"Proceedings of the 24th International Florida Artificial Intelligence Research Society, FLAIRS - 24","Malpani A., Ravindran B., Murthy H.","Malpani, A., Microsoft Corporation, India; Ravindran, B., Department of Computer Science and Engineering, IIT Madras, India; Murthy, H., Department of Computer Science and Engineering, IIT Madras, India",5.0,,Conference Paper,2011-01-01,Malpani2011PersonalizedIT,"@inproceedings{Malpani2011PersonalizedIT,
  title={Personalized Intelligent Tutoring System Using Reinforcement Learning},
  author={Ankit Malpani and Balaraman Ravindran and Hema Murthy},
  booktitle={FLAIRS Conference},
  year={2011}
}"
Reinforcement learning utilizes proxemics: An avatar learns to manipulate the position of people in immersive virtual reality,"A reinforcement learning (RL) method was used to train a virtual character to move participants to a specified location. The virtual environment depicted an alleyway displayed through a wide field-of-view head-tracked stereo head-mounted display. Based on proxemics theory, we predicted that when the character approached within a personal or intimate distance to the participants, they would be inclined to move backwards out of the way. We carried out a between-groups experiment with 30 female participants, with 10 assigned arbitrarily to each of the following three groups: In the Intimate condition the character could approach within 0.38m and in the Social condition no nearer than 1.2m. In the Random condition the actions of the virtual character were chosen randomly from among the same set as in the RL method, and the virtual character could approach within 0.38m. The experiment continued in each case until the participant either reached the target or 7 minutes had elapsed. The distributions of the times taken to reach the target showed significant differences between the three groups, with 9 out of 10 in the Intimate condition reaching the target significantly faster than the 6 out of 10 who reached the target in the Social condition. Only 1 out of 10 in the Random condition reached the target. The experiment is an example of applied presence theory: we rely on the many findings that people tend to respond realistically in immersive virtual environments, and use this to get people to achieve a task of which they had been unaware. This method opens up the door for many such applications where the virtual environment adapts to the responses of the human participants with the aim of achieving particular goals. © 2012 ACM 1544-3558/2012/03- ART3 ©10.00.",Experimentation; Human Factors,Education,,SCOPUS,1,not used,online,n,n,y,n,n,y,n,y,n,y,n,n,n,n,n,y,Q-Learning,ACM Transactions on Applied Perception,"Kastanis I., Slater M.","Kastanis, I., Facultat de Psicologia, Campus de Mundet - Edifici Teatre, Universitat de Barcelona, Passeig de la Vall d'Hebron 171, 08035 Barcelona, Spain; Slater, M., University College London, United Kingdom, Facultat de Psicologia, Campus de Mundet - Edifici Teatre, ICREA-Universitat de Barcelona, Passeig de la Vall d'Hebron 171, 08035 Barcelona, Spain",5.0,10.1145/2134203.2134206,Article,2012-01-01,Kastanis_2012," @article{Kastanis_2012, title={Reinforcement learning utilizes proxemics}, volume={9}, ISSN={1544-3558}, url={http://dx.doi.org/10.1145/2134203.2134206}, DOI={10.1145/2134203.2134206}, number={1}, journal={ACM Transactions on Applied Perception}, publisher={Association for Computing Machinery (ACM)}, author={Kastanis, Iason and Slater, Mel}, year={2012}, month={Mar}, pages={1–15}}
"
Video summarization using reinforcement learning in eigenspace,"We propose video summarization using reinforcement learning. The importance score of each frame in a video is calculated from the user's actions in handling similar previous frames; if such frames were watched rather than skipped, a high score is assigned. To calculate the score, instead of using raw feature vectors extracted from images, we use feature vectors projected on eigenspace: as a result, we can deal with the features comprehensively. We also give an algorithm that uses the reinforcement learning method to create a personalized video summary. The summarization algorithm is applied to a soccer video to confirm its effectiveness.",,Entertainment,Videos,,1/person,not used,online,n,y,n,n,y,n,n,n,n,y,n,n,n,y,n,y,"RL, not further specified",Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101),K. Masumitsu; T. Echigo,,4.0,10.1109/ICIP.2000.899351,IEEE Conferences,2000-01-01,Masumitsu_2000," @article{Masumitsu_2000, title={Video summarization using reinforcement learning in eigenspace}, url={http://dx.doi.org/10.1109/ICIP.2000.899351}, DOI={10.1109/icip.2000.899351}, journal={Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101)}, publisher={IEEE}, author={Masumitsu, K. and Echigo, T.}, year={2000}}
"
Patient tailored virtual rehabilitation,"Virtual rehabilitation should be adaptable to the patient need and progress. To do so, patient in-game performace and ability are monitored to maintain an adequate level of challenge. A novel adaptation strategy is proposed by which patient control and speed are dynamically interrogated to adjust the game difficulty. The strategy is based on a Markov decision process seeding a therapist-guided reinforcement learning algorithm. The optimal learning scheme for the algorithm is established (α = 0.5). Convergence to an optimal therapeutic plan is demonstrated for patients with non-deterministic behaviour. The proposed adaptation algorithm can enhance existing virtual reality-based motor rehabilitation platforms by tailoring the games response to the patient changing needs. © 2013, Springer-Verlag Berlin Heidelberg.",,Health,,,1/person,not used,online,y,n,n,y,n,n,n,n,y,y,n,y,n,y,n,y,Q-Learning,Biosystems and Biorobotics,"Ávila-Sansores S., Orihuela-Espina F., Enrique-Sucar L.","Ávila-Sansores, S., Optics and Electronics, National Institute for Astrophysics, Puebla, Mexico; Orihuela-Espina, F., Optics and Electronics, National Institute for Astrophysics, Puebla, Mexico; Enrique-Sucar, L., Optics and Electronics, National Institute for Astrophysics, Puebla, Mexico",4.0,10.1007/978-3-642-34546-3_143,Book Chapter,2013-01-01,_vila_Sansores_2013," @article{_vila_Sansores_2013, title={Patient Tailored Virtual Rehabilitation}, ISBN={9783642345463}, ISSN={2195-3570}, url={http://dx.doi.org/10.1007/978-3-642-34546-3_143}, DOI={10.1007/978-3-642-34546-3_143}, journal={Biosystems & Biorobotics}, publisher={Springer Berlin Heidelberg}, author={Ávila-Sansores, Shender and Orihuela-Espina, Felipe and Enrique-Sucar, Luis}, year={2013}, pages={879–883}}
"
User centered and context dependent personalization through experiential transcoding,"The pervasive presence of devices exploited to use and deliver entertainment text-based content can make reading easier to get but more difficult to enjoy, in particular for people with reading-related disabilities. The main solutions that allow overcoming some of the difficulties experienced by users with specific and special needs are based on content adaptation and user profiling. This paper presents a system that aims to improve content legibility by exploiting experiential transcoding techniques. The system we propose tracks users' behaviors so as to provide a tailored adaptation of textual content, in order to fit the needs of a specific user on the several different devices she/he actually uses.",content adaptation;device capabilities;legibility;reinforcement learning;user profiling,Entertainment,,,1/person,not used,batch,y,n,n,y,n,n,y,n,y,n,n,y,n,y,n,y,Q-Learning,2014 IEEE 11th Consumer Communications and Networking Conference (CCNC),S. Ferretti; S. Mirri; C. Prandi; P. Salomoni,,4.0,10.1109/CCNC.2014.6940520,IEEE Conferences,2014-01-01,Ferretti_2014_0," @article{Ferretti_2014_0, title={User centered and context dependent personalization through experiential transcoding}, ISBN={9781479923557}, url={http://dx.doi.org/10.1109/CCNC.2014.6940520}, DOI={10.1109/ccnc.2014.6940520}, journal={2014 IEEE 11th Consumer Communications and Networking Conference (CCNC)}, publisher={IEEE}, author={Ferretti, Stefano and Mirri, Silvia and Prandi, Catia and Salomoni, Paola}, year={2014}, month={Jan}}
"
On-line policy learning and adaptation for real-time personalization of an artificial pancreas,"The dynamic complexity of the glucose-insulin metabolism in diabetic patients is the main obstacle towards widespread use of an artificial pancreas. The significant level of subject-specific glycemic variability requires continuously adapting the control policy to successfully face daily changes in patient's metabolism and lifestyle. In this paper, an on-line selective reinforcement learning algorithm that enables real-time adaptation of a control policy based on ongoing interactions with the patient so as to tailor the artificial pancreas is proposed. Adaptation includes two online procedures: on-line sparsification and parameter updating of the Gaussian process used to approximate the control policy. With the proposed sparsification method, the support data dictionary for on-line learning is modified by checking if in the arriving data stream there exists novel information to be added to the dictionary in order to personalize the policy. Results obtained in silico experiments demonstrate that on-line policy learning is both safe and efficient for maintaining blood glucose variability within the normoglycemic range. © 2014 Elsevier Ltd. All rights reserved.",Diabetes; Gaussian processes; Glycemic variability; On-line sparsification; Policy learning; Reinforcement learning,Health,,,1/person,not used,other,y,n,n,y,n,n,y,n,n,y,y,y,n,y,n,y,Q-Learning,Expert Systems with Applications,"De Paula M., Acosta G.G., Martínez E.C.","De Paula, M., INTELYMEC, Centro de Investigaciones en Física e Ingeniería del Centro - CIFICEN, CONICET, Av. del Valle 5737, Olavarría, Argentina; Acosta, G.G., UNCPBA-Facultad de Ingeniería, Universidad Nacional del Centro de la Provincia de Buenos Aires, Av. del Valle 5737, Olavarría, Argentina; Martínez, E.C., INGAR (CONICET-UTN), Avellaneda 3657, Santa Fe, Argentina",4.0,10.1016/j.eswa.2014.10.038,Article,2015-01-01,De_Paula_2015_0," @article{De_Paula_2015_0, title={On-line policy learning and adaptation for real-time personalization of an artificial pancreas}, volume={42}, ISSN={0957-4174}, url={http://dx.doi.org/10.1016/j.eswa.2014.10.038}, DOI={10.1016/j.eswa.2014.10.038}, number={4}, journal={Expert Systems with Applications}, publisher={Elsevier BV}, author={De Paula, Mariano and Acosta, Gerardo G. and Martínez, Ernesto C.}, year={2015}, month={Mar}, pages={2234–2255}}
"
A learning-based control architecture for an assistive robot providing social engagement during cognitively stimulating activities,"Recent studies have shown that sustained engagement in cognitively stimulating activities has had positive effects on the cognitive functioning of humans. The objective of our work is to develop an intelligent socially assistive robot that can engage individuals in person-centered cognitively stimulating activities. In this paper, we present the design of a novel learning-based control architecture that enables the robot to act as a social motivator by providing assistance, encouragement and celebration during the course of an activity. A hierarchical reinforcement learning (HRL) approach is used to provide the robot with the ability to: (i) learn appropriate assistive behaviors based on the structure of the activity and (ii) personalize the interaction based on the person's affective state during the activity. Preliminary experiments show that the proposed learning-based control architecture is effective in determining the optimal assistive behaviors of the robot during a memory game interaction.",,Education,,IEEE Explore,1,state representation,online,n,n,y,n,n,y,n,n,n,y,n,n,y,n,n,y,hierarchical reinforcement learning,2011 IEEE International Conference on Robotics and Automation,J. Chan; G. Nejat,,4.0,10.1109/ICRA.2011.5980426,IEEE Conferences,2011-01-01,Chan_2011," @article{Chan_2011, title={A learning-based control architecture for an assistive robot providing social engagement during cognitively stimulating activities}, ISBN={9781612843865}, url={http://dx.doi.org/10.1109/ICRA.2011.5980426}, DOI={10.1109/icra.2011.5980426}, journal={2011 IEEE International Conference on Robotics and Automation}, publisher={IEEE}, author={Chan, Jeanie and Nejat, Goldie}, year={2011}, month={May}}
"
Learning cooperative persuasive dialogue policies using framing,"In this paper, we propose a new framework of cooperative persuasive dialogue, where a dialogue system simultaneously attempts to achieve user satisfaction while persuading the user to take some action that achieves a pre-defined system goal. Within this framework, we describe a method for reinforcement learning of cooperative persuasive dialogue policies by defining a reward function that reflects both the system and user goal, and using framing, the use of emotionally charged statements common in persuasive dialogue between humans. In order to construct the various components necessary for reinforcement learning, we first describe a corpus of persuasive dialogues between human interlocutors, then propose a method to construct user simulators and reward functions specifically tailored to persuasive dialogue based on this corpus. Then, we implement a fully automatic text-based dialogue system for evaluating the learned policies. Using the implemented dialogue system, we evaluate the learned policy and the effect of framing through experiments both with a user simulator and with real users. The experimental evaluation indicates that the proposed method is effective for construction of cooperative persuasive dialogue systems. © 2016 Elsevier B.V.",Cooperative persuasive dialogue; Dialogue modeling; Dialogue system; Framing; Reinforcement learning,Education,,SCOPUS,1,not used,unknown,y,n,n,y,n,y,y,y,y,y,n,y,n,y,n,y,"RL, not further specified",Speech Communication,"Hiraoka T., Neubig G., Sakti S., Toda T., Nakamura S.","Hiraoka, T., Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan; Neubig, G., Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan; Sakti, S., Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan; Toda, T., Nagoya University, Furo-cho, Chikusa-kuNagoya, Japan; Nakamura, S., Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan",3.0,10.1016/j.specom.2016.09.002,Article,2016-01-01,Hiraoka_2016," @article{Hiraoka_2016, title={Learning cooperative persuasive dialogue policies using framing}, volume={84}, ISSN={0167-6393}, url={http://dx.doi.org/10.1016/j.specom.2016.09.002}, DOI={10.1016/j.specom.2016.09.002}, journal={Speech Communication}, publisher={Elsevier BV}, author={Hiraoka, Takuya and Neubig, Graham and Sakti, Sakriani and Toda, Tomoki and Nakamura, Satoshi}, year={2016}, month={Nov}, pages={83–96}}
"
Personalized tuning of a reinforcement learning control algorithm for glucose regulation,"Artificial pancreas is in the forefront of research towards the automatic insulin infusion for patients with type 1 diabetes. Due to the high inter- and intra-variability of the diabetic population, the need for personalized approaches has been raised. This study presents an adaptive, patient-specific control strategy for glucose regulation based on reinforcement learning and more specifically on the Actor-Critic (AC) learning approach. The control algorithm provides daily updates of the basal rate and insulin-to-carbohydrate (IC) ratio in order to optimize glucose regulation. A method for the automatic and personalized initialization of the control algorithm is designed based on the estimation of the transfer entropy (TE) between insulin and glucose signals. The algorithm has been evaluated in silico in adults, adolescents and children for 10 days. Three scenarios of initialization to i) zero values, ii) random values and iii) TE-based values have been comparatively assessed. The results have shown that when the TE-based initialization is used, the algorithm achieves faster learning with 98%, 90% and 73% in the A+B zones of the Control Variability Grid Analysis for adults, adolescents and children respectively after five days compared to 95%, 78%, 41% for random initialization and 93%, 88%, 41% for zero initial values. Furthermore, in the case of children, the daily Low Blood Glucose Index reduces much faster when the TE-based tuning is applied. The results imply that automatic and personalized tuning based on TE reduces the learning period and improves the overall performance of the AC algorithm.",,Health,,IEEE Explore,1,state representation,online,y,n,n,y,n,n,n,n,n,y,y,y,n,y,n,y,Actor-Critic,2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),E. Daskalaki; P. Diem; S. G. Mougiakakou,,3.0,10.1109/EMBC.2013.6610293,IEEE Conferences,2013-01-01,Daskalaki_2013," @article{Daskalaki_2013, title={Personalized tuning of a reinforcement learning control algorithm for glucose regulation}, ISBN={9781457702167}, url={http://dx.doi.org/10.1109/EMBC.2013.6610293}, DOI={10.1109/embc.2013.6610293}, journal={2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}, publisher={IEEE}, author={Daskalaki, Elena and Diem, Peter and Mougiakakou, Stavroula G.}, year={2013}, month={Jul}}
"
Controlling blood glucose variability under uncertainty using reinforcement learning and Gaussian processes,"Abstract Automated control of blood glucose (BG) concentration with a fully automated artificial pancreas will certainly improve the quality of life for insulin-dependent patients. Closed-loop insulin delivery is challenging due to inter- and intra-patient variability, errors in glucose sensors and delays in insulin absorption. Responding to the varying activity levels seen in outpatients, with unpredictable and unreported food intake, and providing the necessary personalized control for individuals is a challenging task for existing control algorithms. A novel approach for controlling glycemic variability using simulation-based learning is presented. A policy iteration algorithm that combines reinforcement learning with Gaussian process approximation is proposed. To account for multiple sources of uncertainty, a control policy is learned off-line using an Ito's stochastic model of the glucose-insulin dynamics. For safety and performance, only relevant data are sampled through Bayesian active learning. Results obtained demonstrate that a generic policy is both safe and efficient for controlling subject-specific variability due to a patient's lifestyle and its distinctive metabolic response. © 2015 Elsevier B.V.",Artificial pancreas; Diabetes; Gaussian processes; Policy iteration; Reinforcement learning; Stochastic optimal control,Health,,SCOPUS,1,not used,online,y,n,n,y,n,n,n,n,n,n,y,y,n,y,n,y,"RL, not further specified",Applied Soft Computing Journal,"De Paula M., Ávila L.O., Martínez E.C.","De Paula, M., INTELYMEC - Centro de Investigaciones en Física e Ingeniería del Centro - CIFICEN - CONICET, Facultad de Ingeniería, Universidad Nacional del Centro de la Provincia de Buenos Aires - UNCPBA, Av. del Valle 5737, Olavarría, Argentina; Ávila, L.O., INGAR (CONICET - UTN), Avellaneda 3657, Santa Fe, Argentina; Martínez, E.C., INGAR (CONICET - UTN), Avellaneda 3657, Santa Fe, Argentina",3.0,10.1016/j.asoc.2015.06.041,Article,2015-01-01,De_Paula_2015," @article{De_Paula_2015, title={Controlling blood glucose variability under uncertainty using reinforcement learning and Gaussian processes}, volume={35}, ISSN={1568-4946}, url={http://dx.doi.org/10.1016/j.asoc.2015.06.041}, DOI={10.1016/j.asoc.2015.06.041}, journal={Applied Soft Computing}, publisher={Elsevier BV}, author={De Paula, Mariano and Ávila, Luis Omar and Martínez, Ernesto C.}, year={2015}, month={Oct}, pages={310–332}}
"
Increasing Retrieval Quality in Conversational Recommenders,"A major task of research in conversational recommender systems is personalization. Critiquing is a common and powerful form of feedback, where a user can express her feature preferences by applying a series of directional critiques over the recommendations instead of providing specific preference values. Incremental Critiquing (IC) is a conversational recommender system that uses critiquing as a feedback to efficiently personalize products. The expectation is that in each cycle the system retrieves the products that best satisfy the user's soft product preferences from a minimal information input. In this paper, we present a novel technique that increases retrieval quality based on a combination of compatibility and similarity scores. Under the hypothesis that a user learns during the recommendation process, we propose two novel exponential Reinforcement Learning (RL) approaches for compatibility that take into account both the instant at which the user makes a critique and the number of satisfied critiques. Moreover, we consider that the impact of features on the similarity differs according to the preferences manifested by the user. We propose a Global Weighting (GW) approach that uses a common weight for nearest cases in order to focus on groups of relevant products. We show that our methodology significantly improves recommendation efficiency in four data sets of different sizes in terms of session length in comparison with state-of-the-art approaches. Moreover, our recommender shows higher robustness against noisy user data when compared to classical approaches.",Conversational recommender systems;case-based reasoning;critiquing elicitation;personalization.,Commerce,,IEEE Xplore,1,not used,online,n,y,n,n,y,n,n,y,y,n,n,n,y,y,n,n,"Monte Carlo,Backward TD",IEEE Transactions on Knowledge and Data Engineering,M. S. Llorente; S. E. Guerrero,,3.0,10.1109/TKDE.2011.116,IEEE Journals & Magazines,2012-01-01,Llorente_2012," @article{Llorente_2012, title={Increasing Retrieval Quality in Conversational Recommenders}, volume={24}, ISSN={1041-4347}, url={http://dx.doi.org/10.1109/TKDE.2011.116}, DOI={10.1109/tkde.2011.116}, number={10}, journal={IEEE Transactions on Knowledge and Data Engineering}, publisher={Institute of Electrical and Electronics Engineers (IEEE)}, author={Llorente and Guerrero, Sergio Escalera}, year={2012}, month={Oct}, pages={1876–1888}}
"
Towards a General Supporting Framework for Self-Adaptive Software Systems,"When confronted with their internal and environmental dynamics, various software systems increasingly require self-adaptation capabilities. The vision above requires the self-adaptation approach to tackle these challenges and some software systems have their own specific implementations. However, in this paper a general supporting framework is proposed for software systems to self-adapt with running environmental dynamics and meanwhile fulfill various user requirements. Three key issues are covered in the framework: 1) the overall control architecture, which adopts the double closed-loop style and respectively includes the self-adaptation loop and the self-learning loop; 2) a general descriptive language, which is a application-independent and unified language to represent self-adaptation knowledge about target systems; 3) three implementation mechanisms, including forward reasoning, planning and reinforcement learning using feedback, which are supported by the above descriptive language and executed at runtime in different modules. Finally, one scenario of on-demand services of massive data mining tasks is selected and the case study demonstrates how the framework is customized as required and how the approach works.",Double Closed-loop Control Arthitecture;General Descriptive Language;Hierarchical Task Network;Rete Algorithm;Self-Adaptive Supporting Framework;Self-Adaptive System,Other,,IEEE Xplore,1,not used,unknown,n,n,n,n,n,n,n,n,y,n,n,n,n,y,n,y,Q-Learning,2012 IEEE 36th Annual Computer Software and Applications Conference Workshops,L. Wang; Y. Gao; C. Cao; L. Wang,,2.0,10.1109/COMPSACW.2012.38,IEEE Conferences,2012-01-01,Wang_2012," @article{Wang_2012, title={Towards a General Supporting Framework for Self-Adaptive Software Systems}, ISBN={9780769547589}, url={http://dx.doi.org/10.1109/COMPSACW.2012.38}, DOI={10.1109/compsacw.2012.38}, journal={2012 IEEE 36th Annual Computer Software and Applications Conference Workshops}, publisher={IEEE}, author={Wang, Liangdong and Gao, Yang and Cao, Chun and Wang, Li}, year={2012}, month={Jul}}
"
A task-oriented service personalization scheme for smart environments using reinforcement learning,"Users want IoT environments to provide them with personalized support. These environments therefore need to be able to learn user preferences, such as what temperature the room should be or which lights should be turned on. We propose a novel system that separates the tasks of learning a user's preferences and realizing them within the environment. The system is able to capture user preferences by reinterpreting the problem as an optimization problem and applying inverse reinforcement learning to it. The system is shown to be able to accurately extract simple preferences given a small number of user demonstrations. These preferences are then realized by actuates devices running reinforcement learning-based agents to provide an environment consistent with the learnt preferences, also in situations not included in any user demonstration.",,Smart Home,,IEEE Explore,1/person,not used,batch,n,y,y,n,n,y,y,n,n,y,n,y,y,y,n,y,"SARSA,Inverse Reinforcement Learning",2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops),B. Tegelund; H. Son; D. Lee,,2.0,10.1109/PERCOMW.2016.7457110,IEEE Conferences,2016-01-01,Tegelund_2016," @article{Tegelund_2016, title={A task-oriented service personalization scheme for smart environments using reinforcement learning}, ISBN={9781509019410}, url={http://dx.doi.org/10.1109/PERCOMW.2016.7457110}, DOI={10.1109/percomw.2016.7457110}, journal={2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops)}, publisher={IEEE}, author={Tegelund, Bjorn and Son, Heesuk and Lee, Dongman}, year={2016}, month={Mar}}
"
A Service Recommendation Using Reinforcement Learning for Network-based Robots in Ubiquitous Computing Environments,"Ubiquitous robotic companion (URC ) is a new concept for the network-based robot platform which can enable to be following its master wherever or whenever he/she be in order to provide necessary services. The robot platforms in present normally interest in providing services through the direct interaction in responding to the user's demands. On the other hand, URC services are required to be provided by the means of recognizing the circumstances and taking a user's preference into account. In this paper, we propose a service recommendation scheme for URC robots. The proposed service recommendation, developed based on the reinforcement learning, can be used to provide personalized services by learning users' preferences or tasks through the interaction with users. Using simulation for rapid testing, we evaluate of the proposed scheme under a variety of user modeling types and discount factors.",,Domain Independent,,IEEE Xplore,1/person,not used,online,y,n,n,y,n,n,n,n,n,y,n,n,n,y,n,y,Q-Learning,RO-MAN 2007 - The 16th IEEE International Symposium on Robot and Human Interactive Communication,A. Moon; T. Kang; H. Kim; H. Kim,,2.0,10.1109/ROMAN.2007.4415198,IEEE Conferences,2007-01-01,Moon_2007," @article{Moon_2007, title={A Service Recommendation Using Reinforcement Learning for Network-based Robots in Ubiquitous Computing Environments}, ISBN={9781424416349}, url={http://dx.doi.org/10.1109/ROMAN.2007.4415198}, DOI={10.1109/roman.2007.4415198}, journal={RO-MAN 2007 - The 16th IEEE International Symposium on Robot and Human Interactive Communication}, publisher={IEEE}, author={Moon, Aekyung and Kang, Taegun and Kim, Hyoungsun and Kim, Hyun}, year={2007}}
"
Deep reinforcement learning for automated radiation adaptation in lung cancer:,"Purpose: To investigate deep reinforcement learning (DRL) based on historical treatment plans for developing automated radiation adaptation protocols for nonsmall cell lung cancer (NSCLC) patients that aim to maximize tumor local control at reduced rates of radiation pneumonitis grade 2 (RP2). Methods: In a retrospective population of 114 NSCLC patients who received radiotherapy, a three-component neural networks framework was developed for deep reinforcement learning (DRL) of dose fractionation adaptation. Large-scale patient characteristics included clinical, genetic, and imaging radiomics features in addition to tumor and lung dosimetric variables. First, a generative adversarial network (GAN) was employed to learn patient population characteristics necessary for DRL training from a relatively limited sample size. Second, a radiotherapy artificial environment (RAE) was reconstructed by a deep neural network (DNN) utilizing both original and synthetic data (by GAN) to estimate the transition probabilities for adaptation of personalized radiotherapy patients' treatment courses. Third, a deep Q-network (DQN) was applied to the RAE for choosing the optimal dose in a response-adapted treatment setting. This multicomponent reinforcement learning approach was benchmarked against real clinical decisions that were applied in an adaptive dose escalation clinical protocol. In which, 34 patients were treated based on avid PET signal in the tumor and constrained by a 17.2% normal tissue complication probability (NTCP) limit for RP2. The uncomplicated cure probability (P+) was used as a baseline reward function in the DRL. Results: Taking our adaptive dose escalation protocol as a blueprint for the proposed DRL (GAN + RAE + DQN) architecture, we obtained an automated dose adaptation estimate for use at ∼2/3 of the way into the radiotherapy treatment course. By letting the DQN component freely control the estimated adaptive dose per fraction (ranging from 1-5 Gy), the DRL automatically favored dose escalation/de-escalation between 1.5 and 3.8 Gy, a range similar to that used in the clinical protocol. The same DQN yielded two patterns of dose escalation for the 34 test patients, but with different reward variants. First, using the baseline P+ reward function, individual adaptive fraction doses of the DQN had similar tendencies to the clinical data with an RMSE = 0.76 Gy; but adaptations suggested by the DQN were generally lower in magnitude (less aggressive). Second, by adjusting the P+ reward function with higher emphasis on mitigating local failure, better matching of doses between the DQN and the clinical protocol was achieved with an RMSE = 0.5 Gy. Moreover, the decisions selected by the DQN seemed to have better concordance with patients eventual outcomes. In comparison, the traditional temporal difference (TD) algorithm for reinforcement learning yielded an RMSE = 3.3 Gy due to numerical instabilities and lack of sufficient learning. Conclusion: We demonstrated that automated dose adaptation by DRL is a feasible and a promising approach for achieving similar results to those chosen by clinicians. The process may require customization of the reward function if individual cases were to be considered. However, development of this framework into a fully credible autonomous system for clinical decision support would require further validation on larger multi-institutional datasets. © 2017 American Association of Physicists in Medicine.",adaptive radiotherapy; deep learning; lung cancer; reinforcement learning,Health,,SCOPUS,1,state representation,batch,y,y,n,y,y,n,y,y,n,y,y,n,n,n,n,y,"Deep Reinforcement Learning,DQN",Medical Physics,"Tseng H.-H., Luo Y., Cui S., Chien J.-T., Ten Haken R.K., Naqa I.E.","Tseng, H.-H., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States; Luo, Y., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States; Cui, S., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States; Chien, J.-T., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States, Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan; Ten Haken, R.K., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States; Naqa, I.E., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States",2.0,10.1002/mp.12625,Article,2017-01-01,Tseng_2017," @article{Tseng_2017, title={Deep reinforcement learning for automated radiation adaptation in lung cancer}, volume={44}, ISSN={0094-2405}, url={http://dx.doi.org/10.1002/mp.12625}, DOI={10.1002/mp.12625}, number={12}, journal={Medical Physics}, publisher={Wiley}, author={Tseng, Huan-Hsin and Luo, Yi and Cui, Sunan and Chien, Jen-Tzung and Ten Haken, Randall K. and Naqa, Issam El}, year={2017}, month={Nov}, pages={6690–6705}}
"
Reinforcement Learning of User Preferences for a Ubiquitous Personal Assistant,"New technologies bring a multiplicity of new possibilities for users to work with computers. Not only are spaces more and more equipped with stationary computers or notebooks, but more and more users carry mobile devices with them (smart-phones, personal digital …",,Entertainment,,Scholar,1,not used,online,n,n,y,n,n,y,n,n,y,n,n,n,y,n,n,y,Q-Learning,,,,2.0,,,2011-01-01,Zaidenberg2011ReinforcementLO,"@inproceedings{Zaidenberg2011ReinforcementLO,
  title={Reinforcement Learning of User Preferences for a Ubiquitous Personal Assistant},
  author={Sofia Zaidenberg and Patrick Reignier},
  year={2011}
}"
A Recursive Dialogue Game for Personalized Computer-Aided Pronunciation Training,"Learning languages in addition to the native language is very important for all people in the globalized world today, and computer-aided pronunciation training (CAPT) is attractive since the software can be used anywhere at any time, and repeated as many times as desired. In this paper, we introduce the immersive interaction scenario offered by spoken dialogues to CAPT by proposing a recursive dialogue game to make CAPT personalized. A number of tree-structured sub-dialogues are linked sequentially and recursively as the script for the game. The system policy at each dialogue turn is to select in real-time along the dialogue the best training sentence for each specific individual learner within the dialogue script, considering the learner's learning status and the future possible dialogue paths in the script, such that the learner can have the scores for all pronunciation units considered reaching a predefined standard in a minimum number of turns. The purpose here is that those pronunciation units poorly produced by the specific learner can be offered with more practice opportunities in the future sentences along the dialogue, which enables the learner to improve the pronunciation without having to repeat the same training sentences many times. This makes the learning process for each learner completely personalized. The dialogue policy is modeled by Markov decision process (MDP) with high-dimensional continuous state space, and trained with fitted value iteration using a huge number of simulated learners. These simulated leaners have the behavior similar to real learners, and were generated from a corpus of real learner data. The experiments demonstrated very promising results and a real cloud-based system is also successfully implemented.",Computer-aided pronunciation training (CAPT);Markov decision process;computer-assisted language learning;dialogue game;reinforcement learning,Education,,IEEE Explore,1,not used,batch,y,y,n,y,y,y,n,n,n,y,n,y,n,y,n,y,Q-Learning,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",P. h. Su; C. h. Wu; L. s. Lee,,2.0,10.1109/TASLP.2014.2375572,IEEE Journals & Magazines,2015-01-01,Su_2014," @article{Su_2014, title={A Recursive Dialogue Game for Personalized Computer-Aided Pronunciation Training}, ISSN={2329-9304}, url={http://dx.doi.org/10.1109/TASLP.2014.2375572}, DOI={10.1109/taslp.2014.2375572}, journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, publisher={Institute of Electrical and Electronics Engineers (IEEE)}, author={Su, Pei-Hao and Wu, Chuan-Hsun and Lee, Lin-Shan}, year={2014}, pages={1–1}}
"
"Robot self-preservation and adaptation to user preferences in game play, a preliminary study","It is expected that in a near future, personal robots will be endowed with enough autonomy to function and live in an individual's home. This is while commercial robots are designed with default configuration and factory settings which may often be different to an individual's operating preferences. This paper presents how reinforcement learning is applied and utilised towards personalisation of a robot's behaviour. Two-level reinforcement learning has been implemented: first level is in charge of energy autonomy, i.e. how to survive, and second level is involved in adapting robot's behaviour to user's preferences. In both levels Q-learning algorithm has been applied. First level actions have been learnt in a simulated environment and then the results have been transferred to the real robot. Second level has been fully implemented in the real robot and learnt by human-robot interaction. Finally, experiments showing the performance of the system are presented.",,Other,Robotics,y,1,not used,online,n,n,y,n,n,y,n,n,n,y,n,n,n,y,n,y,Q-Learning,2011 IEEE International Conference on Robotics and Biomimetics,Á. Castro-González; F. Amirabdollahian; D. Polani; M. Malfaz; M. A. Salichs,,2.0,10.1109/ROBIO.2011.6181679,IEEE Conferences,2011-01-01,Castro_Gonzalez_2011," @article{Castro_Gonzalez_2011, title={Robot self-preservation and adaptation to user preferences in game play, a preliminary study}, ISBN={9781457721373}, url={http://dx.doi.org/10.1109/ROBIO.2011.6181679}, DOI={10.1109/robio.2011.6181679}, journal={2011 IEEE International Conference on Robotics and Biomimetics}, publisher={IEEE}, author={Castro-Gonzalez, Alvaro and Amirabdollahian, Farshid and Polani, Daniel and Malfaz, Maria and Salichs, Miguel A.}, year={2011}, month={Dec}}
"
Incorporating prior knowledge into Q-learning for drug delivery individualization,"Individualization of drug delivery in treatment of chronic ailments is a challenge to the physician. Variability of response across patient population requires tailoring the dosing strategies to individual's needs. We have previously demonstrated the potential of reinforcement learning methods to support the physician in the management of anemia. In this paper, we propose the incorporation of prior knowledge into the learning mechanism to further improve the outcomes of the treatment.",,Health,,,1,state representation,online,y,n,n,y,n,n,n,y,n,y,y,y,n,n,n,n,Q-Learning,Fourth International Conference on Machine Learning and Applications (ICMLA'05),A. E. Gaweda; M. K. Muezzinoglu; G. R. Aronoff; A. A. Jacobs; J. M. Zurada; M. E. Brier,,2.0,10.1109/ICMLA.2005.40,IEEE Conferences,2005-01-01,Gaweda," @article{Gaweda, title={Incorporating Prior Knowledge into Q-Learning for Drug Delivery Individualization}, ISBN={0769524958}, url={http://dx.doi.org/10.1109/ICMLA.2005.40}, DOI={10.1109/icmla.2005.40}, journal={Fourth International Conference on Machine Learning and Applications (ICMLA’05)}, publisher={IEEE}, author={Gaweda, A.E. and Muezzinoglu, M.K. and Aronoff, G.R. and Jacobs, A.A. and Zurada, J.M. and Brier, M.E.}}
"
Personalized Course Sequence Recommendations,"Given the variability in student learning, it is becoming increasingly important to tailor courses as well as course sequences to student needs. This paper presents a systematic methodology for offering personalized course sequence recommendations to students. First, a forward-search backward-induction algorithm is developed that can optimally select course sequences to decrease the time required for a student to graduate. The algorithm accounts for prerequisite requirements (typically present in higher level education) and course availability. Second, using the tools of multiarmed bandits, an algorithm is developed that can optimally recommend a course sequence that both reduces the time to graduate while also increasing the overall GPA of the student. The algorithm dynamically learns how students with different contextual backgrounds perform for given course sequences and, then, recommends an optimal course sequence for new students. Using real-world student data from the UCLA Mechanical and Aerospace Engineering Department, we illustrate how the proposed algorithms outperform other methods that do not include student contextual information when making course sequence recommendations.",Personalized education;contextual bandits;course sequence recommendation;dynamic programming,Health,,IEEE Xplore,1/person,not used,n,n,n,n,n,n,n,n,n,n,y,n,n,n,y,n,n,"RL, not further specified",IEEE Transactions on Signal Processing,J. Xu; T. Xing; M. van der Schaar,,2.0,10.1109/TSP.2016.2595495,IEEE Journals & Magazines,2016-01-01,Xu_2016," @article{Xu_2016, title={Personalized Course Sequence Recommendations}, volume={64}, ISSN={1941-0476}, url={http://dx.doi.org/10.1109/TSP.2016.2595495}, DOI={10.1109/tsp.2016.2595495}, number={20}, journal={IEEE Transactions on Signal Processing}, publisher={Institute of Electrical and Electronics Engineers (IEEE)}, author={Xu, Jie and Xing, Tianwei and van der Schaar, Mihaela}, year={2016}, month={Oct}, pages={5340–5352}}
"
An Effective Approach to Continuous User Authentication for Touch Screen Smart Devices,"Due to the rapid increase in the use of personal smart devices, more sensitive data is stored and viewed on these smart devices. This trend makes it easier for attackers to access confidential data by physically compromising (including stealing) these smart devices. Currently, most personal smart devices employ one of the one-time user authentication schemes, such as four-to-six digits, fingerprint or pattern-based schemes. These authentication schemes are often not good enough for securing personal smart devices because the attackers can easily extract all the confidential data from the smart device by breaking such schemes, or by keeping the authenticated session open on a physically compromised smart device. In addition, existing re-authentication or continuous authentication techniques for protecting personal smart devices use centralized architecture and require servers at a centralized location to train and update the learning model used for continuous authentication, which impose additional communication overhead. In this paper, an approach is presented to generating and updating the authentication model on the user's smart device with user's gestures, instead of a centralized server. There are two major advantages in this approach. One is that this approach continuously learns and authenticates finger gestures of the user in the background without requiring the user to provide specific gesture inputs. The other major advantage is to have better authentication accuracy by treating uninterrupted user finger gestures over a short time interval as a single gesture for continuous user authentication.",Touch-screen smart devices;adaptive continuous user authentication;and user re-authentication;reinforcement learning,Other,Security,IEEE Xplore,1/person,not used,online,y,n,n,y,n,n,n,n,n,n,y,n,n,y,n,y,"Value Iteration,Policy Iteration","2015 IEEE International Conference on Software Quality, Reliability and Security",A. B. Buduru; S. S. Yau,,2.0,10.1109/QRS.2015.40,IEEE Conferences,2015-01-01,Buduru_2015," @article{Buduru_2015, title={An Effective Approach to Continuous User Authentication for Touch Screen Smart Devices}, ISBN={9781467379892}, url={http://dx.doi.org/10.1109/QRS.2015.40}, DOI={10.1109/qrs.2015.40}, journal={2015 IEEE International Conference on Software Quality, Reliability and Security}, publisher={IEEE}, author={Buduru, Arun Balaji and Yau, Stephen S.}, year={2015}, month={Aug}}
"
Using Contextual Learning to Improve Diagnostic Accuracy: Application in Breast Cancer Screening,"Clinicians need to routinely make management decisions about patients who are at risk for a disease such as breast cancer. This paper presents a novel clinical decision support tool that is capable of helping physicians make diagnostic decisions. We apply this support system to improve the specificity of breast cancer screening and diagnosis. The system utilizes clinical context (e.g., demographics, medical history) to minimize the false positive rates while avoiding false negatives. An online contextual learning algorithm is used to update the diagnostic strategy presented to the physicians over time. We analytically evaluate the diagnostic performance loss of the proposed algorithm, in which the true patient distribution is not known and needs to be learned, as compared with the optimal strategy where all information is assumed known, and prove that the false positive rate of the proposed learning algorithm asymptotically converges to the optimum. In addition, our algorithm also has the important merit that it can provide individualized confidence estimates about the accuracy of the diagnosis recommendation. Moreover, the relevancy of contextual features is assessed, enabling the approach to identify specific contextual features that provide the most value of information in reducing diagnostic errors. Experiments were conducted using patient data collected at a large academic medical center. Our proposed approach outperforms the current clinical practice by 36% in terms of false positive rate given a 2% false negative rate.",Breast cancer;computer-aided diagnosis system;contextual learning;multiarmed bandit (MAB);online learning,Health,,IEEE Xplore,1/person,state representation,batch,n,y,n,n,y,n,y,y,y,y,n,n,y,n,n,y,"Diagnostic Recommendation Algorithm DRA (proposed),Neural−fuzzy approach,Clinical approach LDA",IEEE Journal of Biomedical and Health Informatics,L. Song; W. Hsu; J. Xu; M. van der Schaar,,2.0,10.1109/JBHI.2015.2414934,IEEE Journals & Magazines,2016-01-01,Song_2016," @article{Song_2016, title={Using Contextual Learning to Improve Diagnostic Accuracy: Application in Breast Cancer Screening}, volume={20}, ISSN={2168-2208}, url={http://dx.doi.org/10.1109/JBHI.2015.2414934}, DOI={10.1109/jbhi.2015.2414934}, number={3}, journal={IEEE Journal of Biomedical and Health Informatics}, publisher={Institute of Electrical and Electronics Engineers (IEEE)}, author={Song, Linqi and Hsu, William and Xu, Jie and van der Schaar, Mihaela}, year={2016}, month={May}, pages={902–914}}
"
Greedy outcome weighted tree learning of optimal personalized treatment rules,"We propose a subgroup identification approach for inferring optimal and interpretable personalized treatment rules with high-dimensional covariates. Our approach is based on a two-step greedy tree algorithm to pursue signals in a high-dimensional space. In the first step, we transform the treatment selection problem into a weighted classification problem that can utilize tree-based methods. In the second step, we adopt a newly proposed tree-based method, known as reinforcement learning trees, to detect features involved in the optimal treatment rules and to construct binary splitting rules. The method is further extended to right censored survival data by using the accelerated failure time model and introducing double weighting to the classification trees. The performance of the proposed method is demonstrated via simulation studies, as well as analyses of the Cancer Cell Line Encyclopedia (CCLE) data and the Tamoxifen breast cancer data. © 2016, The International Biometric Society",,Health,,,1,state representation,batch,y,y,n,y,y,n,n,y,n,y,n,n,n,y,n,y,"Greedy outcome weighted tree,Virtual Twins (VT),MIDAs,LASSO,optsel",Biometrics,"Zhu R., Zhao Y.-Q., Chen G., Ma S., Zhao H.","Zhu, R., University of Illinois at Urbana-Champaign, Champaign, IL, United States; Zhao, Y.-Q., Fred Hutchinson Cancer Research Center, Seattle, WA, United States; Chen, G., Vanderbilt University, Nashville, TN, United States; Ma, S., Yale University, New Haven, CT, United States; Zhao, H., Yale University, New Haven, CT, United States",1.0,10.1111/biom.12593,Article,2017-01-01,Zhu_2016," @article{Zhu_2016, title={Greedy outcome weighted tree learning of optimal personalized treatment rules}, volume={73}, ISSN={0006-341X}, url={http://dx.doi.org/10.1111/biom.12593}, DOI={10.1111/biom.12593}, number={2}, journal={Biometrics}, publisher={Wiley}, author={Zhu, Ruoqing and Zhao, Ying-Qi and Chen, Guanhua and Ma, Shuangge and Zhao, Hongyu}, year={2016}, month={Oct}, pages={391–400}}
"
Improving management of Anemia in End Stage Renal Disease using Reinforcement Learning,We present a reinforcement learning approach to elicit individualized dose adjustment policies for patients suffering anemia due to end stage renal disease. Our goal is to achieve stable steady-state anemia management in patients with exhibiting different levels of treatment response. The approach uses Q-learning with parsimonious parametric representation of the state-action value function. We show that this approach achieves stability even in highly responsive patients.,,Health,,,1/person,not used,online,y,n,n,y,n,n,y,n,n,y,n,y,n,y,n,y,Q-learning with continuous state space; fixed policy,2009 International Joint Conference on Neural Networks,A. E. Gaweda,,1.0,10.1109/IJCNN.2009.5179004,IEEE Conferences,2009-01-01,Gaweda_2009," @article{Gaweda_2009, title={Improving management of Anemia in End Stage Renal Disease using Reinforcement Learning}, ISBN={9781424435487}, url={http://dx.doi.org/10.1109/IJCNN.2009.5179004}, DOI={10.1109/ijcnn.2009.5179004}, journal={2009 International Joint Conference on Neural Networks}, publisher={IEEE}, author={Gaweda, Adam E.}, year={2009}, month={Jun}}
"
"Towards efficient, personalized anesthesia using continuous reinforcement learning for propofol infusion control","We demonstrate the use of reinforcement learning algorithms for efficient and personalized control of patients' depth of general anesthesia during surgical procedures - an important aspect for Neurotechnology. We used the continuous actor-critic learning automaton technique, which was trained and tested in silico using published patient data, physiological simulation and the bispectral index (BIS) of patient EEG. Our two-stage technique learns first a generic effective control strategy based on average patient data (factory stage) and can then fine-tune itself to individual patients (personalization stage). The results showed that the reinforcement learner as compared to a bang-bang controller reduced the dose of the anesthetic agent administered by 9.4% and kept the patient closer to the target state, as measured by RMSE (4.90 compared to 8.47). It also kept the BIS error within a narrow, clinically acceptable range 93.9% of the time. Moreover, the policy was trained using only 50 simulated operations. Being able to learn a control strategy this quickly indicates that the reinforcement learner could also adapt regularly to a patient's changing responses throughout a live operation and facilitate the task of anesthesiologists by prompting them with recommended actions.",,Health,,IEEE Explore,1/person,state representation,batch,y,n,n,y,n,n,n,n,n,y,y,n,n,n,n,y,Actor-Critic,2013 6th International IEEE/EMBS Conference on Neural Engineering (NER),C. Lowery; A. A. Faisal,,1.0,10.1109/NER.2013.6696208,IEEE Conferences,2013-01-01,Lowery_2013," @article{Lowery_2013, title={Towards efficient, personalized anesthesia using continuous reinforcement learning for propofol infusion control}, ISBN={9781467319690}, url={http://dx.doi.org/10.1109/NER.2013.6696208}, DOI={10.1109/ner.2013.6696208}, journal={2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)}, publisher={IEEE}, author={Lowery, Cristobal and Faisal, A. Aldo}, year={2013}, month={Nov}}
"
Learning capabilities for improving automatic transmission control,"We analyzed the gear-box position selection (GPS) problem on automatic transmission (AT) and proposed an algorithm, based on learning control, to improve vehicle behavior and driver satisfaction. Our approach guarantees optimization of vehicle performance and adaptation to the driver's style with road condition sensitivity. This improvement has been achieved by combining three knowledge acquisition sources: embedded dynamic models of powertrain, inductive inspection of driver actions and AT designer expertise; and by adding learning capabilities in order to significantly increase the system autonomy. Technically, GPS raises the following four problems which this paper addresses: (1) To achieve vehicle performance optimization of multiple antagonistic criteria, locally and globally over time, we considered a parametric disciminant function depending on an evaluation of the driver satisfaction and so called driver-style-state functions, as a reward for the system, and applied a reinforcement learning algorithm, derived from Q-learning method and combined with a mechanism to escape local optima. (2) Learning directly from the driver is performed when he selects AT ratio in manual mode. (3) Each driver's personal style is represented by a Glass creation/selection mechanism. (4) GPS raises a few singularities which are addressed by a set of restriction rules derived from AT control expertise.",,Transport,,IEEE Xplore,1/person,state representation,online,y,n,n,y,n,n,n,n,y,y,y,n,n,y,n,n,Q-Learning,"Intelligent Vehicles '94 Symposium, Proceedings of the",L. Fournier,,1.0,10.1109/IVS.1994.639561,IEEE Conferences,1994-01-01,Fournier," @article{Fournier, title={Learning capabilities for improving automatic transmission control}, ISBN={0780321359}, url={http://dx.doi.org/10.1109/IVS.1994.639561}, DOI={10.1109/ivs.1994.639561}, journal={Proceedings of the Intelligent Vehicles  ’94 Symposium}, publisher={IEEE}, author={Fournier, L.}}
"
Reinforcement learning approach towards effective content recommendation in MOOC environments,"Understanding the Learner requirements is an important aspect of any learning environment as it helps to recommend the LOs in a more personalized manner. With the growing demand for MOOCs offered by coursera, edx, etc. the learner information plays a vital role in understanding the extent to which the learners can gain out of such courses. The Learning Management Systems (LMS) across the web uses the explicit (rating, performance, etc.) and implicit feedback (LOs used) obtained through interaction with the learners to derive such information. As the requirements of the learners varies with the individual's interest and learning background, a common approach for recommending LOs may not cater the needs of all the learners. To overcome this issue, this paper proposes reinforcement learning based algorithm to analyze the learner information (derived from both implicit and explicit feedback) and generate the knowledge on the learner's requirements and capabilities inside a specific learning context. The reinforcement learning system (RILS) implemented as a part of this work utilizes the knowledge thus generated in order to recommend the appropriate LOs for the learners. The results have highlighted that the knowledge derived from the learning information analysis proved to effective in generating personalized recommendation policies that can cater the context specific requirements of the learners.",LO recommendation;MOOC;Reinforcement Learning;learning context;learning experience,Education,,,1/person,other,online,n,n,y,n,n,y,n,n,y,y,n,n,y,y,n,y,CRBL,"2014 IEEE International Conference on MOOC, Innovation and Technology in Education (MITE)",V. R. Raghuveer; B. K. Tripathy; T. Singh; S. Khanna,,1.0,10.1109/MITE.2014.7020289,IEEE Conferences,2014-01-01,Raghuveer_2014," @article{Raghuveer_2014, title={Reinforcement learning approach towards effective content recommendation in MOOC environments}, ISBN={9781479968763}, url={http://dx.doi.org/10.1109/MITE.2014.7020289}, DOI={10.1109/mite.2014.7020289}, journal={2014 IEEE International Conference on MOOC, Innovation and Technology in Education (MITE)}, publisher={IEEE}, author={Raghuveer, V. R. and Tripathy, B. K. and Singh, Taranveer and Khanna, Saarthak}, year={2014}, month={Dec}}
"
Autonomous task sequencing for customized curriculum design in reinforcement learning,"Transfer learning is a method where an agent reuses knowledge learned in a source task to improve learning on a target task. Recent work has shown that transfer learning can be extended to the idea of curriculum learning, where the agent incrementally accumulates knowledge over a sequence of tasks (i.e. a curriculum). In most existing work, such curricula have been constructed manually. Furthermore, they are fixed ahead of time, and do not adapt to the progress or abilities of the agent. In this paper, we formulate the design of a curriculum as a Markov Decision Process, which directly models the accumulation of knowledge as an agent interacts with tasks, and propose a method that approximates an execution of an optimal policy in this MDP to produce an agent-specific curriculum. We use our approach to automatically sequence tasks for 3 agents with varying sensing and action capabilities in an experimental domain, and show that our method produces curricula customized for each agent that improve performance relative to learning from scratch or using a different agent's curriculum.",,Domain Independent,,SCOPUS,1/person,state representation,other,y,n,n,y,n,n,y,n,n,y,n,n,n,n,n,y,SARSA,IJCAI International Joint Conference on Artificial Intelligence,"Narvekar S., Sinapov J., Stone P.","Narvekar, S., Department of Computer Science, University of Texas, Austin, United States; Sinapov, J., Department of Computer Science, University of Texas, Austin, United States; Stone, P., Department of Computer Science, University of Texas, Austin, United States",1.0,,Conference Paper,2017-01-01,Narvekar2017AutonomousTS,"@inproceedings{Narvekar2017AutonomousTS,
  title={Autonomous Task Sequencing for Customized Curriculum Design in Reinforcement Learning},
  author={Sanmit Narvekar and Jivko Sinapov and Peter Stone},
  booktitle={IJCAI},
  year={2017}
}"
Angle of arrival estimation in dynamic indoor THz channels with Bayesian filter and reinforcement learning,"This paper presents a novel algorithm to estimate the Angle of Arrival (AoA) in a dynamic indoor Terahertz channel. In a realistic application, the user equipment is often moved by the user during the data transmission and the AoA must be estimated periodically, such that the adaptive directional antenna can be adjusted to realize a high antenna gain. The Bayesian filter is applied to exploit continuity and smoothness of the channel dynamics for the AoA estimation. Reinforcement learning is introduced to adapt the prior transition probabilities between system states, in order to fit the variation of application scenarios and personal habits. The algorithm is validated using the ray launching channel simulator and realistic human movement models.",Bayesian filter;Terahertz communication;angle of arrival estimation;dynamic channel;reinforcement learning,Energy,,IEEE Xplore,1/person,other,batch,y,n,n,y,n,n,n,y,y,n,n,y,n,n,n,y,"RL, not further specified",2016 24th European Signal Processing Conference (EUSIPCO),B. Peng; Q. Jiao; T. Kürner,,1.0,10.1109/EUSIPCO.2016.7760594,IEEE Conferences,2016-01-01,Peng_2016," @article{Peng_2016, title={Angle of arrival estimation in dynamic indoor THz channels with Bayesian filter and reinforcement learning}, ISBN={9780992862657}, url={http://dx.doi.org/10.1109/EUSIPCO.2016.7760594}, DOI={10.1109/eusipco.2016.7760594}, journal={2016 24th European Signal Processing Conference (EUSIPCO)}, publisher={IEEE}, author={Peng, Bile and Jiao, Qi and Kurner, Thomas}, year={2016}, month={Aug}}
"
GongBroker: A Broker Model for Power Trading in Smart Grid Markets,"The Smart Grid market is dynamic and complex, and brokers are widely introduced to better manage this market. This paper proposes an intelligent broker model - GongBroker, with smart trading strategies to cope with the dynamics and complexity. GongBroker first predicts the short-term demands of various consumers, and then buys energy from the wholesale market through auctions, and sells energy to various consumers in the retail market. In order to predict the customer demand, a data-driven method is proposed. All the consumers are hierarchically clustered according to their historical energy consumptions, and different prediction methods are tailored for different customer clusters to predict one-day-ahead hourly energy demand. Based on the predicted demand, the GongBroker employs a Markov Decision Process for the one-day-ahead auction in the wholesale market. To compete with other brokers, GongBroker uses independent reinforcement learning processes to optimize prices for different types of consumers. Experimental results demonstrate that GongBroker not only is competitive in making profit, but also keeps a good supply-demand balance.",Broker Model;Data-driven;Reinforcement Learning;Smart Grid Market,Commerce,,IEEE Xplore,1/group,state representation,other,y,n,n,y,n,n,n,n,n,y,n,n,n,n,n,n,SARSA,2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT),X. Wang; M. Zhang; F. Ren; T. Ito,,1.0,10.1109/WI-IAT.2015.108,IEEE Conferences,2015-01-01,Wang_2015," @article{Wang_2015, title={GongBroker: A Broker Model for Power Trading in Smart Grid Markets}, ISBN={9781467396189}, url={http://dx.doi.org/10.1109/WI-IAT.2015.108}, DOI={10.1109/wi-iat.2015.108}, journal={2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)}, publisher={IEEE}, author={Wang, Xishun and Zhang, Minjie and Ren, Fenghui and Ito, Takayuki}, year={2015}, month={Dec}}
"
Active learning for personalizing treatment,"The personalization of treatment via genetic biomarkers and other risk categories has drawn increasing interest among clinical researchers and scientists. A major challenge here is to construct individualized treatment rules (ITR), which recommend the best treatment for each of the different categories of individuals. In general, ITRs can be constructed using data from clinical trials, however these are generally very costly to run. In order to reduce the cost of learning an ITR, we explore active learning techniques designed to carefully decide whom to recruit, and which treatment to assign, throughout the online conduct of the clinical trial. As an initial investigation, we focus on simple ITRs that utilize a small number of subpopulation categories to personalize treatment. To minimize the maximal uncertainty regarding the treatment effects for each subpopulation, we propose the use of a minimax bandit model and provide an active learning policy for solving it. We evaluate our active learning policy using simulated data and data modeled after a clinical trial involving treatments for depressed individuals. We contrast this policy with other plausible active learning policies. The techniques presented in the paper may be generalized to tackle problems of efficient exploration in other domains.",,Health,,IEEE Explore,1,state representation,online,y,y,n,y,y,n,y,y,n,y,n,y,n,n,n,y,AREOA,2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL),K. Deng; J. Pineau; S. Murphy,,1.0,10.1109/ADPRL.2011.5967348,IEEE Conferences,2011-01-01,Deng_2011," @article{Deng_2011, title={Active learning for personalizing treatment}, ISBN={9781424498871}, url={http://dx.doi.org/10.1109/ADPRL.2011.5967348}, DOI={10.1109/adprl.2011.5967348}, journal={2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)}, publisher={IEEE}, author={Deng, Kun and Pineau, Joelle and Murphy, Susan}, year={2011}, month={Apr}}
"
Locality-sensitive linear bandit model for online social recommendation,"Recommender systems provide personalized suggestions by learning users’ preference based on their historical feedback. To alleviate the heavy relying on historical data, several online recommendation methods are recently proposed and have shown the effectiveness in solving data sparsity and cold start problems in recommender systems. However, existing online recommendation methods neglect the use of social connections among users, which has been proven as an effective way to improve recommendation accuracy in offline settings. In this paper, we investigate how to leverage social connections to improve online recommendation performance. In particular, we formulate the online social recommendation task as a contextual bandit problem and propose a Locality-sensitive Linear Bandit (LS.Lin) method to solve it. The proposed model incorporates users’ local social relations into a linear contextual bandit model and is capable to deal with the dynamic changes of user preference and the network structure. We provide a theoretical analysis to the proposed LS.Lin method and then demonstrate its improved performance for online social recommendation in empirical studies compared with baseline methods. © Springer International Publishing AG 2016.",Linear bandits; Online learning; Social recommendation,Commerce,,,1/person,state representation,online,n,y,n,n,y,n,y,y,y,n,n,n,y,y,n,y,"Random,NetBandits,GOB.lin,and their own flavor (LS.lin)",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"Zhao T., King I.","Zhao, T., Shenzhen Key Laboratory of Rich Media Big Data Analytics and Applications, Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China, Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong; King, I., Shenzhen Key Laboratory of Rich Media Big Data Analytics and Applications, Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China, Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong",1.0,10.1007/978-3-319-46687-3_9,Conference Paper,2016-01-01,Zhao_2016," @article{Zhao_2016, title={Locality-Sensitive Linear Bandit Model for Online Social Recommendation}, ISBN={9783319466873}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-319-46687-3_9}, DOI={10.1007/978-3-319-46687-3_9}, journal={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Zhao, Tong and King, Irwin}, year={2016}, pages={80–90}}
"
Decomposing drama management in educational interactive narrative: A modular reinforcement learning approach,"Recent years have seen growing interest in data-driven approaches to personalized interactive narrative generation and drama management. Reinforcement learning (RL) shows particular promise for training policies to dynamically shape interactive narratives based on corpora of player-interaction data. An important open question is how to design reinforcement learning-based drama managers in order to make effective use of player interaction data, which is often expensive to gather and sparse relative to the vast state and action spaces required by drama management. We investigate an offline optimization framework for training modular reinforcement learning-based drama managers in an educational interactive narrative, CRYSTAL ISLAND. We leverage importance sampling to evaluate drama manager policies derived from different decompositional representations of the interactive narrative. Empirical results show significant improvements in drama manager quality from adopting an optimized modular RL decomposition compared to competing representations. © Springer International Publishing AG 2016.",Drama management; Educational interactive narrative; Intelligent narrative technologies; Modular reinforcement learning,Education,,SCOPUS,1,state representation,batch,n,y,n,n,y,n,y,n,n,y,n,n,y,y,n,n,"Q-Learning,Policy Iteration",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"Wang P., Rowe J., Mott B., Lester J.","Wang, P., North Carolina State University, Raleigh, NC, United States; Rowe, J., North Carolina State University, Raleigh, NC, United States; Mott, B., North Carolina State University, Raleigh, NC, United States; Lester, J., North Carolina State University, Raleigh, NC, United States",1.0,10.1007/978-3-319-48279-8_24,Conference Paper,2016-01-01,Wang_2016," @article{Wang_2016, title={Decomposing Drama Management in Educational Interactive Narrative: A Modular Reinforcement Learning Approach}, ISBN={9783319482798}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-319-48279-8_24}, DOI={10.1007/978-3-319-48279-8_24}, journal={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Wang, Pengcheng and Rowe, Jonathan and Mott, Bradford and Lester, James}, year={2016}, pages={270–282}}
"
Tools for the Precision Medicine Era: How to Develop Highly Personalized Treatment Recommendations from Cohort and Registry Data Using Q-Learning,"Q-Learning is a method of reinforcement learning that employs backwards stagewise estimation to identify sequences of actions that maximize some long-term reward. The method can be applied to sequential multipleassignment randomized trials to develop personalized adaptive treatment strategies (ATSs)-longitudinal practice guidelines highly tailored to time-varying attributes of individual patients. Sometimes, the basis for choosing which ATSs to include in a sequential multiple-assignment randomized trial (or randomized controlled trial) may be inadequate. Nonrandomized data sources may inform the initial design of ATSs, which could later be prospectively validated. In this paper, we illustrate challenges involved in using nonrandomized data for this purpose with a case study from the Center for International Blood and Marrow Transplant Research registry (1995-2007) aimed at 1) determining whether the sequence of therapeutic classes used in graft-versus-host disease prophylaxis and in refractory graft-versus-host disease is associated with improved survival and 2) identifying donor and patient factors with which to guide individualized immunosuppressant selections over time. We discuss how to communicate the potential benefit derived from following an ATS at the population and subgroup levels and how to evaluate its robustness to modeling assumptions. This worked example may serve as a model for developing ATSs from registries and cohorts in oncology and other fields requiring sequential treatment decisions. © 2017 The Author(s).",Adaptive treatment strategies; Dynamic treatment regimes; Graft-versus-host disease; Machine learning; Personalized medicine; Prediction; Q-learning; Registry data,Health,,SCOPUS,1,state representation,batch,n,y,n,n,y,n,y,n,n,y,y,y,n,n,n,y,Q-Learning,American Journal of Epidemiology,"Krakow E.F., Hemmer M., Wang T., Logan B., Arora M., Spellman S., Couriel D., Alousi A., Pidala J., Last M., Lachance S., Moodie E.E.M.","Krakow, E.F., Department of Epidemiology Biostatistics and Occupational Health, Faculty of Medicine, McGill University, 1020 Pine Avenue West, Montreal, QC, Canada, Division of Hematology and Oncology, Department of Medicine, Hopital MaisonneuveRosemont Research Center, University of Montreal, Montreal, QC, Canada; Hemmer, M., Department of Epidemiology Biostatistics and Occupational Health, Faculty of Medicine, McGill University, 1020 Pine Avenue West, Montreal, QC, Canada; Wang, T., Center for International Blood and Marrow Transplant Research, Department of Medicine, Medical College of Wisconsin, Milwaukee, WI, United States, Division of Biostatistics, Institute for Health and Society, Medical College of Wisconsin, Milwaukee, WI, United States; Logan, B., Center for International Blood and Marrow Transplant Research, Department of Medicine, Medical College of Wisconsin, Milwaukee, WI, United States, Division of Biostatistics, Institute for Health and Society, Medical College of Wisconsin, Milwaukee, WI, United States; Arora, M., Division of Hematology, Oncology, and Transplantation, Department of Medicine, University of Minnesota Medical Center, Minneapolis, MN, United States; Spellman, S., Center for International Blood and Marrow Transplant Research, National Marrow Donor Program/Be the Match, Minneapolis, MN, United States; Couriel, D., Division of Hematology/BMT, Department of Internal Medicine, University of Utah Health Care and Huntsman Cancer Center, University of Utah, Salt Lake City, UT, United States; Alousi, A., Division of Cancer Medicine, Department of Stem Cell Transplantation, University of Texas M.D. Anderson Cancer Center, Houston, TX, United States; Pidala, J., Department of Blood and Marrow Transplantation, Moffitt Cancer Center, Tampa, FL, United States; Last, M., Center for International Blood and Marrow Transplant Research, Department of Medicine, Medical College of Wisconsin, Milwaukee, WI, United States, US Department of Defense, Fort Meade, MD, United States; Lachance, S., Division of Hematology and Oncology, Department of Medicine, Hopital MaisonneuveRosemont Research Center, University of Montreal, Montreal, QC, Canada; Moodie, E.E.M., Department of Epidemiology Biostatistics and Occupational Health, Faculty of Medicine, McGill University, 1020 Pine Avenue West, Montreal, QC, Canada",1.0,10.1093/aje/kwx027,Article,2017-01-01,Krakow_2017," @article{Krakow_2017, title={Tools for the Precision Medicine Era: How to Develop Highly Personalized
          Treatment Recommendations From Cohort and Registry Data Using Q-Learning}, volume={186}, ISSN={1476-6256}, url={http://dx.doi.org/10.1093/aje/kwx027}, DOI={10.1093/aje/kwx027}, number={2}, journal={American Journal of Epidemiology}, publisher={Oxford University Press (OUP)}, author={Krakow, Elizabeth F and Hemmer, Michael and Wang, Tao and Logan, Brent and Arora, Mukta and Spellman, Stephen and Couriel, Daniel and Alousi, Amin and Pidala, Joseph and Last, Michael and et al.}, year={2017}, month={Jun}, pages={160–172}}
"
Vito – a generic agent for multi-physics model personalization: Application to heart modeling,"Precise estimation of computational physiological model parameters from patient data is one of the main hurdles towards their clinical applicability. Designing robust estimation algorithms is often a tedious and model-specific process. We propose to use, for the first time to our knowledge, artificial intelligence (AI) concepts to learn how to personalize a computational model, inspired by how an expert manually personalizes. We reformulate the parameter estimation problem in terms of Markov decision process and reinforcement learning. In an off-line phase, the artificial agent, called Vito, automatically learns a representative state-action-state model through data-driven exploration of the computational model under consideration. In other words, Vito learns how the model behaves under change of parameters and how to personalize it. Vito then controls the on-line personalization by exploiting its automatically derived action policy. Because the algorithm is model-independent, personalizing a completely new model would require only adjusting some simple parameters of the agent and defining the observations to match, without the full knowledge of the model itself. Vito was evaluated on two challenging problems: the inverse problem of cardiac electrophysiology and the personalization of a lumped-parameter whole-body circulation model. Obtained results suggested that Vito could achieve equivalent goodness of fit than standard methods, while being more robust (up to 25% higher success rates) and with faster (up to three times) convergence rate. Our AI approach could thus make model personalization algorithms generalizable and self-adaptable to any patient, like a human operator. © Springer International Publishing Switzerland 2015.",,Health,,SCOPUS,1/person,state representation,online,n,y,n,n,y,n,n,y,y,n,n,n,y,n,n,y,Dynamic programming,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"Neumann D., Mansi T., Itu L., Georgescu B., Kayvanpour E., Sedaghat-Hamedani F., Haas J., Katus H., Meder B., Steidl S., Hornegger J., Comaniciu D.","Neumann, D., Imaging and Computer Vision, Siemens Corporate Technology, Princeton, NJ, Romania, Pattern Recognition Lab, FAU Erlangen-Nürnberg, Germany; Mansi, T., Imaging and Computer Vision, Siemens Corporate Technology, Princeton, NJ, Romania; Itu, L., Imaging and Computer Vision, Siemens Corporate Technology, Romania; Georgescu, B., Imaging and Computer Vision, Siemens Corporate Technology, Princeton, NJ, Romania; Kayvanpour, E., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Sedaghat-Hamedani, F., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Haas, J., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Katus, H., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Meder, B., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Steidl, S., Pattern Recognition Lab, FAU Erlangen-Nürnberg, Germany; Hornegger, J., Pattern Recognition Lab, FAU Erlangen-Nürnberg, Germany; Comaniciu, D., Imaging and Computer Vision, Siemens Corporate Technology, Princeton, NJ, Romania",1.0,10.1007/978-3-319-24571-3_53,Conference Paper,2015-01-01,Neumann_2015," @article{Neumann_2015, title={Vito – A Generic Agent for Multi-physics Model Personalization: Application to Heart Modeling}, ISBN={9783319245713}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-319-24571-3_53}, DOI={10.1007/978-3-319-24571-3_53}, journal={Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015}, publisher={Springer International Publishing}, author={Neumann, Dominik and Mansi, Tommaso and Itu, Lucian and Georgescu, Bogdan and Kayvanpour, Elham and Sedaghat-Hamedani, Farbod and Haas, Jan and Katus, Hugo and Meder, Benjamin and Steidl, Stefan and et al.}, year={2015}, pages={442–449}}
"
Latent contextual bandits and their application to personalized recommendations for new users,"Personalized recommendations for new users, also known as the cold-start problem, can be formulated as a contextual bandit problem. Existing contextual bandit algorithms generally rely on features alone to capture user variability. Such methods are inefficient in learning new users' interests. In this paper we propose Latent Contextual Bandits. We consider both the benefit of leveraging a set of learned latent user classes for new users, and how we can learn such latent classes from prior users. We show that our approach achieves a better regret bound than existing algorithms. We also demonstrate the benefit of our approach using a large real world dataset and a preliminary user study.",,Entertainment,,SCOPUS,multiple,other,batch,y,y,n,y,y,y,n,n,n,y,n,n,y,y,y,n,"Contextual Bandits,LinUCB,CLUB",IJCAI International Joint Conference on Artificial Intelligence,"Zhou L., Brunskill E.","Zhou, L., Computer Science Department, Carnegie Mellon University, United States; Brunskill, E., Computer Science Department, Carnegie Mellon University, United States",1.0,,Conference Paper,2016-01-01,Zhou2016LatentCB,"@inproceedings{Zhou2016LatentCB,
  title={Latent Contextual Bandits and their Application to Personalized Recommendations for New Users},
  author={Li Zhou and Emma Brunskill},
  booktitle={IJCAI},
  year={2016}
}"
Trust and privacy correlations in social networks: A deep learning framework,"Online Social Networks (OSNs) remain the focal point of Internet usage. Since the beginning, networking sites tried best to have right privacy mechanisms in place for users, enabling them to share the right content with the right audience. With all these efforts, privacy customizations remain hard for users across the sites. Existing research that address this problem mainly focus on semi-supervised strategies that introduce extra complexity by requiring the user to manually specify initial privacy preferences for their friends. In this work, we suggest an adaptive solution that can dynamically generate privacy labels for users in OSNs. To this end, we introduce a deep reinforcement learning framework that targets two key problems in OSNs like Facebook: the exposure of users' interactions through the network to less trusted direct friends, and the possibility of propagating user updates through direct friends' interactions to indirect friends. By implementing this framework, we aim at understanding how social trust and privacy could be correlated, specifically in a dynamic fashion. We report the ranked dependence between the generated privacy labels and the estimated user trust values, which indicate the ability of the framework to identify the highly trusted users and share with them higher percentages of data. © 2016 IEEE.",,Entertainment,,SCOPUS,1,not used,batch,n,y,n,n,y,n,n,n,n,y,n,n,y,y,y,n,Fitted Q-Iteration,"Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2016","Jaradat S., Dokoohaki N., Matskin M., Ferrari E.","Jaradat, S., KTH, Royal Institute of Technology, Sweden; Dokoohaki, N., KTH, Royal Institute of Technology, Sweden; Matskin, M., KTH, Royal Institute of Technology, Sweden; Ferrari, E., University of Insubria, Italy",1.0,10.1109/ASONAM.2016.7752236,Conference Paper,2016-01-01,Jaradat_2016," @article{Jaradat_2016, title={Trust and privacy correlations in social networks: A deep learning framework}, ISBN={9781509028467}, url={http://dx.doi.org/10.1109/ASONAM.2016.7752236}, DOI={10.1109/asonam.2016.7752236}, journal={2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)}, publisher={IEEE}, author={Jaradat, Shatha and Dokoohaki, Nima and Matskin, Mihhail and Ferrari, Elena}, year={2016}, month={Aug}}
"
On personalizing Web content through reinforcement learning,"Nowadays, a large use of personalization techniques is used to adapt Web content to users’ habits, mainly with the aim of offering appropriate products and services. This paper presents a system that uses personalization and adaptation techniques, in order to transcode or modify contents (e.g., adapt text fonts) so as to meet preferences and needs of elderly users and users with disabilities. Such an adaptation can have a positive effect, in particular for users with some reading-related disabilities (i.e., people with dyslexia, users with low vision, users with color blindness, elderly people.). To avoid issues arising from applying transformations to the whole content, the proposed system uses Web intelligence to perform automatic adaptations on single elements composing a Web page. The transformation is applied on the basis of a reinforcement learning algorithm which manages a user profile. The system is evaluated through simulations and a real assessment, where elderly users where asked to use the system prototype for a time period and to perform some specific tasks. Results of the qualitative evaluation confirm the feasibility of the proposal, showing its validity and the users’ appreciation. © 2016, Springer-Verlag Berlin Heidelberg.",Content adaptation; Legibility; Reinforcement learning; User profiling; Web personalization,Entertainment,,SCOPUS,1/person,state representation,batch,y,n,y,y,n,y,n,n,y,y,n,y,y,y,n,y,Q-Learning,Universal Access in the Information Society,"Ferretti S., Mirri S., Prandi C., Salomoni P.","Ferretti, S., Department of Computer Science and Engineering, Università di Bologna, Via Mura Anteo Zamboni 7, Bologna, Italy; Mirri, S., Department of Computer Science and Engineering, Università di Bologna, Via Mura Anteo Zamboni 7, Bologna, Italy; Prandi, C., Department of Computer Science and Engineering, Università di Bologna, Via Mura Anteo Zamboni 7, Bologna, Italy; Salomoni, P., Department of Computer Science and Engineering, Università di Bologna, Via Mura Anteo Zamboni 7, Bologna, Italy",1.0,10.1007/s10209-016-0463-2,Article,2017-01-01,Ferretti_2016," @article{Ferretti_2016, title={On personalizing Web content through reinforcement learning}, volume={16}, ISSN={1615-5297}, url={http://dx.doi.org/10.1007/s10209-016-0463-2}, DOI={10.1007/s10209-016-0463-2}, number={2}, journal={Universal Access in the Information Society}, publisher={Springer Nature}, author={Ferretti, Stefano and Mirri, Silvia and Prandi, Catia and Salomoni, Paola}, year={2016}, month={Mar}, pages={395–410}}
"
Optimal medication dosing from suboptimal clinical examples: A deep reinforcement learning approach,"Misdosing medications with sensitive therapeutic windows, such as heparin, can place patients at unnecessary risk, increase length of hospital stay, and lead to wasted hospital resources. In this work, we present a clinician-in-the-loop sequential decision making framework, which provides an individualized dosing policy adapted to each patient's evolving clinical phenotype. We employed retrospective data from the publicly available MIMIC II intensive care unit database, and developed a deep reinforcement learning algorithm that learns an optimal heparin dosing policy from sample dosing trails and their associated outcomes in large electronic medical records. Using separate training and testing datasets, our model was observed to be effective in proposing heparin doses that resulted in better expected outcomes than the clinical guidelines. Our results demonstrate that a sequential modeling approach, learned from retrospective data, could potentially be used at the bedside to derive individualized patient dosing policies.",,Health,,IEEE Explore,1,state representation,batch,n,y,n,n,y,n,y,n,n,y,y,y,n,y,n,y,Deep Reinforcement Learning,2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),S. Nemati; M. M. Ghassemi; G. D. Clifford,,1.0,10.1109/EMBC.2016.7591355,IEEE Conferences,2016-01-01,Nemati_2016," @article{Nemati_2016, title={Optimal medication dosing from suboptimal clinical examples: A deep reinforcement learning approach}, ISBN={9781457702204}, url={http://dx.doi.org/10.1109/EMBC.2016.7591355}, DOI={10.1109/embc.2016.7591355}, journal={2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}, publisher={IEEE}, author={Nemati, Shamim and Ghassemi, Mohammad M. and Clifford, Gari D.}, year={2016}, month={Aug}}
"
A unified control framework of HVAC system for thermal and acoustic comforts in office building,"Intelligent building system attracts more and more attention in both academic and industrial communities. Learning human comfort requirements and incorporating it into building control system is one of the important issues. In the traditional HVAC control system, the thermal comfort and the acoustic comfort are often conflicted and we lack of a scheme to trade off them well. In this paper, we propose a unified control framework based on reinforcement learning to balance the multiple dimension comforts, including the thermal and acoustic comforts. We utilize the user's complaints in thermal and acoustic sensations as feedback and combine the current environment and devices information to learn the personalized optimal control policy using online Q-learning. The challenge caused by the complaints is coped with an incorporated perception estimation scheme in the Q-learning reward design. Both simulation results and the field experimental results demonstrate the effectiveness of the algorithm, especially in the adaptivity to the individual tradeoff between thermal and acoustic comfort.",,Energy,,IEEE Xplore,1/person,not used,online,y,n,y,y,n,y,n,n,n,y,n,n,n,y,n,y,Q-Learning,2013 IEEE International Conference on Automation Science and Engineering (CASE),Y. Zhao; Q. Zhao; L. Xia; Z. Cheng; F. Wang; F. Song,,1.0,10.1109/CoASE.2013.6653964,IEEE Conferences,2013-01-01,Zhao_2013," @article{Zhao_2013, title={A unified control framework of HVAC system for thermal and acoustic comforts in office building}, ISBN={9781479915156}, url={http://dx.doi.org/10.1109/CoASE.2013.6653964}, DOI={10.1109/coase.2013.6653964}, journal={2013 IEEE International Conference on Automation Science and Engineering (CASE)}, publisher={IEEE}, author={Zhao, Yin and Zhao, Qianchuan and Xia, Li and Cheng, Zhijin and Wang, Fulin and Song, Fangting}, year={2013}, month={Aug}}
"
An interactive learning and adaptation framework for adaptive robot assisted therapy,"In this paper, we present an interactive learning and adaptation framework. The framework combines Interactive Reinforcement Learning methods to effectively adapt and refine a learned policy to cope with new users. We argue that implicit feedback provided by the primary user and guidance from a secondary user can be integrated to the adaptation mechanism, resulting at a tailored and safe interaction. We illustrate this framework with a use case in Robot Assisted Therapy, presenting a Robot Yoga Trainer that monitors a yoga training session, adjusting the session parameters based on human motion activity recognition and evaluation through depth data, to assist the user complete the session, following a Reinforcement Learning approach. Copyright 2016 is held by the owner/author(s).",Adaptive Robot Assisted Therapy; Interactive Reinforcement Learning; Policy Adaptation,Health,,SCOPUS,1,state representation,unknown,n,n,n,n,n,n,n,n,y,y,y,n,y,n,y,y,Interactive Reinforcement Learning,ACM International Conference Proceeding Series,"Tsiakas K., Papakostas M., Chebaa B., Ebert D., Karkaletsis V., Makedon F.","Tsiakas, K., CSE Department, University of Texas at Arlington, United States; Papakostas, M., CSE Department, University of Texas at Arlington, United States; Chebaa, B., Department of Psychology, HERACLEIA Lab, University of Texas at Arlington, United States; Ebert, D., CSE Department, University of Texas at Arlington, United States; Karkaletsis, V., National Center for Scientific Research Demokritos, Athens, Greece; Makedon, F., CSE Department, University of Texas at Arlington, United States",1.0,10.1145/2910674.2935857,Conference Paper,2016-01-01,Tsiakas_2016_0," @article{Tsiakas_2016_0, title={An Interactive Learning and Adaptation Framework for Adaptive Robot Assisted Therapy}, ISBN={9781450343374}, url={http://dx.doi.org/10.1145/2910674.2935857}, DOI={10.1145/2910674.2935857}, journal={Proceedings of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments - PETRA  ’16}, publisher={ACM Press}, author={Tsiakas, Konstantinos and Papakostas, Michalis and Chebaa, Benjamin and Ebert, Dylan and Karkaletsis, Vangelis and Makedon, Fillia}, year={2016}}
"
Exploiting Reinforcement Learning to Profile Users and Personalize Web Pages,"In this paper, we present a Web content adaptation system that is able to automatically adapt textual elements of Web pages, based on the user profile and preferences. The system employs Web intelligence to perform these automatic adaptations on single elements composing a Web page. In particular, a reinforcement learning algorithm, i.e. Q-learning, based on the idea of reward/punishment is utilized as the machine learning system that manages the user profile. Based on it, the user profile is updated, so that automatic adaptations can be effectively performed while surfing the Web. We created a simulation scenario to test our approach over different users with specific preferences and/or different kinds of disabilities. Simulation results confirm the viability of the proposal.",Web personalization;content adaptation;legibility;reinforcement learning;user profiling,Entertainment,,IEEE Xplore,1/person,state representation,batch,y,n,y,y,n,y,n,n,y,y,n,y,y,y,n,y,Q-Learning,2014 IEEE 38th International Computer Software and Applications Conference Workshops,S. Ferretti; S. Mirri; C. Prandi; P. Salomoni,,1.0,10.1109/COMPSACW.2014.45,IEEE Conferences,2014-01-01,Ferretti_2014," @article{Ferretti_2014, title={Exploiting Reinforcement Learning to Profile Users and Personalize Web Pages}, ISBN={9781479935789}, url={http://dx.doi.org/10.1109/COMPSACW.2014.45}, DOI={10.1109/compsacw.2014.45}, journal={2014 IEEE 38th International Computer Software and Applications Conference Workshops}, publisher={IEEE}, author={Ferretti, Stefano and Mirri, Silvia and Prandi, Catia and Salomoni, Paola}, year={2014}, month={Jul}}
"
Personalised Human-Robot Co-Adaptation in Instructional Settings using Reinforcement Learning,"In the domain of robotic tutors, personalised tutoring has startedto receive scientists’ attention, but is still relatively underexplored. Previ-ous work using reinforcement learning (RL) has addressed personalisedtutoring from the perspective of affective policy learning. In this paperwe build on previous work on affective policy learning that used RL tolearn  what  robot’s  supportive  behaviours  are  preferred  by  users  in  aneducational  scenario.  We  propose  a  RL  framework  for  personalisationthat selects a robot’s supportive behaviours to maximize user’s task per-formance in a learning scenario where a Pepper robot acting as a tutorhelps people learning how to solve grid-based logic puzzles. This workis relevant for the development of persuasive embodied agents and so-cial robots used to support users in different scenarios. In particular, thispaper makes a contribution towards the development of algorithms forhuman-robot co-adaptation that enable robots and agents to select effec-tive strategies to establish long-term relationships with human users",,Education,,Google Scholar,1,not used,unknown,n,n,y,n,n,y,y,n,n,y,n,n,y,n,n,y,"RL, not further specified",Persuasive Embodied Agents for Behavior Change (PEACH2017) Workshop at the International conference on Intelligent Virtual Agents (IVA2017),,,1.0,,,2017-01-01,gao2017personalised,"@inproceedings{gao2017personalised,
  title={Personalised human-robot co-adaptation in instructional settings using reinforcement learning},
  author={Gao, Alex Yuan and Barendregt, Wolmet and Castellano, Ginevra},
  booktitle={IVA Workshop on Persuasive Embodied Agents for Behavior Change: PEACH 2017, August 27, Stockholm, Sweden},
  year={2017}
}
"
Multi-objective reinforcement learning algorithm and its application in drive system,"Generally, reinforcement learning (RL) is used to design neurocontroller for control system with single objective. When facing multi-objective system, it is necessary to design the neurocontroller according to the personal preference. This paper proposed a multi-objective reinforcement learning algorithm (MORLA) to design neurocontroller with the personal preference. It transformed the multi-objective into synthetical objective and applied parallel genetic algorithm (PGA) to evolve the neurocontroller according to the synthetical objective. To establish the synthetical objective, the objective weight which represents the personal preference is calculated by solving the constrained optimization problem (COP) at the end of each generation. The COP requires not only the biggest variance of the synthetical objective in the population, but also requires the weight to fit the designerpsilas preference. After acquiring the weights, the PGA can select the elitists from the population according to the designerpsilas preference and design a satisfying neurocontroller by evolutionary operations. At last, the MORLA is used to design neurocontroller for a speed-controlled induction motor drive with indirect vector control. This paper designed several neurocontrollers with different personal preferences for the drive system. The simulation results show the feasibility and validity of the MORLA.",,Transport,,IEEE Xplore,1,not used,unknown,y,n,n,y,n,n,n,n,n,y,y,n,n,y,n,y,"GA,RL, not further specified",2008 34th Annual Conference of IEEE Industrial Electronics,Zhang Huajun; Zhao Jin; Wang Rui; Ma Tan,,1.0,10.1109/IECON.2008.4757965,IEEE Conferences,2008-01-01,Zhang_Huajun_2008," @article{Zhang_Huajun_2008, title={Multi-objective reinforcement learning algorithm and its application in drive system}, ISBN={9781424417674}, url={http://dx.doi.org/10.1109/IECON.2008.4757965}, DOI={10.1109/iecon.2008.4757965}, journal={2008 34th Annual Conference of IEEE Industrial Electronics}, publisher={IEEE}, author={Zhang Huajun and Zhao Jin and Wang Rui and Ma Tan}, year={2008}, month={Nov}}
"
Multi-agents and learning: Implications for Webusage mining,"Characterization of user activities is an important issue in the design and maintenance of websites. Server weblog files have abundant information about the user's current interests. This information can be mined and analyzed therefore the administrators may be able to guide the users in their browsing activity so they may obtain relevant information in a shorter span of time to obtain user satisfaction. Web-based technology facilitates the creation of personally meaningful and socially useful knowledge through supportive interactions, communication and collaboration among educators, learners and information. This paper suggests a new methodology based on learning techniques for a Web-based Multiagent-based application to discover the hidden patterns in the user's visited links. It presents a new approach that involves unsupervised, reinforcement learning, and cooperation between agents. It is utilized to discover patterns that represent the user's profiles in a sample website into specific categories of materials using significance percentages. These profiles are used to make recommendations of interesting links and categories to the user. The experimental results of the approach showed successful user pattern recognition, and cooperative learning among agents to obtain user profiles. It indicates that combining different learning algorithms is capable of improving user satisfaction indicated by the percentage of precision, recall, the progressive category weight and F1-measure. © 2015.Production and hosting by Elsevier B.V.",Cooperative learning; Personalized web search; Recommendation system; Reinforcement learning; Unsupervised learning,Education,,SCOPUS,1/group,not used,online,y,n,n,y,n,n,n,y,n,y,n,n,y,y,n,n,Q-Learning,Journal of Advanced Research,"Lotfy H.M.S., Khamis S.M.S., Aboghazalah M.M.","Lotfy, H.M.S., Computer Science, Mathematics Department, Faculty of Science, AinShams University, Cairo, Egypt; Khamis, S.M.S., Computer Science, Mathematics Department, Faculty of Science, AinShams University, Cairo, Egypt; Aboghazalah, M.M., Pure Math. and Computer Science, Menoufia University, Menoufia, Egypt",1.0,10.1016/j.jare.2015.06.005,Article,2016-01-01,Lotfy_2016," @article{Lotfy_2016, title={Multi-agents and learning: Implications for Webusage mining}, volume={7}, ISSN={2090-1232}, url={http://dx.doi.org/10.1016/j.jare.2015.06.005}, DOI={10.1016/j.jare.2015.06.005}, number={2}, journal={Journal of Advanced Research}, publisher={Elsevier BV}, author={Lotfy, Hewayda M.S. and Khamis, Soheir M.S. and Aboghazalah, Maie M.}, year={2016}, month={Mar}, pages={285–295}}
"
A Reinforcement Learning Approach to Emotion-based Automatic Playlist Generation,"A novel trend emerged in music exploration is to organize and search songs according to their emotions. However, research on automatic playlist generation (APG) primarily focuses on metadata and audio similarity. Mainstream solutions view APG as a static problem. This paper argues that the APG problem is better modeled as a continuous optimization problem, and proposes an adaptive preference model for personalized APG based on emotions. The main idea is to collect a user's behavior in music playing, e.g., rating, skipping and replaying, as immediate feedback in learning the user's preferences for music emotion within a playlist. Reinforcement learning is adopted to learn the user's current preferences, which are used to generate personalized playlists. Learning parameters are tuned by simulation of two hypothetical users. A two-month user study is conducted to evaluate the APG solutions. The results show that the proposed approach reduces the Miss Ratio by 10% in comparison with the baseline approach.",automatic playlist generation;reinforcement learning;song emotion,Entertainment,,y,1,not used,batch,n,n,n,n,n,y,y,n,y,y,n,n,y,y,n,n,"SARSA,Q-Learning",2010 International Conference on Technologies and Applications of Artificial Intelligence,C. Y. Chi; R. T. H. Tsai; J. Y. Lai; J. Y. j. Hsu,,1.0,10.1109/TAAI.2010.21,IEEE Conferences,2010-01-01,Chi_2010," @article{Chi_2010, title={A Reinforcement Learning Approach to Emotion-based Automatic Playlist Generation}, ISBN={9781424486687}, url={http://dx.doi.org/10.1109/TAAI.2010.21}, DOI={10.1109/taai.2010.21}, journal={2010 International Conference on Technologies and Applications of Artificial Intelligence}, publisher={IEEE}, author={Chi, Chung-Yi and Tsai, Richard Tzong-Han and Lai, Jeng-You and Hsu, Jane Yung-jen}, year={2010}, month={Nov}}
"
A learning multi-agent system for personalized information filtering,"A multi-agent hybrid learning approach to the problem of personalized information filtering is proposed in this paper. There are four agents in the multi-agent model. The problem is modeled as Monte Carlo reinforcement learning. Our proposed algorithm is modified Monte Carlo method combined with features of unsupervised suffix tree clustering and supervised backpropagation network. We argue that this proposed approach could precisely capture the user's interest without repeatedly asking for his/her explicit rates and converge to the user's interest quickly. A conclusion is drawn that our approach is efficient, precise and converges more quickly compared with existing approaches. A prototype system is being developed.",,Education,,y,1,not used,unknown,n,n,n,n,n,n,n,n,n,y,n,n,n,n,n,n,Monte Carlo,"Fourth International Conference on Information, Communications and Signal Processing, 2003 and the Fourth Pacific Rim Conference on Multimedia. Proceedings of the 2003 Joint",Junhua Chen; Zhonghua Yang,,1.0,10.1109/ICICS.2003.1292790,IEEE Conferences,2003-01-01,Junhua_Chen," @article{Junhua_Chen, title={A learning multi-agent system for personalized information fiftering}, ISBN={0780381858}, url={http://dx.doi.org/10.1109/ICICS.2003.1292790}, DOI={10.1109/icics.2003.1292790}, journal={Fourth International Conference on Information, Communications and Signal Processing, 2003 and the Fourth Pacific Rim Conference on Multimedia. Proceedings of the 2003 Joint}, publisher={IEEE}, author={Junhua Chen and Zhonghua Yang}}
"
Recipe tuning by reinforcement learning in the SandS ecosystem,"The Social and Smart (SandS) project ecosystem is compounded of household appliance users sharing recipes for the used of appliances, an intermediate control layer, and an intelligent social layer which aims to optimize the appliance recipes maximizing user satisfaction. We consider two aspects of the social intelligence, the innovation producing new recipes for unkown user tasks, and the adaptation to personalize the recipe to an individual user on the basis of his/her specific feedback. The second aspect is proposed to be dealt with by Reinforcement Learning approach, thus user feedback becomes the system reward. In this paper we discuss such an architecture based on the actor-critic approach, providing some experimental results on synthetic datasets that demonstrate the feasibility of the approach, previous to real life implementations.",Reinforcement Learning;Social computing;Social networks;subconscious social intelligence,Smart Home,,IEEE Xplore,1/person,not used,online,y,n,n,y,n,n,n,n,y,n,n,n,n,n,n,y,Actor-Critic,2014 6th International Conference on Computational Aspects of Social Networks,B. Fernandez-Gauna; M. Graña,,0.0,10.1109/CASoN.2014.6920422,IEEE Conferences,2014-01-01,Fernandez_Gauna_2014," @article{Fernandez_Gauna_2014, title={Recipe tuning by reinforcement learning in the SandS ecosystem}, ISBN={9781479959396}, url={http://dx.doi.org/10.1109/CASoN.2014.6920422}, DOI={10.1109/cason.2014.6920422}, journal={2014 6th International Conference on Computational Aspects of Social Networks}, publisher={IEEE}, author={Fernandez-Gauna, Borja and Grana, Manuel}, year={2014}, month={Jul}}
"
Dynamic Class of Service mapping for Quality of Experience control in future networks,"The present work introduces a novel approach to cope with some key limitations of the present communication networks. In particular, the need for effectively managing heterogeneous resources over heterogeneous networks while guaranteeing personalized Quality of Experience (QoE) requirements to the applications, claims for a full cognitive approach which is realized through the introduction of an appropriate Future Internet architecture. The paper, taking this architecture in mind, introduces the innovative concept of dynamic association between applications and Classes of Services, performed by means of proper Reinforcement Learning algorithms. The resulting procedure guarantees QoE personalization, requires low processing capabilities and entails limited signalling overhead.",,Entertainment,,IEEE Explore,1,not used,unknown,y,n,n,y,n,n,y,n,n,n,n,n,n,n,n,n,other,WTC 2014; World Telecommunications Congress 2014,F. D. Priscoli; L. Fogliati; A. Palo; A. Pietrabissa,,0.0,,VDE Conferences,2014-01-01,priscoli2014dynamic,"@inproceedings{priscoli2014dynamic,
  title={Dynamic Class of Service mapping for Quality of Experience control in future networks},
  author={Priscoli, Francesco Delli and Fogliati, Laura and Palo, Andi and Pietrabissa, Antonio},
  booktitle={WTC 2014; World Telecommunications Congress 2014},
  pages={1--6},
  year={2014},
  organization={VDE}
}
"
Personalized Web recommendations: supporting epistemic information about end-users,The online recommendations are a popular presence in the Web sites world due to their potential to increase the customers' satisfaction. The ability to represent epistemic information about the clients' beliefs is important to understand their needs. This paper presents a recommender system based on reinforcement learning. The system represents concepts presented on a Web site by epistemic logical programs and uses a similarity measure between programs in order to facilitate generalization. A prototype of this system and experiments are presented.,,Entertainment,,IEEE Xplore,1,other,batch,n,y,n,n,y,n,n,y,n,y,n,n,y,y,n,y,SARSA,The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05),M. Preda; D. Popescu,,0.0,10.1109/WI.2005.115,IEEE Conferences,2005-01-01,Preda," @article{Preda, title={Personalized Web Recommendations: Supporting Epistemic Information about End-Users}, ISBN={076952415X}, url={http://dx.doi.org/10.1109/WI.2005.115}, DOI={10.1109/wi.2005.115}, journal={The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI’05)}, publisher={IEEE}, author={Preda, M. and Popescu, D.}}
"
Reducing Delay during Vertical Handover,"The next generation of wireless networks will provide heterogeneous wireless technologies to a mobile user, in which they can move using terminals with multiple access interfaces and non-real-time or real-time services. The most important issue in such environment is the Always Best Connected (ABC) concept allowing the best connectivity to applications anywhere at any time. With the growing of communication world lot of research is carried out in the field of wireless networks specifically next generation networks. The integration and interoperability of various wireless networks will lead to number of challenges. One of the challenges is the handovers across various/heterogeneous wireless networks as well as reduction of call drop probability. Number of researches has proposed solutions for this challenge. This paper proposes a solution to this problem for the improvement of end-to-end Quality of Service supporting high data rates, real time transmission over wide area and enabling users to specify their personal preferences using Markov Decision Process (MDP). The proposed method uses delay as its basic parameter to select a network during handovers. The selection of the network amongst the existing wireless network considers the ongoing application of the user during the handover. Through simulations we show that our algorithm reduces call drop probability and satisfies user's requirements.",Markov Decision process;Reinforcement Learning;Reward;Transition Probability;Vertical Handover,Entertainment,,IEEE Explore,1,not used,unknown,y,n,n,y,n,n,y,n,n,n,n,n,n,y,n,y,"RL, not further specified",2015 International Conference on Computing Communication Control and Automation,N. Bagdure; B. Ambudkar,,0.0,10.1109/ICCUBEA.2015.44,IEEE Conferences,2015-01-01,Bagdure_2015," @article{Bagdure_2015, title={Reducing Delay during Vertical Handover}, ISBN={9781479968923}, url={http://dx.doi.org/10.1109/ICCUBEA.2015.44}, DOI={10.1109/iccubea.2015.44}, journal={2015 International Conference on Computing Communication Control and Automation}, publisher={IEEE}, author={Bagdure, Nilakshee and Ambudkar, Bhavna}, year={2015}, month={Feb}}
"
Smart Lifelong Learning System Based on Q-Learning,"The majority of current web-based learning systems are closed learning environments where courses and learning materials are fixed and the only dynamic aspect is the organization of the material that can be adapted to allow a relatively individualized learning environment. In this paper, we propose an evolving web-based learning system which can adapt itself to its learners. More specifically, the novelty with respect to the system lies in its ability to find relevant content on the web, and its ability to personalize and adapt this content based on the system's observation of its learners and the accumulated ratings given by the learners. Hence, although learners do not have direct interaction with the open Web, the system can retrieve relevant information related to them and their situated learning characteristics. Lifelong learning scenarios have particular differences in their need for personalized recommendations that make not possible reusing existing general approaches of recommender systems. The paper describes those challenges and we propose a hybrid technique based on machine learning to recognize learner preferences and predict theirs required contents with high accuracy.",Learning Promotion;Lifelong Learning;Machine Learning;Q Learning;Recommender Systems;Reinforcement Learning,Education,,IEEE Xplore,1,not used,online,n,n,y,n,n,y,n,n,n,y,n,n,n,y,n,y,Q-Learning,2010 Seventh International Conference on Information Technology: New Generations,A. A. Kardan; O. R. B. Speily,,0.0,10.1109/ITNG.2010.140,IEEE Conferences,2010-01-01,Kardan_2010," @article{Kardan_2010, title={Smart Lifelong Learning System Based on Q-Learning}, ISBN={9781424462704}, url={http://dx.doi.org/10.1109/ITNG.2010.140}, DOI={10.1109/itng.2010.140}, journal={2010 Seventh International Conference on Information Technology: New Generations}, publisher={IEEE}, author={Kardan, Ahmad A. and Speily, Omid R.B.}, year={2010}}
"
A learning strategy for paging in mobile environments,"The essence of designing a good paging strategy is to incorporate user mobility characteristics in a predictive mechanism that reduces the average paging cost with as little computational effort as possible. We introduce a novel paging scheme based on the concept of reinforcement learning. Learning endows the paging mechanism with the predictive power necessary to determine a mobile terminal's position, without having to extract a location probability distribution for each specific user. The proposed algorithm is compared against a heuristic randomized learning strategy akin to reinforcement learning, that we invented for this purpose and performs better than the case where no learning is used at all. It is shown that if the user normally moves only among a fraction of cells in the location area, significant savings can be achieved over the randomized strategy, without excessive time to train the network.",,Communication,,IEEE Xplore,1/person,not used,online,y,n,n,y,n,n,y,y,n,y,n,n,n,y,n,y,Q-Learning,2003 5th European Personal Mobile Communications Conference (Conf. Publ. No. 492),I. Koukoutsidis; P. Demestichas; M. Theologou,,0.0,10.1049/cp:20030322,IET Conferences,2003-01-01,Koukoutsidis_2003," @article{Koukoutsidis_2003, title={A learning strategy for paging in mobile environments}, ISBN={0852967535}, url={http://dx.doi.org/10.1049/cp:20030322}, DOI={10.1049/cp:20030322}, journal={5th European Personal Mobile Communications Conference 2003}, publisher={IEE}, author={Koukoutsidis, I.}, year={2003}}
"
Emotion-driven learning agent for setting rich presence in mobile telephony,"Presence or personal status information is going to be an integral part of human life in the near future. With the possibility of personalizing user preferences in a fine grained way, mobile presence will appeal to most users. Among all the advantages, one of the most annoying problems is to set the presence status manually each time. This paper discusses the development of an intelligent agent based presence client that will learn and make decisions on behalf of the user about his or her presence status. The decision is emotion driven and the learning depends on real world experience. The proposed system utilizes a neural network (NN) based emotion-driven agent to learn user preferences. As a NN learning algorithm, two approaches based on Differential Evolution and Reinforcement have been proposed, of which either one can be used. Rich presence status is set using a scripting language named Call Processing Language; and SIP is used for publishing the presence to others.",ANN;CPL;DE;Presence;SIP;context aware;learning agent;neural network;reinforcement,Communication,,IEEE Xplore,1,state representation,online,n,n,y,n,n,y,n,y,n,y,n,n,n,n,y,y,NN,2008 11th International Conference on Computer and Information Technology,S. Saha; R. Quazi,,0.0,10.1109/ICCITECHN.2008.4803023,IEEE Conferences,2008-01-01,Saha_2008," @article{Saha_2008, title={Emotion-driven learning agent for setting rich presence in mobile telephony}, ISBN={9781424421350}, url={http://dx.doi.org/10.1109/ICCITECHN.2008.4803023}, DOI={10.1109/iccitechn.2008.4803023}, journal={2008 11th International Conference on Computer and Information Technology}, publisher={IEEE}, author={Saha, Sumanta and Quazi, Rumana}, year={2008}, month={Dec}}
"
Personalizing robot behavior for interruption in social human-robot interaction,"People engaging in an activity usually has individual tolerance to be interrupted [1], [2]. Humans subconsciously adapt their behaviors to draw other one's attention and to get into a conversation based on their historical experiences, but robots often fail to be aware of humans' feeling and thus interrupt their users repeatedly. To endow service robots with such socially acceptable ability, we propose an online human-aware interactive learning framework in this paper, under which the robot personalizes its behaviors according to both observed user's attention and its conjecture about user's awareness of itself. To this purpose, the correlation between the robot's theory of awareness, user's attention and robot behavior are explored through reinforcement learning techniques. The conducted experiment shows that the robot can personalize its interruption strategy, and the optimal policies converged for at least 26 episodes.",,Domain Independent,,IEEE Xplore,1/person,state representation,online,n,n,y,n,n,y,n,n,n,n,n,y,y,n,n,n,Q-Learning,2014 IEEE International Workshop on Advanced Robotics and its Social Impacts,Y. S. Chiang; T. S. Chu; C. D. Lim; T. Y. Wu; S. H. Tseng; L. C. Fu,,0.0,10.1109/ARSO.2014.7020978,IEEE Conferences,2014-01-01,Chiang_2014," @article{Chiang_2014, title={Personalizing robot behavior for interruption in social human-robot interaction}, ISBN={9781479969685}, url={http://dx.doi.org/10.1109/ARSO.2014.7020978}, DOI={10.1109/arso.2014.7020978}, journal={2014 IEEE International Workshop on Advanced Robotics and its Social Impacts}, publisher={IEEE}, author={Chiang, Yi-Shiu and Chu, Ting-Sheng and Lim, Chung Dial and Wu, Tung-Yen and Tseng, Shih-Huan and Fu, Li-Chen}, year={2014}, month={Sep}}
"
Investigating Deep Reinforcement Learning Techniques in Personalized Dialogue Generation,"In this paper, we propose a personalized dialogue generation system, which combines reinforcement learning techniques with an attention-based hierarchical recurrent encoderdecoder model. Firstly, we incorporate user-specific information into the decoder to …",,Education,,y,1,not used,batch,n,y,n,n,y,n,n,n,n,y,n,n,y,y,n,y,"Q-Learning,Actor-Critic",,,,0.0,,,2018-01-01,yang2018investigating,"@inproceedings{yang2018investigating,
  title={Investigating Deep Reinforcement Learning Techniques in Personalized Dialogue Generation},
  author={Yang, Min and Qu, Qiang and Lei, Kai and Zhu, Jia and Zhao, Zhou and Chen, Xiaojun and Huang, Joshua Z},
  booktitle={Proceedings of the 2018 SIAM International Conference on Data Mining},
  pages={630--638},
  year={2018},
  organization={SIAM}
}
"
A Fast Interactive Search System for Healthcare Services,"In this paper we describe the design, development, and evaluation of a general human-machine interaction search system, and its potential and use in the context of a collaboration project with SAP and Saffron. The objective of a specialized version of the system is to provide medical and healthcare information services to users via interactive search for personalized patient needs. Patients usually have questions regarding healthcare, including those which concern illness symptoms, duration and types of treatment, possible drug effects, and more. Authorized personnel would often be ideal in responding to such needs; however they could potentially be very expensive, and not easy to support and maintain. If patients could have access to information at their home, by means of i-phone or online access, this could save time, doctor office visit expenses, as well as valuable and restricted medical time. What is more, information concerning other anonymized and similar patient cases provides knowledge and perspective on a wide range of patient issues. From the doctors' perspective, they typically need to spend time on differential analysis about new patient cases: study symptoms, research possible causes, rank results by emergency priority and treat them accordingly. A search system that would direct a doctor (or patient/user) to similar patient cases would save significant amount of manual search time. The powerful new feature of this system is the storage and mining of past patient cases knowledge, to create metadata to be used in the subsequent retrieval of relevant documents. Finally, the interactive search system would speed up identification of rare cases; for instance, symptoms that do not appear commonly in past cases may require special treatment or expert referral. We build a model which dynamically learns medical needs of interacting MDs and patients. The model works on free or unstructured text, allowing disambiguation of vague words and flexibility in descri- ing medical needs. In addition, both experts with an advanced knowledge of medical terminology, and beginning users using basic medical terms, can achieve high search relevance. Furthermore, our approach obviates the need for the assignment of tags or labels, such as treatment, symptoms, causes, to documents, to respond effectively to user queries. In particular, we build a temporal difference algorithm to predict user's information needs by incorporating both current and predicted knowledge into learning the user profile. Our source of information about the user consists of submitted queries and feedback on the returned results. We tested our system on publicly available medical data (OhsuMed TREC dataset 2002) and we achieved a significant improvement in retrieval accuracy, compared to the literature. We provide quantitative results as well as demonstration screenshots which illustrate a) the value of interaction (user time spent with system versus results accuracy), b) the value of using medical terminology understanding, when compared with simple general words, and c) the value of allowing the maximum number of feedback submissions to vary.",healthcare information services;interactive retrieval;medical data retrieval;reinforcement learning;temporal difference,Health,,IEEE Explore,1,not used,batch,n,y,n,n,y,n,n,n,n,n,n,n,n,n,n,y,SARSA,2012 Annual SRII Global Conference,M. Daltayanni; C. Wang; R. Akella,,0.0,10.1109/SRII.2012.65,IEEE Conferences,2012-01-01,Daltayanni_2012," @article{Daltayanni_2012, title={A Fast Interactive Search System for Healthcare Services}, ISBN={9780769547701}, url={http://dx.doi.org/10.1109/SRII.2012.65}, DOI={10.1109/srii.2012.65}, journal={2012 Annual SRII Global Conference}, publisher={IEEE}, author={Daltayanni, Maria and Wang, Chunye and Akella, Ram}, year={2012}, month={Jul}}
"
Reinforcement-Learning-Based Personalization of Head-Related Transfer Functions,"Accurately perceiving spatial locations of virtual sounds using stereo earphones or headphones requires individual head-related transfer functions (HRTFs) for each listener. However, accurate HRTF measurement is usually difficult. While previous studies have …",,Entertainment,,Google Scholar,1,not used,unknown,y,n,y,y,n,y,y,n,y,n,n,n,n,n,n,y,"RL, not further specified",Journal of the Audio Engineering Society,,,0.0,,,2018-01-01,Srivihok2005IntelligentAF_0,"@inproceedings{Srivihok2005IntelligentAF_0,
  title={Intelligent Agent for e-Tourism: Personalization Travel Support Agent using Reinforcement Learning},
  author={Anongnart Srivihok and Pisit Sukonmanee},
  booktitle={WWW 2005},
  year={2005}
}"
Multi-objective reinforcement learning algorithm and its improved convergency method,"This paper proposes a multi-objective reinforcement learning algorithm (MORLA) and uses simultaneous perturbation stochastic approximation (SPSA) to improve the convergence of it. Usually, reinforcement learning (RL) is used to design neurocontroller for control system with single objective. When facing multi-objective system, it is necessary to design the neurocontroller according to the personal preference. The MORLA can transform the multi-objective into synthetical objective and applies parallel genetic algorithm (PGA) to evolve the neurocontroller according to the synthetical objective. To establish the synthetical objective, the objective weight which represents the personal preference is calculated by solving the constrained optimization problem (COP) at the end of each generation. The COP requires not only the biggest variance of the synthetical objective in the population, but also requires the weight to fit the designer's preference. After acquiring the weights, the PGA can select the elitists from the population according to the designer's preference and design a satisfying neurocontroller by evolutionary operations. In addition, although GA has good global search ability, it descends slowly at local area. This paper applies SPSA algorithm to search optimal solution when GA is vibrating at local area. The SPSA converges fast by efficient gradient approximation that relies on measurements of the objective function. The hybrid algorithm accelerates the learning speed of reinforcement learning. At last, the MORLA is used to design neurocontroller for a speed-controlled induction motor drive with indirect vector control. With different personal preferences for the drive system, the simulation results show the feasibility and validity of the MORLA.",SPSA;multi-objective reinforcement learning;speed-controlled,Energy,,IEEE Xplore,1/person,state representation,online,y,n,n,y,n,n,n,n,n,y,n,n,n,y,n,y,MORLA,2011 6th IEEE Conference on Industrial Electronics and Applications,Z. Jin; Z. Huajun,,,10.1109/ICIEA.2011.5976002,IEEE Conferences,2011-01-01,Jin_2011," @article{Jin_2011, title={Multi-objective reinforcement learning algorithm and its improved convergency method}, ISBN={9781424487547}, url={http://dx.doi.org/10.1109/ICIEA.2011.5976002}, DOI={10.1109/iciea.2011.5976002}, journal={2011 6th IEEE Conference on Industrial Electronics and Applications}, publisher={IEEE}, author={Jin, Zhao and Huajun, Zhang}, year={2011}, month={Jun}}
"
Automatically Learning User Preferences for Personalized Service Composition,"With the rapid growth of the web services technologies, users often leverage various web services to perform their daily activities, such as on-line shopping. Due to the massive amount of web services available, a user faces numerous choices to meet their personal preferences when selecting the desired services from the web services with the similar functionality. Therefore, it becomes tedious and cumbersome tasks for users to discover and compose services. To reduce user's cognitive burden, it is critical to support automated service composition and make efficient recommendation of personalized services to achieve user's overall goals. However, existing approaches only offer users with limited options designed for the interest of a group of users without considering individual users' interests. To allow users to compose personalized services without much manual specification, we propose a machine learning approach that applies a learning-to-rank algorithm, RankBoost, to automatically learn user preferences and the prioritization of the preferences from users' historical data. Moreover, our approach uses the multi-objective reinforcement learning (MORL) algorithm to make trade-offs among user preferences and recommends a collection of services to achieve the highest objective. We conduct an empirical study to evaluate our approach by collecting the historical data from 12 subjects. The results demonstrate that our approach outperforms the two well-established baseline approaches by 100%-200% in terms of precision on recommending services.",,Commerce,,IEEE Xplore,1,state representation,batch,n,y,n,n,y,n,n,y,n,y,n,n,n,n,n,y,Q-Learning,2017 IEEE International Conference on Web Services (ICWS),Y. Zhao; S. Wang; Y. Zou; J. Ng; T. Ng,,,10.1109/ICWS.2017.93,IEEE Conferences,2017-01-01,Zhao_2017," @article{Zhao_2017, title={Automatically Learning User Preferences for Personalized Service Composition}, ISBN={9781538607527}, url={http://dx.doi.org/10.1109/ICWS.2017.93}, DOI={10.1109/icws.2017.93}, journal={2017 IEEE International Conference on Web Services (ICWS)}, publisher={IEEE}, author={Zhao, Yu and Wang, Shaohua and Zou, Ying and Ng, Joanna and Ng, Tinny}, year={2017}, month={Jun}}
"
A Reinforcement Learning-Based Adaptive Learning System,"With the plethora of educational and e-learning systems and the great variation in students’ personal and social factors that affect their learning behaviors and outcomes, it has become mandatory for all educational systems to adapt to the variability of these factors for each student. Since there is a large number of factors that need to be taken into consideration, the task is very challenging. In this paper, we present an approach that adapts to the most influential factors in a way that varies from one learner to another, and in different learning settings, including individual and collaborative learning. The approach utilizes reinforcement learning for building an intelligent environment that, not only provides a method for suggesting suitable learning materials, but also provides a methodology for accounting for the continuously-changing students’ states and acceptance of technology. We evaluate our system through simulations. The obtained results are promising and show the feasibility of the proposed approach. © 2018, Springer International Publishing AG.",Adaptive learning; Computer-supported collaborative learning; Reinforcement Learning,Education,,,1,state representation,online,y,n,n,y,n,n,n,n,y,y,n,y,n,y,n,y,"Q-Learning,but not completely clear.",Advances in Intelligent Systems and Computing,"Shawky D., Badawi A.","Shawky, D., Center for Learning Technologies, University of Science and Technology, Zewail City, Giza, Egypt; Badawi, A., Center for Learning Technologies, University of Science and Technology, Zewail City, Giza, Egypt",,10.1007/978-3-319-74690-6_22,Conference Paper,2018-01-01,Shawky_2018," @article{Shawky_2018, title={A Reinforcement Learning-Based Adaptive Learning System}, ISBN={9783319746906}, ISSN={2194-5365}, url={http://dx.doi.org/10.1007/978-3-319-74690-6_22}, DOI={10.1007/978-3-319-74690-6_22}, journal={Advances in Intelligent Systems and Computing}, publisher={Springer International Publishing}, author={Shawky, Doaa and Badawi, Ashraf}, year={2018}, pages={221–231}}
"
Intelligent agent for E-tourism: Personalization travel support agent using reinforcement learning,"Web personalization and one to one marketing have been introduced as strategy and marketing tools. By using historical and present information of customers, organizations can learn, predict customer's behaviors and develop products to fit potential customers. In this study, a Personalization Travel Support System is introduced to manage traveling information for user. It provides the information that matches the users' interests. This system applies the Reinforcement Learning to analyze, learn customer behaviors and recommend products to meet customer interests. There are two learning approaches using in this study. First, Personalization Learner by Group Properties is learning from all users in one group to find the group interests of travel information by using given data on user ages and genders. Second, Personalization Learner by User Behavior: user profile, user behaviors and trip features will be analyzed to find the unique interest of each web user. The results from this study reveal that it is possible to develop Personalization Travel Support System. Using weighted trip features improve effectiveness and increase the accuracy of the personalized engine. Precision, Recall and Harmonic Mean of the learned system are higher than the original one. This study offers useful information regarding the areas of personalization of web support system.",Intelligent agent; Personalization; Recommendation algorithm; Reinforcement Learning,Entertainment,,SCOPUS,multiple,state representation,batch,n,y,n,n,n,y,y,n,n,y,n,n,y,y,y,y,Q-Learning,CEUR Workshop Proceedings,"Srivihok A., Sukonmanee P.","Srivihok, A., Department of Computer Science, Faculty of Science, Kasetsart University, Bangkok 10900, Thailand; Sukonmanee, P., Department of Computer Science, Faculty of Science, Kasetsart University, Bangkok 10900, Thailand",,,Conference Paper,2005-01-01,Srivihok2005IntelligentAF,"@inproceedings{Srivihok2005IntelligentAF,
  title={Intelligent Agent for e-Tourism: Personalization Travel Support Agent using Reinforcement Learning},
  author={Anongnart Srivihok and Pisit Sukonmanee},
  booktitle={WWW 2005},
  year={2005}
}"
A Unified Contextual Bandit Framework for Long- and Short-Term Recommendations,"We present a unified contextual bandit framework for recommendation problems that is able to capture long- and short-term interests of users. The model is devised in dual space and the derivation is consequentially carried out using Fenchel-Legrende conjugates and thus leverages to a wide range of tasks and settings. We detail two instantiations for regression and classification scenarios and obtain well-known algorithms for these special cases. The resulting general and unified framework allows for quickly adapting contextual bandits to different applications at-hand. The empirical study demonstrates that the proposed long- and short-term framework outperforms both, short-term and long-term models on data. Moreover, a tweak of the combined model proves beneficial in cold start problems. © 2017, Springer International Publishing AG.",Contextual bandits; Dual optimization; Personalization; Recommendation,Commerce,,SCOPUS,1,other,batch,n,y,n,n,y,n,y,n,n,y,n,n,y,y,n,y,Contextual Bandits,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"Tavakol M., Brefeld U.","Tavakol, M., Leuphana Universität Lüneburg, Lüneburg, Germany, Technische Universität Darmstadt, Darmstadt, Germany; Brefeld, U., Leuphana Universität Lüneburg, Lüneburg, Germany",,10.1007/978-3-319-71246-8_17,Conference Paper,2017-01-01,Tavakol_2017," @article{Tavakol_2017, title={A Unified Contextual Bandit Framework for Long- and Short-Term Recommendations}, ISBN={9783319712468}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-319-71246-8_17}, DOI={10.1007/978-3-319-71246-8_17}, journal={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Tavakol, M. and Brefeld, U.}, year={2017}, pages={269–284}}
"
A scalable approach for periodical personalized recommendations,"We develop a highly scalable and effective contextual bandit approach towards periodical personalized recommendations. The online bootstrapping-based technique provides a princi- pled way for UCB-type exploitation-exploration algorithms, while being able to handle arbitrary sized datasets, well suited to learn the ever evolving user preference drift from streaming data, and essentially parameter-free. We further introduce techniques to handle arbitrary sized feature spaces using feature hashing, leverage existing state-of-art machine learning via learning reduction, and increase cache hits by managing bootstrapped models in memory effectively. The resulted model trains on millions of examples and billions of features within minutes on a single personal computer. It shows persistent performance in both offline and online eval- uation. We observe around 10% click through rate (CTR) and conversion lift over a collaborative filtering approach in real-world A/B testing across more than 40 million users on the major Ticketmaster email recommendation product. © 2016 ACM.",Contextual Bandits; Learning Reductions; Online Learning; Personalization; Recommender Systems; Scalability,Commerce,,,1,state representation,unknown,n,y,y,n,y,y,y,y,n,y,n,n,y,y,n,y,Contextual bandit with BS (OBS); epsilon greedy; Collaborative filtering; LR; random; Thompson sampling,RecSys 2016 - Proceedings of the 10th ACM Conference on Recommender Systems,"Qin Z., Rishabh I., Carnahan J.","Qin, Z., Ticketmaster, Hollywood, CA, United States; Rishabh, I., Ticketmaster, Hollywood, CA, United States; Carnahan, J., Ticketmaster, Hollywood, CA, United States",,10.1145/2959100.2959139,Conference Paper,2016-01-01,Qin_2016," @article{Qin_2016, title={A Scalable Approach for Periodical Personalized Recommendations}, ISBN={9781450340359}, url={http://dx.doi.org/10.1145/2959100.2959139}, DOI={10.1145/2959100.2959139}, journal={Proceedings of the 10th ACM Conference on Recommender Systems - RecSys  ’16}, publisher={ACM Press}, author={Qin, Zhen and Rishabh, Ish and Carnahan, John}, year={2016}}
"
Data-driven inverse learning of passenger preferences in urban public transits,"Urban public transit planning is crucial in reducing traffic congestion and enabling green transportation. However, there is no systematic way to integrate passengers' personal preferences in planning public transit routes and schedules so as to achieve high occupancy rates and efficiency gain of ride-sharing. In this paper, we take the first step tp exact passengers' preferences in planning from history public transit data. We propose a data-driven method to construct a Markov decision process model that characterizes the process of passengers making sequential public transit choices, in bus routes, subway lines, and transfer stops/stations. Using the model, we integrate softmax policy iteration into maximum entropy inverse reinforcement learning to infer the passenger's reward function from observed trajectory data. The inferred reward function will enable an urban planner to predict passengers' route planning decisions given some proposed transit plans, for example, opening a new bus route or subway line. Finally, we demonstrate the correctness and accuracy of our modeling and inference methods in a large-scale (three months) passenger-level public transit trajectory data from Shenzhen, China. Our method contributes to smart transportation design and human-centric urban planning.",,Transport,,,1/group,state representation,unknown,n,y,n,n,y,n,n,n,n,y,n,n,n,y,n,y,Inverse Reinforcement Learning,2017 IEEE 56th Annual Conference on Decision and Control (CDC),G. Wu; Y. Ding; Y. Li; J. Luo; F. Zhang; J. Fu,,,10.1109/CDC.2017.8264410,IEEE Conferences,2017-01-01,Wu_2017," @article{Wu_2017, title={Data-driven inverse learning of passenger preferences in urban public transits}, ISBN={9781509028733}, url={http://dx.doi.org/10.1109/CDC.2017.8264410}, DOI={10.1109/cdc.2017.8264410}, journal={2017 IEEE 56th Annual Conference on Decision and Control (CDC)}, publisher={IEEE}, author={Wu, Guojun and Ding, Yichen and Li, Yanhua and Luo, Jun and Zhang, Fan and Fu, Jie}, year={2017}, month={Dec}}
"
A learning model for personalized adaptive cruise control,"This paper develops a learning model for personalized adaptive cruise control that can learn from human demonstration online and mimic a human driver's driving strategies in the dynamic traffic environment. Under the framework of the proposed model, reinforcement learning is used to capture the human-desired driving strategy, and the proportion-integration-differentiation controller is adopted to convert the learning strategy to low-level control commands. The performance of the learning model is tested in the simulation environment built in a driving simulator using PreScan. Experimental results show that the learning model can duplicate human driving strategies with acceptable errors. Moreover, compared with the traditional adaptive cruise control, the proposed model can provide better driving comfort and smoothness in the dynamic situation.",,Transport,,IEEE Explore,1/person,state representation,online,y,n,n,y,n,n,y,n,n,y,y,y,n,n,n,y,Q-Learning,2017 IEEE Intelligent Vehicles Symposium (IV),X. Chen; Y. Zhai; C. Lu; J. Gong; G. Wang,,,10.1109/IVS.2017.7995748,IEEE Conferences,2017-01-01,Chen_2017," @article{Chen_2017, title={A learning model for personalized adaptive cruise control}, ISBN={9781509048045}, url={http://dx.doi.org/10.1109/IVS.2017.7995748}, DOI={10.1109/ivs.2017.7995748}, journal={2017 IEEE Intelligent Vehicles Symposium (IV)}, publisher={IEEE}, author={Chen, Xin and Zhai, Yong and Lu, Chao and Gong, Jianwei and Wang, Gang}, year={2017}, month={Jun}}
"
Smart Cable-Driven Camera Robotic Assistant,"This paper describes the mechanical design and a cognition system for a novel concept-of-camera robotic assistant. The system combines the advantages of intra-abdominal devices and autonomous camera navigation. The robotic assistant is composed of a magnetic intra-abdominal camera robot with two internal cable-driven degrees of freedom and an external robot that handles an external magnet. The intelligence of the robot is implemented in a cognitive architecture based on a long-term memory that stores the robot's knowledge and learning capabilities to improve the robot's behavior. The navigation strategy combines a reactive control based on instrument tracking and a proactive control based on predefined behaviors, depending on the actual state of the task. The robot's learning capabilities include a semantic customization, to adapt the camera's behavior to the surgeon's preferences, and reinforcement learning, to improve the camera navigation strategy. This paper details both the hardware implementation of the system and the software implementation in a robotic operating system architecture. The cognition system and the performance of the cable-driven mechanism have been validated with a set of in vitro experiments. Moreover, the camera robot has been evaluated through an in-vivo experiment in a pig.",Cognitive robotics;mechatronics;medical robotics;robot control;robot motion,Health,,IEEE Xplore,1/person,not used,online,y,n,n,y,n,y,n,n,y,y,y,n,y,n,n,y,Q-Learning,IEEE Transactions on Human-Machine Systems,I. Rivas-Blanco; C. López-Casado; C. J. Pérez-del-Pulgar; F. García-Vacas; J. C. Fraile; V. F. Muñoz,,,10.1109/THMS.2017.2767286,IEEE Journals & Magazines,2018-01-01,Rivas_Blanco_2018," @article{Rivas_Blanco_2018, title={Smart Cable-Driven Camera Robotic Assistant}, volume={48}, ISSN={2168-2305}, url={http://dx.doi.org/10.1109/THMS.2017.2767286}, DOI={10.1109/thms.2017.2767286}, number={2}, journal={IEEE Transactions on Human-Machine Systems}, publisher={Institute of Electrical and Electronics Engineers (IEEE)}, author={Rivas-Blanco, I. and Lopez-Casado, C. and Perez-del-Pulgar, C. J. and Garcia-Vacas, F. and Fraile, J. C. and Munoz, V. F.}, year={2018}, month={Apr}, pages={183–196}}
"
An interactive multisensing framework for personalized human robot collaboration and assistive training using reinforcement learning,"There is a recent trend of research and applications of Cyber- Physical Systems (CPS) in manufacturing to enhance humanrobot collaboration and production. In this paper, we propose a CPS framework for personalized Human-Robot Collaboration and Training to promote safe human-robot collaboration in manufacturing environments. We propose a human-centric CPS approach that focuses on multimodal human behavior monitoring and assessment, to promote human worker safety and enable human training in Human- Robot Collaboration tasks. We present the architecture of our proposed system, our experimental testbed and our proposed methods for multimodal physiological sensing, human state monitoring and interactive robot adaptation, to enable personalized interaction. © 2017 ACM.",Cyber Physical Systems; Human Robot Collaboration; Intelligent Manufacturing; Vocational Assessment and Training,Other,Manufacturing,SCOPUS,1/person,state representation,online,y,n,n,n,n,n,n,n,y,y,y,n,n,n,n,n,Q-Learning,ACM International Conference Proceeding Series,"Tsiakas K., Papakostas M., Papakostas M., Bell M., Mihalcea R., Wang S., Burzo M., Makedon F.","Tsiakas, K., HERACLEIA Human-Centered Computing Laboratory, CSE Dept., University of Texas, Arlington, United States; Papakostas, M., HERACLEIA Human-Centered Computing Laboratory, CSE Dept., University of Texas, Arlington, United States; Papakostas, M., HERACLEIA Human-Centered Computing Laboratory, CSE Dept., University of Texas, Arlington, United States; Bell, M., Dept. of Psychiatry, Yale School of Medicine, United States; Mihalcea, R., CSE Dept., University of Michigan, United States; Wang, S., Industrial Engineering Dept., University of Texas, Arlington, United States; Burzo, M., Mechanical Engineering Dept., University of Michigan-Flint, United States; Makedon, F., HERACLEIA Human-Centered Computing Laboratory, CSE Dept., University of Texas, Arlington, United States",,10.1145/3056540.3076191,Conference Paper,2017-01-01,Tsiakas_2017," @article{Tsiakas_2017, title={An Interactive Multisensing Framework for Personalized Human Robot Collaboration and Assistive Training Using Reinforcement Learning}, ISBN={9781450352277}, url={http://dx.doi.org/10.1145/3056540.3076191}, DOI={10.1145/3056540.3076191}, journal={Proceedings of the 10th International Conference on PErvasive Technologies Related to Assistive Environments  - PETRA  ’17}, publisher={ACM Press}, author={Tsiakas, Konstantinos and Papakostas, Michalis and Theofanidis, Michail and Bell, Morris and Mihalcea, Rada and Wang, Shouyi and Burzo, Mihai and Makedon, Fillia}, year={2017}}
"
Emergency Navigation in Confined Spaces Using Dynamic Grouping,"The performance of Emergency Management Systems (EMS) in confined spaces is highly dependent on the decision algorithm employed for the safe navigation of the evacuees to the available exits. In the algorithm proposed in this paper, we have considered evacuees under two groups, based on their age and physical condition, and we tailor two routing metrics, one for each group, in finding suitable paths for the evacuees. A dynamic grouping mechanism that can adjust an evacuee's group, and therefore routing metric, according to its on-going health condition is employed during the evacuation. To implement the routing metrics, we have used the Cognitive Packet Network (CPN) with random neural networks (RNN) and reinforcement learning. The CPN is an adaptive routing protocol that is loop-free at all times and easily handles multiple quality of service (QoS) metrics. Simulation results show that allowing the navigation system to be sensitive to the on-going health conditions and mobility of the evacuees, using our proposed dynamic grouping, can achieve higher survival rates.",Cognitive Packet Network;Dynamic Grouping;Emergency navigation;QoS driven protocol,Transport,,,1/group,not used,unknown,y,n,n,y,n,n,y,y,n,y,n,y,n,n,n,y,RNN based RL,"2015 9th International Conference on Next Generation Mobile Applications, Services and Technologies",H. Bi; O. J. Akinwande; E. Gelenbe,,,10.1109/NGMAST.2015.12,IEEE Conferences,2015-01-01,Bi_2015," @article{Bi_2015, title={Emergency Navigation in Confined Spaces Using Dynamic Grouping}, ISBN={9781479986606}, url={http://dx.doi.org/10.1109/NGMAST.2015.12}, DOI={10.1109/ngmast.2015.12}, journal={2015 9th International Conference on Next Generation Mobile Applications, Services and Technologies}, publisher={IEEE}, author={Bi, Huibo and Akinwande, Olumide J and Gelenbe, Erol}, year={2015}, month={Sep}}
"
A reinforcement learning approach to weaning of mechanical ventilation in intensive care units,"The management of invasive mechanical ventilation, and the regulation of sedation and analgesia during ventilation, constitutes a major part of the care of patients admitted to intensive care units. Both prolonged dependence on mechanical ventilation and premature extubation are associated with increased risk of complications and higher hospital costs, but clinical opinion on the best protocol for weaning patients off of a ventilator varies. This work aims to develop a decision support tool that uses available patient information to predict time-to-extubation readiness and to recommend a personalized regime of sedation dosage and ventilator support. To this end, we use off-policy reinforcement learning algorithms to determine the best action at a given patient state from sub-optimal historical ICU data. We compare treatment policies from fitted Qiteration with extremely randomized trees and with feedforward neural networks, and demonstrate that the policies learnt show promise in recommending weaning protocols with improved outcomes, in terms of minimizing rates of reintubation and regulating physiological stability.",,Health,,SCOPUS,1,state representation,batch,n,y,n,n,y,n,y,y,n,y,n,y,n,n,n,y,"Q-Learning,Fitted Q-Iteration","Uncertainty in Artificial Intelligence - Proceedings of the 33rd Conference, UAI 2017","Prasad N., Cheng L.-F., Chivers C., Draugelis M., Engelhardt B.E.","Prasad, N., Computer Science, Princeton University, United States; Cheng, L.-F., Electrical Engineering, Princeton University, United States; Chivers, C., Penn Medicine, United States; Draugelis, M., Penn Medicine, United States; Engelhardt, B.E., Computer Science, Princeton University, United States",,,Conference Paper,2017-01-01,Prasad2017ARL,"@article{Prasad2017ARL,
  title={A Reinforcement Learning Approach to Weaning of Mechanical Ventilation in Intensive Care Units},
  author={Niranjani Prasad and Li-Fang Cheng and Corey Chivers and Michael Draugelis and Barbara Elizabeth Engelhardt},
  journal={CoRR},
  year={2017},
  volume={abs/1704.06300}
}"
OWLS: Observational wireless life-enhancing system,"Socially assistive robotics technologies for individuals, who have been affected by age-related disabilities and similar types of disorders, have become popular options for facilitating natural independence and uninterrupted mobility. Wireless wearable sensor systems enable proactive personal health management and the ubiquitous monitoring of vital signs to keep an active watch on immediate health conditions. In this paper, we develop a system, called OWLS, where multiple wearable sensors, software agents, robots and health analysis technology, have been integrated into a single personal therapy solution (SPTS). Our system uses a reinforcement learning algorithm to make decisions about the user's current health conditions, and to take appropriate actions, as necessary (i.e, contacting outside parties). We show that the approach of non-invasive monitoring, when combined with an alert system, makes this a desirable SPTS in future health care. Copyright © 2016, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",,Health,,,1/person,not used,online,n,n,y,n,n,y,n,n,n,y,n,n,y,y,n,y,Q-Learning,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","Zheng H., Jumadinova J.","Zheng, H., Allegheny College, 520 North Main Street, Allegheny, PA, United States; Jumadinova, J., Allegheny College, 520 North Main Street, Allegheny, PA, United States",,,Conference Paper,2016-01-01,Zheng2016OWLSOW,"@inproceedings{Zheng2016OWLSOW,
  title={OWLS: Observational Wireless Life-enhancing System (Extended Abstract)},
  author={Hanzhong Zheng and Janyl Jumadinova},
  booktitle={AAMAS},
  year={2016}
}"
Adaptive Behavior Generation for Child-Robot Interaction,"Social robots are increasingly applied in assistive settings where they interact with human users to support them in their daily life. There, abilities for a robust and reliable social interaction are required, especially for robots that interact autonomously with humans. Apart from challenges regarding safety and trust, the complexity and difficulty of attaining mutual understanding, engagement or assistance in social interactions that comprise spoken languages and non-verbal behaviors need to be taken into account. In addition, different users or user groups have inter-individual differences with respect to their personal preferences, skills and limitations. This makes it more difficult to develop reliable and understandable robots that work well in different situations or for different users. © 2018 Authors.",Assistive Robotics; Child-Robot-Interaction; Multimodal Social Behavior; Reinforcement Learning,Education,,SCOPUS,1/person,state representation,online,y,y,n,n,y,n,n,n,y,y,y,n,n,n,n,y,Q-Learning,ACM/IEEE International Conference on Human-Robot Interaction,"Hemminghaus J., Kopp S.","Hemminghaus, J., CITEC, Bielefeld University, Bielefeld, Germany; Kopp, S., CITEC, Bielefeld University, Bielefeld, Germany",,10.1145/3173386.3176916,Conference Paper,2018-01-01,Hemminghaus_2018," @article{Hemminghaus_2018, title={Adaptive Behavior Generation for Child-Robot Interaction}, ISBN={9781450356152}, url={http://dx.doi.org/10.1145/3173386.3176916}, DOI={10.1145/3173386.3176916}, journal={Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction  - HRI  ’18}, publisher={ACM Press}, author={Hemminghaus, Jacqueline and Kopp, Stefan}, year={2018}}
"
An MARL-Based Distributed Learning Scheme for Capturing User Preferences in a Smart Environment,"Providing a personalized service to a user in a smart environment has been one of the key goals in the area of pervasive computing. The proliferation of individually developed smart devices in the name of Internet of Things opens up a possibility of providing personalized services to a user in an autonomous and distributed manner. As a user's task often involves services supported by multiple devices, capturing a device-specific service preference is not enough to maximize a user's comfort. In this paper, we propose a distributed learning scheme for capturing multiple device service preferences in a smart environment. We exploit multi-agent reinforcement learning (MARL) method where each smart device acts as a reinforcement learning agent to incrementally and cooperatively capture a user specific preference of a task. Experiments confirm that smart devices with the proposed scheme are able to capture multiple device service preferences from a small number of interactions with a user and an environment. Also, the proposed transfer learning method improves learning performance for a new task.",context aware;distributed learning;personalization;smart device;user preference,Smart Home,,,1,state representation,online,n,n,y,n,n,y,n,n,n,y,n,n,y,y,n,y,Q-Learning,2017 IEEE International Conference on Services Computing (SCC),J. Lim; H. Son; D. Lee; D. Lee,,,10.1109/SCC.2017.24,IEEE Conferences,2017-01-01,Lim_2017," @article{Lim_2017, title={An MARL-Based Distributed Learning Scheme for Capturing User Preferences in a Smart Environment}, ISBN={9781538620052}, url={http://dx.doi.org/10.1109/SCC.2017.24}, DOI={10.1109/scc.2017.24}, journal={2017 IEEE International Conference on Services Computing (SCC)}, publisher={IEEE}, author={Lim, Junsung and Son, Heesuk and Lee, Daekeun and Lee, Dongman}, year={2017}, month={Jun}}
"
Learning from demonstration: Teaching a myoelectric prosthesis with an intact limb via reinforcement learning,"Prosthetic arms should restore and extend the capabilities of someone with an amputation. They should move naturally and be able to perform elegant, coordinated movements that approximate those of a biological arm. Despite these objectives, the control of modern-day prostheses is often nonintuitive and taxing. Existing devices and control approaches do not yet give users the ability to effect highly synergistic movements during their daily-life control of a prosthetic device. As a step towards improving the control of prosthetic arms and hands, we introduce an intuitive approach to training a prosthetic control system that helps a user achieve hard-to-engineer control behaviours. Specifically, we present an actor-critic reinforcement learning method that for the first time promises to allow someone with an amputation to use their non-amputated arm to teach their prosthetic arm how to move through a wide range of coordinated motions and grasp patterns. We evaluate our method during the myoelectric control of a multi-joint robot arm by non-amputee users, and demonstrate that by using our approach a user can train their arm to perform simultaneous gestures and movements in all three degrees of freedom in the robot's hand and wrist based only on information sampled from the robot and the user's above-elbow myoelectric signals. Our results indicate that this learning-from-demonstration paradigm may be well suited to use by both patients and clinicians with minimal technical knowledge, as it allows a user to personalize the control of his or her prosthesis without having to know the underlying mechanics of the prosthetic limb. These preliminary results also suggest that our approach may extend in a straightforward way to next-generation prostheses with precise finger and wrist control, such that these devices may someday allow users to perform fluid and intuitive movements like playing the piano, catching a ball, and comfortably shaking hands.",,Health,,IEEE Explore,1,not used,online,n,n,y,n,n,y,y,n,y,n,y,n,y,n,n,y,Actor-Critic,2017 International Conference on Rehabilitation Robotics (ICORR),G. Vasan; P. M. Pilarski,,,10.1109/ICORR.2017.8009453,IEEE Conferences,2017-01-01,Vasan_2017," @article{Vasan_2017, title={Learning from demonstration: Teaching a myoelectric prosthesis with an intact limb via reinforcement learning}, ISBN={9781538622964}, url={http://dx.doi.org/10.1109/ICORR.2017.8009453}, DOI={10.1109/icorr.2017.8009453}, journal={2017 International Conference on Rehabilitation Robotics (ICORR)}, publisher={IEEE}, author={Vasan, Gautham and Pilarski, Patrick M.}, year={2017}, month={Jul}}
"
Adaptive treatment allocation using sub-sampled Gaussian processes,"Personalized medicine targets the customization of treatment strategies to patients' individual characteristics. Here we consider the problem of optimizing personalized pharmacological treatment strategies for cancer. We focus primarily on developing effective strategies to collect the data necessary for the construction of personalized treatments. We formulate this problem as a contextual bandit and present a new algorithm based on repeated sub-sampling for robust data collection in this framework. We present a case study showing experiments on a simulation setting, built from real data collected in a previous animal experiments. Promising results in this case study have since lead us to deploy this strategy in a partner wet lab to allocate treatments for the next phase of animal experiments. Copyright © 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,Health,,SCOPUS,1/person,state representation,batch,y,y,n,y,y,n,n,n,y,n,n,n,n,n,n,y,Contextual Bandits,AAAI Fall Symposium - Technical Report,"Durand A., Pineau J.","Durand, A., Department of Electrical Engineering and Computer Engineering Universite, Laval, QC, Canada; Pineau, J., School of Computer Science, McGill University, Montreal, Canada",,,Conference Paper,2015-01-01,durand2015adaptive,"@inproceedings{durand2015adaptive,
  title={Adaptive treatment allocation using sub-sampled gaussian processes},
  author={Durand, Audrey and Pineau, Joelle},
  booktitle={2015 AAAI Fall Symposium Series},
  year={2015}
}
"
Using Options to Accelerate Learning of New Tasks According to Human Preferences,"Over the years, people need to incorporate a wider range of information and multiple objectives for their decision making. Nowadays, humans are dependent on computer systems to interpret and take profit from the huge amount of available data on the Internet. Hence, varied services, such as location- based systems, must combine a huge quantity of raw data to give the desired response to the user. However, as humans have different preferences, the optimal answer is different for each user profile, and few systems offer the service of solving tasks in a customized manner for each user. Reinforcement Learning (RL) has been used to autonomously train systems to solve (or assist on) decision-making tasks according to user preferences. However, the learning process is very slow and require many interactions with the environment. Therefore, we here propose to reuse knowledge from previous tasks to accelerate the learning process in a new task. Our proposal, called Multiobjective Options, accelerates learning while providing a customized solution according to the current user preferences. Our experiments in the Tourist World Domain show that our proposal learns faster and better than regular learning, and that the achieved solutions follow user preferences. © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,Transport,,SCOPUS,1/person,state representation,online,y,n,n,y,n,n,n,y,y,y,n,n,n,y,n,y,"Multiobjective Options (MO-Opt),Q-Learning",AAAI Workshop - Technical Report,"Bonini R.C., Da Silva F.L., Spina E., Costa A.H.R.","Bonini, R.C., Escola Politdcnica da Universidade de São Paulo, São Paulo, Brazil; Da Silva, F.L., Escola Politdcnica da Universidade de São Paulo, São Paulo, Brazil; Spina, E., Escola Politdcnica da Universidade de São Paulo, São Paulo, Brazil; Costa, A.H.R., Escola Politdcnica da Universidade de São Paulo, São Paulo, Brazil",,,Conference Paper,2017-01-01,Peng2004AutomaticNA_0,"@inproceedings{Peng2004AutomaticNA_0,
  title={Automatic Navigation Among Mobile DTV Services},
  author={Chengyuan Peng and Petri Vuorimaa},
  booktitle={ICEIS},
  year={2004}
}"
Contextual multi-armed bandit algorithms for personalized learning action selection,"Optimizing the selection of learning resources and practice questions to address each individual student's needs has the potential to improve students' learning efficiency. In this paper, we study the problem of selecting a personalized learning action for each student (e.g. watching a lecture video, working on a practice question, etc.), based on their prior performance, in order to maximize their learning outcome. We formulate this problem using the contextual multi-armed bandits framework, where students' prior concept knowledge states (estimated from their responses to questions in previous assessments) correspond to contexts, the personalized learning actions correspond to arms, and their performance on future assessments correspond to rewards. We propose three new Bayesian policies to select personalized learning actions for students that each exhibits advantages over prior work, and experimentally validate them using real-world datasets.",contextual bandits;personalized learning,Education,,IEEE Xplore,1/group,state representation,batch,n,y,n,n,y,n,n,n,n,y,n,n,n,n,n,y,Contextual Bandits,"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",I. Manickam; A. S. Lan; R. G. Baraniuk,,,10.1109/ICASSP.2017.7953377,IEEE Conferences,2017-01-01,Manickam_2017," @article{Manickam_2017, title={Contextual multi-armed bandit algorithms for personalized learning action selection}, ISBN={9781509041176}, url={http://dx.doi.org/10.1109/ICASSP.2017.7953377}, DOI={10.1109/icassp.2017.7953377}, journal={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, publisher={IEEE}, author={Manickam, Indu and Lan, Andrew S. and Baraniuk, Richard G.}, year={2017}, month={Mar}}
"
Personalized response generation by Dual-learning based domain adaptation,"Open-domain conversation is one of the most challenging artificial intelligence problems, which involves language understanding, reasoning, and the utilization of common sense knowledge. The goal of this paper is to further improve the response generation, using personalization criteria. We propose a novel method called PRGDDA (Personalized Response Generation by Dual-learning based Domain Adaptation) which is a personalized response generation model based on theories of domain adaptation and dual learning. During the training procedure, PRGDDA first learns the human responding style from large general data (without user-specific information), and then fine-tunes the model on a small size of personalized data to generate personalized conversations with a dual learning mechanism. We conduct experiments to verify the effectiveness of the proposed model on two real-world datasets in both English and Chinese. Experimental results show that our model can generate better personalized responses for different users. © 2018 Elsevier Ltd",Deep reinforcement learning; Domain adaptation; Dual learning; Personalized response generation,Domain Independent,,SCOPUS,1,state representation,online,n,y,n,n,y,y,y,y,n,y,n,y,y,y,n,y,Policy Gradient,Neural Networks,"Yang M., Tu W., Qu Q., Zhao Z., Chen X., Zhu J.","Yang, M., Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Tu, W., School of Information Management and Engineering, Shanghai University of Finance and Economics, Shanghai, China; Qu, Q., Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Zhao, Z., School of Computing Science, Zhejiang University, Hangzhou, China; Chen, X., College of Computer Science and Software, Shenzhen University, Shenzhen, China; Zhu, J., School of Computer Science, South China Normal University, Guangzhou, China",,10.1016/j.neunet.2018.03.009,Article,2018-01-01,Yang_2018," @article{Yang_2018, title={Personalized response generation by Dual-learning based domain adaptation}, volume={103}, ISSN={0893-6080}, url={http://dx.doi.org/10.1016/j.neunet.2018.03.009}, DOI={10.1016/j.neunet.2018.03.009}, journal={Neural Networks}, publisher={Elsevier BV}, author={Yang, Min and Tu, Wenting and Qu, Qiang and Zhao, Zhou and Chen, Xiaojun and Zhu, Jia}, year={2018}, month={Jul}, pages={72–82}}
"
A self-adaptive system for improving autonomy and public spaces accessibility for elderly,"Nowadays, there is an increasing need to provide a safe and independent living for cognitively deficient population. Notably, we have to improve seniors’ autonomy and their public spaces accessibility. Giving these observations, the aim of this paper is to provide a personalized adaptive assisting system for elderly. More precisely, this paper presents the specification and implementation of a self-organizing multi-agent system able to abstract the different distributed components involved in user’s environment. This system is able to detect different possible situations that a user could face in his daily outdoors activities and propose accordingly appropriate actions. This system not only learns user’s habits from its perceptions but also improves its recommendations thanks to feedbacks provided by stakeholders (family, doctors …) following a reinforcement learning reasoning. Finally, we present our system evaluation specially its learning capabilities through different scenarios that have been generated automatically. © Springer International Publishing AG 2018.",AMAS theory; Assisted living system; Multi-agent system; Reinforcement learning,Health,,SCOPUS,1/person,not used,unknown,y,n,n,y,n,n,n,n,n,y,n,n,n,n,n,y,"RL, not further specified","Smart Innovation, Systems and Technologies","Triki S., Hanachi C.","Triki, S., 118 Route de Narbonne, Toulouse, France; Hanachi, C., 2 Rue du Doyen-Gabriel-Marty, Toulouse, France",,10.1007/978-3-319-59394-4_6,Conference Paper,2018-01-01,Triki_2017," @article{Triki_2017, title={A Self-adaptive System for Improving Autonomy and Public Spaces Accessibility for Elderly}, ISBN={9783319593944}, ISSN={2190-3026}, url={http://dx.doi.org/10.1007/978-3-319-59394-4_6}, DOI={10.1007/978-3-319-59394-4_6}, journal={Smart Innovation, Systems and Technologies}, publisher={Springer International Publishing}, author={Triki, Sameh and Hanachi, Chihab}, year={2017}, month={May}, pages={53–66}}
"
Adaptive interventions treatment modelling and regimen optimization using Sequential Multiple Assignment Randomized Trials (SMART) and Q-learning,"Nowadays, pharmacological practices are focused on a single best treatment to treat a disease which sounds impractical as the same treatment may not work the same way for every patient. Thus, there is a need of shift towards more patient-centric rather than disease-centric approach, in which personal characteristics of a patient or biomarkers are used to determine the tailored optimal treatment. The ""one size fits all"" concept is contradicted by research area of personalized medicine. The Sequential Multiple Assignment Randomized Trial (SMART) is a multi-stage trials to inform the development of dynamic treatment regimens (DTR's). In SMART, a subject is randomized through different stages of treatment where each stage corresponds to a treatment decision. These types of adaptive interventions are individualized and are repeatedly adjusted across time based on patient's individual clinical characteristics and ongoing performance. The reinforcement learning (Q-learning), a computational algorithm for optimization of treatment regimens to maximize desired clinical outcome is used in optimizing the sequence of treatments. This statistical model contains regression analysis for function approximation of data from clinical trials. The model will predict a series of regimens across time, depending on the biomarkers of a new participant for optimizing the weight management decision rules.",Dynamic treatment regimens (DTR); Personalized medicine; Q-learning algorithm; Regression analysis; Sequential Multiple Assignment Randomized Trial (SMART),Health,,SCOPUS,1,state representation,unknown,n,y,n,n,y,n,y,n,n,y,n,n,y,n,n,y,Q-Learning,67th Annual Conference and Expo of the Institute of Industrial Engineers 2017,"Baniya A., Herrmann S., Qiao Q., Lu H.","Baniya, A., Department of Electrical Engineering and Computer Science, United States, Department of Construction and Operations Management, South Dakota State University, United States; Herrmann, S., Sanford Health Research SD, United States; Qiao, Q., Department of Electrical Engineering and Computer Science, United States; Lu, H., Department of Construction and Operations Management, South Dakota State University, United States",,,Conference Paper,2017-01-01,Baniya2018AdaptiveIT,"@inproceedings{Baniya2018AdaptiveIT,
  title={Adaptive Interventions Treatment Modelling and Regimen Optimization Using Sequential Multiple Assignment Randomized Trials (Smart) and Q-Learning},
  author={Abiral Baniya},
  year={2018}
}"
Automatic navigation among mobile DTV services,"Limited number of input buttons on a mobile device, such as mobile phones and PDAs, restricts people's access to digital broadcast services. In this paper, we present a reinforcement learning approach to automatically navigating among services in mobile digital television systems. Our approach uses standard Q-learning algorithm as a theory basis to predict next button for the user by learning usage patterns from interaction experiences. We did the experiment using a modified algorithm in test system. The experimental results demonstrate that the performance is good and the method is feasible and appropriate in practice.",Automatic navigation; Button prediction; Exploration; Intelligent user interface; Reinforcement learning,Entertainment,,y,1/person,not used,online,n,n,y,n,n,y,y,n,y,y,n,n,y,y,n,y,Q-Learning,ICEIS 2004 - Proceedings of the Sixth International Conference on Enterprise Information Systems,"Peng C., Vuorimaa P.","Peng, C., Telecom. Software Multimedia Lab., Dept. of Compter Sci. and Eng., Helsinki University of Technology, P.O. Box 5400, FIN-02015, Finland; Vuorimaa, P., Telecom. Software Multimedia Lab., Dept. of Compter Sci. and Eng., Helsinki University of Technology, P.O. Box 5400, FIN-02015, Finland",,,Conference Paper,2004-01-01,Peng2004AutomaticNA,"@inproceedings{Peng2004AutomaticNA,
  title={Automatic Navigation Among Mobile DTV Services},
  author={Chengyuan Peng and Petri Vuorimaa},
  booktitle={ICEIS},
  year={2004}
}"
Self-learning system for personalized e-learning,"Knowledge is a basic requirement in the era of globalization. The act of acquiring knowledge is learning and with the advent of technology, learning has become easier. E-Learning is a popular learning method which uses the web or other technologies to promote learning. A very traditional learning method is discussed and some new methods with the integration of modern technology have been discussed. Although technologies like artificial intelligence, cloud computing, ontology, fuzzy logic etc. have been used in learning systems they are still less automated, not personalized and do not pose the capability to self-learn. A method is proposed to increase the automation and self-learning in the e-learning management systems. Further, the personalization of learning for the user so that every type of learner can learn from the type and level of the content the learner seems fit is the main aim. The proposed system is feasible, economical and scalable which makes it suitable for every level.",Machine learning;e-learning system;learning management system;reinforcement learning,Education,,IEEE Xplore,1,not used,unknown,n,n,n,n,n,n,n,n,n,n,n,n,n,n,n,y,"RL, not further specified",2017 International Conference on Emerging Trends in Computing and Communication Technologies (ICETCCT),V. Pant; S. Bhasin; S. Jain,,,10.1109/ICETCCT.2017.8280344,IEEE Conferences,2017-01-01,Pant_2017," @article{Pant_2017, title={Self-learning system for personalized e-learning}, ISBN={9781538611470}, url={http://dx.doi.org/10.1109/ICETCCT.2017.8280344}, DOI={10.1109/icetcct.2017.8280344}, journal={2017 International Conference on Emerging Trends in Computing and Communication Technologies (ICETCCT)}, publisher={IEEE}, author={Pant, Vishal and Bhasin, Shivansh and Jain, Subhi}, year={2017}, month={Nov}}
"
Optimal learning control of oxygen saturation using a policy iteration algorithm and a proof-of-concept in an interconnecting three-tank system,"In this work, “policy iteration algorithm” (PIA) is applied for controlling arterial oxygen saturation that does not require mathematical models of the plant. This technique is based on nonlinear optimal control to solve the Hamilton–Jacobi–Bellman equation. The controller is synthesized using a state feedback configuration based on an unidentified model of complex pathophysiology of pulmonary system in order to control gas exchange in ventilated patients, as under some circumstances (like emergency situations), there may not be a proper and individualized model for designing and tuning controllers available in time. The simulation results demonstrate the optimal control of oxygenation based on the proposed PIA by iteratively evaluating the Hamiltonian cost functions and synthesizing the control actions until achieving the converged optimal criteria. Furthermore, as a practical example, we examined the performance of this control strategy using an interconnecting three-tank system as a real nonlinear system. © 2016 Elsevier Ltd",Biomedical control system; Closed-loop ventilation; Control of oxygen saturation; Optimal control; Policy iteration algorithm; Reinforcement learning,Health,,SCOPUS,1/person,other,unknown,y,n,n,y,n,n,n,n,n,y,y,n,n,n,n,y,Policy Iteration,Control Engineering Practice,"Pomprapa A., Leonhardt S., Misgeld B.J.E.","Pomprapa, A., Philips Chair for Medical Information Technology, RWTH Aachen University, Aachen, Germany; Leonhardt, S., Philips Chair for Medical Information Technology, RWTH Aachen University, Aachen, Germany; Misgeld, B.J.E., Philips Chair for Medical Information Technology, RWTH Aachen University, Aachen, Germany",,10.1016/j.conengprac.2016.07.014,Article,2017-01-01,Pomprapa_2017," @article{Pomprapa_2017, title={Optimal learning control of oxygen saturation using a policy iteration algorithm and a proof-of-concept in an interconnecting three-tank system}, volume={59}, ISSN={0967-0661}, url={http://dx.doi.org/10.1016/j.conengprac.2016.07.014}, DOI={10.1016/j.conengprac.2016.07.014}, journal={Control Engineering Practice}, publisher={Elsevier BV}, author={Pomprapa, Anake and Leonhardt, Steffen and Misgeld, Berno J.E.}, year={2017}, month={Feb}, pages={194–203}}
"
Deep Reinforcement Learning for Dynamic Treatment Regimes on Medical Registry Data,"In this paper, we propose the first deep reinforce-ment learning framework to estimate the optimal Dynamic Treat-ment Regimes from observational medical data. This framework is more flexible and adaptive for high dimensional action and state spaces than existing reinforcement learning methods to model real life complexity in heterogeneous disease progression and treatment choices, with the goal to provide doctor and patients the data-driven personalized decision recommendations. The proposed deep reinforcement learning framework contains a supervised learning step to predict the most possible expert actions; and a deep reinforcement learning step to estimate the long term value function of Dynamic Treatment Regimes. We motivated and implemented the proposed framework on a data set from the Center for International Bone Marrow Transplant Research (CIBMTR) registry database, focusing on the sequence of prevention and treatments for acute and chronic graft versus host disease. We showed results of the initial implementation that demonstrates promising accuracy in predicting human expert decisions and initial implementation for the reinforcement learning step.",,Health,,IEEE Xplore,1,state representation,online,n,y,n,n,y,n,n,n,n,y,y,n,y,n,n,y,Deep Reinforcement Learning,2017 IEEE International Conference on Healthcare Informatics (ICHI),Y. Liu; B. Logan; N. Liu; Z. Xu; J. Tang; Y. Wang,,,10.1109/ICHI.2017.45,IEEE Conferences,2017-01-01,Liu_2017," @article{Liu_2017, title={Deep Reinforcement Learning for Dynamic Treatment Regimes on Medical Registry Data}, ISBN={9781509048816}, url={http://dx.doi.org/10.1109/ICHI.2017.45}, DOI={10.1109/ichi.2017.45}, journal={2017 IEEE International Conference on Healthcare Informatics (ICHI)}, publisher={IEEE}, author={Liu, Ying and Logan, Brent and Liu, Ning and Xu, Zhiyuan and Tang, Jian and Wang, Yangzhi}, year={2017}, month={Aug}}
"
Personalized response generation via domain adaptation,"In this paper, we propose a novel personalized response generation model via domain adaptation (PRG-DM). First, we learn the human responding style from large general data (without user-specific information). Second, we fine tune the model on a small size of personalized data to generate personalized responses with a dual learning mechanism. Moreover, we propose three new rewards to characterize good conversations that are personalized, informative and grammatical. We employ the policy gradient method to generate highly rewarded responses. Experimental results show that our model can generate better personalized responses for different users. © 2017 Copyright held by the owner/author(s).",Domain adaptation; Reinforcement learning; Response generation,Domain Independent,,SCOPUS,1,state representation,online,n,y,n,n,y,y,y,y,n,y,n,y,y,y,n,y,Policy Gradient,SIGIR 2017 - Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,"Yang M., Zhao Z., Zhao W., Chen X., Zhu J., Zhou L., Cao Z.","Yang, M., Tencent AI Lab, United States; Zhao, Z., Zhejiang University, China; Zhao, W., Tencent, China; Chen, X., Normal University, China; Zhu, J., IIE, Chinese Academy of Sciences, China; Zhou, L., South China Normal University, China; Cao, Z., Tencent AI Lab, United States",,10.1145/3077136.3080706,Conference Paper,2017-01-01,Yang_2017," @article{Yang_2017, title={Personalized Response Generation via Domain adaptation}, ISBN={9781450350228}, url={http://dx.doi.org/10.1145/3077136.3080706}, DOI={10.1145/3077136.3080706}, journal={Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval  - SIGIR  ’17}, publisher={ACM Press}, author={Yang, Min and Zhao, Zhou and Zhao, Wei and Chen, Xiaojun and Zhu, Jia and Zhou, Lianqiang and Cao, Zigang}, year={2017}}
"
Use of reinforcement learning in two real applications,"In this paper, we present two sucessful applications of Reinforcement Learning (RL) in real life. First, the optimization of anemia management in patients undergoing Chronic Renal Failure is presented. The aim is to individualize the treatment (Erythropoietin dosages) in order to stabilize patients within a targeted range of Hemoglobin (Hb). Results show that the use of RL increases the ratio of patients within the desired range of Hb. Thus, patients' quality of life is increased, and additionally, Health Care System reduces its expenses in anemia management. Second, RL is applied to modify a marketing campaign in order to maximize long-term profits. RL obtains an individualized policy depending on customer characteristics that increases long-term profits at the end of the campaign. Results in both problems show the robustness of the obtained policies and suggest their use in other real-life problems. © 2008 Springer Berlin Heidelberg.",,Domain Independent,,SCOPUS,1,state representation,batch,n,y,n,n,y,n,y,n,n,n,y,y,n,n,n,y,Q-Learning,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"Martín-Guerrero J.D., Soria-Olivas E., Martínez-Sober M., Serrrano-López A.J., Magdalena-Benedito R., Gómez-Sanchis J.","Martín-Guerrero, J.D., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain; Soria-Olivas, E., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain; Martínez-Sober, M., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain; Serrrano-López, A.J., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain; Magdalena-Benedito, R., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain; Gómez-Sanchis, J., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain",,10.1007/978-3-540-89722-4_15,Conference Paper,2008-01-01,Mart_n_Guerrero_2008," @article{Mart_n_Guerrero_2008, title={Use of Reinforcement Learning in Two Real Applications}, ISBN={9783540897224}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-540-89722-4_15}, DOI={10.1007/978-3-540-89722-4_15}, journal={Recent Advances in Reinforcement Learning}, publisher={Springer Berlin Heidelberg}, author={Martín-Guerrero, José D. and Soria-Olivas, Emilio and Martínez-Sober, Marcelino and Serrrano-López, Antonio J. and Magdalena-Benedito, Rafael and Gómez-Sanchis, Juan}, year={2008}, pages={191–204}}
"
Interactive learning and adaptation for robot assisted therapy for people with dementia,"In this paper, we present an adaptive cognitive music game designed to monitor and improve the attention levels of peo- ple with dementia. The goal of this game is to provide a customized protocol based on user needs and preferences, following the Reinforcement Learning (RL) framework. The game adjusts its parameters (e.g., diffculty level) so as to help the user complete the task successfully, while keeping them engaged. The main contribution of this paper is an interactive learning and adaptation framework that enables and facilitates the adaptation of the robot behavior towards new users, providing a safe, tailored and effcient interac- tion. Copyright 2016 is held by the owner/author(s).",Interactive Reinforcement Learning; Music Therapy; Policy Adaptation; Robot Assisted Therapy; Robot Learning and Behavior Adaptation,Health,,SCOPUS,1,state representation,unknown,n,n,n,n,n,n,n,n,n,y,n,n,n,n,n,y,Interactive Reinforcement Learning,ACM International Conference Proceeding Series,"Tsiakas K., Abellanoza C., Makedon F.","Tsiakas, K., CSE Department, HERACLEIA Lab, University of Texas at Arlington, United States; Abellanoza, C., Department of Psychology, Cognitive Neuroscience of Memory Lab, University of Texas at Arlington, United States; Makedon, F., CSE Department, HERACLEIA Lab, University of Texas at Arlington, United States",,10.1145/2910674.2935849,Conference Paper,2016-01-01,Tsiakas_2016," @article{Tsiakas_2016, title={Interactive Learning and Adaptation for Robot Assisted Therapy for People with Dementia}, ISBN={9781450343374}, url={http://dx.doi.org/10.1145/2910674.2935849}, DOI={10.1145/2910674.2935849}, journal={Proceedings of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments - PETRA  ’16}, publisher={ACM Press}, author={Tsiakas, Konstantinos and Abellanoza, Cheryl and Makedon, Fillia}, year={2016}}
"
Towards learning reward functions from user interactions,"In the physical world, people have dynamic preferences, e.g., the same situation can lead to satisfaction for some humans and to frustration for others. Personalization is called for. The same observation holds for online behavior with interactive systems. It is natural to represent the behavior of users who are engaging with interactive systems such as a search engine or a recommender system, as a sequence of actions where each next action depends on the current situation and the user reward of taking a particular action. By and large, current online evaluation metrics for interactive systems such as search engines or recommender systems, are static and do not reflect differences in user behavior. They rarely capture or model the reward experienced by a user while interacting with an interactive system.We argue that knowing a user's reward function is essential for an interactive system as both for learning and evaluation. We propose to learn users' reward functions directly from observed interaction traces. In particular, we present how users' reward functions can be uncovered directly using inverse reinforcement learning techniques. We also show how to incorporate user features into the learning process. Our main contribution is a novel and dynamic approach to restore a user's reward function. We present an analytic approach to this problem and complement it with initial experiments using the interaction logs of a cultural heritage institution that demonstrate the feasibility of the approach by uncovering different reward functions for different user groups. © 2017 Copyright held by the owner/author(s).",Interactive systems; Inverse reinforcement learning; Online evaluation,Commerce,,SCOPUS,1/group,state representation,batch,n,y,n,n,y,n,n,n,n,y,n,n,y,y,n,y,Inverse Reinforcement Learning,ICTIR 2017 - Proceedings of the 2017 ACM SIGIR International Conference on the Theory of Information Retrieval,"Li Z., Kiseleva J., De Rijke M., Grotov A.","Li, Z., University of Amsterdam, Amsterdam, Netherlands; Kiseleva, J., UserSat.com, University of Amsterdam, Amsterdam, Netherlands; De Rijke, M., University of Amsterdam, Amsterdam, Netherlands; Grotov, A., University of Amsterdam, Amsterdam, Netherlands",,10.1145/3121050.3121098,Conference Paper,2017-01-01,Li_2017," @article{Li_2017, title={Towards Learning Reward Functions from User Interactions}, ISBN={9781450344906}, url={http://dx.doi.org/10.1145/3121050.3121098}, DOI={10.1145/3121050.3121098}, journal={Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval  - ICTIR  ’17}, publisher={ACM Press}, author={Li, Ziming and Kiseleva, Julia and de Rijke, Maarten and Grotov, Artem}, year={2017}}
"
Making better recommendations with online profiling agents,"In recent years, we have witnessed the success of autonomous agents applying machine learning techniques across a wide range of applications. However, agents applying the same machine learning techniques in online applications have not been so successful. Even agent-based hybrid recommender systems that combine information filtering techniques with collaborative filtering techniques have only been applied with considerable success to simple consumer goods such as movies, books, clothing and food. Complex, adaptive autonomous agent systems that can handle complex goods such as real estate, vacation plans, insurance, mutual funds, and mortgage have yet emerged. To a large extent, the reinforcement learning methods developed to aid agents in learning have been more successfully deployed in offline applications. The inherent limitations in these methods have rendered them somewhat ineffective in online applications. In this paper, we postulate that a small amount of prior knowledge and human-provided input can dramatically speed up online learning. We will demonstrate that our agent HumanE - with its prior knowledge or ""experiences"" about the real estate domain -can effectively assist users in identifying requirements, especially unstated ones, quickly and unobtrusively.",Electronic profiling; Experience; Inference; Intelligent agents; Interactive learning; Personalization; Reinforcement learning; User preferences,Commerce,,y,1,not used,online,n,n,y,n,n,y,n,n,n,y,n,n,n,y,n,n,"RL, not further specified",Proceedings of the National Conference on Artificial Intelligence,"Oh D., Tan C.L.","Oh, D., School of Computing, National University of Singapore, 3 Science Drive 2, Singapore 117543, Singapore; Tan, C.L., School of Computing, National University of Singapore, 3 Science Drive 2, Singapore 117543, Singapore",,,Conference Paper,2004-01-01,Oh2004MakingBR,"@article{Oh2004MakingBR,
  title={Making Better Recommendations with Online Profiling Agents},
  author={Danny Oh and Chew Lim Tan},
  journal={AI Magazine},
  year={2004},
  volume={26},
  pages={29-40}
}"
Reinforcement learning for game personalization on edge devices,"Good progress has been shown recently in the area of active learning, specifically, Reinforcement learning (RL). In this paper, the authors show how RL can be used to personalize games based on user-interaction with the game. The work uses Deep Q network models (DQN) and the open source framework OpenAI to build an RL model that is able to optimize the gamer's engagement level in a game. The authors define an example quantitative measure of gamer engagement and incorporate that into the DQN learning reward function. The gamer experience optimization is empirically demonstrated using a game of Pong. Simulation testing and analysis of results indicate adapted RL models increase engagement reward values, thus enhancing gamer experience. The contribution of this paper is twofold: (1) using RL, it paves the path for wider adaptation to user-behavior, starting with gaming, and (2) it shows analysis and feasibility of an RL algorithm on an edge device (Personal Computer) in real-time.",artificial intelligence;computer games;edge computing;game personalization;reinforcement learning,Entertainment,,y,1,not used,online,y,n,n,y,n,n,n,n,n,y,n,y,n,y,n,y,DQN,2018 International Conference on Information and Computer Technologies (ICICT),A. Bodas; B. Upadhyay; C. Nadiger; S. Abdelhak,,,10.1109/INFOCT.2018.8356853,IEEE Conferences,2018-01-01,Bodas_2018," @article{Bodas_2018, title={Reinforcement learning for game personalization on edge devices}, ISBN={9781538653845}, url={http://dx.doi.org/10.1109/INFOCT.2018.8356853}, DOI={10.1109/infoct.2018.8356853}, journal={2018 International Conference on Information and Computer Technologies (ICICT)}, publisher={IEEE}, author={Bodas, Anand and Upadhyay, Bhargav and Nadiger, Chetan and Abdelhak, Sherine}, year={2018}, month={Mar}}
"
MPRL: Multiple-Periodic Reinforcement Learning for difficulty adjustment in rehabilitation games,"Generally, the difficulty level of a therapeutic game is regulated manually by a therapist. However, home-based rehabilitation games require a technique for automatic difficulty adjustment. This paper proposes a personalized difficulty adjustment technique for a rehabilitation game that automatically regulates difficulty settings based on a patient's skills in real-time. To this end, ideas from reinforcement learning are used to dynamically adjust the difficulty of a game. We show that difficulty adjustment is a multiple-objective problem, in which some objectives might be evaluated at different periods. To address this problem, we propose and use Multiple-Periodic Reinforcement Learning (MPRL) that makes it possible to evaluate different objectives of difficulty adjustment in separate periods. The results of experiments show that MPRL outperforms traditional Multiple-Objective Reinforcement Learning (MORL) in terms of user satisfaction parameters as well as improving the movement skills of patients.",,Health,,IEEE Explore,1,not used,online,n,n,y,n,n,y,n,y,n,y,n,n,y,n,n,y,Multiple-Periodic Reinforcement Learning,2017 IEEE 5th International Conference on Serious Games and Applications for Health (SeGAH),Y. A. Sekhavat,,,10.1109/SeGAH.2017.7939260,IEEE Conferences,2017-01-01,Sekhavat_2017," @article{Sekhavat_2017, title={MPRL: Multiple-Periodic Reinforcement Learning for difficulty adjustment in rehabilitation games}, ISBN={9781509054824}, url={http://dx.doi.org/10.1109/SeGAH.2017.7939260}, DOI={10.1109/segah.2017.7939260}, journal={2017 IEEE 5th International Conference on Serious Games and Applications for Health (SeGAH)}, publisher={IEEE}, author={Sekhavat, Yoones A.}, year={2017}, month={Apr}}
"
Agent-based assistance in ambient assisted living through reinforcement learning and semantic technologies: (Short paper),"For impaired people, the conduction of certain daily life activities is problematic due to motoric and cognitive handicaps. For that reason, assistive agents in ambient assisted environments provide services that aim at supporting elderly and impaired people. However, these agents act in complex stochastic and indeterministic environments where the concrete effects of a performed action are usually unknown at design time. Furthermore, they have to perform varying tasks according to the user’s context and needs, wherefore an agent has to be flexible and able to recognize required capabilities in a certain situation in order to provide adequate, unobtrusive assistance. Hence, an expressive representation framework is required that relates user-specific impairments to required agent capabilities. This work presents an approach which (a) describes and links user impairments and capabilities using the formal, model-theoretic semantics expressed in OWL2 DL ontologies, (b) computes optimal policies through Reinforcement Learning and propagates these in an agent network. The presented approach improves the collaborative, personalized and adequate assistance of assistive agents and tailors the agent-based services to the user’s missing capabilities. © 2017, Springer International Publishing AG.",,Health,,y,1,not used,online,n,n,y,n,n,y,n,n,n,y,y,n,n,n,n,y,Q-Learning,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"Merkle N., Zander S.","Merkle, N., FZI Forschungszentrum Informatik am KIT, Information Process Engineering, Haid-und-Neu-Str. 10-14, Karlsruhe, Germany; Zander, S., Institute for Computer Science, University of Applied Sciences Darmstadt, Schöfferstrasse 8B, Darmstadt, Germany",,10.1007/978-3-319-69459-7_12,Conference Paper,2017-01-01,Merkle_2017," @article{Merkle_2017, title={Agent-Based Assistance in Ambient Assisted Living Through Reinforcement Learning and Semantic Technologies}, ISBN={9783319694597}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-319-69459-7_12}, DOI={10.1007/978-3-319-69459-7_12}, journal={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Merkle, Nicole and Zander, Stefan}, year={2017}, pages={180–188}}
"
Personalizing a Service Robot by Learning Human Habits from Behavioral Footprints,"For a domestic personal robot, personalized services are as important as predesigned tasks, because the robot needs to adjust the home state based on the operator's habits. An operator's habits are composed of cues, behaviors, and rewards. This article introduces behavioral footprints to describe the operator's behaviors in a house, and applies the inverse reinforcement learning technique to extract the operator's habits, represented by a reward function. We implemented the proposed approach with a mobile robot on indoor temperature adjustment, and compared this approach with a baseline method that recorded all the cues and behaviors of the operator. The result shows that the proposed approach allows the robot to reveal the operator's habits accurately and adjust the environment state accordingly. © 2015 THE AUTHORS. Published by Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company",behavioral footprints; habit learning; personalized robot,Health,,SCOPUS,1/person,state representation,online,y,n,y,y,n,y,y,y,n,y,y,n,n,n,n,y,"Inverse Reinforcement Learning,baseline methods",Engineering,"Li K., Meng M.Q.-H.","Li, K., California Institute of Technology, Pasadena, CA, United States; Meng, M.Q.-H., The Chinese University of Hong Kong, Hong Kong, China",,10.15302/J-ENG-2015024,Article,2015-01-01,Li_2015," @article{Li_2015, title={Personalizing a Service Robot by Learning Human Habits from Behavioral Footprints}, volume={1}, ISSN={2095-8099}, url={http://dx.doi.org/10.15302/J-ENG-2015024}, DOI={10.15302/j-eng-2015024}, number={1}, journal={Engineering}, publisher={Elsevier BV}, author={Li, Kun and Meng, Max Q.-H.}, year={2015}, month={Mar}, pages={079–084}}
"
Learning social relations for culture aware interaction,"Each person has their private physical and/or psychological area where they do not want to share with others during social interactions. This area gives them comfort about interactions and its size usually depends on various factors such as culture, personal traits, and acquaintanceship. This issue may also arise in case of human-robot interaction, especially when the robot is required to generate a socially competent interaction strategy toward people they are interacting with. Here, we propose a new robot exploration strategy to socially interact with people by considering the social relationship between the robot and each person. To that end, two definitions of interaction area are made: (1) Acceptable area allowed to be shared with other people and robots, and (2) Private area where a human does not want to be interfered by others. Based on these definitions, the robot can optimize the path to maximize the frequency/degree of visiting the acceptable area of each person and to minimize the frequency/degree of trespassing into the private area of them at the same time in an iterative way. In this paper, the social force model (SFM) of each person, based on the potential field concept, is designed by a fuzzy inference system and its parameter is optimized by the reinforcement learning model during interactions. We have shown that the proposed model can generate a suitable SFM of each person, which was quite similar to a ground truth model, allowing to plan a path to simultaneously optimize the two factors of interaction area, respectively.",,Domain Independent,,IEEE Xplore,1/person,state representation,online,y,n,n,y,n,n,n,n,n,n,y,n,y,y,n,n,R-Learning,2017 14th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI),P. Patompak; S. Jeong; I. Nilkhamhang; N. Y. Chong,,,10.1109/URAI.2017.7992879,IEEE Conferences,2017-01-01,Patompak_2017," @article{Patompak_2017, title={Learning social relations for culture aware interaction}, ISBN={9781509030569}, url={http://dx.doi.org/10.1109/URAI.2017.7992879}, DOI={10.1109/urai.2017.7992879}, journal={2017 14th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)}, publisher={IEEE}, author={Patompak, Pakpoom and Jeong, Sungmoon and Nilkhamhang, Itthisek and Chong, Nak Young}, year={2017}, month={Jun}}
"
Personalizing mobile fitness apps using reinforcement learning,"Despite the vast number of mobile fitness applications (apps) and their potential advantages in promoting physical activity, many existing apps lack behavior-change features and are not able to maintain behavior change motivation. This paper describes a novel fitness app called CalFit, which implements important behavior-change features like dynamic goal setting and self-monitoring. CalFit uses a reinforcement learning algorithm to generate personalized daily step goals that are challenging but attainable. We conducted the Mobile Student Activity Reinforcement (mSTAR) study with 13 college students to evaluate the efficacy of the CalFit app. The control group (receiving goals of 10,000 steps/day) had a decrease in daily step count of 1,520 (SD ± 740) between baseline and 10-weeks, compared to an increase of 700 (SD ± 830) in the intervention group (receiving personalized step goals). The difference in daily steps between the two groups was 2,220, with a statistically significant p = 0:039. © 2018 Copyright for the individual papers remains with the authors.",Fitness app; Goal setting; Interface design; Mobile app; Personalization; Physical activity,Health,,SCOPUS,1,not used,online,n,n,y,n,n,y,y,n,n,y,n,n,y,y,n,y,Inverse Reinforcement Learning,CEUR Workshop Proceedings,"Zhou M., Mintz Y., Fukuoka Y., Goldberg K., Flowers E., Kaminsky P., Castillejo A., Aswani A.","Zhou, M., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Mintz, Y., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Fukuoka, Y., Department of Physiological, Nursing Institute for Health and Aging, School of Nursing, University of California, San Francisco, CA, United States; Goldberg, K., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Flowers, E., Department of Physiological Nursing, School of Nursing, University of California, San Francisco, CA, United States; Kaminsky, P., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Castillejo, A., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Aswani, A., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States",,,Conference Paper,2018-01-01,Zhou2018PersonalizingMF,"@inproceedings{Zhou2018PersonalizingMF,
  title={Personalizing Mobile Fitness Apps using Reinforcement Learning},
  author={Mo Zhou and Yonatan Dov Mintz and Yoshimi Fukuoka and Kenneth Y. Goldberg and Elena Flowers and Philip Kaminsky and Alejandro Castillejo and Anil Aswani},
  booktitle={IUI Workshops},
  year={2018}
}"
Interactive narrative personalization with deep reinforcement learning,"Data-driven techniques for interactive narrative generation are the subject of growing interest. Reinforcement learning (RL) offers significant potential for devising data-driven interactive narrative generators that tailor players' story experiences by inducing policies from player interaction logs. A key open question in RL-based interactive narrative generation is how to model complex player interaction patterns to learn effective policies. In this paper we present a deep RL-based interactive narrative generation framework that leverages synthetic data produced by a bipartite simulated player model. Specifically, the framework involves training a set of Q-networks to control adaptable narrative event sequences with long short-term memory network-based simulated players. We investigate the deep RL framework's performance with an educational interactive narrative, Crystal Island. Results suggest that the deep RL-based narrative generation framework yields effective personalized interactive narratives.",,Education,,SCOPUS,1,state representation,batch,y,n,n,y,n,n,n,n,n,y,n,n,y,y,n,n,"DQN,DDQN,Q-Learning,Async QN,Async DQN",IJCAI International Joint Conference on Artificial Intelligence,"Wang P., Rowe J., Min W., Mott B., Lester J.","Wang, P., Department of Computer Science, North Carolina State University, Raleigh, NC, United States; Rowe, J., Department of Computer Science, North Carolina State University, Raleigh, NC, United States; Min, W., Department of Computer Science, North Carolina State University, Raleigh, NC, United States; Mott, B., Department of Computer Science, North Carolina State University, Raleigh, NC, United States; Lester, J., Department of Computer Science, North Carolina State University, Raleigh, NC, United States",,,Conference Paper,2017-01-01,Wang2017InteractiveNP,"@inproceedings{Wang2017InteractiveNP,
  title={Interactive Narrative Personalization with Deep Reinforcement Learning},
  author={Pengcheng Wang and Jonathan P. Rowe and Wookhee Min and Bradford W. Mott and James C. Lester},
  booktitle={IJCAI},
  year={2017}
}"
Multi-task learning for contextual bandits,"Contextual bandits are a form of multi-armed bandit in which the agent has access to predictive side information (known as the context) for each arm at each time step, and have been used to model personalized news recommendation, ad placement, and other applications. In this work, we propose a multi-task learning framework for contextual bandit problems. Like multi-task learning in the batch setting, the goal is to leverage similarities in contexts for different arms so as to improve the agent's ability to predict rewards from contexts. We propose an upper confidence bound-based multi-task learning algorithm for contextual bandits, establish a corresponding regret bound, and interpret this bound to quantify the advantages of learning in the presence of high task (arm) similarity. We also describe an effective scheme for estimating task similarity from data, and demonstrate our algorithm's performance on several data sets. © 2017 Neural information processing systems foundation. All rights reserved.",,Commerce,,SCOPUS,multiple,state representation,batch,n,y,n,n,y,n,y,n,n,n,n,n,n,n,n,y,UCB,Advances in Neural Information Processing Systems,"Deshmukh A.A., Dogan U., Scott C.","Deshmukh, A.A., Department of EECS, University of Michigan, Ann Arbor, Ann Arbor, MI, United States; Dogan, U., Microsoft Research, Cambridge, United Kingdom; Scott, C., Department of EECS, University of Michigan, Ann Arbor, Ann Arbor, MI, United States",,,Conference Paper,2017-01-01,Deshmukh2017MultiTaskLF,"@inproceedings{Deshmukh2017MultiTaskLF,
  title={Multi-Task Learning for Contextual Bandits},
  author={Aniket Anand Deshmukh and {\""U}r{\""u}n Dogan and Clayton Scott},
  booktitle={NIPS},
  year={2017}
}"
Path planning with user route preference - A reward surface approximation approach using orthogonal Legendre polynomials,"As self driving cars become more ubiquitous, users would look for natural ways of informing the car AI about their personal choice of routes. This choice is not always dictated by straightforward logic such as shortest distance or shortest time, and can be influenced by hidden factors, such as comfort and familiarity. This paper presents a path learning algorithm for such applications, where from limited positive demonstrations, an autonomous agent learns the user's path preference and honors that choice in its route planning, but has the capability to adopt alternate routes, if the original choice(s) become impractical. The learning problem is modeled as a Markov decision process. The states (way-points) and actions (to move from one way-point to another) are pre-defined according to the existing network of paths between the origin and destination and the user's demonstration is assumed to be a sample of the preferred path. The underlying reward function which captures the essence of the demonstration is computed using an inverse reinforcement learning algorithm and from that the entire path mirroring the expert's demonstration is extracted. To alleviate the problem of state space explosion when dealing with a large state space, the reward function is approximated using a set of orthogonal polynomial basis functions with a fixed number of coefficients regardless of the size of the state space. A six fold reduction in total learning time is achieved compared to using simple basis functions, that has dimensionality equal to the number of distinct states.",,Transport,,IEEE Xplore,1,not used,online,n,n,y,n,n,y,n,n,n,y,n,n,n,n,n,y,Inverse Reinforcement Learning,2016 IEEE International Conference on Automation Science and Engineering (CASE),A. R. Srinivasan; S. Chakraborty,,,10.1109/COASE.2016.7743527,IEEE Conferences,2016-01-01,Srinivasan_2016," @article{Srinivasan_2016, title={Path planning with user route preference - A reward surface approximation approach using orthogonal Legendre polynomials}, ISBN={9781509024094}, url={http://dx.doi.org/10.1109/COASE.2016.7743527}, DOI={10.1109/coase.2016.7743527}, journal={2016 IEEE International Conference on Automation Science and Engineering (CASE)}, publisher={IEEE}, author={Srinivasan, Aravinda Ramakrishnan and Chakraborty, Subhadeep}, year={2016}, month={Aug}}
"
Model-free machine learning in biomedicine: Feasibility study in type 1 diabetes,"Although reinforcement learning (RL) is suitable for highly uncertain systems, the applicability of this class of algorithms to medical treatment may be limited by the patient variability which dictates individualised tuning for their usually multiple algorithmic parameters. This study explores the feasibility of RL in the framework of artificial pancreas development for type 1 diabetes (T1D). In this approach, an Actor-Critic (AC) learning algorithm is designed and developed for the optimisation of insulin infusion for personalised glucose regulation. AC optimises the daily basal insulin rate and insulin:carbohydrate ratio for each patient, on the basis of his/her measured glucose profile. Automatic, personalised tuning of AC is based on the estimation of information transfer (IT) from insulin to glucose signals. Insulinto-glucose IT is linked to patient-specific characteristics related to total daily insulin needs and insulin sensitivity (SI). The AC algorithm is evaluated using an FDA-accepted T1D simulator on a large patient database under a complex meal protocol, meal uncertainty and diurnal SI variation. The results showed that 95.66% of time was spent in normoglycaemia in the presence of meal uncertainty and 93.02% when meal uncertainty and SI variation were simultaneously considered. The time spent in hypoglycaemia was 0.27% in both cases. The novel tuning method reduced the risk of severe hypoglycaemia, especially in patients with low SI. © 2016 Daskalaki et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,Communication,,SCOPUS,1,not used,unknown,n,y,n,y,n,n,n,n,n,y,y,y,y,n,n,y,Actor-Critic,PLoS ONE,"Daskalaki E., Diem P., Mougiakakou S.G.","Daskalaki, E., Diabetes Technology Research Group, ARTORG Center for Biomedical Engineering Research, University of Bern, Murtenstrasse 50, Bern, Switzerland; Diem, P., Division of Endocrinology, Diabetes and Clinical Nutrition, Bern University Hospital Inselspital, Bern, Switzerland; Mougiakakou, S.G., Diabetes Technology Research Group, ARTORG Center for Biomedical Engineering Research, University of Bern, Murtenstrasse 50, Bern, Switzerland, Division of Endocrinology, Diabetes and Clinical Nutrition, Bern University Hospital Inselspital, Bern, Switzerland",,10.1371/journal.pone.0158722,Article,2016-01-01,Daskalaki_2016," @article{Daskalaki_2016, title={Model-Free Machine Learning in Biomedicine: Feasibility Study in Type 1 Diabetes}, volume={11}, ISSN={1932-6203}, url={http://dx.doi.org/10.1371/journal.pone.0158722}, DOI={10.1371/journal.pone.0158722}, number={7}, journal={PLOS ONE}, publisher={Public Library of Science (PLoS)}, author={Daskalaki, Elena and Diem, Peter and Mougiakakou, Stavroula G.}, editor={Maedler, KathrinEditor}, year={2016}, month={Jul}, pages={e0158722}}
"
Synergies Between Evolutionary Computation and Multiagent Reinforcement Learning: The Benefits of Exchanging Solutions,"In many real-world situations in which resources are scarce, aligning the optimum of the system with the optimum of agents can be conflicting. For instance, in traffic assignment, the system's and the agents' welfare may not be aligned. In order to deal with this, in this paper a new approach is proposed, based on a synergy between: (i) a global optimization process in which the traffic authority employs metaheuristics, and (ii) reinforcement learning processes that run at each individual driver agent. Both the agents and the system authority exchange solutions that are incorporated by the other party in order to come up with an assignment of routes.","evolutionary computation, multiagent systems, reinforcement learning",Transport,,ACM-DL,1/person,not used,unknown,y,n,n,y,n,n,y,n,n,n,n,n,n,n,n,y,Q-Learning,Proceedings of the Genetic and Evolutionary Computation Conference Companion,Ana L. C. Bazzan,,,10.1145/3067695.3075970,article,2017-01-01,Bazzan_2017," @article{Bazzan_2017, title={Synergies between evolutionary computation and multiagent reinforcement learning}, ISBN={9781450349390}, url={http://dx.doi.org/10.1145/3067695.3075970}, DOI={10.1145/3067695.3075970}, journal={Proceedings of the Genetic and Evolutionary Computation Conference Companion on   - GECCO  ’17}, publisher={ACM Press}, author={Bazzan, Ana L. C.}, year={2017}}
"
A Contextual Bandits Framework for Personalized Learning Action Selection.,,,Education,,DBLP,1,not used,batch,n,y,n,n,y,n,n,y,y,n,n,n,y,n,n,y,"A-CLUB,CLUB",,"Andrew S. Lan, Richard G. Baraniuk",,,,Conference and Workshop Papers,2016-01-01,Lan2016ACB,"@inproceedings{Lan2016ACB,
  title={A Contextual Bandits Framework for Personalized Learning Action Selection},
  author={Andrew S. Lan and Richard G. Baraniuk},
  booktitle={EDM},
  year={2016}
}"
TacTex'13: A Champion Adaptive Power Trading Agent,"Sustainable energy systems of the future could no longer rely on the current paradigm that energy supply follows demand. Since many of the renewable energy resources do not produce power on demand, there is a need for new market structures that motivate sustainable behaviors by participants. The Power Trading Agent Competition(Power TAC) is a new annual competition that focuses on the design and operation of future retail power markets, specifically in smart grid environments with renewable energy production, smart metering, and autonomous agents acting on behalf of customers and retailers. It uses a rich, open-source simulation platform that is based on real-world data and state-of-the-art customer models. Its purpose is to help researchers understand the dynamics of customer and retailer decision-making, as well as the robustness of proposed market designs. This research contributes to the former, by introducing TacTex'13, the champion agent from the inaugural competition in 2013. TacTex'13 learns and adapts to the environment in which it operates, by heavily relying on reinforcement-learning and prediction methods. We formalize the complex decision-making problem that TacTex'13 faces, and approximate its solution in TacTex'13's constituent components. We examine the success of the complete agent through analysis of competition results.","energy trading, machine learning, smart-grid",Energy,,ACM-DL,1,not used,online,y,n,n,y,n,n,n,y,n,y,n,y,n,y,n,y,"RL, not further specified",Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems,Daniel  Urieli and Peter  Stone,,,,article,2014-01-01,Urieli2014TacTex13AC,"@inproceedings{Urieli2014TacTex13AC,
  title={TacTex'13: a champion adaptive power trading agent},
  author={Daniel Urieli and Peter Stone},
  booktitle={AAMAS},
  year={2014}
}"
An Efficient Bandit Algorithm for Realtime Multivariate Optimization,"Optimization is commonly employed to determine the content of web pages, such as to maximize conversions on landing pages or click-through rates on search engine result pages. Often the layout of these pages can be decoupled into several separate decisions. For example, the composition of a landing page may involve deciding which image to show, which wording to use, what color background to display, etc. Thus, optimization is a combinatorial problem over an exponentially large decision space. Randomized experiments do not scale well to this setting, and therefore, in practice, one is typically limited to optimizing a single aspect of a web page at a time. This represents a missed opportunity in both the speed of experimentation and the exploitation of possible interactions between layout decisions.","a/b testing, hill-climbing, multi-armed bandit, multivariate optimization",Commerce,,ACM-DL,1,not used,other,y,y,y,y,y,y,n,n,n,y,n,n,n,y,n,y,Contextual Bandits,Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,Daniel N. Hill and Houssam  Nassif and Yi  Liu and Anand  Iyer and S.V.N.  Vishwanathan,,,10.1145/3097983.3098184,article,2017-01-01,Hill_2017," @article{Hill_2017, title={An Efficient Bandit Algorithm for Realtime Multivariate Optimization}, ISBN={9781450348874}, url={http://dx.doi.org/10.1145/3097983.3098184}, DOI={10.1145/3097983.3098184}, journal={Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  - KDD  ’17}, publisher={ACM Press}, author={Hill, Daniel N. and Nassif, Houssam and Liu, Yi and Iyer, Anand and Vishwanathan, S.V.N.}, year={2017}}
"
Learning User Preferences by Observing User-Items Interactions in an IoT Augmented Space,,"cultural heritage, implicit feedback, internet of things, inverse reinforcement learning, museum, recommder systems, reinforcement learning, user modelling",Entertainment,,ACM-DL,1,not used,unknown,n,y,n,n,y,n,n,n,n,y,n,n,y,y,n,y,MLIRL,"Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization",David  Massimo and Mehdi  Elahi and Francesco  Ricci,,,10.1145/3099023.3099070,article,2017-01-01,Massimo_2017," @article{Massimo_2017, title={Learning User Preferences by Observing User-Items Interactions in an IoT Augmented Space}, ISBN={9781450350679}, url={http://dx.doi.org/10.1145/3099023.3099070}, DOI={10.1145/3099023.3099070}, journal={Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization  - UMAP  ’17}, publisher={ACM Press}, author={Massimo, David and Elahi, Mehdi and Ricci, Francesco}, year={2017}}
"
Directing Exploratory Search: Reinforcement Learning from User Interactions with Keywords,,"adaptive interfaces, data mining, information filtering, machine learning, recommender systems",Entertainment,,ACM-DL,1/person,not used,online,y,n,y,y,n,y,y,n,y,n,n,n,y,y,n,n,LinRel,Proceedings of the 2013 International Conference on Intelligent User Interfaces,Dorota  Glowacka and Tuukka  Ruotsalo and Ksenia  Konuyshkova and kumaripaba  Athukorala and Samuel  Kaski and Giulio  Jacucci,,,10.1145/2449396.2449413,article,2013-01-01,Glowacka_2013," @article{Glowacka_2013, title={Directing exploratory search}, ISBN={9781450319652}, url={http://dx.doi.org/10.1145/2449396.2449413}, DOI={10.1145/2449396.2449413}, journal={Proceedings of the 2013 international conference on Intelligent user interfaces - IUI  ’13}, publisher={ACM Press}, author={Glowacka, Dorota and Ruotsalo, Tuukka and Konuyshkova, Ksenia and Athukorala, kumaripaba and Kaski, Samuel and Jacucci, Giulio}, year={2013}}
"
DJ-MC: A Reinforcement-Learning Agent for Music Playlist Recommendation,"In recent years, there has been growing 
focus on the study of automated recommender systems. Music 
recommendation systems serve as a prominent domain for such works, both 
from an academic and a commercial perspective. A fundamental aspect of 
music perception is that music is experienced in temporal context and in
 sequence. In this work we present DJ-MC, a novel reinforcement-learning
 framework for music recommendation that does not recommend songs 
individually but rather song sequences, or playlists, based on a model 
of preferences for both songs and song transitions. The model is learned
 online and is uniquely adapted for each listener. To reduce exploration
 time, DJ-MC exploits user feedback to initialize a model, which it 
subsequently updates by reinforcement. We evaluate our framework with 
human participants using both real song and playlist data. Our results 
indicate that DJ-MC's ability to recommend sequences of songs provides a
 significant improvement over more straightforward approaches, which do 
not take transitions into account.
",,Entertainment,,ACM-DL,1,not used,online,n,y,n,y,n,y,y,y,y,n,n,n,n,n,n,y,"RL, not further specified",Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,Elad  Liebman and Maytal  Saar-Tsechansky and Peter  Stone,,,,article,2015-01-01,Liebman2015DJMCAR,"@inproceedings{Liebman2015DJMCAR,
  title={DJ-MC: A Reinforcement-Learning Agent for Music Playlist Recommendation},
  author={Elad Liebman and Peter Stone},
  booktitle={AAMAS},
  year={2015}
}"
Clustering Household Preferences in Local Electricity Markets,"The current hierarchical, fossil-fuel based energy system is shifting towards a sustainable system based on distributed renewable generation. Simultaneously, energy end consumers become increasingly important as active prosumers. Local electricity markets (LEMs), on which prosumers and consumers can trade electricity locally, enable sustainable, distributed local electricity balances with an active involvement of the end customers. However, trading needs to be automated, and specified to the household's specific preferences in terms of price and electricity source. We show how intelligent agent strategies can fulfill both objectives. To this end, we conduct a multi-agent simulation of a LEM between 100 households and a community storage in a merit order LEM. LEM agents maximize their individual utility via automated Erev-Roth reinforcement learning. The learning strategies take into account the households' individual electricity preferences. To this end, agent preferences are grouped into truly greens, price-sensitive greens, and non adopters. The evaluation of the strategies is based on the agents' revenues, costs and electricity source mix. It shows that reinforcement learning can represent household preferences on LEMs.","Local electricity market, household agents, intelligent agents, reinforcement learning",Energy,,ACM-DL,1/person,other,online,y,n,n,y,n,n,n,n,n,n,n,n,n,n,n,y,Erev-Roth,Proceedings of the Ninth International Conference on Future Energy Systems,Esther  Mengelkamp and Christof  Weinhardt,,,10.1145/3208903.3214348,article,2018-01-01,Mengelkamp_2018_0," @article{Mengelkamp_2018_0, title={Clustering Household Preferences in Local Electricity Markets}, ISBN={9781450357678}, url={http://dx.doi.org/10.1145/3208903.3214348}, DOI={10.1145/3208903.3214348}, journal={Proceedings of the Ninth International Conference on Future Energy Systems  - e-Energy  ’18}, publisher={ACM Press}, author={Mengelkamp, Esther and Weinhardt, Christof}, year={2018}}
"
Intelligent Agent Strategies for Residential Customers in Local Electricity Markets,,"Local electricity market, intelligent agents, reinforcement learning",Energy,,,1/person,other,online,y,n,n,y,n,n,n,y,n,y,n,y,n,y,n,y,Erev-Roth RL and extensions,Proceedings of the Ninth International Conference on Future Energy Systems,Esther  Mengelkamp and Johannes  G&#228;rttner and Christof  Weinhardt,,,10.1145/3208903.3208907,article,2018-01-01,Mengelkamp_2018," @article{Mengelkamp_2018, title={Intelligent Agent Strategies for Residential Customers in Local Electricity Markets}, ISBN={9781450357678}, url={http://dx.doi.org/10.1145/3208903.3208907}, DOI={10.1145/3208903.3208907}, journal={Proceedings of the Ninth International Conference on Future Energy Systems  - e-Energy  ’18}, publisher={ACM Press}, author={Mengelkamp, Esther and Gärttner, Johannes and Weinhardt, Christof}, year={2018}}
"
Learning User Preferences for Wireless Services Provisioning,,,Communication,,ACM-DL,1,not used,unknown,n,n,y,n,n,y,n,n,n,y,n,n,n,n,n,y,Multi-layer neural networks,Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 1,G.  Lee and S.  Bauer and P.  Faratin and J.  Wroclawski,,,10.1109/AAMAS.2004.161,article,2004-01-01,Lee2004LearningUP,"@article{Lee2004LearningUP,
  title={Learning user preferences for wireless services provisioning},
  author={George Lee and Steven Bauer and Peyman Faratin and John Wroclawski},
  journal={Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, 2004. AAMAS 2004.},
  year={2004},
  pages={480-487}
}"
Ad Recommendation Systems for Life-Time Value Optimization,"The main objective in the ad recommendation problem is to find a strategy that, for each visitor of the website, selects the ad that has the highest probability of being clicked. This strategy could be computed using supervised learning or contextual bandit algorithms, which treat two visits of the same user as two separate independent visitors, and thus, optimize greedily for a single step into the future. Another approach would be to use reinforcement learning (RL) methods, which differentiate between two visits of the same user and two different visitors, and thus, optimizes for multiple steps into the future or the life-time value (LTV) of a customer. While greedy methods have been well-studied, the LTV approach is still in its infancy, mainly due to two fundamental challenges: how to compute a good LTV strategy and how to evaluate a solution using historical data to ensure its “safety” before deployment. In this paper, we tackle both of these challenges by proposing to use a family of off-policy evaluation techniques with statistical guarantees about the performance of a new strategy. We apply these methods to a real ad recommendation problem, both for evaluating the final performance and for optimizing the parameters of the RL algorithm. Our results show that our LTV optimization algorithm equipped with these off-policy evaluation techniques outperforms the greedy approaches. They also give fundamental insights on the difference between the click through rate (CTR) and LTV metrics for performance evaluation in the ad recommendation problem.","ad recommendation, off-policy evaluation, reinforcement learning",Commerce,,ACM-DL,1,state representation,batch,n,y,n,n,y,n,n,n,n,y,n,n,y,y,n,n,Fitted Q-Iteration,Proceedings of the 24th International Conference on World Wide Web,Georgios  Theocharous and Philip S. Thomas and Mohammad  Ghavamzadeh,,,10.1145/2740908.2741998,article,2015-01-01,Theocharous_2015," @article{Theocharous_2015, title={Ad Recommendation Systems for Life-Time Value Optimization}, ISBN={9781450334730}, url={http://dx.doi.org/10.1145/2740908.2741998}, DOI={10.1145/2740908.2741998}, journal={Proceedings of the 24th International Conference on World Wide Web - WWW  ’15 Companion}, publisher={ACM Press}, author={Theocharous, Georgios and Thomas, Philip S. and Ghavamzadeh, Mohammad}, year={2015}}
"
DRN: A Deep Reinforcement Learning Framework for News Recommendation,,"deep Q-Learning, news recommendation, reinforcement learning",Commerce,,ACM-DL,1,state representation,batch,n,n,n,n,n,y,n,y,n,y,n,n,n,y,n,n,DDQN,Proceedings of the 2018 World Wide Web Conference,Guanjie  Zheng and Fuzheng  Zhang and Zihan  Zheng and Yang  Xiang and Nicholas Jing  Yuan and Xing  Xie and Zhenhui  Li,,,10.1145/3178876.3185994,article,2018-01-01,Zheng_2018," @article{Zheng_2018, title={DRN}, ISBN={9781450356398}, url={http://dx.doi.org/10.1145/3178876.3185994}, DOI={10.1145/3178876.3185994}, journal={Proceedings of the 2018 World Wide Web Conference on World Wide Web  - WWW  ’18}, publisher={ACM Press}, author={Zheng, Guanjie and Zhang, Fuzheng and Zheng, Zihan and Xiang, Yang and Yuan, Nicholas Jing and Xie, Xing and Li, Zhenhui}, year={2018}}
"
Automatic Computer Game Balancing: A Reinforcement Learning Approach,,"adaptive agents, game balancing, reinforcement learning",Entertainment,,ACM-DL,1,state representation,unknown,y,n,y,y,n,y,y,n,n,y,n,y,n,y,n,y,Q-Learning,Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multiagent Systems,Gustavo  Andrade and Geber  Ramalho and Hugo  Santana and Vincent  Corruble,,,10.1145/1082473.1082648,article,2005-01-01,Andrade_2005," @article{Andrade_2005, title={Automatic computer game balancing}, ISBN={1595930930}, url={http://dx.doi.org/10.1145/1082473.1082648}, DOI={10.1145/1082473.1082648}, journal={Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems  - AAMAS  ’05}, publisher={ACM Press}, author={Andrade, Gustavo and Ramalho, Geber and Santana, Hugo and Corruble, Vincent}, year={2005}}
"
Real-Time Robot Personality Adaptation Based on Reinforcement Learning and Social Signals,,"adaptation, dialog, introversion/extraversion, personality, reinforcement learning, social robotics, social signals",Entertainment,,ACM-DL,1/person,state representation,online,y,n,n,y,n,n,n,n,n,y,n,n,n,y,n,n,Q-Learning,Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction,Hannes  Ritschel and Elisabeth  Andr&#233;,,,10.1145/3029798.3038381,article,2017-01-01,Ritschel_2017," @article{Ritschel_2017, title={Real-Time Robot Personality Adaptation based on Reinforcement Learning and Social Signals}, ISBN={9781450348850}, url={http://dx.doi.org/10.1145/3029798.3038381}, DOI={10.1145/3029798.3038381}, journal={Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction - HRI  ’17}, publisher={ACM Press}, author={Ritschel, Hannes and André, Elisabeth}, year={2017}}
"
Optimal Testing for Crowd Workers,,"crowdsourcing, reinforcement learning",Domain Independent,,ACM-DL,1/person,state representation,online,y,n,y,y,n,y,y,y,n,y,y,y,y,y,n,n,EM,Proceedings of the 2016 International Conference on Autonomous Agents &#38; Multiagent Systems,Jonathan  Bragg and   Mausam and Daniel S. Weld,,,,article,2016-01-01,Bragg2016OptimalTF,"@inproceedings{Bragg2016OptimalTF,
  title={Optimal Testing for Crowd Workers},
  author={Jonathan Bragg and Mausam and Daniel S. Weld},
  booktitle={AAMAS},
  year={2016}
}"
Learning to Collaborate: Multi-Scenario Ranking via Multi-Agent Reinforcement Learning,,"joint optimization, learning to rank, multi-agent learning, reinforcement learning",Commerce,,ACM-DL,1,state representation,batch,y,n,n,y,n,n,n,n,n,y,n,n,n,y,n,n,MA-RDPG,Proceedings of the 2018 World Wide Web Conference,Jun  Feng and Heng  Li and Minlie  Huang and Shichen  Liu and Wenwu  Ou and Zhirong  Wang and Xiaoyan  Zhu,,,10.1145/3178876.3186165,article,2018-01-01,Feng_2018," @article{Feng_2018, title={Learning to Collaborate}, ISBN={9781450356398}, url={http://dx.doi.org/10.1145/3178876.3186165}, DOI={10.1145/3178876.3186165}, journal={Proceedings of the 2018 World Wide Web Conference on World Wide Web  - WWW  ’18}, publisher={ACM Press}, author={Feng, Jun and Li, Heng and Huang, Minlie and Liu, Shichen and Ou, Wenwu and Wang, Zhirong and Zhu, Xiaoyan}, year={2018}}
"
Personalizing a Dialogue System With Transfer Reinforcement Learning.,"It is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient. Personalized dialogue systems trained on a small dataset can overfit and make it difficult to adapt to different user needs. One way to solve this problem is to consider a collection of multiple users' data as a source domain and an individual user's data as a target domain, and to perform a transfer learning from the source to the target domain. By following this idea, we propose ""PETAL""(PErsonalized Task-oriented diALogue), a transfer-learning framework based on POMDP to learn a personalized dialogue system. The system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user. This framework can avoid the negative transfer problem by considering differences between source and target users. The policy in the personalized POMDP can learn to choose different actions appropriately for different users. Experimental results on a real-world coffee-shopping data and simulation data show that our personalized dialogue system can choose different optimal actions for different users, and thus effectively improve the dialogue quality under the personalized setting.",,Commerce,,DBLP,multiple,other,other,y,y,n,y,y,n,y,y,n,y,n,n,y,y,n,n,SARSA,,"Kaixiang Mo, Yu Zhang, Shuangyin Li, Jiajun Li, Qiang Yang ",,,,Conference and Workshop Papers,2018-01-01,Mo2018PersonalizingAD,"@inproceedings{Mo2018PersonalizingAD,
  title={Personalizing a Dialogue System With Transfer Reinforcement Learning},
  author={Kaixiang Mo and Yu Zhang and Shuangyin Li and Jiajun Li and Qiang Yang},
  booktitle={AAAI},
  year={2018}
}"
Modelling User Behaviors with Evolving Users and Catalogs of Evolving Items,,"evolving items, evolving users, online learning, recommender systems, reinforcement learning, sequental decision making",Commerce,,ACM-DL,1,state representation,unknown,n,n,n,n,n,n,n,n,n,y,n,n,n,n,n,y,Contextual Bandits,"Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization",Leonardo  Cella,,,10.1145/3099023.3102251,article,2017-01-01,Cella_2017," @article{Cella_2017, title={Modelling User Behaviors with Evolving Users and Catalogs of Evolving Items}, ISBN={9781450350679}, url={http://dx.doi.org/10.1145/3099023.3102251}, DOI={10.1145/3099023.3102251}, journal={Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization  - UMAP  ’17}, publisher={ACM Press}, author={Cella, Leonardo}, year={2017}}
"
Adaptive Cognitive Orthotics: Combining Reinforcement Learning and Constraint-based Temporal Reasoning,,,Health,,ACM-DL,1,not used,online,y,n,n,y,n,n,n,n,n,y,y,n,n,y,n,y,Q-Learning,Proceedings of the Twenty-first International Conference on Machine Learning,Matthew  Rudary and Satinder  Singh and Martha E. Pollack,,,10.1145/1015330.1015411,article,2004-01-01,Rudary_2004," @article{Rudary_2004, title={Adaptive cognitive orthotics}, ISBN={1581138285}, url={http://dx.doi.org/10.1145/1015330.1015411}, DOI={10.1145/1015330.1015411}, journal={Twenty-first international conference on Machine learning  - ICML  ’04}, publisher={ACM Press}, author={Rudary, Matthew and Singh, Satinder and Pollack, Martha E.}, year={2004}}
"
Intelligent Adapted e-Learning System Based on Deep Reinforcement Learning,,"Decision Support System, Deep Neural network, E-learning, Learning Management System (LMS), Personalized learning, Reinforcement Learning",Education,,,1,not used,unknown,n,y,n,n,y,n,n,n,n,y,n,n,n,n,n,y,Deep Reinforcement Learning,Proceedings of the 2Nd International Conference on Computing and Wireless Communication Systems,Mohammed  El Fouki and Noura  Aknin and K. Ed El. Kadiri,,,10.1145/3167486.3167574,article,2017-01-01,El_Fouki_2017," @article{El_Fouki_2017, title={Intelligent Adapted e-Learning System based on Deep Reinforcement Learning}, ISBN={9781450353069}, url={http://dx.doi.org/10.1145/3167486.3167574}, DOI={10.1145/3167486.3167574}, journal={Proceedings of the 2nd International Conference on Computing and Wireless Communication Systems  - ICCWCS’17}, publisher={ACM Press}, author={El Fouki, Mohammed and Aknin, Noura and El. Kadiri, K. Ed}, year={2017}}
"
Cross Channel Optimized Marketing by Reinforcement Learning,,"CRM, cost sensitive learning, customer life time value, reinforcement learning, targeted marketing",Commerce,,ACM-DL,1,state representation,batch,n,y,n,n,y,n,n,n,n,y,n,n,n,n,y,y,Q-Learning,Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,Naoki  Abe and Naval  Verma and Chid  Apte and Robert  Schroko,,,10.1145/1014052.1016912,article,2004-01-01,Abe_2004," @article{Abe_2004, title={Cross channel optimized marketing by reinforcement learning}, ISBN={1581138889}, url={http://dx.doi.org/10.1145/1014052.1016912}, DOI={10.1145/1014052.1016912}, journal={Proceedings of the 2004 ACM SIGKDD international conference on Knowledge discovery and data mining  - KDD  ’04}, publisher={ACM Press}, author={Abe, Naoki and Verma, Naval and Apte, Chid and Schroko, Robert}, year={2004}}
"
Using Personality Models As Prior Knowledge to Accelerate Learning About Stress-Coping Preferences: (Demonstration),,"human-agent teamwork, human-behaviour models, reinforcement learning",Health,,ACM-DL,multiple,other,unknown,n,n,n,n,n,n,n,n,y,n,n,n,n,y,n,y,Q-Learning,Proceedings of the 2016 International Conference on Autonomous Agents &#38; Multiagent Systems,Sebastian  Ahrndt and Marco  L&#252;tzenberger and Stephen M. Prochnow,,,,article,2016-01-01,Ahrndt2016UsingPM,"@inproceedings{Ahrndt2016UsingPM,
  title={Using Personality Models as Prior Knowledge to Accelerate Learning About Stress-Coping Preferences: (Demonstration)},
  author={Sebastian Ahrndt and Marco L{\""u}tzenberger and Stephen M. Prochnow},
  booktitle={AAMAS},
  year={2016}
}"
"Reinforcement Learning: The Sooner the Better, or the Later the Better?",,"delayed reward, immediate reward, pedagogical strategy, problem solving, reinforcement learning, worked example",Education,,ACM-DL,1,not used,unknown,n,y,n,n,y,n,n,n,n,y,n,n,n,n,n,y,"RL, not further specified",Proceedings of the 2016 Conference on User Modeling Adaptation and Personalization,Shitian  Shen and Min  Chi,,,10.1145/2930238.2930247,article,2016-01-01,Shen_2016," @article{Shen_2016, title={Reinforcement Learning}, ISBN={9781450343688}, url={http://dx.doi.org/10.1145/2930238.2930247}, DOI={10.1145/2930238.2930247}, journal={Proceedings of the 2016 Conference on User Modeling Adaptation and Personalization - UMAP  ’16}, publisher={ACM Press}, author={Shen, Shitian and Chi, Min}, year={2016}}
"
Designing Intelligent Sales-agent for Online Selling,,"abstract argumentation framework, negotiation, persuasion, reinforcement learning, sales-agent",Commerce,,ACM-DL,1,state representation,online,y,n,y,n,n,y,n,n,n,y,n,n,n,y,n,y,TD-Learning,Proceedings of the 7th International Conference on Electronic Commerce,Shiu-li  Huang and Fu-ren  Lin,,,10.1145/1089551.1089605,article,2005-01-01,Huang_2005," @article{Huang_2005, title={Designing intelligent sales-agent for online selling}, ISBN={1595931120}, url={http://dx.doi.org/10.1145/1089551.1089605}, DOI={10.1145/1089551.1089605}, journal={Proceedings of the 7th international conference on Electronic commerce  - ICEC  ’05}, publisher={ACM Press}, author={Huang, Shiu-li and Lin, Fu-ren}, year={2005}}
"
LEAP: Learning to Prescribe Effective and Safe Treatment Combinations for Multimorbidity,,"multi-instance multilabel learning, multimorbidity, treatment recommendation",Health,,ACM-DL,1,other,unknown,n,y,n,n,y,n,y,y,n,y,y,y,n,n,n,y,Model-Free Policy-based Reinforcement Learning,Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,Yutao  Zhang and Robert  Chen and Jie  Tang and Walter F. Stewart and Jimeng  Sun,,,10.1145/3097983.3098109,article,2017-01-01,Zhang_2017," @article{Zhang_2017, title={LEAP}, ISBN={9781450348874}, url={http://dx.doi.org/10.1145/3097983.3098109}, DOI={10.1145/3097983.3098109}, journal={Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  - KDD  ’17}, publisher={ACM Press}, author={Zhang, Yutao and Chen, Robert and Tang, Jie and Stewart, Walter F. and Sun, Jimeng}, year={2017}}
"
