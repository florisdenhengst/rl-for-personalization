{
  "(non-linear) contextual bandit": "Contextual Bandits",
  "Actor Critic": "Actor-Critic",
  "Actor Critic RL": "Actor-Critic",
  "Actor-Critic": "Actor-Critic",
  "Actor-critic": "Actor-Critic",
  "Actor-critic RL": "Actor-Critic",
  "Actor-critict reinforcement learning": "Actor-Critic",
  "Bayes-UCB": "UCB",
  "Bayesian reinforcement learning": "RL, not further specified",
  "Bootstrap CB": "Contextual Bandits",
  "CB": "Contextual Bandits",
  "CB variants with a lot of different variants (10+)": "Contextual Bandits",
  "Contextual Bandits": "Contextual Bandits",
  "Contextual bandit": "Contextual Bandits",
  "Contextual bandit combined with stacking (called HyperTS and HyperTSFB)": "Contextual Bandits",
  "Contextual bandit with BS (OBS)": "Contextual Bandits",
  "Contextual bandits": "Contextual Bandits",
  "DRL": "Deep Reinforcement Learning",
  "DYNA-Q": "DYNA-Q",
  "Deep RL": "Deep Reinforcement Learning",
  "Dyna-Q": "DYNA-Q",
  "Fitted Q-Iteration": "Fitted Q-Iteration",
  "Fitted Q-iteration": "Fitted Q-Iteration",
  "IRL with Optimal Policy (IRL-OP) and Appreticeship Learning (AL)": "Inverse Reinforcement Learning",
  "IRL with Softmax Policy Iteration (IRL-SP)": "Inverse Reinforcement Learning",
  "Inverse RL": "Inverse Reinforcement Learning",
  "Inverse Reinforcement Learning": "Inverse Reinforcement Learning",
  "Inverse reinforcement learning": "Inverse Reinforcement Learning",
  "KMTL-UCB": "UCB",
  "KMTL-UCB-Est": "UCB",
  "Kernel-UCB-Ind": "UCB",
  "Kernel-UCB-Pool": "UCB",
  "Latent Contextual Bandits": "Contextual Bandits",
  "LinUCB": "LinUCB",
  "Local Thompson Sampling (LTS)": "Thompson Sampling",
  "Maximum Entropy IRL": "Inverse Reinforcement Learning",
  "Maximum Entropy Inverse RL": "Maximum Entropy IRL",
  "Missing": "RL, not further specified",
  "Multi-agent reinforcement learning": "Multi-agent Reinforcement Learning",
  "Naive LinUCB": "LinUCB",
  "Neural Fitted Q Iteration": "Fitted Q-Iteration",
  "Online Reinforcement Learning": "RL, not further specified",
  "Own unnamed variant": "RL, not further specified",
  "Policy Iteration": "Policy Iteration",
  "Policy iteration": "Policy Iteration",
  "Policy iteration with epsilon greedy or softmax": "Policy Iteration",
  "Q-Learning": "Q-Learning",
  "Q-learning": "Q-Learning",
  "Q-learning (Vari-RL(Q) and Batch-AU)": "Q-Learning",
  "Q-learning (slight variation) with different initializations.": "Q-Learning",
  "Q-learning with Bayesian active learning": "Q-Learning",
  "Q-learning with continuous state space": "Q-Learning",
  "RL": "RL, not further specified",
  "RL (DJ-MC)": "RL, not further specified",
  "RL (no specific algorithm was mentioned)": "RL, not further specified",
  "RL (they have a formalisation but don't specify a name)": "other",
  "RL framework": "RL, not further specified",
  "RL?": "RL, not further specified",
  "Random": "Random",
  "SARSA": "SARSA",
  "SARSA(lambda)": "SARSA",
  "Sarsa": "SARSA",
  "Sarsa(\u03bb) with \u000f-greedy action selection": "SARSA",
  "Seems Q-learning": "Q-Learning",
  "TD(0)": "TD-Learning",
  "Thompson Sampling": "Thompson Sampling",
  "Thompson Sampling (with and without Regularization)": "Thompson Sampling",
  "Thompson sampling": "Thompson Sampling",
  "Unspecified": "RL, not further specified",
  "WAIR (seems Q-learning variant)": "Q-Learning",
  "contextual multi-armed bandit": "Contextual Bandits",
  "contextual multi-armed bandits (MAB)": "Contextual Bandits",
  "continuous actor-critic learning automaton (CACLA)": "Actor-Critic",
  "Actor-critic": "Actor-Critic",
  "e-greedy CB": "Contextual Bandits",
  "fixed policy": "baseline methods",
  "implemented Rl": "RL, not further specified",
  "policy iteration algorithm": "Policy Iteration",
  "random": "Random",
  "Reinforcement Learning": "RL, not further specified"
}
