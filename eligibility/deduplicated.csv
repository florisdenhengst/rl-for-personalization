Title,Abstract,Access Type,Affiliations,Art. No.,Author Affiliations,Authors,Authors with affiliations,Citation Count,Copyright Year,DOI,Database,DOI counts,DOI count,Date Added To Xplore,End Page,Funding Information,IEEE Terms,INSPEC Controlled Terms,INSPEC Non-Controlled Terms,ISBN,ISSN,Issue,Issue Date,Journal,Source Title,Keywords,License,Local ID,Meeting Date,Mesh_Terms,Online Date,PDF Link,Page count,Page end,Page start,Pages,Patent Citation Count,Publisher,Reference Count,Source,Source ISBN,Source ISSN,Source Title,Start Page,Type,URL,Unnamed: 12,Volume,Volume.1,Year,acronym,booktitle,cluster_id,conf_loc,description,edition,issue_date,issue_no,key,month,num_pages,num_versions,number,pages,pdf_url,publisher_loc,url_citation,url_citations,venue
Adapting difficulty levels in personalized robot-child tutoring interactions,".Social roliots can be used to tutor children in one-on-one interactions. Because students have different learning needs, they consequenUy require complex, non-scripted teaching behaviors that adapt to the learning needs of each child. As a result of this, robot tutors are more effective given a means of adaptively customizing the pace and content of a student's curriculum. In this paper we propose a reinforcement learning-based approach that affords such capabilities to a tutoring robot, with the goals of fostering measurable learning gains and sustained engagement. We outline an architecture in which the robot uses reinforcement learning to adapt the difficulty of its exercises. Further, we describe a proposed study capable of evaluating the effectiveness of our Intelligent Tutoring System. © Copyright 2014, Association for the Advancement of Artificial Intelligence (www.aaia.org). All rights reserved.",,"Yale University, 51 Prospect St., New Haven, CT, United States",,,"Ramacliandran A., Scassellati B.","Ramacliandran, A., Yale University, 51 Prospect St., New Haven, CT, United States; Scassellati, B., Yale University, 51 Prospect St., New Haven, CT, United States",3,,,SCOPUS,0,,,,,,,,,,,,,AAAI Workshop - Technical Report,,,2-s2.0-84974815499,,,,,,59,56,,,,,Scopus,,,AAAI Workshop - Technical Report,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974815499&partnerID=40&md5=75adeaedcf63bcb3f1c3d765aa90e4d7,,WS-14-07,,2014,,,,,,,,,,,,,,,,,,,
Customizing treatment to the patient: Adaptive treatment strategies,[No abstract available],,"Institute for Social Research, University of Michigan, 426 Thompson St, Ann Arbor, MI 48106-1248, United States; The Methodology Center, Department of Human Development and Family Studies, The Pennsylvania State University, University Park, PA 16802, United States; University of Texas, Southwestern Medical Center at Dallas, Dallas, TX 75390, United States",,,"Murphy S.A., Collins L.M., Rush A.J.","Murphy, S.A., Institute for Social Research, University of Michigan, 426 Thompson St, Ann Arbor, MI 48106-1248, United States; Collins, L.M., The Methodology Center, Department of Human Development and Family Studies, The Pennsylvania State University, University Park, PA 16802, United States; Rush, A.J., University of Texas, Southwestern Medical Center at Dallas, Dallas, TX 75390, United States",48,,10.1016/j.drugalcdep.2007.02.001,SCOPUS,1,,,,,,,,,,SUPPL. 2,,,Drug and Alcohol Dependence,Clinical decision support; Control engineering; Experimental design; Individualized care; Reinforcement learning; Stepped care; Structured treatment interruptions,,2-s2.0-34047213941,,,,,,S3,S1,,,,,Scopus,,,Drug and Alcohol Dependence,,Editorial,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047213941&doi=10.1016%2fj.drugalcdep.2007.02.001&partnerID=40&md5=e148da8ef07ce304ef8306445b05a86a,,88,,2007,,,,,,,,,,,,,,,,,,,
Dynamic Information Retrieval Modeling,"<p> Big data and human-computer information retrieval (HCIR) are changing IR. They capture the dynamic changes in the data and dynamic interactions of users with IR systems. A dynamic system is one which changes or adapts over time or a sequence of events. Many modern IR systems and data exhibit these characteristics which are largely ignored by conventional techniques. What is missing is an ability for the model to change over time and be responsive to stimulus. Documents, relevance, users and tasks all exhibit dynamic behavior that is captured in data sets typically collected over long time spans and models need to respond to these changes. Additionally, the size of modern datasets enforces limits on the amount of learning a system can achieve. Further to this, advances in IR interface, personalization and ad display demand models that can react to users in real time and in an intelligent, contextual way. </p> <p>In this book we provide a comprehensive and up to-date introduction to Dynamic Information Retrieval Modeling, the statistical modeling of IR systems that can adapt to change. We define <i>dynamics</i>, what it means within the context of IR and highlight examples of problems where dynamics play an important role. We cover techniques ranging from classic relevance feedback to the latest applications of partially observable Markov decision processes (POMDPs) and a handful of useful algorithms and tools for solving IR problems incorporating dynamics. </p> <p>The theoretical component is based around the Markov Decision Process (MDP), a mathematical framework taken from the field of Artificial Intelligence (AI) that enables us to construct models that change according to sequential inputs. We define the framework and the algorithms commonly used to optimize over it and generalize it to the case where the inputs aren't reliable. We explore the topic of reinforcement learning more broadly and introduce nother tool known as a Multi-Armed Bandit which is useful for cases where exploring model parameters is beneficial. Following this we introduce theories and algorithms which can be used to incorporate dynamics into an IR model before presenting an array of state-of-the-art research that already does, such as in the areas of session search and online advertising. </p> <p>Change is at the heart of modern Information Retrieval systems and this book will help equip the reader with the tools and knowledge needed to understand <i>Dynamic Information Retrieval Modeling</i>. </p>",,,,,G. H. Yang; M. Sloan; J. Wang,,,2016,10.2200/S00718ED1V01Y201605ICR049,IEEE Xplore,1,,20160706,,,,,,,,,,,Dynamic Information Retrieval Modeling,Markov decision process;dynamic information retrieval;information retrieval;information retrieval evaluation;information retrieval models;recommender systems;reinforcement learning,,,,,,https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=7503460.pdf&bkn=7503459&pdfType=book,,,,,,Morgan & Claypool,,,97816270552,,Dynamic Information Retrieval Modeling,,Morgan and Claypool eBooks,,,,,2016,,,,,,,,,,,,,,,,,,,
Reward Sensitivity of ACC as an Intermediate Phenotype between DRD4-521T and Substance Misuse,"<para>The development and expression of the midbrain dopamine system is determined in part by genetic factors that vary across individuals such that dopamine-related genes are partly responsible for addiction vulnerability. However, a complete account of how dopamine-related genes predispose individuals to drug addiction remains to be developed. Adopting an intermediate phenotype approach, we investigated whether reward-related electrophysiological activity of ACC—a cortical region said to utilize dopamine reward signals to learn the value of extended, context-specific sequences of goal-directed behaviors—mediates the influence of multiple dopamine-related functional polymorphisms over substance use. We used structural equation modeling to examine whether two related electrophysiological phenomena associated with the control and reinforcement learning functions of ACC—theta power and the reward positivity—mediated the relationship between the degree of substance misuse and genetic polymorphisms that regulate dopamine processing in frontal cortex. Substance use data were collected from 812 undergraduate students. One hundred ninety-six returned on a subsequent day to participate in an electrophysiological experiment and to provide saliva samples for DNA analysis. We found that these electrophysiological signals mediated a relationship between the DRD4-521T dopamine receptor genotype and substance misuse. Our results provide a theoretical framework that bridges the gap between genes and behavior in drug addiction and illustrate how future interventions might be individually tailored for specific genetic and neurocognitive profiles.</para>",,,,1University of Victoria,T. E. Baker; T. Stockwell; G. Barnes; R. Haesevoets; C. B. Holroyd,,,,10.1162/jocn_a_00905,IEEE Xplore,1,,20160204,471,,,,,,0898-929X;0898929X,3,March 2016,,Journal of Cognitive Neuroscience,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395964,,,,,,MIT Press,,,,,Journal of Cognitive Neuroscience,460,MIT Press Journals,,,28,,2016,,,,,,,,,,,,,,,,,,,
Past Makes Future: Role of pFC in Prediction,"<para>The pFC enables the essential human capacities for predicting future events and preadapting to them. These capacities rest on both the structure and dynamics of the human pFC. Structurally, pFC, together with posterior association cortex, is at the highest hierarchical level of cortical organization, harboring neural networks that represent complex goal-directed actions. Dynamically, pFC is at the highest level of the perception–action cycle, the circular processing loop through the cortex that interfaces the organism with the environment in the pursuit of goals. In its predictive and preadaptive roles, pFC supports cognitive functions that are critical for the temporal organization of future behavior, including planning, attentional set, working memory, decision-making, and error monitoring. These functions have a common future perspective and are dynamically intertwined in goal-directed action. They all utilize the same neural infrastructure: a vast array of widely distributed, overlapping, and interactive cortical networks of personal memory and semantic knowledge, named cognits, which are formed by synaptic reinforcement in learning and memory acquisition. From this cortex-wide reservoir of memory and knowledge, pFC generates purposeful, goal-directed actions that are preadapted to predicted future events.</para>",,,,"1University of California, Los Angeles",J. M. Fuster; S. L. Bressler,,0,,10.1162/jocn_a_00746,IEEE Xplore,1,,20150305,654,,,,,,0898-929X;0898929X,4,April 2015,,Journal of Cognitive Neuroscience,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7052276,,,,,,MIT Press,,,,,Journal of Cognitive Neuroscience,639,MIT Press Journals,,,27,,2015,,,,,,,,,,,,,,,,,,,
Closed-loop algorithm to detect human face using color and reinforcement learning,"A closed-loop algorithm to detect human face using color information and reinforcement learning is presented in this paper. By using a skin-color selector, the regions with color like that of human skin are selected as candidates for human face. In the next stage, the candidates are matched with a face model and given an evaluation of the match degree by the matching module. And if the evaluation of the match result is too low, a reinforcement learning stage will start to search the best parameters of the skin-color selector. It has been tested using many photos of various ethnic groups under various lighting conditions, such as different light source, high light and shadow. And the experiment result proved that this algorithm is robust to the varying lighting conditions and personal conditions.",,"Inst. of Info. Syst. and Elec. Eng., Zhejiang Univ., Hangzhou 310027, China",,,"Wu D.-H., Ye X.-Q., Gu W.-K.","Wu, D.-H., Inst. of Info. Syst. and Elec. Eng., Zhejiang Univ., Hangzhou 310027, China; Ye, X.-Q., Inst. of Info. Syst. and Elec. Eng., Zhejiang Univ., Hangzhou 310027, China; Gu, W.-K., Inst. of Info. Syst. and Elec. Eng., Zhejiang Univ., Hangzhou 310027, China",,,10.1631/jzus.2002.0072,SCOPUS,1,,,,,,,,,,1,,,Journal of Zhejinag University: Science,Human face detection; Reinforcement learning; Skin-color selector,,2-s2.0-0036050298,,,,,,76,72,,,,,Scopus,,,Journal of Zhejinag University: Science,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036050298&doi=10.1631%2fjzus.2002.0072&partnerID=40&md5=8597918e640b41e2055b3140993f6bbb,,3,,2002,,,,,,,,,,,,,,,,,,,
Dynamic treatment regimes,"A dynamic treatment regime consists of a sequence of decision rules, one per stage of intervention, that dictate how to individualize treatments to patients, based on evolving treatment and covariate history. These regimes are particularly useful for managing chronic disorDers and fit well into the larger paradigm of personalized medicine. They provide one way to operationalize a clinical decision support system. Statistics plays a key role in the construction of evidence-based dynamic treatment regimes-informing the best study design as well as efficient estimation and valid inference. Owing to the many novel methodological challenges this area offers, it has been growing in popularity among statisticians in recent years. In this article, we review the key developments in this exciting field of research. In particular, we discuss the sequential multiple assignment randomized trial designs, estimation techniques like Q-learning and marginal structural models, and several inference techniques designed to address the associated nonstandard asymptotics. We reference software whenever available. We also outline some important future directions. © 2014 by Annual Reviews.",,"Duke-NUS Graduate Medical School, National University of Singapore, Singapore, Singapore; Department of Statistics and Institute for Social Research, University of Michigan, Ann Arbor, MI 48109, United States",,,"Chakraborty B., Murphy S.A.","Chakraborty, B., Duke-NUS Graduate Medical School, National University of Singapore, Singapore, Singapore; Murphy, S.A., Department of Statistics and Institute for Social Research, University of Michigan, Ann Arbor, MI 48109, United States",24,,10.1146/annurev-statistics-022513-115553,SCOPUS,1,,,,,,,,,,,,,Annual Review of Statistics and Its Application,Dynamic treatment regime; Nonregularity; Q-learning; Reinforcement learning; Sequential randomization,,2-s2.0-84906077445,,,,,,464,447,,,,,Scopus,,,Annual Review of Statistics and Its Application,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906077445&doi=10.1146%2fannurev-statistics-022513-115553&partnerID=40&md5=ce47ee4c06d6e2fbd95907910f5528cc,,1,,2014,,,,,,,,,,,,,,,,,,,
Hardware implementation of FAST-based reinforcement learning algorithm,"A FAST-based (Flexible Adaptable-Size Topology) reinforcement learning chip is implemented in this article. Basically, the FAST is an ART-like (Adaptive Resonance Theory) mechanism. The ART is characterized as one of unsupervised learning neural network models, facilitated to solve stability-plasticity dilemma. The chip is a self organizing architecture which consists of three main structures including similarity, learning, and pruning. Dynamically adjusting the size of sensitivity regions of each neuron and adaptively pruning one of the neurons when an input pattern activates more than one neuron, the chip can preserve hardware resources (available neurons) to accommodate more categories. The clustered result by the implemented chip is then sent to an AHC (Adaptive Heuristic Critic) architecture (emulated by a personal computer) to learn to balance an inverted pendulum system which is also emulated by the personal computer for verifying the implemented architecture. © 2005 IEEE.",,"Department of Electrical, National Chung-Cheng University, Chiayi 711, Taiwan; Department of Computer Science and Information Engineering, National Formosa University, Yunlin 632, Taiwan",,,"Hwang K.-S., Hsu Y.-P., Hsieh H.-W., Lin H.-Y.","Hwang, K.-S., Department of Electrical, National Chung-Cheng University, Chiayi 711, Taiwan; Hsu, Y.-P., Department of Computer Science and Information Engineering, National Formosa University, Yunlin 632, Taiwan; Hsieh, H.-W., Department of Electrical, National Chung-Cheng University, Chiayi 711, Taiwan; Lin, H.-Y., Department of Electrical, National Chung-Cheng University, Chiayi 711, Taiwan",5,,,SCOPUS,0,,,,,,,,,,,,,"Proceedings of the 2005 IEEE International Workshop on VLSI Design and Video Technology, IWVDVT 2005",,,2-s2.0-23844518746,,,,,,146,143,,,,,Scopus,,,"Proceedings of the 2005 IEEE International Workshop on VLSI Design and Video Technology, IWVDVT 2005",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23844518746&partnerID=40&md5=d9d8f4f08fe1f1ca5c563113c35f8811,,,,2005,,,,,,,,,,,,,,,,,,,
Personalized-Proctorial Instruction fir Dynamic Systems and Control,A highly promising teaching technique using reinforcement learning theory and individually paced instruction is analyzed and shown to offer several advantages in conducting dynamic systems and control courses. © 1972 ASME.,,"Mechanical Engineering Department, University of Texas at Austin, Austin, TX, United States",,,Hoberock L.L.,"Hoberock, L.L., Mechanical Engineering Department, University of Texas at Austin, Austin, TX, United States",1,,10.1115/1.3426564,SCOPUS,1,,,,,,,,,,2,,,"Journal of Dynamic Systems, Measurement and Control, Transactions of the ASME",,,2-s2.0-85024532643,,,,,,167,165,,,,,Scopus,,,"Journal of Dynamic Systems, Measurement and Control, Transactions of the ASME",,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024532643&doi=10.1115%2f1.3426564&partnerID=40&md5=c287f59b6aa7640be6e383ac9b1045db,,94,,1972,,,,,,,,,,,,,,,,,,,
Changes in the digital divide: A case from Belgium: Colloquium,"A major concern about the use of information and communication technology (ICT) in adult education is that the digital divide may reinforce the already existing learning divide, because it runs along the same lines of socio-economic class, gender and ethnicity. A case study of family day care providers (FDC) in Flanders, Belgium is presented to study the potentialities and problems with low professional status, as well as a target group for enhancing professionalism. A study was conducted to assess to what degree technology may be integrated in FDC provider's experience, cultures and beliefs. An analysis of covariance (ANCOVA) of the perceived PC skills as dependant variable and including education, previous work experiences with a PC, previous adult training in PC use, and age as independent variables showed that only age as independent variable was interrelated with PC skills. The result also showed that anxiety and motivation in the sample need to be considered as personal variables that may not run along tradition lines of the digital divide.",,"Ghent University, SocialWelfare Studies, Dunantlaan 2, B-9000 Gent, Belgium",,,"Vandenbroeck M., Verschelden G., Boonaert T., Haute L.V.","Vandenbroeck, M., Ghent University, SocialWelfare Studies, Dunantlaan 2, B-9000 Gent, Belgium; Verschelden, G.; Boonaert, T.; Haute, L.V.",4,,10.1111/j.1467-8535.2007.00698.x,SCOPUS,1,,,,,,,,,,4,,,British Journal of Educational Technology,,,2-s2.0-34250010931,,,,,,743,742,,,,,Scopus,,,British Journal of Educational Technology,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250010931&doi=10.1111%2fj.1467-8535.2007.00698.x&partnerID=40&md5=0e7cb21f6ecfac56d03ef05ac6363b9f,,38,,2007,,,,,,,,,,,,,,,,,,,
Increasing Retrieval Quality in Conversational Recommenders,"A major task of research in conversational recommender systems is personalization. Critiquing is a common and powerful form of feedback, where a user can express her feature preferences by applying a series of directional critiques over the recommendations instead of providing specific preference values. Incremental Critiquing (IC) is a conversational recommender system that uses critiquing as a feedback to efficiently personalize products. The expectation is that in each cycle the system retrieves the products that best satisfy the user's soft product preferences from a minimal information input. In this paper, we present a novel technique that increases retrieval quality based on a combination of compatibility and similarity scores. Under the hypothesis that a user learns during the recommendation process, we propose two novel exponential Reinforcement Learning (RL) approaches for compatibility that take into account both the instant at which the user makes a critique and the number of satisfied critiques. Moreover, we consider that the impact of features on the similarity differs according to the preferences manifested by the user. We propose a Global Weighting (GW) approach that uses a common weight for nearest cases in order to focus on groups of relevant products. We show that our methodology significantly improves recommendation efficiency in four data sets of different sizes in terms of session length in comparison with state-of-the-art approaches. Moreover, our recommender shows higher robustness against noisy user data when compared to classical approaches.",,,,"Universitat de Barcelona, Barcelona",M. S. Llorente; S. E. Guerrero,,3,,10.1109/TKDE.2011.116,IEEE Xplore,1,,20120817,1888,,Cognition;Current measurement;Learning systems;Monte Carlo methods;Recommender systems;Space exploration,,,,1041-4347;10414347,10,Oct. 2012,,IEEE Transactions on Knowledge and Data Engineering,Conversational recommender systems;case-based reasoning;critiquing elicitation;personalization.,,,,,20110609,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5871618,,,,,,IEEE,38,,,,IEEE Transactions on Knowledge and Data Engineering,1876,IEEE Journals & Magazines,,,24,,2012,,,,,,,,,,,,,,,,,,,
Anti-lock braking systems data-driven control using Q-learning,"A model-free tire slip control solution for a fast, highly nonlinear Anti-lock Braking System (ABS) is proposed in this work via a reinforcement Q-learning optimal control approach. The solution is tailored around a batch neural fitted scheme using two neural networks to approximate the value function and the controller, respectively. The transition samples are collected from the process through interaction by online exploiting the current iteration controller (or policy) under an ε-greedy exploration strategy. The ABS process fits this type of learning-by-interaction since it does not need an initial stabilizing controller. The validation case studies carried out on a real laboratory setup reveal that high control system performance can be achieved after several tens of interaction episodes with the controlled process. Insightful comments on the observed control behavior in a set of real-time experiments are offered along with performance comparisons with several other controllers.",,,,"Department of Automation and Applied Informatics, Politehnica University of Timisoara, Timisoara, Romania",M. B. Radac; R. E. Precup; R. C. Roman,,,,10.1109/ISIE.2017.8001283,IEEE Xplore,1,,20170807,423,,,automotive components;braking;greedy algorithms;learning (artificial intelligence);motion control;neurocontrollers;optimal control,ε-greedy exploration strategy;anti-lock braking system;batch neural fitted scheme;data-driven control;iteration controller;learning-by-interaction;model-free tire slip control;neural networks;nonlinear ABS;optimal control;reinforcement Q-learning;value function,,,,19-21 June 2017,,2017 IEEE 26th International Symposium on Industrial Electronics (ISIE),Actor critic control;Anti-lock Braking System;Q-learning;model-free control;neural networks,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8001283,,,,,,IEEE,,,Electronic:978-1-5090-1412-5; POD:978-1-5090-1413-2,,2017 IEEE 26th International Symposium on Industrial Electronics (ISIE),418,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
A learning multi-agent system for personalized information filtering,"A multi-agent hybrid learning approach to the problem of personalized information filtering is proposed in this paper. There are four agents in the multi-agent model. The problem is modeled as Monte Carlo reinforcement learning. Our proposed algorithm is modified Monte Carlo method combined with features of unsupervised suffix tree clustering and supervised backpropagation network. We argue that this proposed approach could precisely capture the user's interest without repeatedly asking for his/her explicit rates and converge to the user's interest quickly. A conclusion is drawn that our approach is efficient, precise and converges more quickly compared with existing approaches. A prototype system is being developed.",,,,"Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore, Singapore",Junhua Chen; Zhonghua Yang,,1,,10.1109/ICICS.2003.1292790,IEEE Xplore,1,,20040504,1868 vol.3,,Backpropagation algorithms;Clustering algorithms;Contacts;Information filtering;Information filters;Information retrieval;Monte Carlo methods;Multiagent systems;Search engines;Supervised learning,Internet;Monte Carlo methods;backpropagation;information filters;multi-agent systems,Monte Carlo reinforcement learning;backpropagation network;learning multiagent system;personalized information filtering;suffix tree clustering,,,,15-18 Dec. 2003,,"Fourth International Conference on Information, Communications and Signal Processing, 2003 and the Fourth Pacific Rim Conference on Multimedia. Proceedings of the 2003 Joint",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1292790,,,,,,IEEE,9,,POD:0-7803-8185-8,,"Fourth International Conference on Information, Communications and Signal Processing, 2003 and the Fourth Pacific Rim Conference on Multimedia. Proceedings of the 2003 Joint",1864,IEEE Conferences,,,3,,2003,,,,,,,,,,,,,,,,,,,
A distributed reinforcement learning approach to maximize resource utilization and control handover dropping in multimedia wireless networks,"A new scheme to maximize resource utilization in a cellular network while respecting constraints on handover dropping probability is proposed and analyzed. The constraints are set for each traffic class separately and have to be respected by the network independently of the area in a localized manner. The problem is formulated as a Markov Decision Process (MDP) and solved by making use of the model-free simulation-based Q-learning algorithm that runs at each cell. Integration of the handover limit in the model is achieved by observing which of the new call arrivals, at a particular state of the system, are mostly responsible for violation of the handover dropping limit. Through trial and error, the algorithm proceeds to the statistical elimination of new admissions in the system, those causing excessive dropping. Results obtained via the proposed Reinforcement Learning (RL) based approach are compared with a resource allocation that takes into consideration heterogeneous and unevenly distributed traffic over the geographical area under consideration. For the scenarios examined, comparable results and performance are observed with an advantage for RL in blocking and utilization.",,,,"Motorola Labs, Gif-sur-Yvette, France",E. Alexandri; G. Martinez; D. Zeghlache,,3,,10.1109/PIMRC.2002.1046544,IEEE Xplore,1,,20021210,2253 vol.5,,Bandwidth;Communication system traffic control;Intelligent networks;Land mobile radio cellular systems;Learning;Quality of service;Resource management;Telecommunication traffic;Traffic control;Wireless networks,Markov processes;cellular radio;learning (artificial intelligence);multimedia communication;probability,Markov decision process;call arrivals;cellular network;distributed reinforcement learning approach;handover dropping;handover dropping probability;handover limit;model-free simulation-based Q-learning algorithm;multimedia wireless networks;resource utilization;statistical elimination;traffic class;unevenly distributed traffic,,,,15-18 Sept. 2002,,"The 13th IEEE International Symposium on Personal, Indoor and Mobile Radio Communications",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1046544,,,,,,IEEE,4,,POD:0-7803-7589-0,,"The 13th IEEE International Symposium on Personal, Indoor and Mobile Radio Communications",2249,IEEE Conferences,,,5,,2002,,,,,,,,,,,,,,,,,,,
An Actor-Critic based controller for glucose regulation in type 1 diabetes,"A novel adaptive approach for glucose control in individuals with type 1 diabetes under sensor-augmented pump therapy is proposed. The controller, is based on Actor-Critic (AC) learning and is inspired by the principles of reinforcement learning and optimal control theory. The main characteristics of the proposed controller are (i) simultaneous adjustment of both the insulin basal rate and the bolus dose, (ii) initialization based on clinical procedures, and (iii) real-time personalization. The effectiveness of the proposed algorithm in terms of glycemic control has been investigated in silico in adults, adolescents and children under open-loop and closed-loop approaches, using announced meals with uncertainties in the order of ±25% in the estimation of carbohydrates.The results show that glucose regulation is efficient in all three groups of patients, even with uncertainties in the level of carbohydrates in the meal. The percentages in the A. +. B zones of the Control Variability Grid Analysis (CVGA) were 100% for adults, and 93% for both adolescents and children.The AC based controller seems to be a promising approach for the automatic adjustment of insulin infusion in order to improve glycemic control. After optimization of the algorithm, the controller will be tested in a clinical trial. © 2012 Elsevier Ireland Ltd.",,"ARTORG Center for Biomedical Engineering Research, Diabetes Technology Research Group, University of Bern, Murtenstrasse 50, 3010 Bern, Switzerland; Division of Endocrinology, Diabetes and Clinical Nutrition, University Hospital, Inselspital, University of Bern, 3010 Bern, Switzerland",,,"Daskalaki E., Diem P., Mougiakakou S.G.","Daskalaki, E., ARTORG Center for Biomedical Engineering Research, Diabetes Technology Research Group, University of Bern, Murtenstrasse 50, 3010 Bern, Switzerland; Diem, P., Division of Endocrinology, Diabetes and Clinical Nutrition, University Hospital, Inselspital, University of Bern, 3010 Bern, Switzerland; Mougiakakou, S.G., ARTORG Center for Biomedical Engineering Research, Diabetes Technology Research Group, University of Bern, Murtenstrasse 50, 3010 Bern, Switzerland",6,,10.1016/j.cmpb.2012.03.002,SCOPUS,1,,,,,,,,,,2,,,Computer Methods and Programs in Biomedicine,Actor-Critic; Closed-loop control; Glucose control; Reinforcement learning,,2-s2.0-84873458229,,,,,,125,116,,,,,Scopus,,,Computer Methods and Programs in Biomedicine,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873458229&doi=10.1016%2fj.cmpb.2012.03.002&partnerID=40&md5=9fd626af8395921d3a16e711438dc11d,,109,,2013,,,,,,,,,,,,,,,,,,,
A Reinforcement Learning Approach to Emotion-based Automatic Playlist Generation,"A novel trend emerged in music exploration is to organize and search songs according to their emotions. However, research on automatic playlist generation (APG) primarily focuses on metadata and audio similarity. Mainstream solutions view APG as a static problem. This paper argues that the APG problem is better modeled as a continuous optimization problem, and proposes an adaptive preference model for personalized APG based on emotions. The main idea is to collect a user's behavior in music playing, e.g., rating, skipping and replaying, as immediate feedback in learning the user's preferences for music emotion within a playlist. Reinforcement learning is adopted to learn the user's current preferences, which are used to generate personalized playlists. Learning parameters are tuned by simulation of two hypothetical users. A two-month user study is conducted to evaluate the APG solutions. The results show that the proposed approach reduces the Miss Ratio by 10% in comparison with the baseline approach.",,,,"Dept. of CSIE, Nat. Taiwan Univ., Taipei, Taiwan",C. Y. Chi; R. T. H. Tsai; J. Y. Lai; J. Y. j. Hsu,,1,,10.1109/TAAI.2010.21,IEEE Xplore,1,,20110120,65,,,learning (artificial intelligence);music;optimisation;user interfaces,adaptive preference model;continuous optimization problem;emotion-based automatic playlist generation;music emotion;reinforcement learning;user preference,,2376-6816;23766816,,18-20 Nov. 2010,,2010 International Conference on Technologies and Applications of Artificial Intelligence,automatic playlist generation;reinforcement learning;song emotion,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5695433,,,,,,IEEE,20,,Electronic:978-0-7695-4253-9; POD:978-1-4244-8668-7,,2010 International Conference on Technologies and Applications of Artificial Intelligence,60,IEEE Conferences,,,,,2010,,,,,,,,,,,,,,,,,,,
A Q-learning-based multi-rate transmission control scheme for RRC in WCDMA systems,"A Q-learning-based multirate transmission control scheme (Q-MRTC) for radio resource control (RRC) in WCDMA systems is proposed. The RRC problem is modelled as a semi-Markov decision process (SMDP). We successfully apply a real-time reinforcement learning algorithm, named Q-learning, to accurately estimate the transmission cost for the multi-rate transmission control. For the cost function approximation, we apply the feature extraction method to map the original state space into a more compact set which represents the resultant interference profile. Simulation results show that the Q-MRTC can achieve higher system throughput and better users' satisfaction index, by an amount of 87% and 50%, respectively, than the interference-based multi-rate transmission control scheme, while keeping the QoS requirement.",,,,,Fang-Ching Ren; Chung-Ju Chang; Yih-Shen Chen,,0,,10.1109/PIMRC.2002.1045263,IEEE Xplore,1,,20021210,1426 vol.3,,Control systems;Cost function;Feature extraction;Function approximation;Interference;Learning;Multiaccess communication;Radio control;State-space methods;Throughput,Markov processes;broadband networks;code division multiple access;costing;feature extraction;function approximation;learning (artificial intelligence);multiuser channels;radio networks;radiofrequency interference;telecommunication control,Q-learning-based multirate transmission control;QoS;RRC;WCDMA systems;feature extraction method;function approximation;interference profile;real-time reinforcement learning algorithm;semi-Markov decision process;simulation results;state space mapping;system throughput;transmission cost estimation;users satisfaction index,,,,15-18 Sept. 2002,,"The 13th IEEE International Symposium on Personal, Indoor and Mobile Radio Communications",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1045263,,,,,,IEEE,12,,POD:0-7803-7589-0,,"The 13th IEEE International Symposium on Personal, Indoor and Mobile Radio Communications",1422,IEEE Conferences,,,3,,2002,,,,,,,,,,,,,,,,,,,
Reinforcement learning utilizes proxemics: An avatar learns to manipulate the position of people in immersive virtual reality,"A reinforcement learning (RL) method was used to train a virtual character to move participants to a specified location. The virtual environment depicted an alleyway displayed through a wide field-of-view head-tracked stereo head-mounted display. Based on proxemics theory, we predicted that when the character approached within a personal or intimate distance to the participants, they would be inclined to move backwards out of the way. We carried out a between-groups experiment with 30 female participants, with 10 assigned arbitrarily to each of the following three groups: In the Intimate condition the character could approach within 0.38m and in the Social condition no nearer than 1.2m. In the Random condition the actions of the virtual character were chosen randomly from among the same set as in the RL method, and the virtual character could approach within 0.38m. The experiment continued in each case until the participant either reached the target or 7 minutes had elapsed. The distributions of the times taken to reach the target showed significant differences between the three groups, with 9 out of 10 in the Intimate condition reaching the target significantly faster than the 6 out of 10 who reached the target in the Social condition. Only 1 out of 10 in the Random condition reached the target. The experiment is an example of applied presence theory: we rely on the many findings that people tend to respond realistically in immersive virtual environments, and use this to get people to achieve a task of which they had been unaware. This method opens up the door for many such applications where the virtual environment adapts to the responses of the human participants with the aim of achieving particular goals. © 2012 ACM 1544-3558/2012/03- ART3 ©10.00.",,"University College London, United Kingdom; Facultat de Psicologia, Campus de Mundet - Edifici Teatre, Universitat de Barcelona, Passeig de la Vall d'Hebron 171, 08035 Barcelona, Spain; Facultat de Psicologia, Campus de Mundet - Edifici Teatre, ICREA-Universitat de Barcelona, Passeig de la Vall d'Hebron 171, 08035 Barcelona, Spain",3,,"Kastanis I., Slater M.","Kastanis, I., Facultat de Psicologia, Campus de Mundet - Edifici Teatre, Universitat de Barcelona, Passeig de la Vall d'Hebron 171, 08035 Barcelona, Spain; Slater, M., University College London, United Kingdom, Facultat de Psicologia, Campus de Mundet - Edifici Teatre, ICREA-Universitat de Barcelona, Passeig de la Vall d'Hebron 171, 08035 Barcelona, Spain",5,,10.1145/2134203.2134206,SCOPUS,1,,,,,,,,,,1,,,ACM Transactions on Applied Perception,Experimentation; Human Factors,,2-s2.0-84859474974,,,,,,,,,,,,Scopus,,,ACM Transactions on Applied Perception,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859474974&doi=10.1145%2f2134203.2134206&partnerID=40&md5=83e08cb9da9427c493cfb59a2296e9f2,,9,,2012,,,,,,,,,,,,,,,,,,,
Design considerations of reinforcement learning power controllers in Wireless Body Area Networks,"A Wireless Body Area Network (WBAN) comprises a number of tiny devices implanted in/on the body that sample physiological signals of the human body and send them to a coordinator node for medical or other purposes. As these miniature devices run on built-in batteries, energy is the most valuable resource in WBANs. This makes signal interference between neighboring WBANs a serious threat because it causes energy waste in these systems. To mitigate this internetwork interference, we propose a dynamic power control mechanism in WBANs which employs reinforcement learning (RL) to learn from experience and improve its performance. This paper presents guidelines in designing efficient RL power controllers in WBANs and provides an analysis of the effect of the reward function, discount factor, learning rate and eligibility trace parameter where the main performance criteria used are convergence and solution optimality in terms of throughput and energy consumption per bit.",,,,"Department of Engineering, Macquarie University, Sydney, Australia",R. Kazemi; R. Vesilo; E. Dutkiewicz; R. P. Liu,,1,,10.1109/PIMRC.2012.6362688,IEEE Xplore,1,,20121129,2036,,Convergence;Energy consumption;Games;Interference;Power control;Signal to noise ratio;Wireless communication,biomedical communication;body area networks;control engineering computing;control system synthesis;interference suppression;learning (artificial intelligence);medical computing;medical control systems;power control;prosthetics,RL;WBAN;eligibility trace parameter;energy consumption;energy waste;human body implantation;internetwork interference mitigation;medical coordinator node;physiological signal;power control mechanism;reinforcement learning;signal interference;wireless body area network,,2166-9570;21669570,,9-12 Sept. 2012,,"2012 IEEE 23rd International Symposium on Personal, Indoor and Mobile Radio Communications - (PIMRC)",Game;Interference;Power Control;Reinforcement Learning;WBAN;convergence;multi agent;optimality,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6362688,,,,,,IEEE,14,,Electronic:978-1-4673-2569-1; POD:978-1-4673-2566-0; USB:978-1-4673-2568-4,,"2012 IEEE 23rd International Symposium on Personal, Indoor and Mobile Radio Communications - (PIMRC)",2030,IEEE Conferences,,,,,2012,,,,,,,,,,,,,,,,,,,
Dynamic power control in Wireless Body Area Networks using reinforcement learning with approximation,"A Wireless Body Area Network (WBAN) is made up of multiple tiny physiological sensors implanted in/on the human body with each sensor equipped with a wireless transceiver that communicates to a coordinator in a star topology. Energy is the scarcest resource in WBANs. Power control mechanisms to achieve a certain level of utility while using as little power for transmission as possible can play an important role in reducing energy consumption in such very energy-constrained networks. In this paper, we propose a novel power controller to mitigate internetwork interference in WBANs and increase the maximum achievable throughput with the minimum energy consumption. The proposed power controller employs reinforcement learning with approximation to learn from the environment and improve its performance. We compare the performance of the proposed controller to two other power controllers, one based on game theory and the other one based on fuzzy logic. Simulation results show that compared to the other two approaches, RLPC provides a substantial saving in energy consumption per bit, with a substantial increase in network lifetime.",,,,"Department of Electronic Engineering, Macquarie University, Sydney, Australia",R. Kazemi; R. Vesilo; E. Dutkiewicz; Ren Liu,,6,,10.1109/PIMRC.2011.6139908,IEEE Xplore,1,,20120126,2208,,Approximation methods;Energy consumption;Interference;Power control;Sensors;Throughput;Wireless sensor networks,approximation theory;body area networks;control engineering computing;fuzzy control;game theory;interference suppression;learning (artificial intelligence);power control;radio transceivers;telecommunication control;telecommunication network reliability,WBAN;approximation;dynamic power control;energy consumption reduction;energy-constrained networks;fuzzy logic;game theory;interference mitigation;network lifetime;physiological sensors;reinforcement learning;wireless body area networks;wireless transceiver,,2166-9570;21669570,,11-14 Sept. 2011,,"2011 IEEE 22nd International Symposium on Personal, Indoor and Mobile Radio Communications",Dynamic Power Control;Fuzzy Logic;Game Theory;Interference;Reinforcement Learning;WBAN,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139908,,,,,,IEEE,17,,Electronic:978-1-4577-1348-4; POD:978-1-4577-1346-0; USB:978-1-4577-1347-7,,"2011 IEEE 22nd International Symposium on Personal, Indoor and Mobile Radio Communications",2203,IEEE Conferences,,,,,2011,,,,,,,,,,,,,,,,,,,
"Reward Processing, Neuroeconomics, and Psychopathology","Abnormal reward processing is a prominent transdiagnostic feature of psychopathology. The present review provides a framework for considering the different aspects of reward processing and their assessment, and highlights recent insights from the field of neuroeconomics that may aid in understanding these processes. Although altered reward processing in psychopathology has often been treated as a general hypo- or hyperresponsivity to reward, increasing data indicate that a comprehensive understanding of reward dysfunction requires characterization within more specific reward-processing domains, including subjective valuation, discounting, hedonics, reward anticipation and facilitation, and reinforcement learning. As such, more nuanced models of the nature of these abnormalities are needed. We describe several processing abnormalities capable of producing the types of selective alterations in reward-related behavior observed in different forms of psychopathology, including (mal)adaptive scaling and anchoring, dysfunctional weighting of reward and cost variables, competition between valuation systems, and reward prediction error signaling. © 2017 by Annual Reviews. All rights reserved.",,"Department of Psychology, Department of Psychiatry, Vanderbilt University, Nashville, TN, United States; Department of Psychology, Emory University, Atlanta, GA, United States",,,"Zald D.H., Treadway M.T.","Zald, D.H., Department of Psychology, Department of Psychiatry, Vanderbilt University, Nashville, TN, United States; Treadway, M.T., Department of Psychology, Emory University, Atlanta, GA, United States",2,,10.1146/annurev-clinpsy-032816-044957,SCOPUS,1,,,,,,,,,,,,,Annual Review of Clinical Psychology,Addiction; Anhedonia; Computational psychiatry; Dopamine; Major depression; Prediction error,,2-s2.0-85019122756,,,,,,495,471,,,,,Scopus,,,Annual Review of Clinical Psychology,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019122756&doi=10.1146%2fannurev-clinpsy-032816-044957&partnerID=40&md5=a8b5422deb2aa507c9408e66bf128fa2,,13,,2017,,,,,,,,,,,,,,,,,,,
Controlling blood glucose variability under uncertainty using reinforcement learning and Gaussian processes,"Abstract Automated control of blood glucose (BG) concentration with a fully automated artificial pancreas will certainly improve the quality of life for insulin-dependent patients. Closed-loop insulin delivery is challenging due to inter- and intra-patient variability, errors in glucose sensors and delays in insulin absorption. Responding to the varying activity levels seen in outpatients, with unpredictable and unreported food intake, and providing the necessary personalized control for individuals is a challenging task for existing control algorithms. A novel approach for controlling glycemic variability using simulation-based learning is presented. A policy iteration algorithm that combines reinforcement learning with Gaussian process approximation is proposed. To account for multiple sources of uncertainty, a control policy is learned off-line using an Ito's stochastic model of the glucose-insulin dynamics. For safety and performance, only relevant data are sampled through Bayesian active learning. Results obtained demonstrate that a generic policy is both safe and efficient for controlling subject-specific variability due to a patient's lifestyle and its distinctive metabolic response. © 2015 Elsevier B.V.",,"INTELYMEC - Centro de Investigaciones en Física e Ingeniería del Centro - CIFICEN - CONICET, Facultad de Ingeniería, Universidad Nacional del Centro de la Provincia de Buenos Aires - UNCPBA, Av. del Valle 5737, Olavarría, Argentina; INGAR (CONICET - UTN), Avellaneda 3657, Santa Fe, Argentina",3038,,"De Paula M., Ávila L.O., Martínez E.C.","De Paula, M., INTELYMEC - Centro de Investigaciones en Física e Ingeniería del Centro - CIFICEN - CONICET, Facultad de Ingeniería, Universidad Nacional del Centro de la Provincia de Buenos Aires - UNCPBA, Av. del Valle 5737, Olavarría, Argentina; Ávila, L.O., INGAR (CONICET - UTN), Avellaneda 3657, Santa Fe, Argentina; Martínez, E.C., INGAR (CONICET - UTN), Avellaneda 3657, Santa Fe, Argentina",3,,10.1016/j.asoc.2015.06.041,SCOPUS,1,,,,,,,,,,,,,Applied Soft Computing Journal,Artificial pancreas; Diabetes; Gaussian processes; Policy iteration; Reinforcement learning; Stochastic optimal control,,2-s2.0-84937399738,,,,,,332,310,,,,,Scopus,,,Applied Soft Computing Journal,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937399738&doi=10.1016%2fj.asoc.2015.06.041&partnerID=40&md5=44631a8b252ce7b708616dfd8199006e,,35,,2015,,,,,,,,,,,,,,,,,,,
A context-aware content delivery framework for QoS in mobile cloud,"According to increasing performance of mobile devices, like smart phone, tablet PC and etc, and diffusing network infrastructures, like LTE, WiFi and etc, various types of content delivery services based on PC services can serve into mobile devices using cloud. In this paper we proposed content delivery framework with SDN (Software Defined Networking) and CCN (Content Centric Networking) to improve content delivery QoS in mobile cloud environment. Additionally to serve autonomic optimal services, we proposed reinforcement learning based context-aware content delivery scheme. Using our framework, we can guarantee QoS to provide context-aware content delivery scheme. © 2014 IEEE.",,"Department of Computer Engineering, Kyung Hee University, Yong In, South Korea",6996607,,"Haw R., Alam M.G.R., Hong C.S.","Haw, R., Department of Computer Engineering, Kyung Hee University, Yong In, South Korea; Alam, M.G.R., Department of Computer Engineering, Kyung Hee University, Yong In, South Korea; Hong, C.S., Department of Computer Engineering, Kyung Hee University, Yong In, South Korea",,,10.1109/APNOMS.2014.6996607,SCOPUS,1,,,,,,,,,,,,,APNOMS 2014 - 16th Asia-Pacific Network Operations and Management Symposium,Content centric network; Content delivery; Context-aware; Machine learning; Mobile cloud; Software defined networking,,2-s2.0-84941044465,,,,,,,,,,,,Scopus,,,APNOMS 2014 - 16th Asia-Pacific Network Operations and Management Symposium,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941044465&doi=10.1109%2fAPNOMS.2014.6996607&partnerID=40&md5=cceffa9dfa231b42ba00a42e92bbba90,,,,2014,,,,,,,,,,,,,,,,,,,
Reinforcement-Learning-Based Personalization of Head-Related Transfer Functions,"Accurately perceiving spatial locations of virtual sounds using stereo earphones or headphones requires individual head-related transfer functions (HRTFs) for each listener. However, accurate HRTF measurement is usually difficult. While previous studies have …",,,,,,,0,,,Google Scholar,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,http://www.aes.org/e-lib/browse.cfm?elib=19563,,,,2018,,,,,,,,,,,,0,,,,,,,
Towards adaptive dialogue systems for assistive living environments,"Adaptive Dialogue Systems can be seen as smart interfaces that typically use natural language (spoken or written) as a means of communication. They are being used in many applications, such as customer service, in-car interfaces, even in rehabilitation, and therefore it is essential that these systems are robust, scalable and quickly adaptable in order to cope with changing user or system needs or environmental conditions. Making Dialogue Systems adaptive means overcoming several challenges, such as scalability or lack of training data. Achieving adaptation online has thus been an even greater challenge. We propose to build such a system, that will operate in an Assistive Living Environment and provide its services as a coach to patients that need to perform rehabilitative exercises. We are currently in the process of developing it, using Robot Operating System on a robotic platform.",,"Deptartment of Computer Science and Engineering, University of Texas, Arlington, United States; Institute of Informatics and Telecommunications, National Center for Scientific Research Demokritos, Greece",,,"Papangelis A., Karkaletsis V., Huang H.","Papangelis, A., Deptartment of Computer Science and Engineering, University of Texas, Arlington, United States; Karkaletsis, V., Institute of Informatics and Telecommunications, National Center for Scientific Research Demokritos, Greece; Huang, H., Deptartment of Computer Science and Engineering, University of Texas, Arlington, United States",1,,10.1145/2451176.2451185,SCOPUS,1,,,,,,,,,,,,,"International Conference on Intelligent User Interfaces, Proceedings IUI",Adaptive dialogue systems; Dialogue management; Personalization; Reinforcement learning,,2-s2.0-84875830088,,,,,,32,29,,,,,Scopus,,,"International Conference on Intelligent User Interfaces, Proceedings IUI",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875830088&doi=10.1145%2f2451176.2451185&partnerID=40&md5=53894c815dba91d26bada3c5954a70c6,,,,2013,,,,,,,,,,,,,,,,,,,
Green Wi-Fi Implementation and Management in Dense Autonomous Environments for Smart Cities,"Advanced informatics technologies facilitate the construction of green smart cities, especially the Wi-Fi implementation and management, for rapidly increasing personal Wi-Fi devices in autonomous environments residing in nonoverlapped channels often result in low energy efficiency and severe cochannel interference. In this paper, a green Wi-Fi management framework is constructed in order to reduce the overall energy consumption through turning off a portion of access points (APs) and aggregating their users to the other active APs. A Tabu-search-assisted active AP selection algorithm is proposed to minimize the power consumption with a seamless wireless converge. For the active APs, based on our defined metric airtime cost that is integrated by the in-range interference and the hidden terminal interference, a reinforcement-learning-aided AP self-management algorithm is proposed to dynamically adjust APs' channels in the partially overlapped channel space. Extensive simulations and field experiments demonstrate that the power consumption can be reduced by about 65%, and the airtime cost of APs can be reduced by 50% compared with the typical least congestion channel search algorithm.",,,,"Department of Electrical Engineering, Tsinghua University, Beijing, China",Y. Zhang; C. Jiang; J. Wang; Z. Han; J. Yuan; J. Cao,,,,10.1109/TII.2017.2785820,IEEE Xplore,1,,20180404,1563,National Basic Research Program of China; National Natural Science Foundation of China; US NSF; Young Elite Scientists Sponsorship; ,Energy consumption;Green products;Informatics;Interference;Power demand;Wireless communication;Wireless fidelity,cochannel interference;computer network management;energy conservation;green computing;learning (artificial intelligence);power consumption;radiofrequency interference;search problems;smart cities;telecommunication power management;wireless LAN;wireless channels,AP self-management algorithm;Tabu-search-assisted active AP selection algorithm;access points;advanced informatics technologies;cochannel interference;defined metric airtime cost;dense autonomous environments;energy consumption reduction;green Wi-Fi management framework;green smart cities;hidden terminal interference;in-range interference;least congestion channel search algorithm;low energy efficiency;metric airtime cost;nonoverlapped channels;partially overlapped channel space;personal Wi-Fi devices;power consumption;reinforcement-learning-aided AP self-management algorithm,,1551-3203;15513203,4,April 2018,,IEEE Transactions on Industrial Informatics,Energy efficiency;green Wi-Fi;partially overlapped channels (POCs);self-management,,,,,20171221,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8233146,,,,,,IEEE,,,,,IEEE Transactions on Industrial Informatics,1552,IEEE Journals & Magazines,,,14,,2018,,,,,,,,,,,,,,,,,,,
A personalized and integrative comparison-shopping engine and its applications,"Agents are the catalysts for commerce on the Web today. For example, comparison-shopping agents mediate the interactions between consumers and suppliers in order to yield markets that are more efficient. However, today's shopping agents are price-dominated, unreflective of the nature of supplier/consumer differentiation or the changing course of differentiation over time. This paper aims to tackle this dilemma and advances shopping agents into a stage where both kinds of differentiation are taken into account for enhanced understanding of the realities. We call them personalized and integrative shopping agents. These agents can leverage the interactive power of the Web for a more accurate understanding of consumer's preferences. This paper then presents a comparison-shopping engine that can be easily instantiated to become personalized and integrative shopping agents. This engine comprises of a product/merchant information collector, a consumer behavior extractor, a user profile manager, and an on-line learning personalized ranking module. We have built this engine and instantiated a comparison-shopping system for collecting preliminary evaluation results. The results show that this system is quite promising in overcoming the reality challenges of comparison shopping. In order to strengthen the contributions of this engine, we also gave a fielded application of this engine for personalized travel information discovery and explained the great potentials of this engine for a variety of comparison-shopping tasks. © 2002 Elsevier Science B.V. All rights reserved.",,"Information Management Department, Fu-Jen University, 242 Taipei, Taiwan",,,Yuan S.-T.,"Yuan, S.-T., Information Management Department, Fu-Jen University, 242 Taipei, Taiwan",38,,10.1016/S0167-9236(02)00077-5,SCOPUS,1,,,,,,,,,,2,,,Decision Support Systems,(Multi-) agent systems; Comparison shopping; Consumer valuation models; Neural networks; Reinforcement learning,,2-s2.0-0037209477,,,,,,156,139,,,,,Scopus,,,Decision Support Systems,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037209477&doi=10.1016%2fS0167-9236%2802%2900077-5&partnerID=40&md5=37491eee2acf272198efc1cf74b290e9,,34,,2003,,,,,,,,,,,,,,,,,,,
Distributed agent-based air traffic flow management,"Air traffic flow management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today. The FAA estimates that in 2005 alone, there were over 322,000 hours of delays at a cost to the industry in excess of three billion dollars. Finding reliable and adaptive solutions to the flow management problem is of paramount importance if the Next Generation Air Transportation Systems are to achieve the stated goal of accommodating three times the current traffic volume. This problem is particularly complex as it requires the integration and/or coordination of many factors including: new data (e.g., changing weather info), potentially conflicting priorities (e.g., different airlines), limited resources (e.g., air traffic controllers) and very heavy traffic volume (e.g., over 40,000 flights over the US airspace). In this paper we use FACET - an air traffic flow simulator developed at NASA and used extensively by the FAA and industry - to test a multi-agent algorithm for traffic flow management. An agent is associated with a fix (a specific location in 2D space) and its action consists of setting the separation required among the airplanes going though that fix. Agents use reinforcement learning to set this separation and their actions speed up or slow down traffic to manage congestion. Our FACET based results show that agents receiving personalized rewards reduce congestion by up to 45% over agents receiving a global reward and by up to 67% over a current industry approach (Monte Carlo estimation). © 2007 IFAAMAS.",,"Oregon State University, 204 Rogers Hall, Corvallis, OR 97331, United States; UCSC, NASA Ames Research Center, Mailstop 269-3, Moffett Field, CA 94035, United States",255,,"Tumer K., Agogino A.","Tumer, K., Oregon State University, 204 Rogers Hall, Corvallis, OR 97331, United States; Agogino, A., UCSC, NASA Ames Research Center, Mailstop 269-3, Moffett Field, CA 94035, United States",23,,10.1145/1329125.1329434,SCOPUS,1,,,,,,,,,,,,,Proceedings of the International Conference on Autonomous Agents,Air traffic control; Multiagent systems; Optimization; Reinforcement learning,,2-s2.0-60349116249,,,,,,349,342,,,,,Scopus,,,Proceedings of the International Conference on Autonomous Agents,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349116249&doi=10.1145%2f1329125.1329434&partnerID=40&md5=e15919384d9d2bae22084e828e135bbc,,,,2007,,,,,,,,,,,,,,,,,,,
Studies on Drivers' Driving Styles Based on Inverse Reinforcement Learning,"Although advanced driver assistance systems (ADAS) have been widely introduced in automotive industry to enhance driving safety and comfort, and to reduce drivers' driving burden, they do not in general reflect different drivers' driving styles or customized with individual personalities. This can be important to comfort and enjoyable driving experience, and to improved market acceptance. However, it is challenging to understand and further identify drivers' driving styles due to large number and great variations of driving population. Previous research has mainly adopted physical approaches in modeling drivers' driving behavior, which however are often very much limited, if not impossible, in capturing human drivers' driving characteristics. This paper proposes a reinforcement learning based approach, in which the driving styles are formulated through drivers' learning processes from interaction with surrounding environment. Based on the reinforcement learning theory, driving action can be treated as maximizing a reward function. Instead of calibrating the unknown reward function to satisfy driver's desired response, we try to recover it from the human driving data, utilizing maximum likelihood inverse reinforcement learning (MLIRL). An IRL-based longitudinal driving assistance system is also proposed in this paper. Firstly, large amount of real world driving data is collected from a test vehicle, and the data is split into two sets for training and for testing purposes respectively. Then, the longitudinal acceleration is modeled as a Boltzmann distribution in human driving activity. The reward function is denoted as a linear combination of some kernelized basis functions. The driving style parameter vector is estimated using MLIRL based on the training set. Finally, a learning-based longitudinal driving assistance algorithm is developed and evaluated on the testing set. The results demonstrate that the proposed method can satisfactorily reflect human drivers' driving behavior. © 2018 SAE International; General Motors LLC.",,"Jilin University, China; General Motors LLC, China",,,"Jiang Y., Deng W., Wang J., Zhu B.","Jiang, Y., Jilin University, China; Deng, W., Jilin University, China; Wang, J., General Motors LLC, China; Zhu, B., Jilin University, China",,,10.4271/2018-01-0612,SCOPUS,1,,,,,,,,,,,,,SAE Technical Papers,,,2-s2.0-85045526693,,,,,,,,,,,,Scopus,,,SAE Technical Papers,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045526693&doi=10.4271%2f2018-01-0612&partnerID=40&md5=b739715ea81417e25a5fbccb4160734d,,2018-April,,2018,,,,,,,,,,,,,,,,,,,
Model-free machine learning in biomedicine: Feasibility study in type 1 diabetes,"Although reinforcement learning (RL) is suitable for highly uncertain systems, the applicability of this class of algorithms to medical treatment may be limited by the patient variability which dictates individualised tuning for their usually multiple algorithmic parameters. This study explores the feasibility of RL in the framework of artificial pancreas development for type 1 diabetes (T1D). In this approach, an Actor-Critic (AC) learning algorithm is designed and developed for the optimisation of insulin infusion for personalised glucose regulation. AC optimises the daily basal insulin rate and insulin:carbohydrate ratio for each patient, on the basis of his/her measured glucose profile. Automatic, personalised tuning of AC is based on the estimation of information transfer (IT) from insulin to glucose signals. Insulinto-glucose IT is linked to patient-specific characteristics related to total daily insulin needs and insulin sensitivity (SI). The AC algorithm is evaluated using an FDA-accepted T1D simulator on a large patient database under a complex meal protocol, meal uncertainty and diurnal SI variation. The results showed that 95.66% of time was spent in normoglycaemia in the presence of meal uncertainty and 93.02% when meal uncertainty and SI variation were simultaneously considered. The time spent in hypoglycaemia was 0.27% in both cases. The novel tuning method reduced the risk of severe hypoglycaemia, especially in patients with low SI. © 2016 Daskalaki et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",Open Access,"Diabetes Technology Research Group, ARTORG Center for Biomedical Engineering Research, University of Bern, Murtenstrasse 50, Bern, Switzerland; Division of Endocrinology, Diabetes and Clinical Nutrition, Bern University Hospital Inselspital, Bern, Switzerland", e0158722,,"Daskalaki E., Diem P., Mougiakakou S.G.","Daskalaki, E., Diabetes Technology Research Group, ARTORG Center for Biomedical Engineering Research, University of Bern, Murtenstrasse 50, Bern, Switzerland; Diem, P., Division of Endocrinology, Diabetes and Clinical Nutrition, Bern University Hospital Inselspital, Bern, Switzerland; Mougiakakou, S.G., Diabetes Technology Research Group, ARTORG Center for Biomedical Engineering Research, University of Bern, Murtenstrasse 50, Bern, Switzerland, Division of Endocrinology, Diabetes and Clinical Nutrition, Bern University Hospital Inselspital, Bern, Switzerland",,,10.1371/journal.pone.0158722,SCOPUS,1,,,,,,,,,,7,,,PLoS ONE,,,2-s2.0-84979704088,,,,,,,,,,,,Scopus,,,PLoS ONE,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979704088&doi=10.1371%2fjournal.pone.0158722&partnerID=40&md5=ef957c6dffd5fa2b20f63df7b81047f6,,11,,2016,,,,,,,,,,,,,,,,,,,
Augmenting practical cross-layer MAC schedulers via offline reinforcement learning,"An automated offline design process for optimized cross-layer schedulers can produce augmented scheduling algorithms tailored to a target deployment scenario. We discuss the application of ODDS, a reinforcement learning technique we introduced, for augmenting LTE MAC scheduler algorithms of practical significance. ODDS observes the correlation between the value of a utility function and the cross-layer state parameters seen by an instrumented baseline scheduler, determining the best actions via an offline Monte Carlo exploration of the problem space. The result of the ODDS process is a compact definition of a scheduling policy that has been optimized for a target scenario and utility function. In this paper we instrument a production scheduler definition to evaluate the potential of augmented schedulers in practical use, and experiment with awareness to application traffic properties by using a multi-class utility function, yielding scheduling policies that behave differently depending on the features of individual traffic classes.",,,,Nokia Bell Labs,F. Pianese; P. J. Danielsen,,,,10.1109/PIMRC.2017.8292409,IEEE Xplore,1,,20180215,6,,Algorithm design and analysis;Measurement;Optimization;Production;Scheduling algorithms;Throughput;Training,Long Term Evolution;Monte Carlo methods;access protocols;learning (artificial intelligence);telecommunication scheduling;telecommunication traffic,LTE MAC scheduler algorithms;ODDS process;augmented schedulers;augmented scheduling algorithms;automated offline design process;cross-layer state parameters;instrumented baseline scheduler;multiclass utility function;offline Monte Carlo exploration;offline reinforcement learning;optimized cross-layer schedulers;practical cross-layer MAC schedulers;production scheduler definition;reinforcement learning technique;scheduling policy;target deployment scenario,,,,8-13 Oct. 2017,,"2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8292409,,,,,,IEEE,,,Electronic:978-1-5386-3531-5; POD:978-1-5386-3532-2; Paper:978-1-5386-3529-2; USB:978-1-5386-3530-8,,"2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",1,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
An experimental approach to robotic grasping using a connectionist architecture and generic grasping functions,"An experimental approach to robotic grasping is presented. This approach is based on developing a generic representation of grasping rules, which allows learning them from experiments between the object and the robot. A modular connectionist design arranged in subsumption layers is used to provide a mapping between sensory inputs and robot actions. Reinforcement feedback is used to select between different grasping rules and to reduce the number of failed experiments. This is particularly critical for applications in the personal service robot environment. Simulated experiments on a 15-object database show that the system is capable of learning grasping rules for each object in a finite number of experiments as well as generalizing from experiments on one object to grasping from another",,,,"Dept. of Syst. Design Eng., Waterloo Univ., Ont., Canada",M. A. Moussa; M. S. Kamel,,17,,10.1109/5326.669561,IEEE Xplore,1,,20020806,253,,Databases;Grasping;Grippers;Humans;Intelligent robots;Manipulators;Performance evaluation;Robot sensing systems;Service robots;System testing,feedback;learning systems;manipulators;neural nets,connectionist architecture;generic grasping functions;generic grasping rule representation;learning;modular connectionist design;object database;personal service robot environment;reinforcement feedback;robot actions;robotic grasping;sensory inputs;simulated experiments;subsumption layers,,1094-6977;10946977,2,May 1998,,"IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=669561,,,,,,IEEE,26,,,,"IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",239,IEEE Journals & Magazines,,,28,,1998,,,,,,,,,,,,,,,,,,,
Research on user's interest of web personalized information service,"An important problem in Web Personalized Information Service is to describe, measure, acquire and update a user's interest information. Aiming at the problem, this paper puts forward a new approach: Firstly, a general model for the user's interest information is constructed; Secondly, the learning agent initializes the interestingness of the user's interest topic according to his background information; Lastly, the learning agent updates the interestingness dynamically, based on his browsing actions or behaviors, and on his feedbacks by adopting Q-Learning algorithm of Reinforcement Learning. The above approach has four advantages: 1)the user need not describe and edit his profile; 2)the interestingness of the user's interest topic can be computed and quantified; 3)the agent can track the user's interest navigation and expansion dynamically and adapt to it; 4)the approach can be implemented easily.",,"Department of Compute Science and Engineering, Shanghai Jiaotong University, Shanghai 200030, China; School of Information Engineering, East China Jiaotong University, Nanchang 330013, China; School of Computer Information Engineering, Jiangxi Normal University, Nanchang 330022, China",,,"Zhong M., Wang M., Lu R.","Zhong, M., Department of Compute Science and Engineering, Shanghai Jiaotong University, Shanghai 200030, China, School of Information Engineering, East China Jiaotong University, Nanchang 330013, China; Wang, M., School of Computer Information Engineering, Jiangxi Normal University, Nanchang 330022, China; Lu, R., Department of Compute Science and Engineering, Shanghai Jiaotong University, Shanghai 200030, China",,,,SCOPUS,0,,,,,,,,,,4,,,Journal of Computational Information Systems,Interestingness; Personalized information service; Q-learning; User interest model; Web,,2-s2.0-33748151446,,,,,,965,959,,,,,Scopus,,,Journal of Computational Information Systems,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748151446&partnerID=40&md5=a62c93d05f8749b3abd44d7597fa92b5,,1,,2005,,,,,,,,,,,,,,,,,,,
Personalized tuning of a reinforcement learning control algorithm for glucose regulation,"Artificial pancreas is in the forefront of research towards the automatic insulin infusion for patients with type 1 diabetes. Due to the high inter- and intra-variability of the diabetic population, the need for personalized approaches has been raised. This study presents an adaptive, patient-specific control strategy for glucose regulation based on reinforcement learning and more specifically on the Actor-Critic (AC) learning approach. The control algorithm provides daily updates of the basal rate and insulin-to-carbohydrate (IC) ratio in order to optimize glucose regulation. A method for the automatic and personalized initialization of the control algorithm is designed based on the estimation of the transfer entropy (TE) between insulin and glucose signals. The algorithm has been evaluated in silico in adults, adolescents and children for 10 days. Three scenarios of initialization to i) zero values, ii) random values and iii) TE-based values have been comparatively assessed. The results have shown that when the TE-based initialization is used, the algorithm achieves faster learning with 98%, 90% and 73% in the A+B zones of the Control Variability Grid Analysis for adults, adolescents and children respectively after five days compared to 95%, 78%, 41% for random initialization and 93%, 88%, 41% for zero initial values. Furthermore, in the case of children, the daily Low Blood Glucose Index reduces much faster when the TE-based tuning is applied. The results imply that automatic and personalized tuning based on TE reduces the learning period and improves the overall performance of the AC algorithm.",,,,"Diabetes Technology Research Group, ARTORG Center for Biomedical Engineering Research, University of Bern, 3010, Switzerland",E. Daskalaki; P. Diem; S. G. Mougiakakou,,3,,10.1109/EMBC.2013.6610293,IEEE Xplore,1,,20130926,3490,,Algorithm design and analysis;Diabetes;Insulin;Integrated circuits;Learning (artificial intelligence);Sugar;Vectors,blood;entropy;learning (artificial intelligence);medical computing;paediatrics;sugar,Actor-Critic learning approach;TE-based initialization;adolescents;artificial pancreas;automatic insulin infusion;basal rate;control variability grid analysis;glucose regulation;glucose signals;insulin signals;insulin-to-carbohydrate ratio;low-blood glucose index;patient-specific control strategy;personalized tuning;reinforcement learning control algorithm;transfer entropy;type 1 diabetes,,1094-687X;1094687X,,3-7 July 2013,,2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),,,,,"Adolescent;Adult;Algorithms;Blood Glucose;Child;Humans;Insulin;Pancreas, Artificial;Precision Medicine;Signal Processing, Computer-Assisted",,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6610293,,,,,,IEEE,27,,Electronic:978-1-4577-0216-7; POD:978-1-4577-0215-0; USB:978-1-4577-0214-3,,2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),3487,IEEE Conferences,,,,,2013,,,,,,,,,,,,,,,,,,,
Path planning with user route preference - A reward surface approximation approach using orthogonal Legendre polynomials,"As self driving cars become more ubiquitous, users would look for natural ways of informing the car AI about their personal choice of routes. This choice is not always dictated by straightforward logic such as shortest distance or shortest time, and can be influenced by hidden factors, such as comfort and familiarity. This paper presents a path learning algorithm for such applications, where from limited positive demonstrations, an autonomous agent learns the user's path preference and honors that choice in its route planning, but has the capability to adopt alternate routes, if the original choice(s) become impractical. The learning problem is modeled as a Markov decision process. The states (way-points) and actions (to move from one way-point to another) are pre-defined according to the existing network of paths between the origin and destination and the user's demonstration is assumed to be a sample of the preferred path. The underlying reward function which captures the essence of the demonstration is computed using an inverse reinforcement learning algorithm and from that the entire path mirroring the expert's demonstration is extracted. To alleviate the problem of state space explosion when dealing with a large state space, the reward function is approximated using a set of orthogonal polynomial basis functions with a fixed number of coefficients regardless of the size of the state space. A six fold reduction in total learning time is achieved compared to using simple basis functions, that has dimensionality equal to the number of distinct states.",,,,"Department of Mechanical Aerospace and Biomedical Engineering, University of Tennessee, Knoxville, 37996 USA",A. R. Srinivasan; S. Chakraborty,,,,10.1109/COASE.2016.7743527,IEEE Xplore,1,,20161117,1105,,Automobiles;Databases;Learning (artificial intelligence);Markov processes;Path planning;Planning;Real-time systems,Markov processes;learning (artificial intelligence);mobile robots;path planning;state-space methods,Markov decision process;autonomous agent learns;dimensionality;inverse reinforcement learning algorithm;large state space;orthogonal Legendre polynomials;orthogonal polynomial basis functions;path planning;reward function;reward surface approximation;route planning;self driving cars;six fold reduction;state space explosion;user path preference;user route preference,,,,21-25 Aug. 2016,,2016 IEEE International Conference on Automation Science and Engineering (CASE),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7743527,,,,,,IEEE,,,Electronic:978-1-5090-2409-4; POD:978-1-5090-2410-0; USB:978-1-5090-2408-7,,2016 IEEE International Conference on Automation Science and Engineering (CASE),1100,IEEE Conferences,,,,,2016,,,,,,,,,,,,,,,,,,,
Reinforcement learning based data self-destruction scheme for secured data management,"As technologies and services that leverage cloud computing have evolved, the number of businesses and individuals who use them are increasing rapidly. In the course of using cloud services, as users store and use data that include personal information, research on privacy protection models to protect sensitive information in the cloud environment is becoming more important. As a solution to this problem, a self-destructing scheme has been proposed that prevents the decryption of encrypted user data after a certain period of time using a Distributed Hash Table (DHT) network. However, the existing self-destructing scheme does not mention how to set the number of key shares and the threshold value considering the environment of the dynamic DHT network. This paper proposes a method to set the parameters to generate the key shares needed for the self-destructing scheme considering the availability and security of data. The proposed method defines state, action, and reward of the reinforcement learning model based on the similarity of the graph, and applies the self-destructing scheme process by updating the parameter based on the reinforcement learning model. Through the proposed technique, key sharing parameters can be set in consideration of data availability and security in dynamic DHT network environments. © 2018 by the authors.",,"Department of Computer Science and Engineering, Kyung Hee University, Yongin-si, South Korea; Databank Systems, Daegu, South Korea",136,,"Kim Y.K., Ullah S., Kwon K., Jang Y., Lee J., Hong C.S.","Kim, Y.K., Department of Computer Science and Engineering, Kyung Hee University, Yongin-si, South Korea; Ullah, S., Department of Computer Science and Engineering, Kyung Hee University, Yongin-si, South Korea; Kwon, K., Databank Systems, Daegu, South Korea; Jang, Y., Databank Systems, Daegu, South Korea; Lee, J., Databank Systems, Daegu, South Korea; Hong, C.S., Department of Computer Science and Engineering, Kyung Hee University, Yongin-si, South Korea",,,10.3390/sym10050136,SCOPUS,1,,,,,,,,,,5,,,Symmetry,DHT network; Privacy protection; Reinforcement learning; Self-destructing scheme,,2-s2.0-85047266273,,,,,,,,,,,,Scopus,,,Symmetry,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047266273&doi=10.3390%2fsym10050136&partnerID=40&md5=a2c437f3cefa1531010a989debbacb75,,10,,2018,,,,,,,,,,,,,,,,,,,
Quantifying uncertainty in batch personalized sequential decision making,"As the amount of data collected from individuals increases, there are more opportunities to use it to offer personalized experiences (e.g., using electronic health records to offer personalized treatments). We advocate applying techniques from batch reinforcement learning to predict the range of effectiveness that policies might have for individuals. We identify three sources of uncer-tainty and present a method that addresses all of them. It handles the uncertainty caused by population mismatch by modeling the data as a latent mixture of different subpopulations of individuals, it explicitly quantifies data sparsity by accounting for the limited data available about the underlying models, and incorporates intrinsic stochasticity to yield estimated percentile ranges of the effectiveness of a policy for a particular new individual. Using this approach, we highlight some interesting variability in policy effectiveness amongst HIV patients given a prior patient treatment dataset. Our approach highlights the potential benefit of taking into account individual variability and data limitations when performing batch policy evaluation for new individuals. © Copyright 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Department of Computer Science, Rutgers University, United States; Department of Computer Science, Carnegie Mellon University, United States; Department of Computer Science, Brown University, United States",,,"Marivate V., Chemali J., Brunskill E., Littmanf M.","Marivate, V., Department of Computer Science, Rutgers University, United States; Chemali, J., Department of Computer Science, Carnegie Mellon University, United States; Brunskill, E., Department of Computer Science, Carnegie Mellon University, United States; Littmanf, M., Department of Computer Science, Brown University, United States",1,,,SCOPUS,0,,,,,,,,,,,,,AAAI Workshop - Technical Report,,,2-s2.0-84974800780,,,,,,30,26,,,,,Scopus,,,AAAI Workshop - Technical Report,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974800780&partnerID=40&md5=436733a7134fcc8b7ea47a3703f08edc,,WS-14-08,,2014,,,,,,,,,,,,,,,,,,,
Intelligent real-time therapy: Harnessing the power of machine learning to optimise the delivery of momentary cognitivebehavioural interventions,"Background Experience sampling methodology (ESM) [Csikszentmihalyi, M. & Larson, R. (1987). Validity and reliability of the experience-sampling method. Journal of Nervous and Mental Disease, 175(9), 526536] has been used to elucidate the cognitivebehavioural mechanisms underlying the development and maintenance of complex mental disorders as well as mechanisms involved in resilience from such states. We present an argument for the development of intelligent real-time therapy (iRTT). Machine learning and reinforcement learning specifically may be used to optimise the delivery of interventions by observing and altering the timing of real-time therapies based on ongoing ESM measures.Aims The aims of the present article are to outline the principles of iRTT and to consider how it would be applied to complex problems such as suicide prevention.Methods Relevant literature was identified through use of PychInfo.Results iRTT may provide an important and ecologically valid adjunct to traditional CBT, providing a means of balancing population-based data with individual data, thus addressing the ""knowledgepractice gap"" [Tarrier, N. (2010b). The cognitive and behavioral treatment of PTSD, what is known and what is known to be unknown: How not to fall into the practice gap. Clinical Psychology: Science and Practice, 17(2), 134143] and facilitating the delivery of interventions in situ, thereby addressing the ""therapyreal-world gap"".Conclusions iRTT may provide a platform for the development of individualised and multifaceted momentary intervention strategies that are ecologically valid and aimed at attenuating pathological pathways to complex mental health problems and amplifying pathways associated with resilience. © 2012 Informa UK, Ltd.",,"Greater Manchester West Mental Health NHS Foundation Trust, Department of Psychology, Prestwich Hospital, Prestwich M25 3BL, United Kingdom; School of Psychological Sciences, University of Manchester, Manchester, United Kingdom; School of Community Based Medicine, University of Manchester, Manchester, United Kingdom; Department of Psychology, Institute of Psychiatry, King's College London, London, United Kingdom",,,"Kelly J., Gooding P., Pratt D., Ainsworth J., Welford M., Tarrier N.","Kelly, J., Greater Manchester West Mental Health NHS Foundation Trust, Department of Psychology, Prestwich Hospital, Prestwich M25 3BL, United Kingdom, School of Psychological Sciences, University of Manchester, Manchester, United Kingdom; Gooding, P., School of Psychological Sciences, University of Manchester, Manchester, United Kingdom; Pratt, D., School of Psychological Sciences, University of Manchester, Manchester, United Kingdom; Ainsworth, J., School of Community Based Medicine, University of Manchester, Manchester, United Kingdom; Welford, M., Greater Manchester West Mental Health NHS Foundation Trust, Department of Psychology, Prestwich Hospital, Prestwich M25 3BL, United Kingdom; Tarrier, N., School of Psychological Sciences, University of Manchester, Manchester, United Kingdom, Department of Psychology, Institute of Psychiatry, King's College London, London, United Kingdom",22,,10.3109/09638237.2011.638001,SCOPUS,1,,,,,,,,,,4,,,Journal of Mental Health,Cognitive behaviour therapy (CBT); Ecological momentary intervention (EMI); Experience sampling methodology (ESM); Intelligent real-time therapy (iRTT); Smartphone; Suicide prevention,,2-s2.0-84864214034,,,,,,414,404,,,,,Scopus,,,Journal of Mental Health,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864214034&doi=10.3109%2f09638237.2011.638001&partnerID=40&md5=a00355d44c25f890ea793fe20c824beb,,21,,2012,,,,,,,,,,,,,,,,,,,
Encouraging Physical Activity in Patients With Diabetes: Intervention Using a Reinforcement Learning System,"BACKGROUND: Regular physical activity is known to be beneficial for people with type 2 diabetes. Nevertheless, most of the people who have diabetes lead a sedentary lifestyle. Smartphones create new possibilities for helping people to adhere to their physical activity goals through continuous monitoring and communication, coupled with personalized feedback.OBJECTIVE: The aim of this study was to help type 2 diabetes patients increase the level of their physical activity.METHODS: We provided 27 sedentary type 2 diabetes patients with a smartphone-based pedometer and a personal plan for physical activity. Patients were sent short message service messages to encourage physical activity between once a day and once per week. Messages were personalized through a Reinforcement Learning algorithm so as to improve each participant's compliance with the activity regimen. The algorithm was compared with a static policy for sending messages and weekly reminders.RESULTS: Our results show that participants who received messages generated by the learning algorithm increased the amount of activity and pace of walking, whereas the control group patients did not. Patients assigned to the learning algorithm group experienced a superior reduction in blood glucose levels (glycated hemoglobin [HbA1c]) compared with control policies, and longer participation caused greater reductions in blood glucose levels. The learning algorithm improved gradually in predicting which messages would lead participants to exercise.CONCLUSIONS: Mobile phone apps coupled with a learning algorithm can improve adherence to exercise in diabetic patients. This algorithm can be used in large populations of diabetic patients to improve health and glycemic control. Our results can be expanded to other areas where computer-led health coaching of humans may have a positive impact. Summary of a part of this manuscript has been previously published as a letter in Diabetes Care, 2016.",,"Microsoft Research, Herzeliya, Israel; Technion - Israel Institute of Technology, Faculty of Medicine, Haifa, Israel; Technion - Israel Institute of Technology, Faculty of Electrical Engineering, Haifa, Israel; Technion - Israel Institute of Technology, Faculty of Industrial Engineering, Haifa, Israel; Rambam Healthcare Campus, Institute of Endocrinology, Haifa, Israel",,,"Yom-Tov E., Feraru G., Kozdoba M., Mannor S., Tennenholtz M., Hochberg I.","Yom-Tov, E., Microsoft Research, Herzeliya, Israel; Feraru, G., Technion - Israel Institute of Technology, Faculty of Medicine, Haifa, Israel; Kozdoba, M., Technion - Israel Institute of Technology, Faculty of Electrical Engineering, Haifa, Israel; Mannor, S., Technion - Israel Institute of Technology, Faculty of Electrical Engineering, Haifa, Israel; Tennenholtz, M., Technion - Israel Institute of Technology, Faculty of Industrial Engineering, Haifa, Israel; Hochberg, I., Rambam Healthcare Campus, Institute of Endocrinology, Haifa, Israel",,,10.2196/jmir.7994,SCOPUS,1,,,,,,,,,,10,,,Journal of medical Internet research,diabetes type 2; physical activity; reinforcement learning,,2-s2.0-85042765009,,,,,,,e338,,,,,Scopus,,,Journal of medical Internet research,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042765009&doi=10.2196%2fjmir.7994&partnerID=40&md5=d20112fde48afda622fd704f8b2ae508,,19,,2017,,,,,,,,,,,,,,,,,,,
"Introduce the ""Flipped Classroom"" into the Basic Medical Experimental Teaching by the Medical Virtual Simulation Platform","Based on drawing ""flipped classroom"" experience of practitioners domestic and foreign, together with the full use of modern information tools and the reinforcement of the construction of discipline support system, we build the medical virtual simulation platform. On this basis, we perform reasonable instructional design and good application of ""flipped classroom"" in medical experimental teaching. These efforts adapt the learning requirements of students at different levels, and can enhance students' self-learning ability. These efforts can also train students' creative and independent thinking ability, achieve personalized learning, and improve the quality of medical students' experimental teaching. Construction of medical virtual simulation platform in the school of medicine not only provides a new method for basic medical experimental teaching to enable introduction of ""flipped classroom"" into basic medical experimental teaching, but also promotes implementation of information education and improves the teaching quality of professional disciplines.",,,,"Sch. of Med., Henan Univ., Kaifeng, China",G. Zhenying; B. Huiling; W. Guoying,,,,10.1109/ITME.2015.172,IEEE Xplore,1,,20160310,487,,Adaptation models;Computational modeling;Computers;Education;Online services;Solid modeling;Videos,computer aided instruction;educational institutions;human factors;learning (artificial intelligence);medical computing;teaching,"""Flipped Classroom"";information education;medical experimental teaching;medical virtual simulation platform;modern information tool",,,,13-15 Nov. 2015,,2015 7th International Conference on Information Technology in Medicine and Education (ITME),"""flipped classroom"";basic medicine;experimental teaching",,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429195,,,,,,IEEE,9,,CD-ROM:978-1-4673-8301-1; Electronic:978-1-4673-8302-8; POD:978-1-4673-8303-5,,2015 7th International Conference on Information Technology in Medicine and Education (ITME),484,IEEE Conferences,,,,,2015,,,,,,,,,,,,,,,,,,,
Reinforcement learning for robot soccer,"Batch reinforcement learning methods provide a powerful framework for learning efficiently and effectively in autonomous robots. The paper reviews some recent work of the authors aiming at the successful application of reinforcement learning in a challenging and complex domain. It discusses several variants of the general batch learning framework, particularly tailored to the use of multilayer perceptrons to approximate value functions over continuous state spaces. The batch learning framework is successfully used to learn crucial skills in our soccer-playing robots participating in the RoboCup competitions. This is demonstrated on three different case studies. © 2009 Springer Science+Business Media, LLC.",,"Department of Computer Science, Albert-Ludwigs-Universität Freiburg, Freiburg, Germany",,,"Riedmiller M., Gabel T., Hafner R., Lange S.","Riedmiller, M., Department of Computer Science, Albert-Ludwigs-Universität Freiburg, Freiburg, Germany; Gabel, T., Department of Computer Science, Albert-Ludwigs-Universität Freiburg, Freiburg, Germany; Hafner, R., Department of Computer Science, Albert-Ludwigs-Universität Freiburg, Freiburg, Germany; Lange, S., Department of Computer Science, Albert-Ludwigs-Universität Freiburg, Freiburg, Germany",83,,10.1007/s10514-009-9120-4,SCOPUS,1,,,,,,,,,,1,,,Autonomous Robots,Autonomous learning robots; Batch reinforcement learning; Learning mobile robots; Neural control; RoboCup,,2-s2.0-67650996818,,,,,,73,55,,,,,Scopus,,,Autonomous Robots,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650996818&doi=10.1007%2fs10514-009-9120-4&partnerID=40&md5=6fa390732e9f6fea8e925083bf502037,,27,,2009,,,,,,,,,,,,,,,,,,,
Reinforcement learning system to mitigate small-cell interference through directionality,"Beam-steering techniques using directional antennas are expected to play an important role in wireless network capacity expansion through ubiquitous small-cell deployment. However, integrating directional antennas into the existing wireless PHY and MAC stack of small cells has been challenging due to the added protocol overhead and lack of a robust antenna beam selection technique that can adapt well to environmental changes. This paper presents the design, implementation, and evaluation of LinkPursuit, a novel learning protocol for distributed antenna state selection in directional small-cell networks. LinkPursuit relies on reconfigurable antennas and a synchronous TimeDivision Multiple Access (TDMA) MAC to achieve simultaneous directional transmission and reception. Further, the system employs a practical antenna selection protocol based on the well known adaptive pursuit algorithm from the reinforcement learning literature. We implement a realtime prototype of LinkPursuit on the WARP platform and conduct extensive experiments to evaluate its performance. The empirical results show that appropriate use of directionality in LinkPursuit can result in higher network sum rates than omnidirectional transmission under various degrees of cross-link interference.",,,,"CWC, University of Oulu, Finland",A. Paatelma; D. H. Nguyen; H. Saarnisaari; N. Kandasamy; K. R. Dandekar,,,,10.1109/PIMRC.2017.8292393,IEEE Xplore,1,,20180215,7,,Directive antennas;Media Access Protocol;Slot antennas;Time division multiple access;Transmitting antennas;Wireless communication,access protocols;beam steering;cellular radio;directive antennas;learning (artificial intelligence);radiofrequency interference;time division multiple access,LinkPursuit;adaptive pursuit algorithm;beam-steering techniques;directional antennas;directional small-cell networks;distributed antenna state selection;reconfigurable antennas;reinforcement learning system;robust antenna beam selection technique;small-cell interference;synchronous TimeDivision Multiple Access MAC;ubiquitous small-cell deployment;wireless network capacity expansion,,,,8-13 Oct. 2017,,"2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8292393,,,,,,IEEE,,,Electronic:978-1-5386-3531-5; POD:978-1-5386-3532-2; Paper:978-1-5386-3529-2; USB:978-1-5386-3530-8,,"2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",1,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
Consistent Goal-Directed User Model for Realisitc Man-Machine Task-Oriented Spoken Dialogue Simulation,"Because of the great variability of factors to take into account, designing a spoken dialogue system is still a tailoring task. Rapid design and reusability of previous work is made very difficult. For these reasons, the application of machine learning methods to dialogue strategy optimization has become a leading subject of researches this last decade. Yet, techniques such as reinforcement learning are very demanding in training data while obtaining a substantial amount of data in the particular case of spoken dialogues is time-consuming and therefore expansive. In order to expand existing data sets, dialogue simulation techniques are becoming a standard solution. In this paper we describe a user modeling technique for realistic simulation of man-machine goal-directed spoken dialogues. This model, based on a stochastic description of man-machine communication, unlike previously proposed models, is consistent along the interaction according to its history and a predefined user goal",,,,"&#201;cole Sup&#233;rieure d'&#201;lectricit&#233; - SUPELEC, Metz Campus - STS Team, 2 rue &#201;douard Belin - F-57070 Metz - FRANCE. email: olivier.pietquin@supelec.fr",O. Pietquin,,4,,10.1109/ICME.2006.262563,IEEE Xplore,1,,20061226,428,,Acoustic noise;Automatic speech recognition;History;Learning systems;Man machine systems;Optimization methods;Sociotechnical systems;Speech processing;Stochastic processes;Training data,interactive systems;learning (artificial intelligence);man-machine systems;optimisation;speech processing;stochastic processes;user modelling,machine learning method;optimization;realistic man-machine simulation;spoken dialogue system;stochastic description;task-oriented dialogue simulation technique;user modeling technique,,1945-7871;19457871,,9-12 July 2006,,2006 IEEE International Conference on Multimedia and Expo,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4036627,,,,,,IEEE,14,,CD-ROM:1-4244-0367-7; POD:1-4244-0366-7,,2006 IEEE International Conference on Multimedia and Expo,425,IEEE Conferences,,,,,2006,,,,,,,,,,,,,,,,,,,
Fast and learnable behavioral and cognitive modeling for virtual character animation,"Behavioral and cognitive modeling for virtual characters is a promising field. It significantly reduces the workload on the animator, allowing characters to act autonomously in a believable fashion. It also makes interactivity between humans and virtual characters more practical than ever before. In this paper we present a novel technique where an artificial neural network is used to approximate a cognitive model. This allows us to execute the model much more quickly, making cognitively empowered characters more practical for interactive applications. Through this approach, we can animate several thousand intelligent characters in real time on a PC. We also present a novel technique for how a virtual character, instead of using an explicit model supplied by the user, can automatically learn an unknown behavioral/cognitive model by itself through reinforcement learning. The ability to learn without an explicit model appears promising for helping behavioral and cognitive modeling become more broadly accepted and used in the computer graphics community, as it can further reduce the workload on the animator. Further, it provides solutions for problems that cannot easily be modeled explicitly. Copyright © 2004 John Wiley & Sons, Ltd.",,"Brigham Young University, 3366 TMCB, Provo, UT 84602, United States; Computer Science Department, Brigham Young University, United States; Computer Science Department, Utah State University, United States",,,"Dinerstein J., Egbert P.K., De Garis H., Dinerstein N.","Dinerstein, J., Brigham Young University, 3366 TMCB, Provo, UT 84602, United States; Egbert, P.K., Computer Science Department, Brigham Young University, United States; De Garis, H., Computer Science Department, Utah State University, United States; Dinerstein, N., Computer Science Department, Utah State University, United States",17,,10.1002/cav.8,SCOPUS,1,,,,,,,,,,2,,,Computer Animation and Virtual Worlds,Behavioral modeling; Cognitive modeling; Computer animation; Machine learning; Reinforcement learning; Synthetic characters,,2-s2.0-23444439826,,,,,,108,95,,,,,Scopus,,,Computer Animation and Virtual Worlds,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23444439826&doi=10.1002%2fcav.8&partnerID=40&md5=ad3fb45b1cdf0ae4c85400551d24b452,,15,,2004,,,,,,,,,,,,,,,,,,,
The use of reinforcement learning algorithms to meet the challenges of an artificial pancreas.,"Blood glucose control, for example, in diabetes mellitus or severe illness, requires strict adherence to a protocol of food, insulin administration and exercise personalized to each patient. An artificial pancreas for automated treatment could boost quality of glucose control and patients' independence. The components required for an artificial pancreas are: i) continuous glucose monitoring (CGM), ii) smart controllers and iii) insulin pumps delivering the optimal amount of insulin. In recent years, medical devices for CGM and insulin administration have undergone rapid progression and are now commercially available. Yet, clinically available devices still require regular patients' or caregivers' attention as they operate in open-loop control with frequent user intervention. Dosage-calculating algorithms are currently being studied in intensive care patients [1] , for short overnight control to supplement conventional insulin delivery [2] , and for short periods where patients rest and follow a prescribed food regime [3] . Fully automated algorithms that can respond to the varying activity levels seen in outpatients, with unpredictable and unreported food intake, and which provide the necessary personalized control for individuals is currently beyond the state-of-the-art. Here, we review and discuss reinforcement learning algorithms, controlling insulin in a closed-loop to provide individual insulin dosing regimens that are reactive to the immediate needs of the patient.",,"Fresenius Kabi Deutschland GmbH, Else-Kröner-Strasse 1, 61352 Bad Homburg, Germany.",,,"Bothe M.K., Dickens L., Reichel K., Tellmann A., Ellger B., Westphal M., Faisal A.A.","Bothe, M.K., Fresenius Kabi Deutschland GmbH, Else-Kröner-Strasse 1, 61352 Bad Homburg, Germany.; Dickens, L.; Reichel, K.; Tellmann, A.; Ellger, B.; Westphal, M.; Faisal, A.A.",9,,10.1586/17434440.2013.827515,SCOPUS,1,,,,,,,,,,5,,,Expert review of medical devices,,,2-s2.0-84901870499,,,,,,673,661,,,,,Scopus,,,Expert review of medical devices,,Review,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901870499&doi=10.1586%2f17434440.2013.827515&partnerID=40&md5=5e57f84d94e8248985013780a9d9b267,,10,,2013,,,,,,,,,,,,,,,,,,,
Visual summary of egocentric photostreams by representative keyframes,"Building a visual summary from an egocentric photostream captured by a lifelogging wearable camera is of high interest for different applications (e.g. memory reinforcement). In this paper, we propose a new summarization method based on keyframes selection that uses visual features extracted by means of a convolutional neural network. Our method applies an unsupervised clustering for dividing the photostreams into events, and finally extracts the most relevant keyframe for each event. We assess the results by applying a blind-taste test on a group of 20 people who assessed the quality of the summaries.",,,,"Universitat de Barcelona, Catalonia/Spain",M. Bolaños; R. Mestre; E. Talavera; X. Giró-i-Nieto; P. Radeva,,2,,10.1109/ICMEW.2015.7169863,IEEE Xplore,1,,20150730,6,,Cameras;Feature extraction;Image segmentation;Indexes;Motion segmentation;Videos;Visualization,cameras;convolution;feature extraction;neural nets;pattern clustering;personal computing;unsupervised learning,blind-taste test;convolutional neural network;egocentric photostreams;keyframe selection;lifelogging wearable camera;memory reinforcement;representative keyframes;summarization method;unsupervised clustering;visual feature extraction;visual summary,,,,June 29 2015-July 3 2015,,2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),egocentric;keyframes;lifelogging;summarization,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169863,,,,,,IEEE,17,,Electronic:978-1-4799-7079-7; POD:978-1-4799-7080-3,,2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),1,IEEE Conferences,,,,,2015,,,,,,,,,,,,,,,,,,,
Business process outsourcing enhanced by fuzzy linguistic consensus model,"Business process outsourcing represents a strategic option to obtain the overall improvement of performance in business process management context. It consists in externalizing whole sub-processes (e.g., production, logistics, human resources) of a value chain. Last decade, the concept of value chain moved toward the more flexible concept of value net that implies the assembly of several value chains tailored to specifics, objectives, markets, etc. Thus, the composition of a value chain within a value net environments can be understood as the modeling of a macro business process in which sub-processes can be outsourced. Such composition activity foresees crucial decision-making moments that need to be sustained by a group of decision-makers owning several and heterogeneous competences in order to select the most suitable external providers to which delegate specific sub-processes. This work proposes a framework to enhance business process outsourcing by introducing group decision-making support that relies on a fuzzy linguistic consensus model. In addition, the framework implements algorithms to learn and assign different weights to decision-makers considering the context and time at which they participate in the group decision making. The framework is applied to an Italian footwear company by describing a numerical example. © 2017 Elsevier B.V.",,"Dipartimento di Scienze Aziendali – Management & Innovation Systems, University of Salerno, Fisciano (SA), Italy; Department of Computer Science and Artificial Intelligence, University of Granada, Granada, Spain; Department of Electrical and Computer Engineering, Faculty of Engineering, King Abdulaziz University, Jeddah, Saudi Arabia",,,"Ciasullo M.V., Fenza G., Loia V., Orciuoli F., Troisi O., Herrera-Viedma E.","Ciasullo, M.V., Dipartimento di Scienze Aziendali – Management & Innovation Systems, University of Salerno, Fisciano (SA), Italy; Fenza, G., Dipartimento di Scienze Aziendali – Management & Innovation Systems, University of Salerno, Fisciano (SA), Italy; Loia, V., Dipartimento di Scienze Aziendali – Management & Innovation Systems, University of Salerno, Fisciano (SA), Italy; Orciuoli, F., Dipartimento di Scienze Aziendali – Management & Innovation Systems, University of Salerno, Fisciano (SA), Italy; Troisi, O., Dipartimento di Scienze Aziendali – Management & Innovation Systems, University of Salerno, Fisciano (SA), Italy; Herrera-Viedma, E., Department of Computer Science and Artificial Intelligence, University of Granada, Granada, Spain, Department of Electrical and Computer Engineering, Faculty of Engineering, King Abdulaziz University, Jeddah, Saudi Arabia",,,10.1016/j.asoc.2017.12.020,SCOPUS,1,,,,,,,,,,,,,Applied Soft Computing Journal,Business processes outsourcing; Context awareness; Fuzzy linguistic consensus model; Reinforcement learning; Value net,,2-s2.0-85039847146,,,,,,444,436,,,,,Scopus,,,Applied Soft Computing Journal,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039847146&doi=10.1016%2fj.asoc.2017.12.020&partnerID=40&md5=978d181b7df8a4116a9a4b237f0e7148,,64,,2018,,,,,,,,,,,,,,,,,,,
Multi-agents and learning: Implications for Webusage mining,"Characterization of user activities is an important issue in the design and maintenance of websites. Server weblog files have abundant information about the user's current interests. This information can be mined and analyzed therefore the administrators may be able to guide the users in their browsing activity so they may obtain relevant information in a shorter span of time to obtain user satisfaction. Web-based technology facilitates the creation of personally meaningful and socially useful knowledge through supportive interactions, communication and collaboration among educators, learners and information. This paper suggests a new methodology based on learning techniques for a Web-based Multiagent-based application to discover the hidden patterns in the user's visited links. It presents a new approach that involves unsupervised, reinforcement learning, and cooperation between agents. It is utilized to discover patterns that represent the user's profiles in a sample website into specific categories of materials using significance percentages. These profiles are used to make recommendations of interesting links and categories to the user. The experimental results of the approach showed successful user pattern recognition, and cooperative learning among agents to obtain user profiles. It indicates that combining different learning algorithms is capable of improving user satisfaction indicated by the percentage of precision, recall, the progressive category weight and F1-measure. © 2015.Production and hosting by Elsevier B.V.",Open Access,"Computer Science, Mathematics Department, Faculty of Science, AinShams University, Cairo, Egypt; Pure Math. and Computer Science, Menoufia University, Menoufia, Egypt",,,"Lotfy H.M.S., Khamis S.M.S., Aboghazalah M.M.","Lotfy, H.M.S., Computer Science, Mathematics Department, Faculty of Science, AinShams University, Cairo, Egypt; Khamis, S.M.S., Computer Science, Mathematics Department, Faculty of Science, AinShams University, Cairo, Egypt; Aboghazalah, M.M., Pure Math. and Computer Science, Menoufia University, Menoufia, Egypt",1,,10.1016/j.jare.2015.06.005,SCOPUS,1,,,,,,,,,,2,,,Journal of Advanced Research,Cooperative learning; Personalized web search; Recommendation system; Reinforcement learning; Unsupervised learning,,2-s2.0-84959451312,,,,,,295,285,,,,,Scopus,,,Journal of Advanced Research,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959451312&doi=10.1016%2fj.jare.2015.06.005&partnerID=40&md5=df411efcfc997150cd8dc04bbd48d678,,7,,2016,,,,,,,,,,,,,,,,,,,
Discovering virtual interest groups across chat rooms,"Chat has becoming an increasingly popular communication tool in our everyday life. When the number of related concurrent chat rooms gets large, tracking them 24x7 becomes very difficult. To address this research problem, we have developed VIGIR (Virtual Interest Group & Information Recommender), a tool for automatic chat room monitoring. The tool builds adaptive interest models for chat users, which are used to provide a number of personalized services including finding virtual interest groups (VIGs) for chat users. Dynamic identification of the VIG addresses the distributed user collaboration challenge, which is acute problem especially in military operations. VIGIR extends our prior work in user interest modeling into the domain of real-time text-based communications. We have evaluated the effectiveness of VIGIR in two studies. The first is a user-centred evaluation where we have achieved a precision at 60% and recall at 80% for VIG identification. In the second study using military chat data, we have demonstrated an average precision of 45% to 50%. In addition, we have shown that the precision for predicting VIG increases over time as more data become available.",,"SAIC, Inc., 1710 SAIC Drive, McLean, VA 22102, United States",,,"Li H., Lau J., Alonso R.","Li, H., SAIC, Inc., 1710 SAIC Drive, McLean, VA 22102, United States; Lau, J., SAIC, Inc., 1710 SAIC Drive, McLean, VA 22102, United States; Alonso, R., SAIC, Inc., 1710 SAIC Drive, McLean, VA 22102, United States",1,,,SCOPUS,0,,,,,,,,,,,,,KMIS 2012 - Proceedings of the International Conference on Knowledge Management and Information Sharing,Chat; IRC; Machine learning; Reinforcement learning; User modeling; Virtual interest group; XMPP,,2-s2.0-84881594185,,,,,,157,152,,,,,Scopus,,,KMIS 2012 - Proceedings of the International Conference on Knowledge Management and Information Sharing,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881594185&partnerID=40&md5=c6e60adc01232386122bd4ff27e82e5f,,,,2012,,,,,,,,,,,,,,,,,,,
Using Contextual Learning to Improve Diagnostic Accuracy: Application in Breast Cancer Screening,"Clinicians need to routinely make management decisions about patients who are at risk for a disease such as breast cancer. This paper presents a novel clinical decision support tool that is capable of helping physicians make diagnostic decisions. We apply this support system to improve the specificity of breast cancer screening and diagnosis. The system utilizes clinical context (e.g., demographics, medical history) to minimize the false positive rates while avoiding false negatives. An online contextual learning algorithm is used to update the diagnostic strategy presented to the physicians over time. We analytically evaluate the diagnostic performance loss of the proposed algorithm, in which the true patient distribution is not known and needs to be learned, as compared with the optimal strategy where all information is assumed known, and prove that the false positive rate of the proposed learning algorithm asymptotically converges to the optimum. In addition, our algorithm also has the important merit that it can provide individualized confidence estimates about the accuracy of the diagnosis recommendation. Moreover, the relevancy of contextual features is assessed, enabling the approach to identify specific contextual features that provide the most value of information in reducing diagnostic errors. Experiments were conducted using patient data collected at a large academic medical center. Our proposed approach outperforms the current clinical practice by 36% in terms of false positive rate given a 2% false negative rate.",,,,"Department of Electrical Engineering, University of California, Los Angeles, Los Angeles, CA, USA",L. Song; W. Hsu; J. Xu; M. van der Schaar,,2,,10.1109/JBHI.2015.2414934,IEEE Xplore,1,,20170520,914,10.13039/100000181 - U.S. Air Force Office of Scientific Research; ,Biopsy;Breast cancer;Clustering algorithms;Context;Imaging,cancer;learning (artificial intelligence);medical diagnostic computing;patient diagnosis,breast cancer screening;clinical decision support tool;contextual features;diagnostic accuracy;disease;false positive rates;online contextual learning algorithm,,2168-2194;21682194,3,May 2016,,IEEE Journal of Biomedical and Health Informatics,Breast cancer;computer-aided diagnosis system;contextual learning;multiarmed bandit (MAB);online learning,,,,"Aged;Algorithms;Breast Neoplasms;Diagnosis, Computer-Assisted;Early Detection of Cancer;Electronic Health Records;Female;Humans;Mammography;Middle Aged;Signal Processing, Computer-Assisted",20150320,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7064753,,,,,,IEEE,42,,,,IEEE Journal of Biomedical and Health Informatics,902,IEEE Journals & Magazines,,,20,,2016,,,,,,,,,,,,,,,,,,,
Inter-cluster connection in cognitive wireless mesh networks based on intelligent network coding,"Cognitive wireless mesh networks have great flexibility to improve the spectrum utilization by opportunistically accessing the authorized frequency bands, within which the secondary users (SUs) should not violate the quality of service (QoS) requirement of the primary users (PUs) while transmitting. In this paper, we consider inter-cluster connection among neighboring clusters under the framework of cognitive wireless mesh networks. Corresponding to the neighboring clusters, all nodes operate in half-duplex mode; hence exchanging control message usually needs four time slots by traditional scheme, which leads to a loss in networking and spectral efficiency especially at the gateway node. A novel scheme based on network coding is proposed, which needs only two time slots. Our simulation experiments reveal the following findings: the performances of traditional inter-cluster connection and network coding based inter-cluster connection are comparable. Next, how to choose optimal signal amplification factor at the gateway node according to the wireless environment is discussed. And we present an intelligent policy based on reinforcement learning to solve the problem. Theoretical analysis and numerical results both show the policy can achieve optimal throughput for the SUs in the long run.",,,,"Key Laboratory of Integrate Information Network Technology, Zhejiang University, Zheda Road 38, Hangzhou, 310027, China",X. Chen; Z. Zhao; H. Zhang; T. Jiang; D. Grace,,3,,10.1109/PIMRC.2009.5449924,IEEE Xplore,1,,20100415,1256,,Chromium;Cognitive radio;Frequency;Intelligent networks;Laboratories;Learning;Network coding;Quality of service;Relays;Wireless mesh networks,cognitive radio;network coding;wireless mesh networks,QoS;cognitive wireless mesh networks;gateway node;half-duplex mode;intelligent network coding;intelligent policy;intercluster connection;optimal signal amplification factor;primary users;quality of service;reinforcement learning;secondary users;spectrum utilization,,2166-9570;21669570,,13-16 Sept. 2009,,"2009 IEEE 20th International Symposium on Personal, Indoor and Mobile Radio Communications",CogMesh;cluster;cognitive radio;cognitive wireless mesh networks;network coding;reinforcement learning;relaying,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5449924,,,,,,IEEE,15,,Electronic:978-1-4244-5123-4; POD:978-1-4244-5122-7,,"2009 IEEE 20th International Symposium on Personal, Indoor and Mobile Radio Communications",1251,IEEE Conferences,,,,,2009,,,,,,,,,,,,,,,,,,,
Concept maps and learning objects,"Concept maps constitute one of the tools frequently used in learning management as they offer the possibility to personalize learning, share knowledge and reinforce learning to learn skills. At the same time, many initiatives or standards are being developed rapidly to make the contents in different learning management systems and learning environments compatible. This paper states the need to combine the technique of Concept Maps with initiatives that package contents developed by IMS to produce more portable and powerful content. A model to create tools for learning management is proposed.",,,,"Dept. of Comput. Sci., Agrarian Univ. of Havana, Cuba",L. I. Navarro; M. M. Such; D. M. Martin; C. P. Sancho; P. P. Peco,,1,,10.1109/ICALT.2005.90,IEEE Xplore,1,,20050919,265,,Computer science;Content management;Education;Environmental management;Knowledge management;Packaging;Power system management;Power system modeling;Psychology;Standards development,computer aided instruction;educational administrative data processing,concept maps;knowledge sharing;learning environments;learning management systems;learning objects;learning personalization;learning reinforcement;skills learning,,2161-3761;21613761,,5-8 July 2005,,Fifth IEEE International Conference on Advanced Learning Technologies (ICALT'05),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1508670,,,,,,IEEE,8,,POD:0-7695-2338-2,,Fifth IEEE International Conference on Advanced Learning Technologies (ICALT'05),263,IEEE Conferences,,,,,2005,,,,,,,,,,,,,,,,,,,
Genetic reinforcement learning for scheduling heterogeneous machines,"Concerns the development of a learning-based heuristic for scheduling heterogeneous machines. List scheduling methods are flexible enough to be used for a large class of problems, including the heterogeneous machine problem. However, designing a priority rule requires insight into the characteristics of the problem. We propose the iterative list scheduling, which refines priority rules while generating a number of schedules. We also show that the iterative list scheduling can be formulated as a reinforcement learning problem, defining states and actions. Due to the large number of possible states, reinforcement learning algorithms which use value functions in constructing an optimal policy may not be suitable for scheduling problems. Encoding the policies of reinforcement learning into genetic algorithms leads to the genetic reinforcement learning (GRL), which directly works with the policies rather than the values of states. A GRL-based scheduler, EVIS (Evolutionary Intracell Scheduler), has been applied to problems such as the heterogeneous machine scheduling, the job-shop scheduling, the flow-shop scheduling, and the open-shop scheduling problems. The proposed model of EVIS, which has the linear order of population-fitness convergence, was verified with computer experiments. Even without fine tuning of EVIS, the quality of solutions found by EVIS was comparable to that of problem-tailored heuristics for most of the problem instances",,,,"Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA",G. H. Kim; C. S. G. Lee,,3,,10.1109/ROBOT.1996.506586,IEEE Xplore,1,,20020806,2803 vol.3,,Biological cells;Encoding;Genetic algorithms;Genetic engineering;Iterative algorithms;Job shop scheduling;Learning;Processor scheduling;Scheduling algorithm;Space exploration,genetic algorithms;heuristic programming;iterative methods;learning (artificial intelligence);production control,EVIS;Evolutionary Intracell Scheduler;genetic reinforcement learning;heterogeneous machine scheduling;iterative list scheduling;learning-based heuristic;linear-order population-fitness convergence;optimal policy;reinforcement learning problem,,1050-4729;10504729,,22-28 Apr 1996,,Proceedings of IEEE International Conference on Robotics and Automation,,,,22 Apr 1996-28 Apr 1996,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=506586,,,,,,IEEE,16,,POD:0-7803-2988-0,,Proceedings of IEEE International Conference on Robotics and Automation,2798,IEEE Conferences,,,3,,1996,,,,,,,,,,,,,,,,,,,
Conflict acts as an implicit cost in reinforcement learning,"Conflict has been proposed to act as a cost in action selection, implying a general function of medio-frontal cortex in the adaptation to aversive events. Here we investigate if response conflict acts as a cost during reinforcement learning by modulating experienced reward values in cortical and striatal systems. Electroencephalography recordings show that conflict diminishes the relationship between reward-related frontal theta power and cue preference yet it enhances the relationship between punishment and cue avoidance. Individual differences in the cost of conflict on reward versus punishment sensitivity are also related to a genetic polymorphism associated with striatal D1 versus D2 pathway balance (DARPP-32). We manipulate these patterns with the D2 agent cabergoline, which induces a strong bias to amplify the aversive value of punishment outcomes following conflict. Collectively, these findings demonstrate that interactive cortico-striatal systems implicitly modulate experienced reward and punishment values as a function of conflict. © 2014 Macmillan Publishers Limited. All rights reserved.",,"Department of Psychology, University of New Mexico, 1 University of New Mexico, Albuquerque, NM, United States; Department of Cognitive, Linguistic and Psychological Sciences, Brown University, 190 Thayer Street, Providence, RI, United States; Department of Neuroscience, Brown University, Box GL-N, Providence, RI, United States; Department of Psychiatry, Brown University, Box G-A1, Providence, RI, United States; Brown Institute for Brain Science, Brown University, Box 1953 2 Stimson Ave., Providence, RI, United States",6394,,"Cavanagh J.F., Masters S.E., Bath K., Frank M.J.","Cavanagh, J.F., Department of Psychology, University of New Mexico, 1 University of New Mexico, Albuquerque, NM, United States; Masters, S.E., Department of Cognitive, Linguistic and Psychological Sciences, Brown University, 190 Thayer Street, Providence, RI, United States; Bath, K., Department of Neuroscience, Brown University, Box GL-N, Providence, RI, United States; Frank, M.J., Department of Cognitive, Linguistic and Psychological Sciences, Brown University, 190 Thayer Street, Providence, RI, United States, Department of Psychiatry, Brown University, Box G-A1, Providence, RI, United States, Brown Institute for Brain Science, Brown University, Box 1953 2 Stimson Ave., Providence, RI, United States",23,,10.1038/ncomms6394,SCOPUS,1,,,,,,,,,,,,,Nature Communications,,,2-s2.0-84922975531,,,,,,,,,,,,Scopus,,,Nature Communications,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922975531&doi=10.1038%2fncomms6394&partnerID=40&md5=a995090841dde725bac87fa45a86e1e2,,5,,2014,,,,,,,,,,,,,,,,,,,
Knowledge sharing approaches for distributed agents system,"Considering situations in a multi-agent system, if there are tremendous number of agents sharing knowledge with each other, it is complicated activities hard to be solved. This thesis proposed a method that all agents just connect with a server to alleviate the complexity of the experiences exchange activities. The server collects learning knowledge loaded from all the agents, merges the knowledge, and shares the knowledge to all agents which lack akin experiences. The agents utilized the proposed Pheromone Mechanism in Ant Colony Algorithm to evaluate whether an experience is worthy to upload to the server. Meanwhile, to deal with the problem of massive data processing, this thesis used the open source software, Apache Hadoop, along with the MapReduce programming model. The agents can take shared experiences integrated with their own knowledge to achieve knowledge sharing and increase the efficiency significantly. The proposed approach in this thesis was implemented by a homemade server and personal computers.",,,,"Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan",K. S. Hwang; C. W. Hsieh; W. C. Jiang,,,,10.1109/ARIS.2017.8297184,IEEE Xplore,1,,20180222,61,,Computational modeling;Data models;Multi-agent systems;Programming;Servers;Training;Trajectory,ant colony optimisation;data handling;learning (artificial intelligence);multi-agent systems;parallel programming;public domain software;software agents,Ant Colony Algorithm;Apache Hadoop;MapReduce programming model;Pheromone Mechanism;distributed agents system;experience exchange;experience sharing;homemade server;knowledge merging;knowledge sharing;learning knowledge;massive data processing;multiagent system;open source software,,,,6-8 Sept. 2017,,2017 International Conference on Advanced Robotics and Intelligent Systems (ARIS),Ant colony algorithm;Distributed computing;Knowledge merging;Knowledge sharing;Reinforcement learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8297184,,,,,,IEEE,,,Electronic:978-1-5386-2419-7; POD:978-1-5386-2420-3; USB:978-1-5386-2418-0,,2017 International Conference on Advanced Robotics and Intelligent Systems (ARIS),59,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
Joint radio resource management for LTE-UMTS coexistence scenarios,"Constantly increasing demand for throughput and quality in wireless communication systems leads to continuous research of wise radio resource management, because of the scarce availability of frequency bands and the consequent capacity limitations. In addition, technology evolution is addressed towards spectral efficient techniques that can offer higher data rates. This is the case of OFDMA (Orthogonal Frequency-Division Multiple Access), introduced by 3GPP as the technology for future Long Term Evolution (LTE). However, given the current penetration of legacy technologies such as UMTS (Universal Mobile Telecommunications System), operators will have to deal with the coexistence of multiple Radio Access Technologies (RATs), so that the exploitation of the complementarities between technologies through Joint Radio Resource Management (JRRM) mechanisms will be needed. In this paper we propose a novel dynamic JRRM algorithm for LTE-UMTS coexistence scenarios. The proposed mechanism is based on Reinforcement Learning (RL) which is considered to be a good candidate to achieve cognition in future reconfigurable networks. The proposed solution implements autonomous RL agents in each base station which decide on the allocation of the most suitable RAT to each user. We give a detailed description of the solution and analyze the behavior under various load conditions. We also demonstrate the capability of the algorithm to adjust in dynamic scenarios.",,,,"Dept. TSC, Universitat Polit&#232;cnica de Catalunya (UPC), c/ Jordi Girona 1-3, 08034, Barcelona, Spain",N. Vučević; J. Pérez-Romero; O. Sallent; R. Agustí,,3,,10.1109/PIMRC.2009.5450181,IEEE Xplore,1,,20100415,16,,3G mobile communication;Availability;Cognition;Heuristic algorithms;Learning;Long Term Evolution;Rats;Resource management;Throughput;Wireless communication,3G mobile communication;OFDM modulation;frequency division multiple access,3GPP;LTE-UMTS coexistence;OFDMA;joint radio resource management;multiple radio access technology;orthogonal frequency-division multiple access;reinforcement learning;spectral efficient technique;universal mobile telecommunications system;wireless communication system,,2166-9570;21669570,,13-16 Sept. 2009,,"2009 IEEE 20th International Symposium on Personal, Indoor and Mobile Radio Communications",LTE;WCDMA;joint radio resource management;reinforcement learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5450181,,,,,1,IEEE,18,,Electronic:978-1-4244-5123-4; POD:978-1-4244-5122-7,,"2009 IEEE 20th International Symposium on Personal, Indoor and Mobile Radio Communications",12,IEEE Conferences,,,,,2009,,,,,,,,,,,,,,,,,,,
Identification and evaluation of performance parameters for RE-BAR bending training simulator,"Construction rebars (steel concrete reinforcing bars) are used to provide structural strength and reinforcement for the concrete structure. This requires the bending and cutting of the rebar to proper size before they can be used for construction. A novel haptic based barbending simulator has been devised which enables the trainees to learn and improve the construction rebar bending skill in a safe and controlled way. With its limited assessment and reporting features, the computerised virtual training simulator proves to be effective in training. Adding the features like personalized skill tracking and predictive performance modeling holds even more potential in supporting the training program. Towards this goal, a user performance modeling needs to be done which includes an initial study on performance parameters, assessment criteria and data collection before remodeling the simulator. This paper presents a study on the performance parameters for the bar bending simulator and also evaluates its effectiveness in modeling expert and novice performances. During this study we also hypothesize the parameters that can distinguish an expert and novice performances which was validated with 2 classification techniques - Support vector machine and J48 Decision tree. While revealing the classification rules J48 algorithm provides 78.94% accuracy where as SVM provides 94.737% accuracy. The study also shows that the 2 performance parameters force applied over time and bend angle accuracy are effective to distinguish expert and novice level of expertize for the construction rebar bending skill.",,,,"AMMACHI Labs, Amrita School of Engineering, Amritapuri, Amrita Vishwa Vidyapeetham, Amrita University, Kerala, India",B. M. Menon; P. Aswathi; S. Deepu; R. R. Bhavani,,,,10.1109/ICCCNT.2017.8204042,IEEE Xplore,1,,20171214,7,,Bars;Data models;Force;Hidden Markov models;Shape;Training;Vehicles,bending;computer based training;construction industry;decision trees;digital simulation;haptic interfaces;mechanical engineering computing;pattern classification;rebar;steel;support vector machines,RE-BAR bending training simulator;assessment criteria;bar bending simulator;bending cutting;computerised virtual training simulator;concrete structure;construction rebar bending skill;construction rebars;data collection;personalized skill tracking;predictive performance modeling;steel concrete;structural strength;training program;user performance modeling,,,,3-5 July 2017,,"2017 8th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",Decision trees;Intelligent systems;Machine learning algorithms;Modeling;Support vector machines;Vocational training,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8204042,,,,,,IEEE,,,Electronic:978-1-5090-3038-5; POD:978-1-5090-3039-2,,"2017 8th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",1,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
Evolution of context-aware user profiles,"Context-awareness and adaptation are key issues in mobile and ubiquitous computing. Applications on mobile devices use context information to adapt themselves to changing environments. User profiles play an important role in these systems as they serve as an individualization filter in a wide range of possible context adaptation parameters. In this paper we propose a modeling approach for the evolution of context-aware user profiles. A motivating scenario, the intelligent selection of a suitable medical expert in an emergency situation, shows the need for context-aware matching of user profiles. This is achieved by a similarity matching algorithm and reinforcement learning.",,,,"Condat AG, Berlin, Germany",J. Thomsen; Y. Vanrompay; Y. Berbers,,1,,10.1109/ICUMT.2009.5345395,IEEE Xplore,1,,20091204,6,,Application software;Computer science;Context modeling;Filters;Intelligent networks;Mobile computing;Multiple signal classification;Personal digital assistants;Runtime;Ubiquitous computing,learning (artificial intelligence);medical computing;mobile computing,adaptation;context-aware user profiles;context-awareness;medical expert;mobile computing;reinforcement learning;similarity matching algorithm;ubiquitous computing,,2157-0221;21570221,,12-14 Oct. 2009,,2009 International Conference on Ultra Modern Telecommunications & Workshops,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5345395,,,,,,IEEE,21,,POD:978-1-4244-3942-3,,2009 International Conference on Ultra Modern Telecommunications & Workshops,1,IEEE Conferences,,,,,2009,,,,,,,,,,,,,,,,,,,
Hierarchical exploration for accelerating contextual bandits,"Contextual bandit learning is an increasingly popular approach to optimizing recommender systems via user feedback, but can be slow to converge in practice due to the need for exploring a large feature space. In this paper, we propose a coarse-to-fine hierarchical approach for encoding prior knowledge that drastically reduces the amount of exploration required. Intuitively, user preferences can be reasonably embedded in a coarse low-dimensional feature space that can be explored efficiently, requiring exploration in the high-dimensional space only as necessary. We introduce a bandit algorithm that explores within this coarse-to-fine spectrum, and prove performance guarantees that depend on how well the coarse space captures the user's preferences. We demonstrate substantial improvement over conventional bandit algorithms through extensive simulation as well as a live user study in the setting of personalized news recommendation. Copyright 2012 by the author(s)/owner(s).",,"iLab, H. John Heinz III College, Carnegie Mellon University, Pittsburgh, PA 15213, United States; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States",,,"Yue Y., Hong S.A., Guestrin C.","Yue, Y., iLab, H. John Heinz III College, Carnegie Mellon University, Pittsburgh, PA 15213, United States; Hong, S.A., School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States; Guestrin, C., School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States",8,,,SCOPUS,0,,,,,,,,,,,,,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",,,2-s2.0-84867112205,,,,,,1902,1895,,,,,Scopus,,,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867112205&partnerID=40&md5=4dc1f9081d737b65dfff6e12543f0bba,,2,,2012,,,,,,,,,,,,,,,,,,,
Multi-task learning for contextual bandits,"Contextual bandits are a form of multi-armed bandit in which the agent has access to predictive side information (known as the context) for each arm at each time step, and have been used to model personalized news recommendation, ad placement, and other applications. In this work, we propose a multi-task learning framework for contextual bandit problems. Like multi-task learning in the batch setting, the goal is to leverage similarities in contexts for different arms so as to improve the agent's ability to predict rewards from contexts. We propose an upper confidence bound-based multi-task learning algorithm for contextual bandits, establish a corresponding regret bound, and interpret this bound to quantify the advantages of learning in the presence of high task (arm) similarity. We also describe an effective scheme for estimating task similarity from data, and demonstrate our algorithm's performance on several data sets. © 2017 Neural information processing systems foundation. All rights reserved.",,"Department of EECS, University of Michigan, Ann Arbor, Ann Arbor, MI, United States; Microsoft Research, Cambridge, United Kingdom",,,"Deshmukh A.A., Dogan U., Scott C.","Deshmukh, A.A., Department of EECS, University of Michigan, Ann Arbor, Ann Arbor, MI, United States; Dogan, U., Microsoft Research, Cambridge, United Kingdom; Scott, C., Department of EECS, University of Michigan, Ann Arbor, Ann Arbor, MI, United States",,,,SCOPUS,0,,,,,,,,,,,,,Advances in Neural Information Processing Systems,,,2-s2.0-85047009825,,,,,,4857,4849,,,,,Scopus,,,Advances in Neural Information Processing Systems,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047009825&partnerID=40&md5=70d38299b0bb67817c5702fd0efabb71,,2017-December,,2017,,,,,,,,,,,,,,,,,,,
Online context-aware recommendation with time varying multi-armed bandit,"Contextual multi-armed bandit problems have gained increasing popularity and attention in recent years due to their capability of leveraging contextual information to deliver online personalized recommendation services (e.g., online advertising and news article selection). To predict the reward of each arm given a particular context, existing relevant research studies for contextual multi-armed bandit problems often assume the existence of a fixed yet unknown reward mapping function. However, this assumption rarely holds in practice, since real-world problems often involve underlying processes that are dynamically evolving over time. In this paper, we study the time varying contextual multi-armed problem where the reward mapping function changes over time. In particular, we propose a dynamical context drift model based on particle learning. In the proposed model, the drift on the reward mapping function is explicitly modeled as a set of random walk particles, where good fitted particles are selected to learn the mapping dynamically. Taking advantage of the fully adaptive inference strategy of particle learning, our model is able to effectively capture the context change and learn the latent parameters. In addition, those learnt parameters can be naturally integrated into existing multi-arm selection strategies such as LinUCB and Thompson sampling. Empirical studies on two real-world applications, including online personalized advertising and news recommendation, demonstrate the effectiveness of our proposed approach. The experimental results also show that our algorithm can dynamically track the changing reward over time and consequently improve the click-through rate. © 2016 ACM.",,"School of Computing and Information Science, Florida International University, Miami, United States",,,"Zeng C., Wang Q., Mokhtari S., Li T.","Zeng, C., School of Computing and Information Science, Florida International University, Miami, United States; Wang, Q., School of Computing and Information Science, Florida International University, Miami, United States; Mokhtari, S., School of Computing and Information Science, Florida International University, Miami, United States; Li, T., School of Computing and Information Science, Florida International University, Miami, United States",8,,10.1145/2939672.2939878,SCOPUS,1,,,,,,,,,,,,,Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,Particle learning; Personalization; Probability matching; Recommender system; Time varying contextual bandit,,2-s2.0-84985020774,,,,,,2034,2025,,,,,Scopus,,,Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985020774&doi=10.1145%2f2939672.2939878&partnerID=40&md5=26e3553e4a237cf5525077e959b4d46e,,13-17-August-2016,,2016,,,,,,,,,,,,,,,,,,,
A highly interpretable fuzzy rule base using ordinal structure for obstacle avoidance of mobile robot,"Conventional fuzzy logic controller is applicable when there are only two fuzzy inputs with usually one output. Complexity increases when there are more than one inputs and outputs making the system unrealizable. The ordinal structure model of fuzzy reasoning has an advantage of managing high-dimensional problem with multiple input and output variables ensuring the interpretability of the rule set. This is achieved by giving an associated weight to each rule in the defuzzification process. In this work, a methodology to design an ordinal fuzzy logic controller with application for obstacle avoidance of Khepera mobile robot is presented. The implementation will show that ordinal structure fuzzy is easier to design with highly interpretable rules compared to conventional fuzzy controller. In order to achieve high accuracy, a specially tailored Genetic Algorithm (GA) approach for reinforcement learning has been proposed to optimize the ordinal structure fuzzy controller. Simulation results demonstrated improved obstacle avoidance performance in comparison with conventional fuzzy controllers. Comparison of direct and incremental GA for optimization of the controller is also presented. © 2010 Elsevier B.V. All rights reserved.",,"Intelligent System and Robotics Laboratory, Institute of Advanced Technology, Universiti Putra Malaysia, 43400 Serdang, Selangor, Malaysia; Department of Computer and Communication Systems, Faculty of Engineering, Universiti Putra Malaysia, 43400 Serdang, Selangor, Malaysia",,,"Samsudin K., Ahmad F.A., Mashohor S.","Samsudin, K., Intelligent System and Robotics Laboratory, Institute of Advanced Technology, Universiti Putra Malaysia, 43400 Serdang, Selangor, Malaysia; Ahmad, F.A., Department of Computer and Communication Systems, Faculty of Engineering, Universiti Putra Malaysia, 43400 Serdang, Selangor, Malaysia; Mashohor, S., Department of Computer and Communication Systems, Faculty of Engineering, Universiti Putra Malaysia, 43400 Serdang, Selangor, Malaysia",37,,10.1016/j.asoc.2010.05.002,SCOPUS,1,,,,,,,,,,2,,,Applied Soft Computing Journal,Fuzzy logic; Genetic algorithm; Mobile robot; Obstacle avoidance; Ordinal structure,,2-s2.0-78751633864,,,,,,1637,1631,,,,,Scopus,,,Applied Soft Computing Journal,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751633864&doi=10.1016%2fj.asoc.2010.05.002&partnerID=40&md5=3eac299086b1245a7062038f727ccc06,,11,,2011,,,,,,,,,,,,,,,,,,,
Dynamic personalization in conversational recommender systems,"Conversational recommender systems are E-Commerce applications which interactively assist online users to acquire their interaction goals during their sessions. In our previous work, we have proposed and validated a methodology for conversational systems which autonomously learns the particular web page to display to the user, at each step of the session. We employed reinforcement learning to learn an optimal strategy, i.e., one that is personalized for a real user population. In this paper, we extend our methodology by allowing it to autonomously learn and update the optimal strategy dynamically (at run-time), and individually for each user. This learning occurs perpetually after every session, as long as the user continues her interaction with the system. We evaluate our approach in an off-line simulation with four simulated users, as well as in an online evaluation with thirteen real users. The results show that an optimal strategy is learnt and updated for each real and simulated user. For each simulated user, the optimal behavior is reasonably adapted to this user’s characteristics, but converges after several hundred sessions. For each real user, the optimal behavior converges only in several sessions. It provides assistance only in certain situations, allowing many users to buy several products together in shorter time and with more page-views and lesser number of query executions. We prove that our approach is novel and show how its current limitations can catered. © Springer-Verlag Berlin Heidelberg 2013.",,"Department of Computer Science, National University of Computer and Emerging Sciences (NUCES), Shah Lateef Town, National Highway, Karachi, Pakistan; Sukkur Institute of Business Administration, Airport Road, Sukkur, Sindh, Pakistan; ECTRL Solutions SRL, Via Solteri 38, Trento, Italy",,,"Mahmood T., Mujtaba G., Venturini A.","Mahmood, T., Department of Computer Science, National University of Computer and Emerging Sciences (NUCES), Shah Lateef Town, National Highway, Karachi, Pakistan; Mujtaba, G., Sukkur Institute of Business Administration, Airport Road, Sukkur, Sindh, Pakistan; Venturini, A., ECTRL Solutions SRL, Via Solteri 38, Trento, Italy",9,,10.1007/s10257-013-0222-3,SCOPUS,1,,,,,,,,,,2,,,Information Systems and e-Business Management,Conversational recommender systems; Dynamic personalization; Individual user; Off-line simulation; On-line experiment; Optimal strategy; Real users; Reinforcement learning,,2-s2.0-84953638450,,,,,,238,213,,,,,Scopus,,,Information Systems and e-Business Management,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953638450&doi=10.1007%2fs10257-013-0222-3&partnerID=40&md5=4692e5b4f1f7c8a9be659d012a702aee,,12,,2014,,,,,,,,,,,,,,,,,,,
Learning the engagement of query processors for intelligent analytics,"Current applications require the processing of huge amounts of data produced by applications or end users personal devices. In such settings, intelligent analytics on top of large scale data are the key research subject for future data driven decision making. Due to the huge amount of data, analytics should be based on an efficient technique for querying big data partitions. Each partition contains only a part of the data and a processor is dedicated to execute queries for the corresponding partition. A Query Controller (QC) is responsible for managing continuous queries and returning the final outcome to users / applications by using the underlying processors. In this paper, we propose a learning scheme to be adopted by the QC for allocating each query to the available processors. We adopt the Q-learning algorithm to calculate the reward that the QC obtains for every allocation between queries and processors. The outcome is an efficient model that derives the optimal allocation for the incoming queries. We provide mathematical formulations for solving the discussed problem and present our simulation results. Through a large number of simulations, we reveal the advantages of the proposed model and give numerical results while comparing our framework with a baseline model. © 2016, Springer Science+Business Media New York.",,"Department of Computer Science, University of Thessaly, Lamia, Greece; Department of Informatics and Telecommunications, University of Athens, Athens, Greece",,,"Kolomvatsos K., Hadjiefthymiades S.","Kolomvatsos, K., Department of Computer Science, University of Thessaly, Lamia, Greece; Hadjiefthymiades, S., Department of Informatics and Telecommunications, University of Athens, Athens, Greece",,,10.1007/s10489-016-0821-z,SCOPUS,1,,,,,,,,,,1,,,Applied Intelligence,Intelligent analytics; Q-learning; Query streams; Reinforcement learning,,2-s2.0-84979686223,,,,,,112,96,,,,,Scopus,,,Applied Intelligence,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979686223&doi=10.1007%2fs10489-016-0821-z&partnerID=40&md5=2f57169e41217e042590dc68523f42e8,,46,,2017,,,,,,,,,,,,,,,,,,,
Exploration in interactive personalized music recommendation: A reinforcement learning approach,"Current music recommender systems typically act in a greedymanner by recommending songs with the highest user ratings. Greedy recommendation, however, is suboptimal over the long term: it does not actively gather information on user preferences and fails to recommend novel songs that are potentially interesting. A successful recommender system must balance the needs to explore user preferences and to exploit this information for recommendation. This article presents a new approach to music recommendation by formulating this exploration-exploitation trade-off as a reinforcement learning task. To learn user preferences, it uses Bayesian model that accounts for both audio content and the novelty of recommendations. A piecewise-linear approximation to the model and a variational inference algorithm help to speed up Bayesian inference. One additional benefit of our approach is a single unified model for both music recommendation and playlist generation. We demonstrate the strong potential of the proposed approach with simulation results and a user study. © 2014 ACM.",,"Department of Computer Science, National University of Singapore, Singapore 117417, Singapore; Computing Science Department, Institute of High Performance Computing, A STAR, Singapore 138632, Singapore",7,,"Wang Y., Wang X., Wang Y., Hsu D.","Wang, Y., Department of Computer Science, National University of Singapore, Singapore 117417, Singapore; Wang, X., Department of Computer Science, National University of Singapore, Singapore 117417, Singapore; Wang, Y., Computing Science Department, Institute of High Performance Computing, A STAR, Singapore 138632, Singapore; Hsu, D., Department of Computer Science, National University of Singapore, Singapore 117417, Singapore",15,,10.1145/2623372,SCOPUS,1,,,,,,,,,,1,,,"ACM Transactions on Multimedia Computing, Communications and Applications",Application; Machine learning; Model; Music; Recommender systems,,2-s2.0-84906856108,,,,,,,,,,,,Scopus,,,"ACM Transactions on Multimedia Computing, Communications and Applications",,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906856108&doi=10.1145%2f2623372&partnerID=40&md5=da52cbcf39aaff70ff8fd51e7294ca75,,11,,2014,,,,,,,,,,,,,,,,,,,
Stochastic data acquisition for answering queries as time goes by,"Data and actions are tightly coupled. On one hand, data analysis results trigger decision making and actions. On the other hand, the action of acquiring data is the very first step in the whole data processing pipeline. Data acquisition almost always has some costs, which could be either monetary costs or computing resource costs such as sensor battery power, network transfers, or I/O costs. Using outdated data to answer queries can avoid the data acquisition costs, but there is a penalty of potentially inaccurate results. Given a sequence of incoming queries over time, we study the problem of sequential decision making on when to acquire data and when to use existing versions to answer each query. We propose two approaches to solve this problem using reinforcement learning and tailored locality-sensitive hashing. A systematic empirical study using two real-world datasets shows that our approaches are effective and efficient. © 2016. VLDB Endowment.",,"University of Massachusetts, Lowell, United States",,,"Li Z., Ge T.","Li, Z., University of Massachusetts, Lowell, United States; Ge, T., University of Massachusetts, Lowell, United States",,,10.14778/3021924.3021942,SCOPUS,1,,,,,,,,,,3,,,Proceedings of the VLDB Endowment,,,2-s2.0-85020435578,,,,,,288,277,,,,,Scopus,,,Proceedings of the VLDB Endowment,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020435578&doi=10.14778%2f3021924.3021942&partnerID=40&md5=9106aa67ac44a645b64311764b00579d,,10,,2016,,,,,,,,,,,,,,,,,,,
Interactive narrative personalization with deep reinforcement learning,"Data-driven techniques for interactive narrative generation are the subject of growing interest. Reinforcement learning (RL) offers significant potential for devising data-driven interactive narrative generators that tailor players' story experiences by inducing policies from player interaction logs. A key open question in RL-based interactive narrative generation is how to model complex player interaction patterns to learn effective policies. In this paper we present a deep RL-based interactive narrative generation framework that leverages synthetic data produced by a bipartite simulated player model. Specifically, the framework involves training a set of Q-networks to control adaptable narrative event sequences with long short-term memory network-based simulated players. We investigate the deep RL framework's performance with an educational interactive narrative, Crystal Island. Results suggest that the deep RL-based narrative generation framework yields effective personalized interactive narratives.",,"Department of Computer Science, North Carolina State University, Raleigh, NC, United States",,,"Wang P., Rowe J., Min W., Mott B., Lester J.","Wang, P., Department of Computer Science, North Carolina State University, Raleigh, NC, United States; Rowe, J., Department of Computer Science, North Carolina State University, Raleigh, NC, United States; Min, W., Department of Computer Science, North Carolina State University, Raleigh, NC, United States; Mott, B., Department of Computer Science, North Carolina State University, Raleigh, NC, United States; Lester, J., Department of Computer Science, North Carolina State University, Raleigh, NC, United States",,,,SCOPUS,0,,,,,,,,,,,,,IJCAI International Joint Conference on Artificial Intelligence,,,2-s2.0-85031928990,,,,,,3858,3852,,,,,Scopus,,,IJCAI International Joint Conference on Artificial Intelligence,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031928990&partnerID=40&md5=147d78aa4f5d60f8ded6123ba8b73b09,,,,2017,,,,,,,,,,,,,,,,,,,
Game theory and neural basis of social decision making,"Decision making in a social group has two distinguishing features. First, humans and other animals routinely alter their behavior in response to changes in their physical and social environment. As a result, the outcomes of decisions that depend on the behavior of multiple decision makers are difficult to predict and require highly adaptive decision-making strategies. Second, decision makers may have preferences regarding consequences to other individuals and therefore choose their actions to improve or reduce the well-being of others. Many neurobiological studies have exploited game theory to probe the neural basis of decision making and suggested that these features of social decision making might be reflected in the functions of brain areas involved in reward evaluation and reinforcement learning. Molecular genetic studies have also begun to identify genetic mechanisms for personal traits related to reinforcement learning and complex social decision making, further illuminating the biological basis of social behavior. © 2008 Nature Publishing Group.",,"Yale University School of Medicine, Department of Neurobiology, SHM B404, 333 Cedar Street, New Haven, CT 06510, United States",,,Lee D.,"Lee, D., Yale University School of Medicine, Department of Neurobiology, SHM B404, 333 Cedar Street, New Haven, CT 06510, United States",139,,10.1038/nn2065,SCOPUS,1,,,,,,,,,,4,,,Nature Neuroscience,,,2-s2.0-41149123141,,,,,,409,404,,,,,Scopus,,,Nature Neuroscience,,Review,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41149123141&doi=10.1038%2fnn2065&partnerID=40&md5=779a41c1bac083442ed4c4aa074053c3,,11,,2008,,,,,,,,,,,,,,,,,,,
Event-related potential-based personal decision process,"Decision making refers to an evaluation process of selecting a particular action from a number of alternative choices in a provided situation. However, in a multi-agent environment, where the outcomes of one's actions change dynamically because they are related to the behavior of others, it becomes difficult to make an optimal decision. Although game theory provides normative solutions for decision making in groups, how such decision-making strategies are altered by experience remains poorly understood. Optimal behavior in a competitive world requires the flexibility to adapt decision strategies based on recent outcomes. To explore the essence of such adaptive decision-making processes, we investigated the learning process in human playing a competitive game with binary choices, using a matching pennies game. In the present study, we tested the hypothesis that this flexibility emerges through a reinforcement learning process, and produced a reinforcement learning model. In addition, event-related brain potentials (ERPs) were recorded while subjects were playing the game. Analyses of ERP data focused on the feedback-related negativity (FRN), an outcome-locked potential thought to reflect a neural prediction error signal. Results show that the magnitude of ERPs after losing to the computer opponent predicted whether subjects would change decision behavior on the subsequent trial.",,"Institute of Information and Technology, Jiangxi Bluesky University, Nanchang 330098, Jiangxi Province, China",,,"Hu J.-F., Li X.-F., Yin J.-H.","Hu, J.-F., Institute of Information and Technology, Jiangxi Bluesky University, Nanchang 330098, Jiangxi Province, China; Li, X.-F., Institute of Information and Technology, Jiangxi Bluesky University, Nanchang 330098, Jiangxi Province, China; Yin, J.-H., Institute of Information and Technology, Jiangxi Bluesky University, Nanchang 330098, Jiangxi Province, China",,,10.3969/j.issn.1673-8225.2009.26.016,SCOPUS,1,,,,,,,,,,26,,,Journal of Clinical Rehabilitative Tissue Engineering Research,,,2-s2.0-69749100603,,,,,,5078,5074,,,,,Scopus,,,Journal of Clinical Rehabilitative Tissue Engineering Research,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69749100603&doi=10.3969%2fj.issn.1673-8225.2009.26.016&partnerID=40&md5=6c84d43df855aa6451429654394c4504,,13,,2009,,,,,,,,,,,,,,,,,,,
Negation scope detection in sentiment analysis: Decision support for news-driven trading,"Decision support for financial news using natural language processing requires robust methods that process all sentences correctly, including those that are negated. To predict the corresponding negation scope, related literature commonly utilizes rule-based algorithms and generative probabilistic models. In contrast, we propose the use of a tailored reinforcement learning method, since it can conquer learning task of arbitrary length. We then perform a thorough comparison with a two-pronged evaluation. First, we compare the predictive performance using a manually-labeled dataset. Here, reinforcement learning outperforms common approaches from the related literature, leading to a balanced classification accuracy of up to 70.17%. Second, we examine how detecting negation scopes can improve the accuracy of sentiment analysis for financial news, leading to an improvement of up to 10.63% in the correlation between news sentiment and stock market returns. This reveals negation scope detection as a crucial leverage in decision support from sentiment. © 2016 Elsevier B.V.",,"Chair for Information Systems Research, University of Freiburg, Platz der Alten Synagoge, Freiburg, Germany",,,"Pröllochs N., Feuerriegel S., Neumann D.","Pröllochs, N., Chair for Information Systems Research, University of Freiburg, Platz der Alten Synagoge, Freiburg, Germany; Feuerriegel, S., Chair for Information Systems Research, University of Freiburg, Platz der Alten Synagoge, Freiburg, Germany; Neumann, D., Chair for Information Systems Research, University of Freiburg, Platz der Alten Synagoge, Freiburg, Germany",5,,10.1016/j.dss.2016.05.009,SCOPUS,1,,,,,,,,,,,,,Decision Support Systems,Decision support; Financial news; Machine learning; Negation scope detection; Sentiment analysis,,2-s2.0-84978808835,,,,,,75,67,,,,,Scopus,,,Decision Support Systems,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978808835&doi=10.1016%2fj.dss.2016.05.009&partnerID=40&md5=ee31c2cc1c8cee3ff33aa745c91d605f,,88,,2016,,,,,,,,,,,,,,,,,,,
Policy gradient reinforcement learning for I/O reordering on storage servers,"Deep customization of storage architectures to the applications they support is often undesirable — nature of application data is dynamic, applications are replaced far more often than storage systems are and usage patterns change dynamically with time. A continuously learning software intervention that dynamically adapts to the changing workload pattern would be the easiest way to bridge this ‘gap’. As borne out by our experiments, the overhead induced by such software interventions turns out to be negligible for large-scale storage systems. Reinforcement Learning offers a way to dynamically learn from a continuous data stream and take appropriate actions towards optimizing a future goal. We adapt policy gradient reinforcement learning to learn a policy that minimizes I/O wait time that in turn maximizes I/O throughput. A set of discrete actions consisting of switches between scheduling schemes is considered to dynamically re-order client-specific I/O operations. Results reveal that I/O reordering policy learned using reinforcement learning results in significant improvement in the overall I/O throughput. © Springer International Publishing AG 2017.",,"International Institute of Information Technology, Bangalore, Bengaluru, India",,,"Dheenadayalan K., Srinivasaraghavan G., Muralidhara V.N.","Dheenadayalan, K., International Institute of Information Technology, Bangalore, Bengaluru, India; Srinivasaraghavan, G., International Institute of Information Technology, Bangalore, Bengaluru, India; Muralidhara, V.N., International Institute of Information Technology, Bangalore, Bengaluru, India",,,10.1007/978-3-319-70087-8_87,SCOPUS,1,,,,,,,,,,,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"Filer, I/O reordering; Overload; Policy gradient; Throughput",,2-s2.0-85035108108,,,,,,859,849,,,,,Scopus,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035108108&doi=10.1007%2f978-3-319-70087-8_87&partnerID=40&md5=f15b7012441253d0502deabd14aefabf,,10634 LNCS,,2017,,,,,,,,,,,,,,,,,,,
Personalizing mobile fitness apps using reinforcement learning,"Despite the vast number of mobile fitness applications (apps) and their potential advantages in promoting physical activity, many existing apps lack behavior-change features and are not able to maintain behavior change motivation. This paper describes a novel fitness app called CalFit, which implements important behavior-change features like dynamic goal setting and self-monitoring. CalFit uses a reinforcement learning algorithm to generate personalized daily step goals that are challenging but attainable. We conducted the Mobile Student Activity Reinforcement (mSTAR) study with 13 college students to evaluate the efficacy of the CalFit app. The control group (receiving goals of 10,000 steps/day) had a decrease in daily step count of 1,520 (SD ± 740) between baseline and 10-weeks, compared to an increase of 700 (SD ± 830) in the intervention group (receiving personalized step goals). The difference in daily steps between the two groups was 2,220, with a statistically significant p = 0:039. © 2018 Copyright for the individual papers remains with the authors.",,"Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Department of Physiological, Nursing Institute for Health and Aging, School of Nursing, University of California, San Francisco, CA, United States; Department of Physiological Nursing, School of Nursing, University of California, San Francisco, CA, United States",,,"Zhou M., Mintz Y., Fukuoka Y., Goldberg K., Flowers E., Kaminsky P., Castillejo A., Aswani A.","Zhou, M., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Mintz, Y., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Fukuoka, Y., Department of Physiological, Nursing Institute for Health and Aging, School of Nursing, University of California, San Francisco, CA, United States; Goldberg, K., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Flowers, E., Department of Physiological Nursing, School of Nursing, University of California, San Francisco, CA, United States; Kaminsky, P., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Castillejo, A., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Aswani, A., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States",,,,SCOPUS,0,,,,,,,,,,,,,CEUR Workshop Proceedings,Fitness app; Goal setting; Interface design; Mobile app; Personalization; Physical activity,,2-s2.0-85044520942,,,,,,,,,,,,Scopus,,,CEUR Workshop Proceedings,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044520942&partnerID=40&md5=fa48eaf14863091847e69eecc5d02618,,2068,,2018,,,,,,,,,,,,,,,,,,,
A hybrid web recommender system based on Q-learning,"Different efforts have been made to address the problem of information overload on the Internet. Recommender systems aim at directing users through this information space, toward the resources that best meet their needs and interests. Web Content Recommendation has been an active application area for Information Filtering, Web Mining and Machine Learning research. Recent studies show that combining the conceptual and usage information can improve the quality of web recommendations. In this paper we exploit this idea to enhance a reinforcement learning framework, primarily devised for web recommendations based on web usage data. A hybrid web recommendation method is proposed by making use of the conceptual relationships among web resources to derive a novel model of the problem, enriched with semantic knowledge about the usage behavior. With our hybrid model for the web page recommendation problem we show the apt and flexibility of the reinforcement learning framework in the web recommendation domain, and demonstrate how it can be extended in order to incorporate various sources of information. We evaluate our method under different settings and show how this method can improve the overall quality of web recommendations. Copyright 2008 ACM.",,"Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran",,,"Taghipour N., Kardan A.","Taghipour, N., Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran; Kardan, A., Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran",11,,10.1145/1363686.1363954,SCOPUS,1,,,,,,,,,,,,,Proceedings of the ACM Symposium on Applied Computing,Machine learning; Personalization; Recommender systems; Reinforcement learning; Web mining,,2-s2.0-56749160807,,,,,,1168,1164,,,,,Scopus,,,Proceedings of the ACM Symposium on Applied Computing,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56749160807&doi=10.1145%2f1363686.1363954&partnerID=40&md5=b3d254c9ed6f9435078120241cad1399,,,,2008,,,,,,,,,,,,,,,,,,,
Adaptive learning based directional MAC protocol for millimeter wave (mmWave) wireless networks,"Directional antennas are used to counter the increased path loss at millimeter wave (mmWave) frequencies (beyond 30 GHz). Usage of directional antennas imply lack of channel sensing on the part of various network nodes. Consequently coordination among various network nodes becomes critical for efficient network operation. In this paper we propose the adaptive learning directional medium access control (AL-DMAC) protocol based on reinforcement learning, to facilitate implicit coordination among various nodes. Our simulation results demonstrate that the AL-DMAC protocol along with a suitably chosen neighbor discovery mechanism yields higher network throughput than both the naive directional slotted ALOHA (DSA) protocol and the memory guided directional medium access control (MD-MAC) protocol. Further the AL-DMAC protocol and the MD-MAC protocol have comparable performance in terms of fairness, measured in terms of the Jain's fairness index.",,,,"Dhirubhai Ambani Institute of Information and Communication Technology (DA-IICT), Gandhinagar - 382007, Gujarat, India",P. Tiwari; D. K. Meena; L. S. Pillutla,,,,10.1109/PIMRC.2017.8292296,IEEE Xplore,1,,20180215,5,,Adaptive learning;Directional antennas;Media Access Protocol;Millimeter wave communication;Millimeter wave technology;Throughput,access protocols;directive antennas;learning (artificial intelligence);millimetre wave communication;telecommunication computing;wireless channels,AL-DMAC protocol;MD-MAC protocol;adaptive learning directional medium access control;channel sensing;directional antennas;directional slotted ALOHA protocol;frequency 30.0 GHz;millimeter wave wireless networks;reinforcement learning,,,,8-13 Oct. 2017,,"2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",MAC protocols and reinforcement learning;directional antennas;mmWave,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8292296,,,,,,IEEE,,,Electronic:978-1-5386-3531-5; POD:978-1-5386-3532-2; Paper:978-1-5386-3529-2; USB:978-1-5386-3530-8,,"2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",1,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
Designing decentralized controllers for distributed-air-jet mems-based micromanipulators by reinforcement learning,"Distributed-air-jet MEMS-based systems have been proposed to manipulate small parts with high velocities and without any friction problems. The control of such distributed systems is very challenging and usual approaches for contact arrayed system don't produce satisfactory results. In this paper, we investigate reinforcement learning control approaches in order to position and convey an object. Reinforcement learning is a popular approach to find controllers that are tailored exactly to the system without any prior model. We show how to apply reinforcement learning in a decentralized perspective and in order to address the global-local trade-off. The simulation results demonstrate that the reinforcement learning method is a promising way to design control laws for such distributed systems. © 2010 Springer Science+Business Media B.V.",,"FEMTO-ST/UFC-ENSMM-UTBM-CNRS, Université de Franche-Comté, Besançon, France; InESS/ULP-CNRS, Université Louis Pasteur, Strasbourg, France",,,"Matignon L., Laurent G.J., Le Fort-Piat N., Chapuis Y.-A.","Matignon, L., FEMTO-ST/UFC-ENSMM-UTBM-CNRS, Université de Franche-Comté, Besançon, France; Laurent, G.J., FEMTO-ST/UFC-ENSMM-UTBM-CNRS, Université de Franche-Comté, Besançon, France; Le Fort-Piat, N., FEMTO-ST/UFC-ENSMM-UTBM-CNRS, Université de Franche-Comté, Besançon, France; Chapuis, Y.-A., InESS/ULP-CNRS, Université Louis Pasteur, Strasbourg, France",8,,10.1007/s10846-010-9396-9,SCOPUS,1,,,,,,,,,,2,,,Journal of Intelligent and Robotic Systems: Theory and Applications,Decentralized control; Distributed control; MEMS-based actuator array; Reinforcement learning; Smart surface,,2-s2.0-77955654600,,,,,,166,145,,,,,Scopus,,,Journal of Intelligent and Robotic Systems: Theory and Applications,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955654600&doi=10.1007%2fs10846-010-9396-9&partnerID=40&md5=96e32a868d4dae09fd21ae5869018a42,,59,,2010,,,,,,,,,,,,,,,,,,,
Personalized web-document filtering using reinforcement learning,"Document filtering is increasingly deployed in Web environments to reduce information overload of users. We formulate online information filtering as a reinforcement learning problem, i.e., TD(0). The goal is to learn user profiles that best represent information needs and thus maximize the expected value of user relevance feedback. A method is then presented that acquires reinforcement signals automatically by estimating user's implicit feedback from direct observations of browsing behaviors. This ""learning by observation"" approach is contrasted with conventional relevance feedback methods which require explicit user feedbacks. Field tests have been performed that involved 10 users reading a total of 18,750 HTML documents during 45 days. Compared to the existing document filtering techniques, the proposed learning method showed superior performance in information quality and adaptation speed to user preferences in online filtering.",,"Biointelligence Lab., Sch. of Comp. Sci. and Engineering, Seoul National University, Seoul, South Korea; Biointelligence Lab., Sch. of Comp. Sci. and Engineering, Seoul National University, Seoul 151-742, South Korea",,,"Zhang B.-T., Seo Y.-W.","Zhang, B.-T., Biointelligence Lab., Sch. of Comp. Sci. and Engineering, Seoul National University, Seoul, South Korea, Biointelligence Lab., Sch. of Comp. Sci. and Engineering, Seoul National University, Seoul 151-742, South Korea; Seo, Y.-W., Biointelligence Lab., Sch. of Comp. Sci. and Engineering, Seoul National University, Seoul, South Korea",41,,10.1080/088395101750363993,SCOPUS,1,,,,,,,,,,7,,,Applied Artificial Intelligence,,,2-s2.0-0242322792,,,,,,685,665,,,,,Scopus,,,Applied Artificial Intelligence,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0242322792&doi=10.1080%2f088395101750363993&partnerID=40&md5=068c1f95f84f5da8db38e9ff190d8805,,15,,2001,,,,,,,,,,,,,,,,,,,
Solving large scale disassembly line balancing problem with uncertainty using reinforcement learning,"Due to increasing environmental concerns, manufacturers are forced to take back their products at the end of products' useful functional life. Manufacturers explore various options including disassembly operations to recover components and subassemblies for reuse, remanufacture, and recycle to extend the life of materials in use and cut down the disposal volume. However, disassembly operations are problematic due to high degree of uncertainty associated with the quality and configuration of product returns. In this research we address the disassembly line balancing problem (DLBP) using a Monte-Carlo based reinforcement learning technique. This reinforcement learning approach is tailored fit to the underlying dynamics of a DLBP. The research results indicate that the reinforcement learning based method is able to perform effectively, even on a complex large scale problem, within a reasonable amount of computational time. The proposed method performed on par or better than the benchmark methods for solving DLBP reported in the literature. Unlike other methods which are usually limited deterministic environments, the reinforcement learning based method is able to operate in deterministic as well as stochastic environments. © 2012 Springer Science+Business Media New York.",,"Department of Mechanical and Industrial Engineering, Northeastern University, 360 Huntington Avenue, Boston, MA 02115, United States",,,"Tuncel E., Zeid A., Kamarthi S.","Tuncel, E., Department of Mechanical and Industrial Engineering, Northeastern University, 360 Huntington Avenue, Boston, MA 02115, United States; Zeid, A., Department of Mechanical and Industrial Engineering, Northeastern University, 360 Huntington Avenue, Boston, MA 02115, United States; Kamarthi, S., Department of Mechanical and Industrial Engineering, Northeastern University, 360 Huntington Avenue, Boston, MA 02115, United States",15,,10.1007/s10845-012-0711-0,SCOPUS,1,,,,,,,,,,4,,,Journal of Intelligent Manufacturing,Cell phone; Disassembly; Disassembly line balancing; Heuristics; PC; Reinforcement learning,,2-s2.0-84904384763,,,,,,659,647,,,,,Scopus,,,Journal of Intelligent Manufacturing,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904384763&doi=10.1007%2fs10845-012-0711-0&partnerID=40&md5=cf5242ed0560e198a2a024f86478a8a9,,25,,2014,,,,,,,,,,,,,,,,,,,
An Effective Approach to Continuous User Authentication for Touch Screen Smart Devices,"Due to the rapid increase in the use of personal smart devices, more sensitive data is stored and viewed on these smart devices. This trend makes it easier for attackers to access confidential data by physically compromising (including stealing) these smart devices. Currently, most personal smart devices employ one of the one-time user authentication schemes, such as four-to-six digits, fingerprint or pattern-based schemes. These authentication schemes are often not good enough for securing personal smart devices because the attackers can easily extract all the confidential data from the smart device by breaking such schemes, or by keeping the authenticated session open on a physically compromised smart device. In addition, existing re-authentication or continuous authentication techniques for protecting personal smart devices use centralized architecture and require servers at a centralized location to train and update the learning model used for continuous authentication, which impose additional communication overhead. In this paper, an approach is presented to generating and updating the authentication model on the user's smart device with user's gestures, instead of a centralized server. There are two major advantages in this approach. One is that this approach continuously learns and authenticates finger gestures of the user in the background without requiring the user to provide specific gesture inputs. The other major advantage is to have better authentication accuracy by treating uninterrupted user finger gestures over a short time interval as a single gesture for continuous user authentication.",,,,"Inf. Assurance Center, Sch. of Comput., Inf., & Decision Syst. Eng., Arizona State Univ., Tempe, AZ, USA",A. B. Buduru; S. S. Yau,,2,,10.1109/QRS.2015.40,IEEE Xplore,1,,20150924,226,,Accuracy;Authentication;Context;Fingers;Object recognition;Performance evaluation,authorisation;computer crime;gesture recognition;message authentication;smart phones;touch sensitive screens,attackers;authentication accuracy;authentication model;centralized server;confidential data access;continuous user authentication;finger gestures authentication;personal smart devices;sensitive data;touch screen smart devices;uninterrupted user finger gestures,,,,3-5 Aug. 2015,,"2015 IEEE International Conference on Software Quality, Reliability and Security",Touch-screen smart devices;adaptive continuous user authentication;and user re-authentication;reinforcement learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272936,,,,,,IEEE,19,,Electronic:978-1-4673-7989-2; POD:978-1-4673-7990-8,,"2015 IEEE International Conference on Software Quality, Reliability and Security",219,IEEE Conferences,,,,,2015,,,,,,,,,,,,,,,,,,,
New Statistical Learning Methods for Estimating Optimal Dynamic Treatment Regimes,"Dynamic treatment regimes (DTRs) are sequential decision rules for individual patients that can adapt over time to an evolving illness. The goal is to accommodate heterogeneity among patients and find the DTR which will produce the best long-term outcome if implemented. We introduce two new statistical learning methods for estimating the optimal DTR, termed backward outcome weighted learning (BOWL), and simultaneous outcome weighted learning (SOWL). These approaches convert individualized treatment selection into an either sequential or simultaneous classification problem, and can thus be applied by modifying existing machine learning techniques. The proposed methods are based on directly maximizing over all DTRs a nonparametric estimator of the expected long-term outcome; this is fundamentally different than regression-based methods, for example, Q-learning, which indirectly attempt such maximization and rely heavily on the correctness of postulated regression models.  We prove that the resulting rules are consistent, and provide finite sample bounds for the errors using the estimated rules. Simulation results suggest the proposed methods produce superior DTRs compared with Q-learning especially in small samples. We illustrate the methods using data from a clinical trial for smoking cessation. Supplementary materials for this article are available online. © 2015 American Statistical Association.",,"Department of Biostatistics and Medical Informatics, University of Wisconsin-MadisonWI, United States; Department of Biostatistics, University of North Carolina at Chapel HillNC, United States; Department of Statistics, North Carolina State UniversityNC, United States; Department of Statistics and Operations Research, University of North Carolina at Chapel Hill, Chapel Hill, NC, United States",,,"Zhao Y.-Q., Zeng D., Laber E.B., Kosorok M.R.","Zhao, Y.-Q., Department of Biostatistics and Medical Informatics, University of Wisconsin-MadisonWI, United States; Zeng, D., Department of Biostatistics, University of North Carolina at Chapel HillNC, United States; Laber, E.B., Department of Statistics, North Carolina State UniversityNC, United States; Kosorok, M.R., Department of Statistics and Operations Research, University of North Carolina at Chapel Hill, Chapel Hill, NC, United States",27,,10.1080/01621459.2014.937488,SCOPUS,1,,,,,,,,,,510,,,Journal of the American Statistical Association,Classification; Personalized medicine; Q-learning; Reinforcement learning; Risk bound; Support vector machine,,2-s2.0-84936797778,,,,,,598,583,,,,,Scopus,,,Journal of the American Statistical Association,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936797778&doi=10.1080%2f01621459.2014.937488&partnerID=40&md5=4be5602bc2c4dc8e467b01449fb353a8,,110,,2015,,,,,,,,,,,,,,,,,,,
Learning social relations for culture aware interaction,"Each person has their private physical and/or psychological area where they do not want to share with others during social interactions. This area gives them comfort about interactions and its size usually depends on various factors such as culture, personal traits, and acquaintanceship. This issue may also arise in case of human-robot interaction, especially when the robot is required to generate a socially competent interaction strategy toward people they are interacting with. Here, we propose a new robot exploration strategy to socially interact with people by considering the social relationship between the robot and each person. To that end, two definitions of interaction area are made: (1) Acceptable area allowed to be shared with other people and robots, and (2) Private area where a human does not want to be interfered by others. Based on these definitions, the robot can optimize the path to maximize the frequency/degree of visiting the acceptable area of each person and to minimize the frequency/degree of trespassing into the private area of them at the same time in an iterative way. In this paper, the social force model (SFM) of each person, based on the potential field concept, is designed by a fuzzy inference system and its parameter is optimized by the reinforcement learning model during interactions. We have shown that the proposed model can generate a suitable SFM of each person, which was quite similar to a ground truth model, allowing to plan a path to simultaneously optimize the two factors of interaction area, respectively.",,,,"School of Information Science, Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Nomi, Ishikawa 923-1292, Japan",P. Patompak; S. Jeong; I. Nilkhamhang; N. Y. Chong,,,,10.1109/URAI.2017.7992879,IEEE Xplore,1,,20170727,31,,Adaptation models;Force;Human-robot interaction;Iron;Mathematical model;Navigation;Robots,fuzzy reasoning;human-robot interaction;learning (artificial intelligence);social aspects of automation,SFM;culture aware interaction;fuzzy inference system;human-robot interaction;potential field concept;reinforcement learning model;robot exploration strategy;social force model;social interactions;social relations;socially competent interaction strategy,,,,June 28 2017-July 1 2017,,2017 14th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7992879,,,,,,IEEE,,,Electronic:978-1-5090-3056-9; POD:978-1-5090-3057-6,,2017 14th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI),26,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
Inducing effective pedagogical strategies using learning context features,"Effective pedagogical strategies are important for e-learning environments. While it is assumed that an effective learning environment should craft and adapt its actions to the user's needs, it is often not clear how to do so. In this paper, we used a Natural Language Tutoring System named Cordillera and applied Reinforcement Learning (RL) to induce pedagogical strategies directly from pre-existing human user interaction corpora. 50 features were explored to model the learning context. Of these features, domain-oriented and system performance features were the most influential while user performance and background features were rarely selected. The induced pedagogical strategies were then evaluated on real users and results were compared with pre-existing human user interaction corpora. Overall, our results show that RL is a feasible approach to induce effective, adaptive pedagogical strategies by using a relatively small training corpus. Moreover, we believe that our approach can be used to develop other adaptive and personalized learning environments. © 2010 Springer-Verlag.",,"Machine Learning Department, Carnegie Mellon University, PA 15213, United States; School of Computing and Informatics, Arizona State University, AZ 85287, United States; Department of Computer Science, University of Pittsburgh, PA 15260, United States; Department of Biomedical Informatics, University of Pittsburgh, Pittsburgh, PA 15260, United States",,,"Chi M., Vanlehn K., Litman D., Jordan P.","Chi, M., Machine Learning Department, Carnegie Mellon University, PA 15213, United States; Vanlehn, K., School of Computing and Informatics, Arizona State University, AZ 85287, United States; Litman, D., Department of Computer Science, University of Pittsburgh, PA 15260, United States; Jordan, P., Department of Biomedical Informatics, University of Pittsburgh, Pittsburgh, PA 15260, United States",10,,10.1007/978-3-642-13470-8_15,SCOPUS,1,,,,,,,,,,,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,,2-s2.0-77954602915,,,,,,158,147,,,,,Scopus,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954602915&doi=10.1007%2f978-3-642-13470-8_15&partnerID=40&md5=7383a8c4df03d5465e9cffbeffaa8641,,6075 LNCS,,2010,,,,,,,,,,,,,,,,,,,
The neural bases of emotion regulation,"Emotions are powerful determinants of behaviour, thought and experience, and they may be regulated in various ways. Neuroimaging studies have implicated several brain regions in emotion regulation, including the ventral anterior cingulate and ventromedial prefrontal cortices, as well as the lateral prefrontal and parietal cortices. Drawing on computational approaches to value-based decision-making and reinforcement learning, we propose a unifying conceptual framework for understanding the neural bases of diverse forms of emotion regulation. © 2015 Macmillan Publishers Limited.",,"Department of Psychiatry and Behavioral Sciences, Stanford Neurosciences Institute, Stanford University, 401 Quarry Road, Stanford, CA, United States; Veterans Affairs Palo Alto Healthcare System, Sierra Pacific Mental Illness, Research, Education and Clinical Center (MIRECC), 3801 Miranda Ave, Palo Alto, CA, United States; Department of Systems Neuroscience, University Medical Center Hamburg- Eppendorf, Martinistr 52, Hamburg, Germany; Department of Psychology, Stanford University, 450 Serra Mall, Stanford, CA, United States",,,"Etkin A., Büchel C., Gross J.J.","Etkin, A., Department of Psychiatry and Behavioral Sciences, Stanford Neurosciences Institute, Stanford University, 401 Quarry Road, Stanford, CA, United States, Veterans Affairs Palo Alto Healthcare System, Sierra Pacific Mental Illness, Research, Education and Clinical Center (MIRECC), 3801 Miranda Ave, Palo Alto, CA, United States; Büchel, C., Department of Systems Neuroscience, University Medical Center Hamburg- Eppendorf, Martinistr 52, Hamburg, Germany; Gross, J.J., Department of Psychology, Stanford University, 450 Serra Mall, Stanford, CA, United States",131,,10.1038/nrn4044,SCOPUS,1,,,,,,,,,,11,,,Nature Reviews Neuroscience,,,2-s2.0-84944740173,,,,,,700,693,,,,,Scopus,,,Nature Reviews Neuroscience,,Review,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944740173&doi=10.1038%2fnrn4044&partnerID=40&md5=13c04bd08c4c85acdbb1093d59a0fae3,,16,,2015,,,,,,,,,,,,,,,,,,,
A 55nm time-domain mixed-signal neuromorphic accelerator with stochastic synapses and embedded reinforcement learning for autonomous micro-robots,"Even as rapid advances are being made in the areas of deep neural networks (DNNs) and convolutional neural networks (CNNs) with most hardware demonstrations geared towards inference in vision-based platforms [1-5], we recognize that true autonomy in intelligent agents will only emerge when such bio-mimetic systems can perform continuous learning through interactions with the environment. Reinforcement learning (RL) presents one such computational paradigm inspired by behaviorist psychology, where autonomous agents take actions in an environment to maximize a notion of cumulative reward. This concept is deeply rooted in the human brain where dopamine mediated neurotransmitters (in the cortex, striatum and thalamus of the brain) have been shown to encourage reward-motivated behavior in all our social interactions (Fig. 7.4.1). In this paper, we present a 690μW (V<sub>CC</sub>=1.2V) neuromorphic accelerator fabricated in 55nm CMOS, which: (1) inherits unique properties of stochastic neural networks, (2) leverages recent advances in Q-learning as an implementation of RL, and (3) demonstrates energy-efficient time-domain mixed-signal (TD-MS) circuit architectures, to provide autonomy to a mobile, self-driving micro-robot at the edge of the cloud, with possible applications in disaster relief, reconnaissance and personal robotics.",,,,"Georgia Institute of Technology, Atlanta, GA",A. Amravati; S. B. Nasir; S. Thangadurai; I. Yoon; A. Raychowdhury,,,,10.1109/ISSCC.2018.8310215,IEEE Xplore,1,,20180312,126,,Biological neural networks;Neuromorphics;Neurons;Semiconductor device measurement;Sensors;Stochastic processes;Synapses,biomimetics;brain;feedforward neural nets;learning (artificial intelligence);microrobots;mobile robots;neurocontrollers;robot vision,55nm time-domain mixed-signal neuromorphic accelerator;CMOS;RL;autonomous agents;behaviorist psychology;bio-mimetic systems;computational paradigm;convolutional neural networks;cumulative reward;deep neural networks;dopamine mediated neurotransmitters;human brain;intelligent agents;mobile self-driving microrobot;personal robotics;reinforcement learning;reward-motivated behavior;social interactions;stochastic neural networks;stochastic synapses;striatum;thalamus,,,,11-15 Feb. 2018,,2018 IEEE International Solid - State Circuits Conference - (ISSCC),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8310215,,,,,,IEEE,,,Electronic:978-1-5090-4940-0; POD:978-1-5386-2227-8,,2018 IEEE International Solid - State Circuits Conference - (ISSCC),124,IEEE Conferences,,,,,2018,,,,,,,,,,,,,,,,,,,
Attention-Aware Face Hallucination via Deep Reinforcement Learning,"Face hallucination is a domain-specific super-resolution problem with the goal to generate high-resolution (HR) faces from low-resolution (LR) input images. In contrast to existing methods that often learn a single patch-to-patch mapping from LR to HR images and are regardless of the contextual interdependency between patches, we propose a novel Attention-aware Face Hallucination (Attention-FH) framework which resorts to deep reinforcement learning for sequentially discovering attended patches and then performing the facial part enhancement by fully exploiting the global interdependency of the image. Specifically, in each time step, the recurrent policy network is proposed to dynamically specify a new attended region by incorporating what happened in the past. The state (i.e., face hallucination result for the whole image) can thus be exploited and updated by the local enhancement network on the selected region. The Attention-FH approach jointly learns the recurrent policy network and local enhancement network through maximizing the long-term reward that reflects the hallucination performance over the whole image. Therefore, our proposed Attention-FH is capable of adaptively personalizing an optimal searching path for each face image according to its own characteristic. Extensive experiments show our approach significantly surpasses the state-of-the-arts on in-the-wild faces with large pose and illumination variations. The state (i.e., face hallucination result for the whole image) can thus be exploited and updated by the local enhancement network on the selected region. The Attention-FH approach jointly learns the recurrent policy network and local enhancement network through maximizing the long-term reward that reflects the hallucination performance over the whole image. Therefore, our proposed Attention-FH is capable of adaptively personalizing an optimal searching path for each face image according to its own characteristic. Extensive experiments show our appro- ch significantly surpasses the state-of-the-arts on in-the-wild faces with large pose and illumination variations.",,,,"Sch. of Data & Comput. Sci., Sun Yat-sen Univ., Guangzhou, China",Q. Cao; L. Lin; Y. Shi; X. Liang; G. Li,,1,,10.1109/CVPR.2017.180,IEEE Xplore,1,,20171109,1664,,Computer vision;Face;History;Image resolution;Learning (artificial intelligence);Neural networks,face recognition;image enhancement;image resolution;learning (artificial intelligence),attention-FH approach;attention-aware face hallucination;deep reinforcement learning;domain-specific super-resolution problem;facial part enhancement;hallucination performance;high-resolution face generation;in-the-wild faces;local enhancement network;low-resolution input images;optimal searching path;patch-to-patch mapping;recurrent policy network,,1063-6919;10636919,,21-26 July 2017,,2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8099663,,,,,,IEEE,,,Electronic:978-1-5386-0457-1; POD:978-1-5386-0458-8,,2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),1656,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
A Three-Layered Mutually Reinforced Model for Personalized Citation Recommendation,"Fast-growing scientific papers pose the problem of rapidly and accurately finding a list of reference papers for a given manuscript. Citation recommendation is an indispensable technique to overcome this obstacle. In this paper, we propose a citation recommendation approach via mutual reinforcement on a three-layered graph, in which each paper, author or venue is represented as a vertex in the paper layer, author layer, and venue layer, respectively. For personalized recommendation, we initiate the random walk separately for each query researcher. However, this has a high computational complexity due to the large graph size. To solve this problem, we apply a three-layered interactive clustering approach to cluster related vertices in the graph. Personalized citation recommendations are then made on the subgraph, generated by the clusters associated with each researcher's needs. When evaluated on the ACL anthology network, DBLP, and CiteSeer ML data sets, the performance of our proposed model-based citation recommendation approach is comparable with that of other state-of-the-art citation recommendation approaches. The results also demonstrate that the personalized recommendation approach is more effective than the nonpersonalized recommendation approach.",,,,"School of Automation, Northwestern Polytechnical University, Xi'an 710072, China.",X. Cai; J. Han; W. Li; R. Zhang; S. Pan; L. Yang,,,,10.1109/TNNLS.2018.2817245,IEEE Xplore,1,,,12,China Postdoctoral Science Foundation; National Natural Science Foundation of China; Research Grants Council of Hong Kong; ,Clustering algorithms;Computational complexity;Context modeling;Data models;Hybrid power systems;Indexes;Learning systems,,,,2162-237X;2162237X,Early Access,,,IEEE Transactions on Neural Networks and Learning Systems,Mutually reinforced model;personalized citation recommendation;three-layered interactive clustering.,,,,,20180412,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8337085,,,,,,IEEE,,,,,IEEE Transactions on Neural Networks and Learning Systems,1,IEEE Early Access Articles,,,Early Access,,2018,,,,,,,,,,,,,,,,,,,
Genetic reinforcement learning approach to the heterogeneous machine scheduling problem,"Focuses on the development of a learning-based heuristic for scheduling heterogeneous machines. Although list scheduling methods have been widely used for a large class of scheduling problems, including the heterogeneous machine scheduling problem, they involve designing priority rules, which usually require a fair amount of insights on the characteristics of the problem to be solved. Instead of elaborate design of priority rules in a single step, we propose an iterative list scheduling process, which refines priority rules while generating a number of schedules. The proposed iterative list scheduling is formulated as a reinforcement learning problem, with states and actions defined in list scheduling. Due to the large number of possible states, reinforcement learning algorithms which use value functions in constructing an optimal policy may not be suitable for scheduling problems. Thus, to directly work with policies rather than the values of states, we propose genetic reinforcement learning (GRL), in which the policies of reinforcement learning are encoded into the chromosomes of genetic algorithms and a near-optimal policy is searched for by genetic algorithms. A GRL-based scheduler, called evolutionary intracell scheduler (EVIS), has been developed and applied to various scheduling problems such as the heterogeneous machine scheduling, the processor scheduling, the job-shop scheduling, the flow-shop scheduling, and the open-shop scheduling problems. The proposed model of EVIS, which has a linear order of population-fitness convergence, is verified by computer experiments. Even without fine tuning EVIS, the quality of solutions achieved by EVIS is comparable to that of problem-tailored heuristics for most of the problem instances",,,,"Dept. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA",Gyoung Hwan Kim; C. S. G. Lee,,6,,10.1109/70.736772,IEEE Xplore,1,,20020806,893,,Approximation algorithms;Biological cells;Cost function;Genetic algorithms;Helium;Iterative algorithms;Job shop scheduling;Learning;Processor scheduling;Scheduling algorithm,genetic algorithms;iterative methods;learning (artificial intelligence);minimisation;production control;scheduling,evolutionary intracell scheduler;flow-shop scheduling;genetic reinforcement learning;heterogeneous machine scheduling problem;iterative list scheduling process;job-shop scheduling;learning-based heuristic;near-optimal policy;open-shop scheduling problems,,1042-296X;1042296X,6,Dec 1998,,IEEE Transactions on Robotics and Automation,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=736772,,,,,,IEEE,67,,,,IEEE Transactions on Robotics and Automation,879,IEEE Journals & Magazines,,,14,,1998,,,,,,,,,,,,,,,,,,,
Personalizing a Service Robot by Learning Human Habits from Behavioral Footprints,"For a domestic personal robot, personalized services are as important as predesigned tasks, because the robot needs to adjust the home state based on the operator's habits. An operator's habits are composed of cues, behaviors, and rewards. This article introduces behavioral footprints to describe the operator's behaviors in a house, and applies the inverse reinforcement learning technique to extract the operator's habits, represented by a reward function. We implemented the proposed approach with a mobile robot on indoor temperature adjustment, and compared this approach with a baseline method that recorded all the cues and behaviors of the operator. The result shows that the proposed approach allows the robot to reveal the operator's habits accurately and adjust the environment state accordingly. © 2015 THE AUTHORS. Published by Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company",Open Access,"California Institute of Technology, Pasadena, CA, United States; The Chinese University of Hong Kong, Hong Kong, China",,,"Li K., Meng M.Q.-H.","Li, K., California Institute of Technology, Pasadena, CA, United States; Meng, M.Q.-H., The Chinese University of Hong Kong, Hong Kong, China",,,10.15302/J-ENG-2015024,SCOPUS,1,,,,,,,,,,1,,,Engineering,behavioral footprints; habit learning; personalized robot,,2-s2.0-84988727903,,,,,,84,79,,,,,Scopus,,,Engineering,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988727903&doi=10.15302%2fJ-ENG-2015024&partnerID=40&md5=bd73d8218407187c4a2a349249f2a84c,,1,,2015,,,,,,,,,,,,,,,,,,,
"Utilization of machine learning methods for assembling, training and understanding autonomous robots","For decades human society has been supported by the proliferation of complex artifacts such as electronic appliances, personal vehicles and mass transportation systems, electrical and communications grids, and in the past few decades, Internet. In the very near future, robots will play increasingly important roles in our daily life. The increase in complexity of the tasks and sometimes physical forms or morphologies of the artifacts consequently requires complex assembling and controlling procedures of them, which soon will be unmanageable by the traditional manufacturing process. The aim of this paper is to give a brief review on the potentials of the non-traditional assembling of complex artifacts, which in this study is symbolized by the creation of autonomous robots. Methods in self-assembling modular robots, real time learning of autonomous robots and a method for giving the comprehensive understanding, albeit intuitively, to human will be explained through some physical experiments.",,,,"Department of Mechanics and Information Technology, Chukyo University, Toyota, Japan",P. Hartono,,2,,10.1109/HSI.2011.5937399,IEEE Xplore,1,,20110704,402,,Learning systems;Morphology;Robot kinematics;Robot sensing systems;Topology;Training,learning (artificial intelligence);mobile robots;robotic assembly,autonomous robots assembling;autonomous robots training;autonomous robots undertanding;machine learning methods;selfassembling modular robots,,2158-2246;21582246,,19-21 May 2011,,"2011 4th International Conference on Human System Interactions, HSI 2011",Autonomous Robots;Modular Robots;Perceptual Understanding;Reinforcement Learning;Self Assembling;Self-Organizing Map,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5937399,,,,,,IEEE,15,,Electronic:978-1-4244-9639-6; Electronic:978-1-4244-9640-2; POD:978-1-4244-9638-9,,"2011 4th International Conference on Human System Interactions, HSI 2011",398,IEEE Conferences,,,,,2011,,,,,,,,,,,,,,,,,,,
Agent-based assistance in ambient assisted living through reinforcement learning and semantic technologies: (Short paper),"For impaired people, the conduction of certain daily life activities is problematic due to motoric and cognitive handicaps. For that reason, assistive agents in ambient assisted environments provide services that aim at supporting elderly and impaired people. However, these agents act in complex stochastic and indeterministic environments where the concrete effects of a performed action are usually unknown at design time. Furthermore, they have to perform varying tasks according to the user’s context and needs, wherefore an agent has to be flexible and able to recognize required capabilities in a certain situation in order to provide adequate, unobtrusive assistance. Hence, an expressive representation framework is required that relates user-specific impairments to required agent capabilities. This work presents an approach which (a) describes and links user impairments and capabilities using the formal, model-theoretic semantics expressed in OWL2 DL ontologies, (b) computes optimal policies through Reinforcement Learning and propagates these in an agent network. The presented approach improves the collaborative, personalized and adequate assistance of assistive agents and tailors the agent-based services to the user’s missing capabilities. © 2017, Springer International Publishing AG.",,"FZI Forschungszentrum Informatik am KIT, Information Process Engineering, Haid-und-Neu-Str. 10-14, Karlsruhe, Germany; Institute for Computer Science, University of Applied Sciences Darmstadt, Schöfferstrasse 8B, Darmstadt, Germany",,,"Merkle N., Zander S.","Merkle, N., FZI Forschungszentrum Informatik am KIT, Information Process Engineering, Haid-und-Neu-Str. 10-14, Karlsruhe, Germany; Zander, S., Institute for Computer Science, University of Applied Sciences Darmstadt, Schöfferstrasse 8B, Darmstadt, Germany",,,10.1007/978-3-319-69459-7_12,SCOPUS,1,,,,,,,,,,,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,,2-s2.0-85032656608,,,,,,188,180,,,,,Scopus,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032656608&doi=10.1007%2f978-3-319-69459-7_12&partnerID=40&md5=34e55c071586274d84d0079823608e8d,,10574 LNCS,,2017,,,,,,,,,,,,,,,,,,,
Learning to give route directions from human demonstrations,"For several applications, robots and other computer systems must provide route descriptions to humans. These descriptions should be natural and intuitive for the human users. In this paper, we present an algorithm that learns how to provide good route descriptions from a corpus of human-written directions. Using inverse reinforcement learning, our algorithm learns how to select the information for the description depending on the context of the route segment. The algorithm then uses the learned policy to generate directions that imitate the style of the descriptions provided by humans, thus taking into account personal as well as cultural preferences and special requirements of the particular user group providing the learning demonstrations. We evaluate our approach in a user study and show that the directions generated by our policy sound similar to human-given directions and substantially more natural than directions provided by commercial web services.",,,,"Department of Computer Science, University of Freiburg, 79110 Freiburg, Germany",S. Oßwald; H. Kretzschmar; W. Burgard; C. Stachniss,,3,,10.1109/ICRA.2014.6907334,IEEE Xplore,1,,20140929,3308,,Computers;Context;Cultural differences;Entropy;Learning (artificial intelligence);Measurement;Web services,control engineering computing;learning (artificial intelligence);mobile robots;path planning,computer systems;cultural preferences;human demonstrations;human-given directions;human-written directions;inverse reinforcement learning;learning demonstrations;personal preferences;robots;route descriptions;route directions;route segment,,1050-4729;10504729,,May 31 2014-June 7 2014,,2014 IEEE International Conference on Robotics and Automation (ICRA),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6907334,,,,,,IEEE,25,,Electronic:978-1-4799-3685-4; POD:978-1-4799-3686-1; USB:978-1-4799-3684-7,,2014 IEEE International Conference on Robotics and Automation (ICRA),3303,IEEE Conferences,,,,,2014,,,,,,,,,,,,,,,,,,,
Dynamic Game Difficulty Scaling Using Adaptive Behavior-Based AI,"Games are played by a wide variety of audiences. Different individuals will play with different gaming styles and employ different strategic approaches. This often involves interacting with nonplayer characters that are controlled by the game AI. From a developer's standpoint, it is important to design a game AI that is able to satisfy the variety of players that will interact with the game. Thus, an adaptive game AI that can scale the difficulty of the game according to the proficiency of the player has greater potential to customize a personalized and entertaining game experience compared to a static game AI. In particular, dynamic game difficulty scaling refers to the use of an adaptive game AI that performs game adaptations in real time during the game session. This paper presents two adaptive algorithms that use ideas from reinforcement learning and evolutionary computation to improve player satisfaction by scaling the difficulty of the game AI while the game is being played. The effects of varying the learning and mutation rates are examined and a general rule of thumb for the parameters is proposed. The proposed algorithms are demonstrated to be capable of matching its opponents in terms of mean scores and winning percentages. Both algorithms are able to generalize well to a variety of opponents.",,,,"Institute for Infocomm Research, Agency for Science Technology and Research (A*STAR), Singapore, Singapore",C. H. Tan; K. C. Tan; A. Tay,,15,,10.1109/TCIAIG.2011.2158434,IEEE Xplore,1,,20111212,301,,Adaptation model;Artificial intelligence;Games;Humans;Pixel;Real time systems;Vehicles,computer games;evolutionary computation;learning (artificial intelligence),adaptive behavior-based artificial intelligence;dynamic game difficulty scaling;evolutionary computation;game adaptation;gaming styles;mutation rates;reinforcement learning,,1943-068X;1943068X,4,Dec. 2011,,IEEE Transactions on Computational Intelligence and AI in Games,Artificial intelligence;behavior based;car racing simulation;game AI;player satisfaction;real-time adaptation,,,,,20110602,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783334,,,,,1,IEEE,37,,,,IEEE Transactions on Computational Intelligence and AI in Games,289,IEEE Journals & Magazines,,,3,,2011,,,,,,,,,,,,,,,,,,,
Multi-objective reinforcement learning algorithm and its application in drive system,"Generally, reinforcement learning (RL) is used to design neurocontroller for control system with single objective. When facing multi-objective system, it is necessary to design the neurocontroller according to the personal preference. This paper proposed a multi-objective reinforcement learning algorithm (MORLA) to design neurocontroller with the personal preference. It transformed the multi-objective into synthetical objective and applied parallel genetic algorithm (PGA) to evolve the neurocontroller according to the synthetical objective. To establish the synthetical objective, the objective weight which represents the personal preference is calculated by solving the constrained optimization problem (COP) at the end of each generation. The COP requires not only the biggest variance of the synthetical objective in the population, but also requires the weight to fit the designerpsilas preference. After acquiring the weights, the PGA can select the elitists from the population according to the designerpsilas preference and design a satisfying neurocontroller by evolutionary operations. At last, the MORLA is used to design neurocontroller for a speed-controlled induction motor drive with indirect vector control. This paper designed several neurocontrollers with different personal preferences for the drive system. The simulation results show the feasibility and validity of the MORLA.",,,,"Department of Control Science and Engineering, Huazhong University of Science and Technology, China",Zhang Huajun; Zhao Jin; Wang Rui; Ma Tan,,1,,10.1109/IECON.2008.4757965,IEEE Xplore,1,,20090123,279,,Algorithm design and analysis;Constraint optimization;Control systems;Convergence;Design engineering;Design optimization;Electronics packaging;Genetic algorithms;Learning;Neurocontrollers,control system synthesis;genetic algorithms;induction motor drives;learning (artificial intelligence);machine control;neurocontrollers;velocity control,MORLA;constrained optimization problem;control system;drive system;indirect vector control;multiobjective reinforcement learning algorithm;neurocontroller;parallel genetic algorithm;speed-controlled induction motor drive,,1553-572X;1553572X,,10-13 Nov. 2008,,2008 34th Annual Conference of IEEE Industrial Electronics,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4757965,,,,,,IEEE,32,,CD-ROM:978-1-4244-1766-7; POD:978-1-4244-1767-4,,2008 34th Annual Conference of IEEE Industrial Electronics,274,IEEE Conferences,,,,,2008,,,,,,,,,,,,,,,,,,,
MPRL: Multiple-Periodic Reinforcement Learning for difficulty adjustment in rehabilitation games,"Generally, the difficulty level of a therapeutic game is regulated manually by a therapist. However, home-based rehabilitation games require a technique for automatic difficulty adjustment. This paper proposes a personalized difficulty adjustment technique for a rehabilitation game that automatically regulates difficulty settings based on a patient's skills in real-time. To this end, ideas from reinforcement learning are used to dynamically adjust the difficulty of a game. We show that difficulty adjustment is a multiple-objective problem, in which some objectives might be evaluated at different periods. To address this problem, we propose and use Multiple-Periodic Reinforcement Learning (MPRL) that makes it possible to evaluate different objectives of difficulty adjustment in separate periods. The results of experiments show that MPRL outperforms traditional Multiple-Objective Reinforcement Learning (MORL) in terms of user satisfaction parameters as well as improving the movement skills of patients.",,,,"Faculty of Multimedia, Tabriz Islamic Art University, Iran",Y. A. Sekhavat,,,,10.1109/SeGAH.2017.7939260,IEEE Xplore,1,,20170608,7,,Biological cells;Silicon,learning (artificial intelligence);medical computing;patient rehabilitation;serious games (computing),home-based rehabilitation games;multiple-objective problem;multiple-periodic reinforcement learning;patient movement skills;therapeutic game,,,,2-4 April 2017,,2017 IEEE 5th International Conference on Serious Games and Applications for Health (SeGAH),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7939260,,,,,,IEEE,,,Electronic:978-1-5090-5482-4; POD:978-1-5090-5483-1,,2017 IEEE 5th International Conference on Serious Games and Applications for Health (SeGAH),1,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
Personalized Course Sequence Recommendations,"Given the variability in student learning, it is becoming increasingly important to tailor courses as well as course sequences to student needs. This paper presents a systematic methodology for offering personalized course sequence recommendations to students. First, a forward-search backward-induction algorithm is developed that can optimally select course sequences to decrease the time required for a student to graduate. The algorithm accounts for prerequisite requirements (typically present in higher level education) and course availability. Second, using the tools of multiarmed bandits, an algorithm is developed that can optimally recommend a course sequence that both reduces the time to graduate while also increasing the overall GPA of the student. The algorithm dynamically learns how students with different contextual backgrounds perform for given course sequences and, then, recommends an optimal course sequence for new students. Using real-world student data from the UCLA Mechanical and Aerospace Engineering Department, we illustrate how the proposed algorithms outperform other methods that do not include student contextual information when making course sequence recommendations.",,,,"Department of Electrical and Computer Engineering, University of Miami, Coral Gables, FL, USA",J. Xu; T. Xing; M. van der Schaar,,2,,10.1109/TSP.2016.2595495,IEEE Xplore,1,,20160824,5352,10.13039/100000001 - National Science Foundation; ,Adaptive systems;Aerospace engineering;Complexity theory;Education;Electronic mail;Heuristic algorithms;Signal processing algorithms,aerospace computing;computer aided instruction;educational courses;engineering education;inference mechanisms;mechanical engineering computing;recommender systems,GPA;UCLA Mechanical and Aerospace Engineering Department;contextual backgrounds;course availability;forward-search backward-induction algorithm;multiarmed bandit tool;personalized course sequence recommendations;prerequisite requirements;student learning;systematic methodology,,1053-587X;1053587X,20,"Oct.15, 15 2016",,IEEE Transactions on Signal Processing,Personalized education;contextual bandits;course sequence recommendation;dynamic programming,,,,,20160727,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7524023,,,,,,IEEE,,,,,IEEE Transactions on Signal Processing,5340,IEEE Journals & Magazines,,,64,,2016,,,,,,,,,,,,,,,,,,,
Reinforcement learning for game personalization on edge devices,"Good progress has been shown recently in the area of active learning, specifically, Reinforcement learning (RL). In this paper, the authors show how RL can be used to personalize games based on user-interaction with the game. The work uses Deep Q network models (DQN) and the open source framework OpenAI to build an RL model that is able to optimize the gamer's engagement level in a game. The authors define an example quantitative measure of gamer engagement and incorporate that into the DQN learning reward function. The gamer experience optimization is empirically demonstrated using a game of Pong. Simulation testing and analysis of results indicate adapted RL models increase engagement reward values, thus enhancing gamer experience. The contribution of this paper is twofold: (1) using RL, it paves the path for wider adaptation to user-behavior, starting with gaming, and (2) it shows analysis and feasibility of an RL algorithm on an edge device (Personal Computer) in real-time.",,,,Intel Corporation,A. Bodas; B. Upadhyay; C. Nadiger; S. Abdelhak,,,,10.1109/INFOCT.2018.8356853,IEEE Xplore,1,,20180510,122,,Adaptation models;Buildings;Games;Learning (artificial intelligence);Mathematical model;Training,computer games;learning (artificial intelligence);mobile computing,DQN learning reward function;RL algorithm;RL model;active learning;deep Q network models;edge device;engagement reward values;game personalization;gamer engagement;gamer experience;open source framework OpenAI;personal computer;reinforcement learning,,,,23-25 March 2018,,2018 International Conference on Information and Computer Technologies (ICICT),artificial intelligence;computer games;edge computing;game personalization;reinforcement learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8356853,,,,,,IEEE,,,Electronic:978-1-5386-5384-5; POD:978-1-5386-5385-2; Paper:978-1-5386-5382-1; USB:978-1-5386-5383-8,,2018 International Conference on Information and Computer Technologies (ICICT),119,IEEE Conferences,,,,,2018,,,,,,,,,,,,,,,,,,,
Towards a learning framework for dancing robots,"How can we make robots learn how to dance? How do humans learn to dance? An emerging culture of dancing robots is becoming more prominent in the research community with more emphasis on how we can show of our own creativity rather than allowing the robots to develop their own cognitive and psychological behaviours to the music being played. There are many different types of music and indeed, many different robots and many ways, in which they can dance to music however, much of the work carried out in this field concern limiting robots to dance in particular ways to a specific music and no adaptive behaviour implemented in them to be able to respond intuitively to music in general. We propose in this paper, a way in which such a problem can begin to be looked into, by introducing fundamental things that should be learnt that are necessary for dancing. We programmed a virtual robot to learn to dance to the beat as well as recognise the downbeat of any time-signature and tailor its movements to the loudness of music, using the Sarsa and the Sarsa(Â¿) algorithms from reinforcement learning as the learning framework. Experimental results show that it is possible to make robots learn to dance to these fundamental rhythmic features of music.",,,,"Computer Science Department and Research School of Informatics, Loughborough University, UK",I. S. Tholley; Q. Meng; P. W. H. Chung,,2,,10.1109/ICCA.2009.5410324,IEEE Xplore,1,,20100208,1586,,Automatic control;Cognitive robotics;Computer science;Humans;Learning;Psychology;Rhythm;Robotics and automation;Robots;Timing,learning systems;robots,Sarsa algorithms;dancing robots;reinforcement learning;time-signature;virtual robot,,1948-3449;19483449,,9-11 Dec. 2009,,2009 IEEE International Conference on Control and Automation,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5410324,,,,,,IEEE,20,,POD:978-1-4244-4706-0,,2009 IEEE International Conference on Control and Automation,1581,IEEE Conferences,,,,,2009,,,,,,,,,,,,,,,,,,,
Experiments on individual strategy updating in iterated snowdrift game under random rematching,"How do people actually play the iterated snowdrift games, particularly under random rematching protocol is far from well explored. Two sets of laboratory experiments on snowdrift game were conducted to investigate human strategy updating rules. Four groups of subjects were modeled by experience-weighted attraction learning theory at individual-level. Three out of the four groups (75%) passed model validation. Substantial heterogeneity is observed among the players who update their strategies in four typical types, whereas rare people behave like belief-based learners even under fixed pairing. Most subjects (63.9%) adopt the reinforcement learning (or alike) rules; but, interestingly, the performance of averaged reinforcement learners suffered. It is observed that two factors seem to benefit players in competition, i.e., the sensitivity to their recent experiences and the overall consideration of forgone payoffs. Moreover, subjects with changing opponents tend to learn faster based on their own recent experience, and display more diverse strategy updating rules than they do with fixed opponent. These findings suggest that most of subjects do apply reinforcement learning alike updating rules even under random rematching, although these rules may not improve their performance. The findings help evolutionary biology researchers to understand sophisticated human behavioral strategies in social dilemmas. © 2015 Elsevier Ltd.",,"Institute of Systems Engineering, College of Management and Economics, Tianjin University, Tianjin, China",,,"Qi H., Ma S., Jia N., Wang G.","Qi, H., Institute of Systems Engineering, College of Management and Economics, Tianjin University, Tianjin, China; Ma, S., Institute of Systems Engineering, College of Management and Economics, Tianjin University, Tianjin, China; Jia, N., Institute of Systems Engineering, College of Management and Economics, Tianjin University, Tianjin, China; Wang, G., Institute of Systems Engineering, College of Management and Economics, Tianjin University, Tianjin, China",2,,10.1016/j.jtbi.2014.12.008,SCOPUS,1,,,,,,,,,,,,,Journal of Theoretical Biology,Evolutionary game theory; Experience-weighted attraction learning; Experimental economics; Matching protocol,,2-s2.0-84922607512,,,,,,12,1,,,,,Scopus,,,Journal of Theoretical Biology,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922607512&doi=10.1016%2fj.jtbi.2014.12.008&partnerID=40&md5=667741f583b8724be5eb3adf098c4c50,,368,,2015,,,,,,,,,,,,,,,,,,,
Reinstated episodic context guides sampling-based decisions for reward,"How does experience inform decisions? In episodic sampling, decisions are guided by a few episodic memories of past choices. This process can yield choice patterns similar to model-free reinforcement learning; however, samples can vary from trial to trial, causing decisions to vary. Here we show that context retrieved during episodic sampling can cause choice behavior to deviate sharply from the predictions of reinforcement learning. Specifically, we show that, when a given memory is sampled, choices (in the present) are influenced by the properties of other decisions made in the same context as the sampled event. This effect is mediated by fMRI measures of context retrieval on each trial, suggesting a mechanism whereby cues trigger retrieval of context, which then triggers retrieval of other decisions from that context. This result establishes a new avenue by which experience can guide choice and, as such, has broad implications for the study of decisions. © 2017 Nature America, Inc., part of Springer Nature. All rights reserved.",,"Neuroscience Institute, Princeton University, Princeton, NJ, United States; Department of Psychology, Princeton University, Princeton, NJ, United States",,,"Bornstein A.M., Norman K.A.","Bornstein, A.M., Neuroscience Institute, Princeton University, Princeton, NJ, United States; Norman, K.A., Neuroscience Institute, Princeton University, Princeton, NJ, United States, Department of Psychology, Princeton University, Princeton, NJ, United States",5,,10.1038/nn.4573,SCOPUS,1,,,,,,,,,,7,,,Nature Neuroscience,,,2-s2.0-85021392099,,,,,,1003,997,,,,,Scopus,,,Nature Neuroscience,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021392099&doi=10.1038%2fnn.4573&partnerID=40&md5=cbb4c23087bc185fa012c8825f2baf87,,20,,2017,,,,,,,,,,,,,,,,,,,
Design and Evaluation of a Self-Learning HTTP Adaptive Video Streaming Client,"HTTP Adaptive Streaming (HAS) is becoming the de facto standard for Over-The-Top (OTT)-based video streaming services such as YouTube and Netflix. By splitting a video into multiple segments of a couple of seconds and encoding each of these at multiple quality levels, HAS allows a video client to dynamically adapt the requested quality during the playout to react to network changes. However, state-of-the-art quality selection heuristics are deterministic and tailored to specific network configurations. Therefore, they are unable to cope with a vast range of highly dynamic network settings. In this letter, a novel Reinforcement Learning (RL)-based HAS client is presented and evaluated. The self-learning HAS client dynamically adapts its behaviour by interacting with the environment to optimize the Quality of Experience (QoE), the quality as perceived by the end-user. The proposed client has been thoroughly evaluated using a network-based simulator and is shown to outperform traditional HAS clients by up to 13% in a mobile network environment.",,,,"Dept. of Inf. Tech., Ghent Univ., Ghent, Belgium",M. Claeys; S. Latre; J. Famaey; F. De Turck,,27,,10.1109/LCOMM.2014.020414.132649,IEEE Xplore,1,,20140416,719,,Adaptive systems;Bandwidth;Bit rate;Convergence;Standards;Streaming media;Video sequences,hypermedia;learning (artificial intelligence);quality of experience;transport protocols;video streaming,OTT media streaming;QoE;mobile network environment;network-based simulator;novel reinforcement learning based HAS client;over-the-top based video streaming services;quality of experience;self-learning HAS client;self-learning HTTP adaptive video streaming client,,1089-7798;10897798,4,April 2014,,IEEE Communications Letters,Streaming media;intelligent agent;learning systems;quality of service,,,,,20140221,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6746772,,,,,,IEEE,11,,,,IEEE Communications Letters,716,IEEE Journals & Magazines,,,18,,2014,,,,,,,,,,,,,,,,,,,
Human EEG uncovers latent generalizable rule structure during learning,"Human cognition is flexible and adaptive, affording the ability to detect and leverage complex structure inherent in the environment and generalize this structure to novel situations. Behavioral studies show that humans impute structure into simple learning problems, even when this tendency affords no behavioral advantage. Here we used electroencephalography to investigate the neural dynamics indicative of such incidental latent structure. Event-related potentials over lateral prefrontal cortex, typically observed for instructed task rules, were stratified according to individual participants' constructed rule sets. Moreover, this individualized latent rule structure could be independently decoded from multielectrode pattern classification. Both neural markers were predictive of participants' ability to subsequently generalize rule structure to new contexts. These EEG dynamics reveal that the human brain spontaneously constructs hierarchically structured representations during learning of simple task rules. © 2014 the authors.",,"Cognitive, Linguistic, and Psychological Sciences, Brown University, Providence, RI 02912, United States; Department of Psychology, University of New Mexico, Albuquerque, NM 87131, United States",,,"Collins A.G.E., Cavanagh J.F., Frank M.J.","Collins, A.G.E., Cognitive, Linguistic, and Psychological Sciences, Brown University, Providence, RI 02912, United States; Cavanagh, J.F., Cognitive, Linguistic, and Psychological Sciences, Brown University, Providence, RI 02912, United States, Department of Psychology, University of New Mexico, Albuquerque, NM 87131, United States; Frank, M.J., Cognitive, Linguistic, and Psychological Sciences, Brown University, Providence, RI 02912, United States",21,,10.1523/JNEUROSCI.3900-13.2014,SCOPUS,1,,,,,,,,,,13,,,Journal of Neuroscience,EEG; Prefrontal cortex; Reinforcement learning; Rules; Task-set,,2-s2.0-84897881519,,,,,,4685,4677,,,,,Scopus,,,Journal of Neuroscience,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897881519&doi=10.1523%2fJNEUROSCI.3900-13.2014&partnerID=40&md5=f000c5ff6973ec9892b793b53bf60707,,34,,2014,,,,,,,,,,,,,,,,,,,
Natural preparation behavior synthesis,"Humans adjust their movements in advance to prepare for the forthcoming action, resulting in efficient and smooth transitions. However, traditional computer animation approaches such as motion graphs simply concatenate a series of actions without taking into account the following one. In this paper, we propose a new method to produce preparation behaviors using reinforcement learning. As an offline process, the system learns the optimal way to approach a target and to prepare for interaction. A scalar value called the level of preparation is introduced, which represents the degree of transition from the initial action to the interacting action. To synthesize the movements of preparation, we propose a customized motion blending scheme based on the level of preparation, which is followed by an optimization framework that adjusts the posture to keep the balance. During runtime, the trained controller drives the character to move to a target with the appropriate level of preparation, resulting in a humanlike behavior. We create scenes in which the character has to move in a complex environment and to interact with objects, such as crawling under and jumping over obstacles while walking. The method is useful not only for computer animation but also for real-time applications such as computer games, in which the characters need to accomplish a series of tasks in a given environment. Copyright © 2013 John Wiley & Sons, Ltd.",,"Northumbria University, United Kingdom; Trinity College Dublin, Ireland; Hong Kong Baptist University, Hong Kong; University of Edinburgh, United Kingdom; University Rennes 2, France",,,"Shum H.P.H., Hoyet L., Ho E.S.L., Komura T., Multon F.","Shum, H.P.H., Northumbria University, United Kingdom; Hoyet, L., Trinity College Dublin, Ireland; Ho, E.S.L., Hong Kong Baptist University, Hong Kong; Komura, T., University of Edinburgh, United Kingdom; Multon, F., University Rennes 2, France",,,10.1002/cav.1546,SCOPUS,1,,,,,,,,,,5-6,,,Computer Animation and Virtual Worlds,Motion blending; Motion synthesis; Posture optimization; Preparation behavior; Reinforcement learning,,2-s2.0-84908543309,,,,,,542,531,,,,,Scopus,,,Computer Animation and Virtual Worlds,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908543309&doi=10.1002%2fcav.1546&partnerID=40&md5=5d313de575efc8dbfeef4e15176ae509,,25,,2014,,,,,,,,,,,,,,,,,,,
Patients with Parkinson's disease learn to control complex systems-an indication for intact implicit cognitive skill learning,"Implicit memory and learning mechanisms are composed of multiple processes and systems. Previous studies demonstrated a basal ganglia involvement in purely cognitive tasks that form stimulus response habits by reinforcement learning such as implicit classification learning. We will test the basal ganglia influence on two cognitive implicit tasks previously described by Berry and Broadbent, the sugar production task and the personal interaction task. Furthermore, we will investigate the relationship between certain aspects of an executive dysfunction and implicit learning. To this end, we have tested 22 Parkinsonian patients and 22 age-matched controls on two implicit cognitive tasks, in which participants learned to control a complex system. They interacted with the system by choosing an input value and obtaining an output that was related in a complex manner to the input. The objective was to reach and maintain a specific target value across trials (dynamic system learning). The two tasks followed the same underlying complex rule but had different surface appearances. Subsequently, participants performed an executive test battery including the Stroop test, verbal fluency and the Wisconsin card sorting test (WCST). The results demonstrate intact implicit learning in patients, despite an executive dysfunction in the Parkinsonian group. They lead to the conclusion that the basal ganglia system affected in Parkinson's disease does not contribute to the implicit acquisition of a new cognitive skill. Furthermore, the Parkinsonian patients were able to reach a specific goal in an implicit learning context despite impaired goal directed behaviour in the WCST, a classic test of executive functions. These results demonstrate a functional independence of implicit cognitive skill learning and certain aspects of executive functions. © 2006 Elsevier Ltd. All rights reserved.",,"Department of Neurology, University Medical Center Schleswig-Holstein, Campus Kiel, Germany",,,"Witt K., Daniels C., Daniel V., Schmitt-Eliassen J., Volkmann J., Deuschl G.","Witt, K., Department of Neurology, University Medical Center Schleswig-Holstein, Campus Kiel, Germany; Daniels, C., Department of Neurology, University Medical Center Schleswig-Holstein, Campus Kiel, Germany; Daniel, V., Department of Neurology, University Medical Center Schleswig-Holstein, Campus Kiel, Germany; Schmitt-Eliassen, J., Department of Neurology, University Medical Center Schleswig-Holstein, Campus Kiel, Germany; Volkmann, J., Department of Neurology, University Medical Center Schleswig-Holstein, Campus Kiel, Germany; Deuschl, G., Department of Neurology, University Medical Center Schleswig-Holstein, Campus Kiel, Germany",16,,10.1016/j.neuropsychologia.2006.04.013,SCOPUS,1,,,,,,,,,,12,,,Neuropsychologia,Cognitive skill learning; Executive dysfunction; Implicit learning; Parkinson's disease,,2-s2.0-33745944539,,,,,,2451,2445,,,,,Scopus,,,Neuropsychologia,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745944539&doi=10.1016%2fj.neuropsychologia.2006.04.013&partnerID=40&md5=c0a0717070a406cc65c19d0e09ac84a5,,44,,2006,,,,,,,,,,,,,,,,,,,
Towards a meta motion planner B: algorithm and applications,"In a companion paper (see Proceedings of ICRA 2001) we developed a framework for rating or comparing navigation packages. For a given environment a navigation package consists of a motion planner and a sensor to be used during navigation. The ability to rate or measure a navigation package is important in order to address issues like sensor customization for an environment and choice of a motion planner in an environment. In this paper we present the algorithm which we use in order to rate a given navigation package. Under the framework which was presented previously, a partially observable Markov decision process is defined. The algorithm searches for an optimal policy to be employed in this decision process. We briefly review the problem and framework, develop the algorithm and present experimental results.",,,,"Dept. of Math., Technion-Israel Inst. of Technol., Haifa, Israel",A. Adam; E. Rivlin; I. Shimshoni,,2,,10.1109/ROBOT.2001.932568,IEEE Xplore,1,,20060418,298 vol.1,,Application software;Computer science;Industrial engineering;Mathematics;Motion measurement;Navigation;Packaging;Paper technology;Path planning;Robot sensing systems,Markov processes;computerised navigation;decision theory;learning (artificial intelligence);path planning;robots;search problems,Markov decision process;meta motion planner;navigation;reinforcement learning;robots;search problem;sensor customization,,1050-4729;10504729,,2001,,Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=932568,,,,,,IEEE,7,,POD:0-7803-6576-3,,Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164),291,IEEE Conferences,,,1,,2001,,,,,,,,,,,,,,,,,,,
Model Learning and Knowledge Sharing for a Multiagent System With Dyna-Q Learning,"In a multiagent system, if agents' experiences could be accessible and assessed between peers for environmental modeling, they can alleviate the burden of exploration for unvisited states or unseen situations so as to accelerate the learning process. Since how to build up an effective and accurate model within a limited time is an important issue, especially for complex environments, this paper introduces a model-based reinforcement learning method based on a tree structure to achieve efficient modeling and less memory consumption. The proposed algorithm tailored a Dyna-Q architecture to multiagent systems by means of a tree structure for modeling. The tree-model built from real experiences is used to generate virtual experiences such that the elapsed time in learning could be reduced. As well, this model is suitable for knowledge sharing. This paper is inspired by the concept of knowledge sharing methods in multiagent systems where an agent could construct a global model from scattered local models held by individual agents. Consequently, it can increase modeling accuracy so as to provide valid simulated experiences for indirect learning at the early stage of learning. To simplify the sharing process, the proposed method applies resampling techniques to grafting partial branches of trees containing required and useful experiences disseminated from experienced peers, instead of merging the whole trees. The simulation results demonstrate that the proposed sharing method can achieve the objectives of sample efficiency and learning acceleration in multiagent cooperation applications.",,,,"Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan",K. S. Hwang; W. C. Jiang; Y. J. Chen,,3,,10.1109/TCYB.2014.2341582,IEEE Xplore,1,,20150413,990,,Decision trees;Mathematical model;Multi-agent systems;Planning;Stochastic processes;Support vector machine classification;Vectors,,,,2168-2267;21682267,5,May 2015,,IEEE Transactions on Cybernetics,Decision tree;Dyna-Q;model sharing;multiagent system,,,,,20140805,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6871355,,,,,,IEEE,20,,,,IEEE Transactions on Cybernetics,978,IEEE Journals & Magazines,,,45,,2015,,,,,,,,,,,,,,,,,,,
Reinforcement learning-based cooperative sensing in cognitive radio ad hoc networks,"In cognitive radio networks, spectrum sensing is a fundamental function for detecting the presence of primary users in licensed frequency bands. Due to multipath fading and shadowing, the performance of detection may be considerably compromised. To improve the detection probability, cooperative sensing is an effective approach for secondary users to cooperate and combat channel impairments. This approach, however, incurs overhead such as sensing delay for reporting local decisions and the increase of control traffic in the network. In this paper, a reinforcement learning-based cooperative sensing method is proposed to address the cooperation overhead problem. By using the proposed cooperative sensing model, the secondary user learns to (i) find the optimal set of cooperating neighbors with minimum control traffic, (ii) minimize the overall cooperative sensing delay, (iii) select independent users for cooperation under correlated shadowing, and (iv) improve the energy efficiency for cooperative sensing. The simulation results show that the proposed reinforcement learning-based cooperative sensing method reduces the overhead of cooperative sensing while effectively improving the detection performance to combat correlated shadowing.",,,,"Broadband Wireless Networking Laboratory, School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, 30332, USA",B. F. Lo; I. F. Akyildiz,,11,,10.1109/PIMRC.2010.5671686,IEEE Xplore,1,,20101217,2249,,Correlation;Delay;Fading;Learning;Markov processes;Sensors;Shadow mapping,ad hoc networks;cognitive radio;cooperative systems;learning (artificial intelligence);telecommunication computing,ad hoc networks;cognitive radio networks;control traffic;cooperative sensing delay;detection performance;multipath fading;reinforcement learning-based cooperative sensing;reinforcement learning-based cooperative sensing method;shadowing;spectrum sensing,,2166-9570;21669570,,26-30 Sept. 2010,,"21st Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5671686,,,,,,IEEE,12,,Electronic:978-1-4244-8016-6; POD:978-1-4244-8017-3,,"21st Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications",2244,IEEE Conferences,,,,,2010,,,,,,,,,,,,,,,,,,,
Cooperative retransmissions using Markov decision process with reinforcement learning,"In cooperative retransmissions, nodes with better channel qualities help other nodes in retransmitting a failed packet to its intended destination. In this paper, we propose a cooperative retransmission scheme where each node makes local decision to cooperate or not to cooperate at what transmission power using a Markov decision process with reinforcement learning. With the reinforcement learning, the proposed scheme avoids solving an Markov decision process with a large number of states. Through simulations, we show that the proposed scheme is robust to collisions, is scalable with regard to the network size, and can provide significant cooperative diversity.",,,,"Institute for Infocomm Research, Agency for Science, Technology & Research (A&#191;STAR), 1 Fusionopolis Way, #21-01 Connexis South Tower, 138632 Singapore",G. N. Shirazi; P. Y. Kong; C. K. Tham,,0,,10.1109/PIMRC.2009.5450098,IEEE Xplore,1,,20100415,656,,Automatic repeat request;Bismuth;Dynamic programming;Learning;Poles and towers;Relays;Robustness;Throughput;Transmitters;Wireless networks,Markov processes;diversity reception;wireless channels,Markov decision process;cooperative diversity;cooperative retransmission;reinforcement learning,,2166-9570;21669570,,13-16 Sept. 2009,,"2009 IEEE 20th International Symposium on Personal, Indoor and Mobile Radio Communications",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5450098,,,,,,IEEE,11,,Electronic:978-1-4244-5123-4; POD:978-1-4244-5122-7,,"2009 IEEE 20th International Symposium on Personal, Indoor and Mobile Radio Communications",652,IEEE Conferences,,,,,2009,,,,,,,,,,,,,,,,,,,
Dynamic information retrieval modeling,"In Dynamic Information Retrieval modeling we model dy-namic systems which change or adapt over time or a se-quence of events using a range of techniques from artificial intelligence and reinforcement learning. Many of the open problems in current IR research can be described as dynamic systems, for instance, session search or computational adver-tising. State of the art research provides solutions to these problems that are responsive to a changing environment, learn from past interactions and predict future utility. Ad-vances in IR interface, personalization and ad display de-mand models that can react to users in real time and in an intelligent, contextual way. The objective of this half-day tutorial is to provide a comprehensive and up-to-date introduction to Dynamic In-formation Retrieval Modeling. We motivate a conceptual model linking static, interactive and dynamic retrieval and use this to define dynamics within the context of IR. We then cover a number of algorithms and techniques from the artificial intelligence (AI) and online learning literature such as Markov Decision Processes (MDP) [1], their partially ob-servable variation (POMDP) [5] and multi-armed bandits [7]. Following this we describe how to identify dynamics in an IR problem and demonstrate how to model them using the described techniques. The remainder of the tutorial will then cover an array of state-of-the-art research on dynamic systems in IR and how they can be modeled using using dynamic IR [2, 6]. We use research on session search [3], multi-page search [4] and online advertising [8] as in-depth examples of such work. This tutorial is of relevance to IR practitioners and re-searchers, where we will present the merits of dynamic infor-mation retrieval modeling and introduce the relevant tech-niques. The content will be of particular interest to re-searchers working in the areas of statistical modeling, per-sonalization and recommendation, and is also relevant to practitioners in Web search, online advertising and anyone who works with big data. After this tutorial, attendees will: • Be able to identify the dynamics in an IR system •Be able to model these dynamics using techniques from AI and reinforcement learning • Have knowledge of the state-of-the-art research in dy-namic information retrieval modeling. Copyright © 2015 ACM.",,"Georgetown University, United Kingdom; University College London, United Kingdom",,,"Yang H., Sloan M., Wang J.","Yang, H., Georgetown University, United Kingdom; Sloan, M., University College London, United Kingdom; Wang, J., University College London, United Kingdom",3,,10.1145/2684822.2697038,SCOPUS,1,,,,,,,,,,,,,WSDM 2015 - Proceedings of the 8th ACM International Conference on Web Search and Data Mining,Dynamic information retrieval modeling; Probabilistic rel-evance model; Reinforcement learning,,2-s2.0-84928710591,,,,,,410,409,,,,,Scopus,,,WSDM 2015 - Proceedings of the 8th ACM International Conference on Web Search and Data Mining,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928710591&doi=10.1145%2f2684822.2697038&partnerID=40&md5=b9991c18a43a17193ce69d06e791c428,,,,2015,,,,,,,,,,,,,,,,,,,
Reinforcement Learning for Active Queue Management in Mobile All-IP Networks,"In future all-IP based wireless networks, like the envisaged in the long term evolution (LTE) architectures for future systems, network providers will have to deal with large traffic volumes with different QoS requirements. In order to increase exploitation of network resources wisely, intelligent adaptive solutions for class based traffic regulation are needed. In particular, active queue management (AQM) is regarded as one of these solutions to provide low queuing delay and high throughput to flows by smart packet discarding. In this paper, we propose a novel AQM solution for future all-IP networks based on a reinforcement learning scheme that allows controlling both the queuing delay and the packet loss of the different service classes. The proposed approach is evaluated through simulations and compared against other algorithms used in the literature, like the random early detection (RED) and the drop from tail (DFT), confirming the benefits of the proposed algorithm.",,,,"Dept. TSC, Universitat Polit&#232;cnica de Catalunya (UPC), Barcelona, Spain",N. Vucevic; J. Perez-Romero; O. Sallent; R. Agusti,,2,,10.1109/PIMRC.2007.4394713,IEEE Xplore,1,,20071204,5,,Bandwidth;Delay;Diffserv networks;Intelligent networks;Learning;Mobile communication;Quality of service;Tail;Telecommunication traffic;Traffic control,IP networks;learning (artificial intelligence);mobile computing;mobility management (mobile radio);queueing theory;telecommunication traffic,AQM;QoS;active queue management;all-IP based wireless networks;class based traffic regulation;drop from tail;intelligent adaptive solutions;mobile all-IP networks;network resources exploitation;queuing delay;random early detection;reinforcement learning,,2166-9570;21669570,,3-7 Sept. 2007,,"2007 IEEE 18th International Symposium on Personal, Indoor and Mobile Radio Communications",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4394713,,,,,,IEEE,15,,CD-ROM:978-1-4244-1144-3; POD:978-1-4244-1143-6,,"2007 IEEE 18th International Symposium on Personal, Indoor and Mobile Radio Communications",1,IEEE Conferences,,,,,2007,,,,,,,,,,,,,,,,,,,
Human-inspired computational fairness,"In many common tasks for multi-agent systems, assuming individually rational agents leads to inferior solutions. Numerous researchers found that fairness needs to be considered in addition to individual reward, and proposed valuable computational models of fairness. In this paper, we argue that there are two opportunities for improvement. First, existing models are not specifically tailored to addressing a class of tasks named social dilemmas, even though such tasks are quite common in the context of multi-agent systems. Second, the models generally rely on the assumption that all agents will and can adhere to these models, which is not always the case. We therefore present a novel computational model, i.e., human-inspired computational fairness. Upon being confronted with social dilemmas, humans may apply a number of fully decentralized sanctioning mechanisms to ensure that optimal, fair solutions emerge, even though some participants may be deciding purely on the basis of individual reward. In this paper, we show how these human mechanisms may be computationally modelled, such that fair and optimal solutions emerge from agents being confronted with social dilemmas. © 2010 The Author(s).",,"Computational Modelling Lab, Vrije Universiteit Brussel, Pleinlaan 2, 1050 Brussels, Belgium; Department of Knowledge Engineering, Maastricht University, PO Box 616, 6200 MD Maastricht, Netherlands",,,"de Jong S., Tuyls K.","de Jong, S., Computational Modelling Lab, Vrije Universiteit Brussel, Pleinlaan 2, 1050 Brussels, Belgium, Department of Knowledge Engineering, Maastricht University, PO Box 616, 6200 MD Maastricht, Netherlands; Tuyls, K., Department of Knowledge Engineering, Maastricht University, PO Box 616, 6200 MD Maastricht, Netherlands",11,,10.1007/s10458-010-9122-9,SCOPUS,1,,,,,,,,,,1,,,Autonomous Agents and Multi-Agent Systems,Fairness; Human-inspired mechanisms; Multi-agent systems; Reinforcement learning; Social dilemmas,,2-s2.0-78651242484,,,,,,126,103,,,,,Scopus,,,Autonomous Agents and Multi-Agent Systems,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651242484&doi=10.1007%2fs10458-010-9122-9&partnerID=40&md5=6377a01ddb7d901ab98beeefaa7b1365,,22,,2011,,,,,,,,,,,,,,,,,,,
Policy search for learning robot control using sparse data,"In many complex robot applications, such as grasping and manipulation, it is difficult to program desired task solutions beforehand, as robots are within an uncertain and dynamic environment. In such cases, learning tasks from experience can be a useful alternative. To obtain a sound learning and generalization performance, machine learning, especially, reinforcement learning, usually requires sufficient data. However, in cases where only little data is available for learning, due to system constraints and practical issues, reinforcement learning can act suboptimally. In this paper, we investigate how model-based reinforcement learning, in particular the probabilistic inference for learning control method (Pilco), can be tailored to cope with the case of sparse data to speed up learning. The basic idea is to include further prior knowledge into the learning process. As Pilco is built on the probabilistic Gaussian processes framework, additional system knowledge can be incorporated by defining appropriate prior distributions, e.g. a linear mean Gaussian prior. The resulting Pilco formulation remains in closed form and analytically tractable. The proposed approach is evaluated in simulation as well as on a physical robot, the Festo Robotino XT. For the robot evaluation, we employ the approach for learning an object pick-up task. The results show that by including prior knowledge, policy learning can be sped up in presence of sparse data.",,,,"Cognitive Systems, Bosch Corporate Research, Germany",B. Bischoff; D. Nguyen-Tuong; H. van Hoof; A. McHutchon; C. E. Rasmussen; A. Knoll; J. Peters; M. P. Deisenroth,,3,,10.1109/ICRA.2014.6907422,IEEE Xplore,1,,20140929,3887,,Computational modeling;Data models;Grasping;Heuristic algorithms;Pneumatic systems;Robots;Valves,Gaussian distribution;Gaussian processes;learning (artificial intelligence);learning systems;probability;robots,Festo Robotino XT;PILCO;dynamic environment;generalization performance;grasping;learning robot control;linear mean Gaussian prior distribution;machine learning;manipulation;model-based reinforcement learning;object pick-up task;physical robot;policy learning;policy search;probabilistic Gaussian processes framework;probabilistic inference for learning control method;sound learning;sparse data;system constraints;uncertain environment,,1050-4729;10504729,,May 31 2014-June 7 2014,,2014 IEEE International Conference on Robotics and Automation (ICRA),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6907422,,,,,,IEEE,14,,Electronic:978-1-4799-3685-4; POD:978-1-4799-3686-1; USB:978-1-4799-3684-7,,2014 IEEE International Conference on Robotics and Automation (ICRA),3882,IEEE Conferences,,,,,2014,,,,,,,,,,,,,,,,,,,
Multi-agent reward analysis for learning in noisy domains,"In many multi-agent learning problems, it is difficult to determine, a priori, the agent reward structure that will lead to good performance. This problem is particularly pronounced in continuous, noisy domains ill-suited to simple table backup schemes commonly used in TD(λ)/Q-learning. In this paper, we present a new reward evaluation method that provides a visualization of the tradeoff between coordination among the agents and the difficulty of the learning problem each agent faces. This method is independent of the learning algorithm and is only a function of the problem domain and the agents' reward structure. We then use this reward property visualization method to determine an effective reward without performing extensive simulations. We test this method in both a static and a dynamic multi-rover learning domain where the agents have continuous state spaces and where their actions are noisy (e.g., the agents' movement decisions are not always carried out properly). Our results show that in the more difficult dynamic domain, the reward efficiency visualization method provides a two order of magnitude speedup in selecting a good reward. Most importantly it allows one to quickly create and verify rewards tailored to the observational limitations of the domain. Copyright 2005 ACM.",,"UC Santa Cruz, NASA Ames Research Center, Mailstop 269-3, Moffett Field, CA 94035, United States; NASA Ames Research Center, Mailstop 269-4, Moffett Field, CA 94035, United States",,,"Agogino A., Tumer K.","Agogino, A., UC Santa Cruz, NASA Ames Research Center, Mailstop 269-3, Moffett Field, CA 94035, United States; Tumer, K., NASA Ames Research Center, Mailstop 269-4, Moffett Field, CA 94035, United States",1,,,SCOPUS,0,,,,,,,,,,,,,Proceedings of the International Conference on Autonomous Agents,Multiagent Systems; Reinforcement Learning; Visualization,,2-s2.0-33644813420,,,,,,240,233,,,,,Scopus,,,Proceedings of the International Conference on Autonomous Agents,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33644813420&partnerID=40&md5=8dd6ac100b9f6688afbab875b55b06cc,,,,2005,,,,,,,,,,,,,,,,,,,
Disambiguating entity references within an ontological model,"In our everyday conversations, entities (persons, companies, etc.) are referred to by natural language identifiers (NLIs). Humans employ personal experience and situational context to interpret such identifiers. However, due to ambiguity, even humans run the risk of misinterpretations. In our prior work, we presented a novel method to resolve entity references in texts under the aspect of ambiguity. We explore ontological background knowledge represented in an RDF(S) graph. The different interpretation possibilities lead to different subgraphs of the underlying ontology, each subgraph describing one consistent, non-ambiguous interpretation of the ambiguous NLIs within the ontological knowledge base. Our domain-independent approach is based on spreading activation and uses a semantic relational ranking. In this paper, we suggest three extensions to our original algorithm. First, we process in a two-step interpretation - -instead of the whole original input text - -at first hand smaller text windows in order to get more precise reference interpretations through a smaller local text context. Second, we extend the spreading-activation algorithm within the RDF(S) graph towards a bidirectional exploration of edges which shall speed-up the algorithm. Third we use reinforcement learning in order to take advantage of re-occurring information. We present first experimental results with these algorithmic extensions and derive directions for future work. © 2011 ACM.",,"FZI, Research Institute for Information Technology, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany",,,"Kleb J., Abecker A.","Kleb, J., FZI, Research Institute for Information Technology, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany; Abecker, A., FZI, Research Institute for Information Technology, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany",,,10.1145/1988688.1988714,SCOPUS,1,,,,,,,,,,,,,ACM International Conference Proceeding Series,graph; named entity; spreading activation,,2-s2.0-79960587711,,,,,,,,,,,,Scopus,,,ACM International Conference Proceeding Series,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960587711&doi=10.1145%2f1988688.1988714&partnerID=40&md5=bbe8b01aaafdf3f880b75ab16cdbda79,,,,2011,,,,,,,,,,,,,,,,,,,
Impedance Control of Robot Manipulator in Contact Task Using Machine Learning,"In performing contact tasks using robot manipulators, force control is essential. One approach is to select appropriate stiffness ellipse at the endpoint of the manipulator, where stiffness ellipse is a geometrical shape of force element represented in the principal axis of task space. In this study, we introduce a novel method to tailor stiffness ellipse required to perform contact tasks by using associative search network. Using appropriate performance indexes in the open-door task experiment, we acquired stiffness ellipse trajectory which optimizes dynamic movement of manipulator. Derived stiffness ellipse (or impedance in general) through learning process can be used for the similar task of learning process",,,,"Department of Mechanical Engineering, Korea University, Seoul, Korea, Tel : +82-2-3290-3868; E-mail: biomimetic@korea.ac.kr",B. Kim; S. Park,,2,,10.1109/SICE.2006.314795,IEEE Xplore,1,,20070226,2594,,Force control;Humans;Impedance;Machine learning;Manipulators;Mechanical engineering;Orbital robotics;Performance analysis;Robot control;Shape,force control;learning (artificial intelligence);manipulators,associative search network;force control;impedance control;machine learning;robot manipulator;stiffness ellipse,,,,18-21 Oct. 2006,,2006 SICE-ICASE International Joint Conference,Associative Search Network;Contact task;impedance control;reinforcement learning;stiffness ellipse,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4108082,,,,,,IEEE,14,,CD-ROM:89-950038-5-5; POD:89-950038-4-7,,2006 SICE-ICASE International Joint Conference,2590,IEEE Conferences,,,,,2006,,,,,,,,,,,,,,,,,,,
Making better recommendations with online profiling agents,"In recent years, we have witnessed the success of autonomous agents applying machine learning techniques across a wide range of applications. However, agents applying the same machine learning techniques in online applications have not been so successful. Even agent-based hybrid recommender systems that combine information filtering techniques with collaborative filtering techniques have only been applied with considerable success to simple consumer goods such as movies, books, clothing and food. Complex, adaptive autonomous agent systems that can handle complex goods such as real estate, vacation plans, insurance, mutual funds, and mortgage have yet emerged. To a large extent, the reinforcement learning methods developed to aid agents in learning have been more successfully deployed in offline applications. The inherent limitations in these methods have rendered them somewhat ineffective in online applications. In this paper, we postulate that a small amount of prior knowledge and human-provided input can dramatically speed up online learning. We will demonstrate that our agent HumanE - with its prior knowledge or ""experiences"" about the real estate domain -can effectively assist users in identifying requirements, especially unstated ones, quickly and unobtrusively.",,"School of Computing, National University of Singapore, 3 Science Drive 2, Singapore 117543, Singapore",,,"Oh D., Tan C.L.","Oh, D., School of Computing, National University of Singapore, 3 Science Drive 2, Singapore 117543, Singapore; Tan, C.L., School of Computing, National University of Singapore, 3 Science Drive 2, Singapore 117543, Singapore",,,,SCOPUS,0,,,,,,,,,,,,,Proceedings of the National Conference on Artificial Intelligence,Electronic profiling; Experience; Inference; Intelligent agents; Interactive learning; Personalization; Reinforcement learning; User preferences,,2-s2.0-9444285517,,,,,,792,785,,,,,Scopus,,,Proceedings of the National Conference on Artificial Intelligence,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-9444285517&partnerID=40&md5=52bdd57d2094350dc0f993039c1ff5df,,,,2004,,,,,,,,,,,,,,,,,,,
Shaping proto-value functions using rewards,"In reinforcement learning (RL), an important sub-problem is learning the value function, which is chiefly influenced by the architecture used to represent value functions. is often expressed as a linear combination of a pre-selected set of basis functions. These basis functions are either selected in an ad-hoc manner or are tailored to the RL task using the domain knowledge. Selecting basis functions in an ad-hoc manner does not give a good approximation of value function while choosing functions using domain knowledge introduces dependency on the task. Thus, a desirable scenario is to have a method to choose basis functions that are task independent, but which also provide a good approximation for the value function. In this paper, we propose a novel task-independent basis function construction method that uses the topology of the underlying state space and the reward structure to build the reward-based Proto Value Functions (RPVFs). The approach we propose gives good approximation for the value function and enhanced learning performance. The performance is demonstrated via experiments on grid-world tasks. © 2016 The Authors and IOS Press.",,"Dept. of Computer Science and Automation, Indian Institute of Science, Bangalore, India",,,"Maity R.K., Lakshminarayanan C., Padakandla S., Bhatnagar S.","Maity, R.K., Dept. of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Lakshminarayanan, C., Dept. of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Padakandla, S., Dept. of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Bhatnagar, S., Dept. of Computer Science and Automation, Indian Institute of Science, Bangalore, India",,,10.3233/978-1-61499-672-9-1690,SCOPUS,1,,,,,,,,,,,,,Frontiers in Artificial Intelligence and Applications,,,2-s2.0-85013127793,,,,,,1691,1690,,,,,Scopus,,,Frontiers in Artificial Intelligence and Applications,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013127793&doi=10.3233%2f978-1-61499-672-9-1690&partnerID=40&md5=3722c7c017b2866e389de907804ab4af,,285,,2016,,,,,,,,,,,,,,,,,,,
Q-LDA: Uncovering latent patterns in text-based sequential decision processes,"In sequential decision making, it is often important and useful for end users to understand the underlying patterns or causes that lead to the corresponding decisions. However, typical deep reinforcement learning algorithms seldom provide such information due to their black-box nature. In this paper, we present a probabilistic model, Q-LDA, to uncover latent patterns in text-based sequential decision processes. The model can be understood as a variant of latent topic models that are tailored to maximize total rewards; we further draw an interesting connection between an approximate maximum-likelihood estimation of Q-LDA and the celebrated Q-learning algorithm. We demonstrate in the text-game domain that our proposed method not only provides a viable mechanism to uncover latent patterns in decision processes, but also obtains state-of-the-art rewards in these games. © 2017 Neural information processing systems foundation. All rights reserved.",,"Microsoft Research, Redmond, WA, United States; Google Inc., Kirkland, WA, United States; Citadel LLC, Seattle/Chicago, United States",,,"Chen J., Wang C., Xiao L., He J., Li L., Deng L.","Chen, J., Microsoft Research, Redmond, WA, United States; Wang, C., Google Inc., Kirkland, WA, United States; Xiao, L., Microsoft Research, Redmond, WA, United States; He, J., Citadel LLC, Seattle/Chicago, United States; Li, L., Google Inc., Kirkland, WA, United States; Deng, L., Citadel LLC, Seattle/Chicago, United States",,,,SCOPUS,0,,,,,,,,,,,,,Advances in Neural Information Processing Systems,,,2-s2.0-85047005909,,,,,,4987,4978,,,,,Scopus,,,Advances in Neural Information Processing Systems,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047005909&partnerID=40&md5=f243dfada6ff6acc567438af82c954c9,,2017-December,,2017,,,,,,,,,,,,,,,,,,,
Optimistic bayesian sampling in contextual-bandit problems,"In sequential decision problems in an unknown environment, the decision maker often faces a dilemma over whether to explore to discover more about the environment, or to exploit current knowledge. We address the exploration-exploitation dilemma in a general setting encompassing both standard and contextualised bandit problems. The contextual bandit problem has recently resurfaced in attempts to maximise click-through rates in web based applications, a task with significant commercial interest. In this article we consider an approach of Thompson (1933) which makes use of samples from the posterior distributions for the instantaneous value of each action. We extend the approach by introducing a new algorithm, Optimistic Bayesian Sampling (OBS), in which the probability of playing an action increases with the uncertainty in the estimate of the action value. This results in better directed exploratory behaviour. We prove that, under unrestrictive assumptions, both approaches result in optimal behaviour with respect to the average reward criterion of Yang and Zhu (2002). We implement OBS and measure its performance in simulated Bernoulli bandit and linear regression domains, and also when tested with the task of personalised news article recommendation on a Yahoo! Front Page Today Module data set. We find that OBS performs competitively when compared to recently proposed benchmark algorithms and outperforms Thompson's method throughout. © 2012 Benedict C. May, Nathan Korda, Anthony Lee and David S. Leslie.",,"School of Mathematics, University of Bristol, Bristol, BS8 1TW, United Kingdom; Oxford-Man Institute, University of Oxford, Walton Well Road, Oxford, OX2 6ED, United Kingdom",,,"May B.C., Korda N., Lee A., Leslie D.S.","May, B.C., School of Mathematics, University of Bristol, Bristol, BS8 1TW, United Kingdom; Korda, N., Oxford-Man Institute, University of Oxford, Walton Well Road, Oxford, OX2 6ED, United Kingdom; Lee, A., Oxford-Man Institute, University of Oxford, Walton Well Road, Oxford, OX2 6ED, United Kingdom; Leslie, D.S., School of Mathematics, University of Bristol, Bristol, BS8 1TW, United Kingdom",40,,,SCOPUS,0,,,,,,,,,,,,,Journal of Machine Learning Research,Contextual bandits; Exploration-exploitation; Multi-armed bandits; Sequential allocation; Thompson sampling,,2-s2.0-84864939787,,,,,,2106,2069,,,,,Scopus,,,Journal of Machine Learning Research,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864939787&partnerID=40&md5=7096900388c4f8aba3fc7a7151cd9980,,13,,2012,,,,,,,,,,,,,,,,,,,
Sequential approaches for learning datum-wise sparse representations,"In supervised classification, data representation is usually considered at the dataset level: one looks for the ""best"" representation of data assuming it to be the same for all the data in the data space. We propose a different approach where the representations used for classification are tailored to each datum in the data space. One immediate goal is to obtain sparse datum-wise representations: our approach learns to build a representation specific to each datum that contains only a small subset of the features, thus allowing classification to be fast and efficient. This representation is obtained by way of a sequential decision process that sequentially chooses which features to acquire before classifying a particular point; this process is learned through algorithms based on Reinforcement Learning. The proposed method performs well on an ensemble of medium-sized sparse classification problems. It offers an alternative to global sparsity approaches, and is a natural framework for sequential classification problems. The method extends easily to a whole family of sparsity-related problem which would otherwise require developing specific solutions. This is the case in particular for cost-sensitive and limited-budget classification, where feature acquisition is costly and is often performed sequentially. Finally, our approach can handle non-differentiable loss functions or combinatorial optimization encountered in more complex feature selection problems. © 2012 The Author(s).",,"UPMC, LIP6, Université Pierre et Marie Curie, 4 Place Jussieu Case 169, Paris 75005, France; LIFL (UMR CNRS) and INRIA Lille Nord-Europe, Université de Lille, Villeneuve d'Ascq, France",,,"Dulac-Arnold G., Denoyer L., Preux P., Gallinari P.","Dulac-Arnold, G., UPMC, LIP6, Université Pierre et Marie Curie, 4 Place Jussieu Case 169, Paris 75005, France; Denoyer, L., UPMC, LIP6, Université Pierre et Marie Curie, 4 Place Jussieu Case 169, Paris 75005, France; Preux, P., LIFL (UMR CNRS) and INRIA Lille Nord-Europe, Université de Lille, Villeneuve d'Ascq, France; Gallinari, P., UPMC, LIP6, Université Pierre et Marie Curie, 4 Place Jussieu Case 169, Paris 75005, France",6,,10.1007/s10994-012-5306-7,SCOPUS,1,,,,,,,,,,1-2,,,Machine Learning,Classification; Features selection; Reinforcement learning; Sequential models; Sparsity,,2-s2.0-84865273017,,,,,,122,87,,,,,Scopus,,,Machine Learning,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865273017&doi=10.1007%2fs10994-012-5306-7&partnerID=40&md5=711ea68e179921d4c90e65895e356bde,,89,,2012,,,,,,,,,,,,,,,,,,,
Automated negotiation in multiple e-Marketplaces by using Learning Personalized Mobile Shopping Agents,"In the B2C- or C2C- based e-Marketplaces, the same merchandise is sold by different sellers in different e-Marketplaces. The trading platforms nowadays has resulted in information explosion that makes buyers unable to retrieve and analyze entire merchandise information easily and, therefore, decreases their negotiation power. Moreover, without the records of buyers ' transaction preference, it 's not easy for most agent systems to help common buyers to increase their negotiation power. In this paper, we propose the Learning Personalized Mobile Shopping Agent (LPMSA) and apply it to three e-Marketplace architectures: alliance, broker, noncooperation. The buyer can dispatch mobile agents to multiple e-Marketplaces for collecting merchandise information, negotiating with sellers, and buying merchandise from the above architectures. Furthermore, the agent can learn more about buyer's preference. As a result, the proposed architecture can not only find suitable trading partners for buyers but also help them to get better deals.",,"Department of Information Management, Chaoyang University of Technology, 168, GiFeng East Road, Wufeng, Taichung County, Taiwan",,,"Lee F.-M., Li L.-H., Liu Y.-C.","Lee, F.-M., Department of Information Management, Chaoyang University of Technology, 168, GiFeng East Road, Wufeng, Taichung County, Taiwan; Li, L.-H., Department of Information Management, Chaoyang University of Technology, 168, GiFeng East Road, Wufeng, Taichung County, Taiwan; Liu, Y.-C., Department of Information Management, Chaoyang University of Technology, 168, GiFeng East Road, Wufeng, Taichung County, Taiwan",,,,SCOPUS,0,,,,,,,,,,,,,"Proceedings of the International Conference on Artificial Intelligence, IC-AI'04",Automated Negotiation; E-Marketplace; Mobile Agent; Reinforcement Learning,,2-s2.0-12744281483,,,,,,783,777,,,,,Scopus,,,"Proceedings of the International Conference on Artificial Intelligence, IC-AI'04",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12744281483&partnerID=40&md5=d315d9774254985d85ee436efabd63d8,,2,,2004,,,,,,,,,,,,,,,,,,,
Personalised Human-Robot Co-Adaptation in Instructional Settings using Reinforcement Learning,"In the domain of robotic tutors, personalised tutoring has started to receive scientists' attention, but is still relatively underexplored. Previous work using reinforcement learning (RL) has addressed personalised tutoring from the perspective of affective policy learning. In …",,,,,,,1,,,Google Scholar,0,,,,,,,,,,,,,,,,,,,,"http://scholar.google.com/scholar?cluster=14209507154038331625&hl=en&as_sdt=0,5",,,,,,,,,,,,,,http://www.diva-portal.org/smash/get/diva2:1162389/FULLTEXT01.pdf,,,,2017,,,1.42095071540383E+019,,,,,,,,,3,,,http://www.diva-portal.org/smash/get/diva2:1162389/FULLTEXT01.pdf,,,"http://scholar.google.com/scholar?cites=14209507154038331625&as_sdt=2005&sciodt=0,5&hl=en",
Multi-agent quality of experience control,"In the framework of the Future Internet, the aim of the Quality of Experience (QoE) Control functionalities is to track the personalized desired QoE level of the applications. The paper proposes to perform such a task by dynamically selecting the most appropriate Classes of Service (among the ones supported by the network), this selection being driven by a novel heuristic Multi-Agent Reinforcement Learning (MARL) algorithm. The paper shows that such an approach offers the opportunity to cope with some practical implementation problems: in particular, it allows to face the so-called “curse of dimensionality” of MARL algorithms, thus achieving satisfactory performance results even in the presence of several hundreds of Agents. © 2017, Institute of Control, Robotics and Systems and The Korean Institute of Electrical Engineers and Springer-Verlag Berlin Heidelberg.",,"Department of Computer, Control and Management Engineering “Antonio Ruberti”, University of Rome “La Sapienza”, via Ariosto 25, Rome, Italy; eCampus University, via Isimbardi 10, Novedrate, Italy",,,"Delli Priscoli F., Di Giorgio A., Lisi F., Monaco S., Pietrabissa A., Celsi L.R., Suraci V.","Delli Priscoli, F., Department of Computer, Control and Management Engineering “Antonio Ruberti”, University of Rome “La Sapienza”, via Ariosto 25, Rome, Italy; Di Giorgio, A., Department of Computer, Control and Management Engineering “Antonio Ruberti”, University of Rome “La Sapienza”, via Ariosto 25, Rome, Italy; Lisi, F., Department of Computer, Control and Management Engineering “Antonio Ruberti”, University of Rome “La Sapienza”, via Ariosto 25, Rome, Italy; Monaco, S., Department of Computer, Control and Management Engineering “Antonio Ruberti”, University of Rome “La Sapienza”, via Ariosto 25, Rome, Italy; Pietrabissa, A., Department of Computer, Control and Management Engineering “Antonio Ruberti”, University of Rome “La Sapienza”, via Ariosto 25, Rome, Italy; Celsi, L.R., Department of Computer, Control and Management Engineering “Antonio Ruberti”, University of Rome “La Sapienza”, via Ariosto 25, Rome, Italy; Suraci, V., eCampus University, via Isimbardi 10, Novedrate, Italy",5,,10.1007/s12555-015-0465-5,SCOPUS,1,,,,,,,,,,2,,,"International Journal of Control, Automation and Systems",Future internet; multi-agent reinforcement learning; quality of experience; quality of service,,2-s2.0-85009932576,,,,,,904,892,,,,,Scopus,,,"International Journal of Control, Automation and Systems",,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009932576&doi=10.1007%2fs12555-015-0465-5&partnerID=40&md5=17ab2471409262bc448290a2494ab7e4,,15,,2017,,,,,,,,,,,,,,,,,,,
"Erratum to: MRL-SCSO: Multi-agent Reinforcement Learning-Based Self-Configuration and Self-Optimization Protocol for Unattended Wireless Sensor Networks (Wireless Personal Communications, (2017), 96, 4, (5061-5079), 10.1007/s11277-016-3729-3)","In the original publication, the variable Er in Eq. (5) should read Er and the three levels mentioned in the text between Eqs. (8) and (9) should have read ‘α, β, and Γ’ instead of ‘α, β, and γ.’. © 2017, Springer Science+Business Media New York.",,"Mobile Ad-hoc and Sensor Network Lab, TIFAC-CORE in Pervasive Computing Technologies, Velammal Engineering College, Chennai, India; Department of Computer Science and Engineering, Sri Krishna College of Engineering and Technology, Coimbatore, India",,,"Pravin Renold A., Chandrakala S.","Pravin Renold, A., Mobile Ad-hoc and Sensor Network Lab, TIFAC-CORE in Pervasive Computing Technologies, Velammal Engineering College, Chennai, India; Chandrakala, S., Department of Computer Science and Engineering, Sri Krishna College of Engineering and Technology, Coimbatore, India",,,10.1007/s11277-016-3832-5,SCOPUS,1,,,,,,,,,,4,,,Wireless Personal Communications,,,2-s2.0-84992349001,,,,,,,5081,,,,,Scopus,,,Wireless Personal Communications,,Erratum,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992349001&doi=10.1007%2fs11277-016-3832-5&partnerID=40&md5=d0b9e0407f8fd1fbae0768b468384dea,,96,,2017,,,,,,,,,,,,,,,,,,,
Recency gets larger as lesions move from anterior to posterior locations within the ventromedial prefrontal cortex,"In the past two decades neuroimaging research has substantiated the important role of the prefrontal cortex (PFC) in decision-making. In the current study, we use the complementary lesion based approach to deepen our knowledge concerning the specific cognitive mechanisms modulated by prefrontal activity. Specifically, we assessed the brain substrates implicated in two decision making dimensions in a sample of prefrontal cortex patients: (a) the tendency to differently weigh recent compared to past experience; and (b) the tendency to differently weigh gains compared to losses. The participants performed the Iowa Gambling Task, a complex experience-based decision-making task [3], which was analyzed with a formal cognitive model (the Expectancy-Valance model; [12]). The results indicated that decisions become influenced by more recent, as opposed to older, events when the damage reaches the posterior sectors of the ventromedial prefrontal cortex (VMPC). Furthermore, the degree of this recency deficit was related to the size of the lesion. These results suggest that the posterior area of the prefrontal cortex directly modulates the capacity to use time-delayed information. In contrast, we did not find similar modulation for the sensitivity to gains versus losses. © 2010 Elsevier B.V.",,"Max Wertheimer Minerva Center for Cognitive Studies, Faculty of Industrial Engineering and Management, Technion - Israel Institute of Technology, Haifa 32000, Israel; USC Neuroscience, University of Southern California, Los Angeles, CA 90033, United States",,,"Hochman G., Yechiam E., Bechara A.","Hochman, G., Max Wertheimer Minerva Center for Cognitive Studies, Faculty of Industrial Engineering and Management, Technion - Israel Institute of Technology, Haifa 32000, Israel; Yechiam, E., Max Wertheimer Minerva Center for Cognitive Studies, Faculty of Industrial Engineering and Management, Technion - Israel Institute of Technology, Haifa 32000, Israel; Bechara, A., USC Neuroscience, University of Southern California, Los Angeles, CA 90033, United States",8,,10.1016/j.bbr.2010.04.023,SCOPUS,1,,,,,,,,,,1,,,Behavioural Brain Research,Cognitive models; Recency; Reinforcement learning; Risk taking,,2-s2.0-77953362266,,,,,,34,27,,,,,Scopus,,,Behavioural Brain Research,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953362266&doi=10.1016%2fj.bbr.2010.04.023&partnerID=40&md5=aa7b60b245add046b6257561846611c7,,213,,2010,,,,,,,,,,,,,,,,,,,
Towards learning reward functions from user interactions,"In the physical world, people have dynamic preferences, e.g., the same situation can lead to satisfaction for some humans and to frustration for others. Personalization is called for. The same observation holds for online behavior with interactive systems. It is natural to represent the behavior of users who are engaging with interactive systems such as a search engine or a recommender system, as a sequence of actions where each next action depends on the current situation and the user reward of taking a particular action. By and large, current online evaluation metrics for interactive systems such as search engines or recommender systems, are static and do not reflect differences in user behavior. They rarely capture or model the reward experienced by a user while interacting with an interactive system.We argue that knowing a user's reward function is essential for an interactive system as both for learning and evaluation. We propose to learn users' reward functions directly from observed interaction traces. In particular, we present how users' reward functions can be uncovered directly using inverse reinforcement learning techniques. We also show how to incorporate user features into the learning process. Our main contribution is a novel and dynamic approach to restore a user's reward function. We present an analytic approach to this problem and complement it with initial experiments using the interaction logs of a cultural heritage institution that demonstrate the feasibility of the approach by uncovering different reward functions for different user groups. © 2017 Copyright held by the owner/author(s).",,"University of Amsterdam, Amsterdam, Netherlands; UserSat.com, University of Amsterdam, Amsterdam, Netherlands",,,"Li Z., Kiseleva J., De Rijke M., Grotov A.","Li, Z., University of Amsterdam, Amsterdam, Netherlands; Kiseleva, J., UserSat.com, University of Amsterdam, Amsterdam, Netherlands; De Rijke, M., University of Amsterdam, Amsterdam, Netherlands; Grotov, A., University of Amsterdam, Amsterdam, Netherlands",,,10.1145/3121050.3121098,SCOPUS,1,,,,,,,,,,,,,ICTIR 2017 - Proceedings of the 2017 ACM SIGIR International Conference on the Theory of Information Retrieval,Interactive systems; Inverse reinforcement learning; Online evaluation,,2-s2.0-85033215376,,,,,,292,289,,,,,Scopus,,,ICTIR 2017 - Proceedings of the 2017 ACM SIGIR International Conference on the Theory of Information Retrieval,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033215376&doi=10.1145%2f3121050.3121098&partnerID=40&md5=ddbe414389c116035f0fe3ceee854a01,,,,2017,,,,,,,,,,,,,,,,,,,
An immersive learning model using evolutionary learning,"In this article, we have proposed an educational model using virtual reality on a mobile platform by personalizing the simulated environments as per user actions. We have also proposed an evolutionary learning algorithm based on which the user learning path is designed and the corresponding simulated learning environment is modified. The main objective of this study is to create a personalized learning path for each student as per their calibre and make the learning immersive and retainable using virtual reality. Our proposed model emulates the innate natural learning process in humans and uses that to customize the virtual simulations of the lessons by applying the evolutionary learning technique. A quasi-experimental study is conducted by taking different case studies to establish the effectiveness of our learning model. The results show that our learning model is immersive and gives long term retention while enhancing creativity through reinforced customization of the simulations. © 2017 Elsevier Ltd",,"Department of Computer Science and Engineering, Kyungpook National University, Daegu, South Korea; Department of Electronics and Telecommunication Engineering, Karpagam College of Engineering, Coimbatore, TN, India",,,"Bhattacharjee D., Paul A., Kim J.H., Karthigaikumar P.","Bhattacharjee, D., Department of Computer Science and Engineering, Kyungpook National University, Daegu, South Korea; Paul, A., Department of Computer Science and Engineering, Kyungpook National University, Daegu, South Korea; Kim, J.H., Department of Computer Science and Engineering, Kyungpook National University, Daegu, South Korea; Karthigaikumar, P., Department of Electronics and Telecommunication Engineering, Karpagam College of Engineering, Coimbatore, TN, India",1,,10.1016/j.compeleceng.2017.08.023,SCOPUS,1,,,,,,,,,,,,,Computers and Electrical Engineering,Education; Evolutionary learning; Immersive learning; Immersive virtual reality; m-learning; Personalized learning; Reinforcement learning,,2-s2.0-85032829128,,,,,,249,236,,,,,Scopus,,,Computers and Electrical Engineering,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032829128&doi=10.1016%2fj.compeleceng.2017.08.023&partnerID=40&md5=e276a65771f4540bb5bcd68a3cc6b0e1,,65,,2018,,,,,,,,,,,,,,,,,,,
On the relationship between learning capability and the Boltzmann-Formula,In this paper a combined use of reinforcement learning and simulated annealing is treated. Most of the simulated annealing methods suggest using heuristic temperature bounds as the basis of annealing. Here a theoretically established approach tailored to reinforcement learning following Softmax action selection policy will be shown. An application example of agent-based routing will also be illustrated. © Springer-Verlag Berlin Heidelberg 2001.,,"Computer and Automation Research Institute, Hungarian Academy of Sciences, Kende u. 13-17, Budapest, Hungary",,,"Stefán P., Monostori L.","Stefán, P., Computer and Automation Research Institute, Hungarian Academy of Sciences, Kende u. 13-17, Budapest, Hungary; Monostori, L., Computer and Automation Research Institute, Hungarian Academy of Sciences, Kende u. 13-17, Budapest, Hungary",2,,,SCOPUS,0,,,,,,,,,,,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,,2-s2.0-77953822952,,,,,,236,227,,,,,Scopus,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953822952&partnerID=40&md5=bb5fa3ea3da126332e9faf22e1a5b4f3,,2070,,2001,,,,,,,,,,,,,,,,,,,
Distributed Q-learning based dynamic spectrum access in high capacity density cognitive cellular systems using secondary LTE spectrum sharing,"In this paper a distributed Q-learning based dynamic spectrum access (DSA) algorithm is applied to a cognitive cellular system designed for providing ultra high capacity density with only secondary access to an LTE channel. Large scale simulations of a stadium temporary event scenario show that the distributed Q-learning based DSA scheme provides robust quality of service (QoS) and extremely high system throughput densities to the users of the stadium network, whilst successfully coexisting with a primary network of macro eNodeBs on the same LTE channel. It is also shown that incorporating spectrum awareness or spectrum sensing based admission control into the DSA algorithm in this scenario does not improve its performance. Therefore, distributed Q-learning based DSA is a viable and easily implementable solution for facilitating secondary LTE spectrum sharing in high capacity density cognitive cellular systems.",,,,"Department of Electronics, University of York, Heslington, YO10 5DD, United Kingdom",N. Morozs; D. Grace; T. Clarke,,4,,10.1109/WPMC.2014.7014863,IEEE Xplore,1,,20150122,467,,Cognitive radio;Data models;Interference;Quality of service;Sensors;Throughput,Long Term Evolution;cellular radio;channel allocation;cognitive radio;learning (artificial intelligence);quality of service;radio spectrum management;signal detection;telecommunication computing;telecommunication congestion control,LTE channel secondary access;admission control;distributed Q-learning;dynamic spectrum access;high capacity density cognitive cellular systems;macro eNodeBs;quality of service;secondary LTE spectrum sharing;spectrum awareness;spectrum sensing,,1347-6890;13476890,,7-10 Sept. 2014,,2014 International Symposium on Wireless Personal Multimedia Communications (WPMC),Cognitive Cellular Systems;Dynamic Spectrum Access;Reinforcement Learning;Small Cells;Spectrum Sharing,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014863,,,,,,IEEE,23,,Electronic:978-9860-3-3407-4,,2014 International Symposium on Wireless Personal Multimedia Communications (WPMC),462,IEEE Conferences,,,,,2014,,,,,,,,,,,,,,,,,,,
A Fast Interactive Search System for Healthcare Services,"In this paper we describe the design, development, and evaluation of a general human-machine interaction search system, and its potential and use in the context of a collaboration project with SAP and Saffron. The objective of a specialized version of the system is to provide medical and healthcare information services to users via interactive search for personalized patient needs. Patients usually have questions regarding healthcare, including those which concern illness symptoms, duration and types of treatment, possible drug effects, and more. Authorized personnel would often be ideal in responding to such needs; however they could potentially be very expensive, and not easy to support and maintain. If patients could have access to information at their home, by means of i-phone or online access, this could save time, doctor office visit expenses, as well as valuable and restricted medical time. What is more, information concerning other anonymized and similar patient cases provides knowledge and perspective on a wide range of patient issues. From the doctors' perspective, they typically need to spend time on differential analysis about new patient cases: study symptoms, research possible causes, rank results by emergency priority and treat them accordingly. A search system that would direct a doctor (or patient/user) to similar patient cases would save significant amount of manual search time. The powerful new feature of this system is the storage and mining of past patient cases knowledge, to create metadata to be used in the subsequent retrieval of relevant documents. Finally, the interactive search system would speed up identification of rare cases; for instance, symptoms that do not appear commonly in past cases may require special treatment or expert referral. We build a model which dynamically learns medical needs of interacting MDs and patients. The model works on free or unstructured text, allowing disambiguation of vague words and flexibility in descri- ing medical needs. In addition, both experts with an advanced knowledge of medical terminology, and beginning users using basic medical terms, can achieve high search relevance. Furthermore, our approach obviates the need for the assignment of tags or labels, such as treatment, symptoms, causes, to documents, to respond effectively to user queries. In particular, we build a temporal difference algorithm to predict user's information needs by incorporating both current and predicted knowledge into learning the user profile. Our source of information about the user consists of submitted queries and feedback on the returned results. We tested our system on publicly available medical data (OhsuMed TREC dataset 2002) and we achieved a significant improvement in retrieval accuracy, compared to the literature. We provide quantitative results as well as demonstration screenshots which illustrate a) the value of interaction (user time spent with system versus results accuracy), b) the value of using medical terminology understanding, when compared with simple general words, and c) the value of allowing the maximum number of feedback submissions to vary.",,,,"Technol. & Inf. Manage., Univ. of California Santa Cruz, Santa Cruz, CA, USA",M. Daltayanni; C. Wang; R. Akella,,0,,10.1109/SRII.2012.65,IEEE Xplore,1,,20120924,534,,Diseases;Information services;Medical diagnostic imaging;Senior citizens;Terminology;Unified modeling language,data mining;health care;human computer interaction;information needs;information retrieval;interactive systems;medical information systems;text analysis,SAP;Saffron;basic medical terms;collaboration project;data mining;data storage;differential analysis;document retrieval;fast interactive search system;free or unstructured text;healthcare information services;human-machine interaction search system;information needs;medical information services;medical terminology;metadata;patient cases knowledge;patient issues;personalized patient needs;publicly available medical data;rare cases;submitted queries;temporal difference algorithm;unstructured text,,2166-0778;21660778,,24-27 July 2012,,2012 Annual SRII Global Conference,healthcare information services;interactive retrieval;medical data retrieval;reinforcement learning;temporal difference,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6311035,,,,,,IEEE,25,,Electronic:978-1-5090-5643-9; POD:978-1-4673-2318-5; USB:978-0-7695-4770-1,,2012 Annual SRII Global Conference,525,IEEE Conferences,,,,,2012,,,,,,,,,,,,,,,,,,,
Clyde: A deep reinforcement learning DOOM playing agent,"In this paper we present the use of deep reinforcement learning techniques in the context of playing partially observable multi-agent 3D games. These techniques have traditionally been applied to fully observable 2D environments, or navigation tasks in 3D environments. We show the performance of Clyde in comparison to other competitors within the context of the ViZDOOM competition that saw 9 bots compete against each other in DOOM death matches. Clyde managed to achieve 3rd place in the ViZDOOM competition held at the IEEE Conference on Computational Intelligence and Games 2016. Clyde performed very well considering its relative simplicity and the fact that we deliberately avoided a high level of customisation to keep the algorithm generic. © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"University of Essex, Colchester, United Kingdom; University of York, York, United Kingdom",,,"Ratcliffe D.S., Devlin S., Kruschwit U., Citi Z.","Ratcliffe, D.S., University of Essex, Colchester, United Kingdom; Devlin, S., University of York, York, United Kingdom; Kruschwit, U., University of Essex, Colchester, United Kingdom; Citi, Z., University of Essex, Colchester, United Kingdom",1,,,SCOPUS,0,,,,,,,,,,,,,AAAI Workshop - Technical Report,,,2-s2.0-85044754102,,,,,,990,983,,,,,Scopus,,,AAAI Workshop - Technical Report,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044754102&partnerID=40&md5=c4dd4a346b27c0a2bdfdef7601a4fc4e,,WS-17-01 - WS-17-15,,2017,,,,,,,,,,,,,,,,,,,
Self-organized femto-to-macro interference coordination with partial information,"In this paper we propose a self-organized method for Intercell Interference Coordination (ICIC) between femto and macro layers. We consider the challenging situation where femtocells are completely autonomous, i.e. they do not receive feedback from the macro network. The absence of a macro to femto interface is compliant with 3GPP Releases 10. We propose a distributed learning approach, based on Reinforcement Learning (RL), for environments characterized by partial information, due to the lack of communication between femtos and macros. The theory behind this approach is funded in the Partially Observable Markov Decision Process (POMDP). The POMDP requires to construct a set of beliefs about the environment. These beliefs are constructed following the spatial Interpolation theory, which allows to estimate the interference perceived by the macrousers. Simulation results show that, through the proposed methodology, femtocells can autonomously learn a transmission power policy to manage the aggregated interference at macrousers. Performances are compared to the complete information situation, which is compliant with the status of Release 11.",,,,"Centre Tecnol. de Telecomunicacions de Catalunya (CTTC), Castelldefels, Spain",A. Galindo-Serrano; L. Giupponi,,0,,10.1109/PIMRCW.2013.6707847,IEEE Xplore,1,,20140109,116,,Estimation;Femtocells;Interference;Interpolation;Macrocell networks;OFDM;Signal to noise ratio,Markov processes;femtocellular radio;learning (artificial intelligence);telecommunication computing,3GPP Releases 10;POMDP;RL;distributed learning approach;femtocell;intercell interference coordination;macrouser;partially observable Markov decision process;reinforcement learning;self-organized femto-to-macro interference coordination;spatial interpolation theory;transmission power policy,,,,8-9 Sept. 2013,,"2013 IEEE 24th International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC Workshops)",Femtocell network;POMDP;interference management;multiagent system;spatial interpolation,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6707847,,,,,1,IEEE,16,,Electronic:978-1-4799-0122-7; POD:978-1-4799-0121-0; USB:978-1-4799-0120-3,,"2013 IEEE 24th International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC Workshops)",111,IEEE Conferences,,,,,2013,,,,,,,,,,,,,,,,,,,
A game-theoretic framework with reinforcement learning for multinode cooperation in wireless networks,"In this paper, a game-theoretic framework based on the iterated prisoner's dilemma (IPD) is proposed to model the repeated dynamic interactions of multiple source nodes when communicating with multiple destinations in ad-hoc wireless networks. In such networks where nodes are autonomous, selfish, and not familiar with other nodes' strategies, fully cooperative behaviors cannot be assumed. Thus, a Q-learning algorithm is proposed to allow network nodes to adapt to and play the IPD game against opponents with a variety of known and unknown strategies. Simulation results illustrate that the proposed Q-learning algorithm allows network nodes to play optimally and achieve their maximum expected return values.",,,,"Dept. of Electr. Eng., Kuwait Univ., Kuwait City, Kuwait",M. W. Baidas,,2,,10.1109/PIMRC.2013.6666280,IEEE Xplore,1,,20131125,986,,Broadcasting;Convergence;Games;Learning (artificial intelligence);Silicon;Thin film transistors;Wireless networks,ad hoc networks;cooperative communication;game theory;learning (artificial intelligence);telecommunication computing,IPD;IPD game;Q-learning algorithm;ad-hoc wireless networks;fully cooperative behaviors;game-theoretic framework;iterated prisoner dilemma;multinode cooperation;multiple source nodes;network nodes;reinforcement learning,,2166-9570;21669570,,8-11 Sept. 2013,,"2013 IEEE 24th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",Amplify-and-forward (AF);Q-learning;cooperation;game-theory;prisoner's dilemma;reinforcement learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6666280,,,,,,IEEE,21,,Electronic:978-1-4673-6235-1; POD:978-1-4673-6233-7; USB:978-1-4673-6234-4,,"2013 IEEE 24th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",981,IEEE Conferences,,,,,2013,,,,,,,,,,,,,,,,,,,
A reinforcement self-learning model on an intelligent behavior avatar in a virtual world,"In this paper, a novel method for personal intelligent behavior avatar (IBA) is proposed to acquire autonomous behavior based on the interactions between user and smart objects in the virtual environment. In this method, the behavior decision model and the self-learning model are integrated by Bayesian networks and reinforcement learning. The Bayesian networks can treat interaction experiences using statistical processes, and the sureness of decision making is represented by certainty factors using stochastic reasoning. The reinforcement learning is implemented by learning experimentation or trial and error mechanisms to improve the performance of IBA through feedback. Therefore, the IBA makes a strategic decision that is approximated and appropriate to the user through the self-learning process by reinforcement learning. Finally, the feasibility of this method is investigated by imitating user's behavior and the results of self-learning process. The results of simulation show that the method is successful in imitating user's behavior and improving the performance of IBA",,,,"Dept. of Inf. Eng., Tamkang Univ., Tamsui, Taiwan",Jui-Fa Chen; Wei-Chuan Lin; Hua-Sheng Bai; Hsiao-Chuan Chao,,0,,10.1109/SUTC.2006.1636185,IEEE Xplore,1,,20060619,,,Artificial intelligence;Avatars;Bayesian methods;Chaos;Decision making;Educational institutions;Information technology;Intelligent agent;Learning;Virtual environment,approximation theory;avatars;belief networks;decision making;heuristic programming;statistical analysis;unsupervised learning,Bayesian network;IBA;approximation theory;behavior decision model;learning experimentation;personal intelligent behavior avatar;reinforcement self-learning model;statistical process;stochastic reasoning;strategic decision making,,,,5-7 June 2006,,"IEEE International Conference on Sensor Networks, Ubiquitous, and Trustworthy Computing (SUTC'06)",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1636185,,,,,,IEEE,11,,POD:0-7695-2553-9,,"IEEE International Conference on Sensor Networks, Ubiquitous, and Trustworthy Computing (SUTC'06)",7 pp.,IEEE Conferences,,,1,,2006,,,,,,,,,,,,,,,,,,,
Constructing an intelligent behavior avatar in a virtual world: a self-learning model based on reinforcement,"In this paper, a novel method for personal intelligent behavior avatar (IBA) is proposed to acquire autonomous behavior based on the interactions between user and smart objects in the virtual environment. In this method, the behavior decision model and the self-learning model are integrated by Bayesian networks and reinforcement learning. The Bayesian networks can treat interaction experiences using statistical processes, and the sureness of decision making is represented by certainty factors using stochastic reasoning. The reinforcement learning is implemented by learning experimentation or trial and error mechanisms to improve the performance of IBA through feedback. Therefore, the IBA makes a strategic decision that is approximated and appropriate to the user through the self-learning process by reinforcement learning. Finally, the feasibility of this method is investigated by imitating user's behavior and the results of self-learning process. The results of simulation show that the method is successful in imitating user's behavior and improving the performance of IBA.",,,,"Dept. of Inf. Eng., Tamkang Univ., Tamsui, Taiwan",Jui-Fa Chen; Wei-Chuan Lin; Hua-Sheng Bai; Chia-Che Yang; Hsiao-Chuan Chao,,1,,10.1109/IRI-05.2005.1506510,IEEE Xplore,1,,20050912,426,,Artificial intelligence;Avatars;Bayesian methods;Chaos;Decision making;Educational institutions;Information technology;Intelligent agent;Learning;Virtual environment,avatars;belief networks;decision making;human computer interaction;inference mechanisms;stochastic processes;unsupervised learning,Bayesian networks;behavior decision model;decision making;personal intelligent behavior avatar;reinforcement learning;self-learning model;statistical process;stochastic reasoning;user behavior;virtual world,,,,15-17 Aug. 2005,,"IRI -2005 IEEE International Conference on Information Reuse and Integration, Conf, 2005.",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1506510,,,,,,IEEE,9,,POD:0-7803-9093-8,,"IRI -2005 IEEE International Conference on Information Reuse and Integration, Conf, 2005.",421,IEEE Conferences,,,,,2005,,,,,,,,,,,,,,,,,,,
Q-learning based power control algorithm for D2D communication,"In this paper, reinforcement learning (RL) based power control algorithm in underlay D2D communication is studied. The approach we use regards D2D communication as a multi-agents system, and power control is achieved by maximizing system capacity while maintaining the requirement of quality of service(QoS) from cellular users. We propose two RL based power control methods for D2D users, i.e., team-Q learning and distributed-Q learning. The former is a centralized method in which only one Q-value table needs to be maintained, while the latter enables D2D users to learn independently and reduces the complexity of Q-value table. Simulation results show the difference of the two Q-learning algorithm in terms of convergence and reward function. In addition, it is shown that through our distributed-Q learning, D2D users not only are able to learn their power in a self-organized way, but also achieve better system performance than that using traditional method in LTE(Long Term Evolution).",,,,"Key Lab of Universal Wireless Communication, Beijing University of Posts and Telecommunications, Beijing, China 100876",S. Nie; Z. Fan; M. Zhao; X. Gu; L. Zhang,,,,10.1109/PIMRC.2016.7794793,IEEE Xplore,1,,20161222,6,,Algorithm design and analysis;Device-to-device communication;Interference;Power control;Quality of service;Radio transmitters;Signal to noise ratio,Long Term Evolution;learning (artificial intelligence);multi-agent systems;power control;quality of service;telecommunication control,LTE;Long Term Evolution;Q-learning based power control algorithm;QoS;cellular users;distributed-Q learning;multiagents system;quality of service;reinforcement learning;system capacity;underlay D2D communication,,,,4-8 Sept. 2016,,"2016 IEEE 27th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",D2D communication;Q-learning;Reinforcement learning;multi-agent system;power control,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7794793,,,,,,IEEE,,,Electronic:978-1-5090-3254-9; POD:978-1-5090-3255-6,,"2016 IEEE 27th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",1,IEEE Conferences,,,,,2016,,,,,,,,,,,,,,,,,,,
Reinforcement learning of cooperative persuasive dialogue policies using framing,"In this paper, we apply reinforcement learning for automatically learning cooperative persuasive dialogue system policies using framing, the use of emotionally charged statements common in persuasive dialogue between humans. In order to apply reinforcement learning, we describe a method to construct user simulators and reward functions specifically tailored to persuasive dialogue based on a corpus of persuasive dialogues between human interlocutors. Then, we evaluate the learned policy and the effect of framing through experiments both with a user simulator and with real users. The experimental evaluation indicates that applying reinforcement learning is effective for construction of cooperative persuasive dialogue systems which use framing.",,"Nara Institute of Science and Technology (NAIST), Nara, Japan",,,"Hiraoka T., Neubig G., Sakti S., Toda T., Nakamura S.","Hiraoka, T., Nara Institute of Science and Technology (NAIST), Nara, Japan; Neubig, G., Nara Institute of Science and Technology (NAIST), Nara, Japan; Sakti, S., Nara Institute of Science and Technology (NAIST), Nara, Japan; Toda, T., Nara Institute of Science and Technology (NAIST), Nara, Japan; Nakamura, S., Nara Institute of Science and Technology (NAIST), Nara, Japan",7,,,SCOPUS,0,,,,,,,,,,,,,"COLING 2014 - 25th International Conference on Computational Linguistics, Proceedings of COLING 2014: Technical Papers",,,2-s2.0-84959910680,,,,,,1717,1706,,,,,Scopus,,,"COLING 2014 - 25th International Conference on Computational Linguistics, Proceedings of COLING 2014: Technical Papers",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959910680&partnerID=40&md5=b153841fdc1b686556212286f40ea1af,,,,2014,,,,,,,,,,,,,,,,,,,
A learning approach to the school bus routing problem,"In this paper, we introduce a solution to the School Bus Routing problem (SBRP), using a reinforcement learning technique that we previously applied in the domain of transportation logistics. This approach consists of bundling transportation requests between several locations in order to construct combinations of items in a cost-efficient way. We investigate how the combination of reinforcement learning and our novel bundling algorithm can be used to increase the efficiency in logistics and how it can be applied to other domains. In particular, we discuss how the SBRP can be transformed to resemble a problem in transportation logistics and thus solve the SBRP using our combined technique. We obtain results comparable to those presented in literature, namely from a cost minimization approach that is specifically tailored to the SBRP. We conclude that our reinforcement learning and bundling algorithms are flexible enough to be applied in different domains and offer significant reductions in the cost of the stakeholders.",,"CoMo, Vrije Universiteit Brussel, Pleinlaan 2, Elsene, Belgium",,,"Van Moffaert K., Van Vreckem B., Mihaylov M., Nowe A.","Van Moffaert, K., CoMo, Vrije Universiteit Brussel, Pleinlaan 2, Elsene, Belgium; Van Vreckem, B., CoMo, Vrije Universiteit Brussel, Pleinlaan 2, Elsene, Belgium; Mihaylov, M., CoMo, Vrije Universiteit Brussel, Pleinlaan 2, Elsene, Belgium; Nowe, A., CoMo, Vrije Universiteit Brussel, Pleinlaan 2, Elsene, Belgium",,,,SCOPUS,0,,,,,,,,,,,,,Belgian/Netherlands Artificial Intelligence Conference,,,2-s2.0-84874008961,,,,,,,,,,,,Scopus,,,Belgian/Netherlands Artificial Intelligence Conference,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874008961&partnerID=40&md5=e7b2d272e1fcee9c2b12161e58cb9536,,,,2011,,,,,,,,,,,,,,,,,,,
State aggregation for reinforcement learning using neuroevolution,"In this paper, we present a new machine learning algorithm, RL-SANE, which uses a combination of neuroevolution (NE) and traditional reinforcement learning (RL) techniques to improve learning performace. RL-SANE is an innovative combination of the neuroevolutionary algorithm NEAT(Stanley, 2004) and the RL algorithm Sarsa(λ)(Sutton and Barto, 1998). It uses the special ability of NEAT to generate and train customized neural networks that provide a means for reducing the size of the state space through state aggregation. Reducing the size of the state space through aggregation enables Sarsa(λ) to be applied to much more difficult problems than standard tabular based approaches. Previous similar work in this area, such as in Whiteson and Stone (Whiteson and Stone, 2006) and Stanley and Miikkulainen (Stanley and Miikkulainen, 2001), have shown positive and promising results. This paper gives a brief overview of neuroevolutionary methods, introduces the RL-SANE algorithm, presents a comparative analysis of RL-SANE to other neuroevolutionary algorithms, and concludes with a discussion of enhancements that need to be made to RL-SANE.",,"Air Force Research Lab., Information Directorate, 525 Brooks Rd., Rome, 13441, United States",,,"Wright R., Gemelli N.","Wright, R., Air Force Research Lab., Information Directorate, 525 Brooks Rd., Rome, 13441, United States; Gemelli, N., Air Force Research Lab., Information Directorate, 525 Brooks Rd., Rome, 13441, United States",3,,,SCOPUS,0,,,,,,,,,,,,,ICAART 2009 - Proceedings of the 1st International Conference on Agents and Artificial Intelligence,Evolutionary algorithms; NeuroEvolution; Reinforcement learning; State aggregation,,2-s2.0-70349454220,,,,,,52,45,,,,,Scopus,,,ICAART 2009 - Proceedings of the 1st International Conference on Agents and Artificial Intelligence,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349454220&partnerID=40&md5=0eebdef21111af2a9eebacdf818ab11f,,,,2009,,,,,,,,,,,,,,,,,,,
Adaptive state space abstraction using neuroevolution,"In this paper, we present a new machine learning algorithm, RL-SANE, which uses a combination of neuroevolution (NE) and traditional reinforcement learning (RL) techniques to improve learning performance. RL-SANE is an innovative combination of the neuroevolutionary algorithm NEAT[9] and the RL algorithm Sarsa(λ)[12]. It uses the special ability of NEAT to generate and train customized neural networks that provide a means for reducing the size of the state space through state aggregation. Reducing the size of the state space through aggregation enables Sarsa(λ) to be applied to much more difficult problems than standard tabular based approaches. Previous similar work in this area, such as in Whiteson and Stone [15] and Stanley and Miikkulainen [10], have shown positive results. This paper gives a brief overview of neuroevolutionary methods, introduces the RL-SANE algorithm, presents a comparative analysis of RL-SANE to other neuroevolutionary algorithms, and concludes with a discussion of enhancements that need to be made to RL-SANE. © 2010 Springer-Verlag Berlin Heidelberg.",,"Air Force Research Laboratory Information Directorate, 525 Brooks Rd., Rome, NY 13441, United States",,,"Wright R., Gemelli N.","Wright, R., Air Force Research Laboratory Information Directorate, 525 Brooks Rd., Rome, NY 13441, United States; Gemelli, N., Air Force Research Laboratory Information Directorate, 525 Brooks Rd., Rome, NY 13441, United States",,,10.1007/978-3-642-11819-7_7,SCOPUS,1,,,,,,,,,,,,,Communications in Computer and Information Science,Evolutionary Algorithms; NeuroEvolution; Reinforcement Learning; State Abstraction,,2-s2.0-77957571304,,,,,,96,84,,,,,Scopus,,,Communications in Computer and Information Science,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957571304&doi=10.1007%2f978-3-642-11819-7_7&partnerID=40&md5=3745801dbd637a95644b184783ea7b30,,67 CCIS,,2010,,,,,,,,,,,,,,,,,,,
Personalized intelligent tutoring system using reinforcement learning,"In this paper, we present a Personalized Intelligent Tutoring System that uses Reinforcement Learning techniques to implicitly learn teaching rules and provide instructions to students based on their needs. The system works on coarsely labeled data with minimum expert knowledge to ease extension to newer domains. Copyright © 2011, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Microsoft Corporation, India; Department of Computer Science and Engineering, IIT Madras, India",,,"Malpani A., Ravindran B., Murthy H.","Malpani, A., Microsoft Corporation, India; Ravindran, B., Department of Computer Science and Engineering, IIT Madras, India; Murthy, H., Department of Computer Science and Engineering, IIT Madras, India",5,,,SCOPUS,0,,,,,,,,,,,,,"Proceedings of the 24th International Florida Artificial Intelligence Research Society, FLAIRS - 24",,,2-s2.0-80052392052,,,,,,562,561,,,,,Scopus,,,"Proceedings of the 24th International Florida Artificial Intelligence Research Society, FLAIRS - 24",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052392052&partnerID=40&md5=6dc8d7ca7906b00d34f9338e4ba9a6bf,,,,2011,,,,,,,,,,,,,,,,,,,
Exploiting Reinforcement Learning to Profile Users and Personalize Web Pages,"In this paper, we present a Web content adaptation system that is able to automatically adapt textual elements of Web pages, based on the user profile and preferences. The system employs Web intelligence to perform these automatic adaptations on single elements composing a Web page. In particular, a reinforcement learning algorithm, i.e. Q-learning, based on the idea of reward/punishment is utilized as the machine learning system that manages the user profile. Based on it, the user profile is updated, so that automatic adaptations can be effectively performed while surfing the Web. We created a simulation scenario to test our approach over different users with specific preferences and/or different kinds of disabilities. Simulation results confirm the viability of the proposal.",,,,"Dept. of Comput. Sci. & Eng., Univ. di Bologna, Bologna, Italy",S. Ferretti; S. Mirri; C. Prandi; P. Salomoni,,1,,10.1109/COMPSACW.2014.45,IEEE Xplore,1,,20140922,257,,Adaptation models;Context;Learning (artificial intelligence);Learning systems;Prototypes;Senior citizens;Web pages,Internet;learning (artificial intelligence);user interfaces,Q-learning;Web content adaptation system;Web intelligence;Web page personalization;machine learning system;reinforcement learning;simulation scenario;textual elements;user preference;user profile,,,,21-25 July 2014,,2014 IEEE 38th International Computer Software and Applications Conference Workshops,Web personalization;content adaptation;legibility;reinforcement learning;user profiling,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903138,,,,,,IEEE,24,,Electronic:978-1-4799-3578-9; POD:978-1-4799-3579-6,,2014 IEEE 38th International Computer Software and Applications Conference Workshops,252,IEEE Conferences,,,,,2014,,,,,,,,,,,,,,,,,,,
Interactive learning and adaptation for robot assisted therapy for people with dementia,"In this paper, we present an adaptive cognitive music game designed to monitor and improve the attention levels of peo- ple with dementia. The goal of this game is to provide a customized protocol based on user needs and preferences, following the Reinforcement Learning (RL) framework. The game adjusts its parameters (e.g., diffculty level) so as to help the user complete the task successfully, while keeping them engaged. The main contribution of this paper is an interactive learning and adaptation framework that enables and facilitates the adaptation of the robot behavior towards new users, providing a safe, tailored and effcient interac- tion. Copyright 2016 is held by the owner/author(s).",,"CSE Department, HERACLEIA Lab, University of Texas at Arlington, United States; Department of Psychology, Cognitive Neuroscience of Memory Lab, University of Texas at Arlington, United States",,,"Tsiakas K., Abellanoza C., Makedon F.","Tsiakas, K., CSE Department, HERACLEIA Lab, University of Texas at Arlington, United States; Abellanoza, C., Department of Psychology, Cognitive Neuroscience of Memory Lab, University of Texas at Arlington, United States; Makedon, F., CSE Department, HERACLEIA Lab, University of Texas at Arlington, United States",,,10.1145/2910674.2935849,SCOPUS,1,,,,,,,,,,,,,ACM International Conference Proceeding Series,Interactive Reinforcement Learning; Music Therapy; Policy Adaptation; Robot Assisted Therapy; Robot Learning and Behavior Adaptation,,2-s2.0-85006062180,,,,,,,,,,,,Scopus,,,ACM International Conference Proceeding Series,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006062180&doi=10.1145%2f2910674.2935849&partnerID=40&md5=ec3f5c09483c1d6eab071621bde732a1,,29-June-2016,,2016,,,,,,,,,,,,,,,,,,,
An interactive learning and adaptation framework for adaptive robot assisted therapy,"In this paper, we present an interactive learning and adaptation framework. The framework combines Interactive Reinforcement Learning methods to effectively adapt and refine a learned policy to cope with new users. We argue that implicit feedback provided by the primary user and guidance from a secondary user can be integrated to the adaptation mechanism, resulting at a tailored and safe interaction. We illustrate this framework with a use case in Robot Assisted Therapy, presenting a Robot Yoga Trainer that monitors a yoga training session, adjusting the session parameters based on human motion activity recognition and evaluation through depth data, to assist the user complete the session, following a Reinforcement Learning approach. Copyright 2016 is held by the owner/author(s).",,"CSE Department, University of Texas at Arlington, United States; Department of Psychology, HERACLEIA Lab, University of Texas at Arlington, United States; National Center for Scientific Research Demokritos, Athens, Greece",,,"Tsiakas K., Papakostas M., Chebaa B., Ebert D., Karkaletsis V., Makedon F.","Tsiakas, K., CSE Department, University of Texas at Arlington, United States; Papakostas, M., CSE Department, University of Texas at Arlington, United States; Chebaa, B., Department of Psychology, HERACLEIA Lab, University of Texas at Arlington, United States; Ebert, D., CSE Department, University of Texas at Arlington, United States; Karkaletsis, V., National Center for Scientific Research Demokritos, Athens, Greece; Makedon, F., CSE Department, University of Texas at Arlington, United States",1,,10.1145/2910674.2935857,SCOPUS,1,,,,,,,,,,,,,ACM International Conference Proceeding Series,Adaptive Robot Assisted Therapy; Interactive Reinforcement Learning; Policy Adaptation,,2-s2.0-85005990789,,,,,,,,,,,,Scopus,,,ACM International Conference Proceeding Series,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005990789&doi=10.1145%2f2910674.2935857&partnerID=40&md5=cc7971a64b3859d4fd701146e04a14f9,,29-June-2016,,2016,,,,,,,,,,,,,,,,,,,
Use of reinforcement learning in two real applications,"In this paper, we present two sucessful applications of Reinforcement Learning (RL) in real life. First, the optimization of anemia management in patients undergoing Chronic Renal Failure is presented. The aim is to individualize the treatment (Erythropoietin dosages) in order to stabilize patients within a targeted range of Hemoglobin (Hb). Results show that the use of RL increases the ratio of patients within the desired range of Hb. Thus, patients' quality of life is increased, and additionally, Health Care System reduces its expenses in anemia management. Second, RL is applied to modify a marketing campaign in order to maximize long-term profits. RL obtains an individualized policy depending on customer characteristics that increases long-term profits at the end of the campaign. Results in both problems show the robustness of the obtained policies and suggest their use in other real-life problems. © 2008 Springer Berlin Heidelberg.",,"Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain",,,"Martín-Guerrero J.D., Soria-Olivas E., Martínez-Sober M., Serrrano-López A.J., Magdalena-Benedito R., Gómez-Sanchis J.","Martín-Guerrero, J.D., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain; Soria-Olivas, E., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain; Martínez-Sober, M., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain; Serrrano-López, A.J., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain; Magdalena-Benedito, R., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain; Gómez-Sanchis, J., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain",,,10.1007/978-3-540-89722-4_15,SCOPUS,1,,,,,,,,,,,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,,2-s2.0-58449123856,,,,,,204,191,,,,,Scopus,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58449123856&doi=10.1007%2f978-3-540-89722-4_15&partnerID=40&md5=fb2f4ac752eba3eca8189bfc230bd0e6,,5323 LNAI,,2008,,,,,,,,,,,,,,,,,,,
Personalized ad recommendation systems for life-time value optimization with guarantees,"In this paper, we propose a framework for using reinforcement learning (RL) algorithms to learn good policies for personalized ad recommendation (PAR) systems. The RL algorithms take into account the long-term effect of an action, and thus, could be more suitable than myopic techniques like supervised learning and contextual bandit, for modern PAR systems in which the number of returning visitors is rapidly growing. However, while myopic techniques have been well-studied in PAR systems, the RL approach is still in its infancy, mainly due to two fundamental challenges: how to compute a good RL strategy and how to evaluate a solution using historical data to ensure its ""safety"" before deployment. In this paper, we propose to use a family of off-policy evaluation techniques with statistical guarantees to tackle both these challenges. We apply these methods to a real PAR problem, both for evaluating the final performance and for optimizing the parameters of the RL algorithm. Our results show that a RL algorithm equipped with these offpolicy evaluation techniques outperforms the myopic approaches. Our results also give fundamental insights on the difference between the click through rate (CTR) and life-time value (LTV) metrics for evaluating the performance of a PAR algorithm.",,"Adobe Research, United States; UMassAmherst, United States; INRIA, France",,,"Theocharous G., Thomas P.S., Ghavamzadeh M.","Theocharous, G., Adobe Research, United States; Thomas, P.S., Adobe Research, United States, UMassAmherst, United States; Ghavamzadeh, M., Adobe Research, United States, INRIA, France",7,,,SCOPUS,0,,,,,,,,,,,,,IJCAI International Joint Conference on Artificial Intelligence,,,2-s2.0-84949758918,,,,,,1812,1806,,,,,Scopus,,,IJCAI International Joint Conference on Artificial Intelligence,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949758918&partnerID=40&md5=7653e3556c282c0be9fdbdfaf07f4fb5,,2015-January,,2015,,,,,,,,,,,,,,,,,,,
Facilitating safe adaptation of learning agents using interactive reinforcement learning,"In this paper, we propose a learning framework for the adaptation of an interactive agent to a new user. We focus on applications where safety and personalization are essential, as Rehabilitation Systems and Robot Assisted Therapy. We argue that interactive learning methods can be utilised and combined into the Reinforcement Learning framework, aiming at a safe and tailored interaction.",,"Computer Science and Engineering Dept., University of Texas, Arlington, United States; Institute of Informatics and Telecommunications, National Centre for Scientific Research Demokritos, Greece",,,Tsiakas K.,"Tsiakas, K., Computer Science and Engineering Dept., University of Texas, Arlington, United States, Institute of Informatics and Telecommunications, National Centre for Scientific Research Demokritos, Greece",4,,10.1145/2876456.2876457,SCOPUS,1,,,,,,,,,,,,,"International Conference on Intelligent User Interfaces, Proceedings IUI",Adaptation; Interaction management; Interactive reinforcement learning; Learning agents,,2-s2.0-85014170682,,,,,,109,106,,,,,Scopus,,,"International Conference on Intelligent User Interfaces, Proceedings IUI",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014170682&doi=10.1145%2f2876456.2876457&partnerID=40&md5=9691ccc76385d92e9a7f6aefae9ddc6c,,,,2016,,,,,,,,,,,,,,,,,,,
Improving user satisfaction in agent-based electronic marketplaces by reputation modelling and adjustable product quality,"In this paper, we propose a market model and learning algorithms for buying and selling agents in electronic marketplaces. We take into account the fact that multiple selling agents may offer the same good with different qualities, and that selling agents may alter the quality of their goods. We also consider the possible existence of dishonest selling agents in the market. In our approach, buying agents learn to maximize their expected value of goods using reinforcement learning. In addition, they model and exploit the reputation of selling agents to avoid interaction with the disreputable ones, and therefore to reduce the risk of purchasing low value goods. Our selling agents learn to maximize their expected profits by using reinforcement learning to adjust product prices, and also by altering product quality to provide more customized value to their goods. This paper focuses on presenting results from experiments investigating the behaviours of buying and selling agents in large-sized electronic marketplaces. Our results confirm that buying and selling agents following the proposed algorithms obtain greater satisfaction than buying and selling agents who only use reinforcement learning, with the buying agents not modelling sellers' reputation and the selling agents not adjusting product quality.",,"School of Computer Science, University of Waterloo, Waterloo, Ont. N2L 3G1, Canada",,,"Tran T., Cohen R.","Tran, T., School of Computer Science, University of Waterloo, Waterloo, Ont. N2L 3G1, Canada; Cohen, R., School of Computer Science, University of Waterloo, Waterloo, Ont. N2L 3G1, Canada",42,,,SCOPUS,0,,,,,,,,,,,,,"Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS 2004",,,2-s2.0-4544220420,,,,,,835,828,,,,,Scopus,,,"Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS 2004",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4544220420&partnerID=40&md5=d1231e95154e8761409eedfa69f28d67,,2,,2004,,,,,,,,,,,,,,,,,,,
Learning cooperative persuasive dialogue policies using framing,"In this paper, we propose a new framework of cooperative persuasive dialogue, where a dialogue system simultaneously attempts to achieve user satisfaction while persuading the user to take some action that achieves a pre-defined system goal. Within this framework, we describe a method for reinforcement learning of cooperative persuasive dialogue policies by defining a reward function that reflects both the system and user goal, and using framing, the use of emotionally charged statements common in persuasive dialogue between humans. In order to construct the various components necessary for reinforcement learning, we first describe a corpus of persuasive dialogues between human interlocutors, then propose a method to construct user simulators and reward functions specifically tailored to persuasive dialogue based on this corpus. Then, we implement a fully automatic text-based dialogue system for evaluating the learned policies. Using the implemented dialogue system, we evaluate the learned policy and the effect of framing through experiments both with a user simulator and with real users. The experimental evaluation indicates that the proposed method is effective for construction of cooperative persuasive dialogue systems. © 2016 Elsevier B.V.",,"Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan; Nagoya University, Furo-cho, Chikusa-kuNagoya, Japan",,,"Hiraoka T., Neubig G., Sakti S., Toda T., Nakamura S.","Hiraoka, T., Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan; Neubig, G., Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan; Sakti, S., Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan; Toda, T., Nagoya University, Furo-cho, Chikusa-kuNagoya, Japan; Nakamura, S., Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan",3,,10.1016/j.specom.2016.09.002,SCOPUS,1,,,,,,,,,,,,,Speech Communication,Cooperative persuasive dialogue; Dialogue modeling; Dialogue system; Framing; Reinforcement learning,,2-s2.0-84989201964,,,,,,96,83,,,,,Scopus,,,Speech Communication,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989201964&doi=10.1016%2fj.specom.2016.09.002&partnerID=40&md5=6d48db88d86248c3e2ce1aa7891e78c2,,84,,2016,,,,,,,,,,,,,,,,,,,
Personalized response generation via domain adaptation,"In this paper, we propose a novel personalized response generation model via domain adaptation (PRG-DM). First, we learn the human responding style from large general data (without user-specific information). Second, we fine tune the model on a small size of personalized data to generate personalized responses with a dual learning mechanism. Moreover, we propose three new rewards to characterize good conversations that are personalized, informative and grammatical. We employ the policy gradient method to generate highly rewarded responses. Experimental results show that our model can generate better personalized responses for different users. © 2017 Copyright held by the owner/author(s).",,"Tencent AI Lab, United States; Zhejiang University, China; Tencent, China; Normal University, China; IIE, Chinese Academy of Sciences, China; South China Normal University, China",,,"Yang M., Zhao Z., Zhao W., Chen X., Zhu J., Zhou L., Cao Z.","Yang, M., Tencent AI Lab, United States; Zhao, Z., Zhejiang University, China; Zhao, W., Tencent, China; Chen, X., Normal University, China; Zhu, J., IIE, Chinese Academy of Sciences, China; Zhou, L., South China Normal University, China; Cao, Z., Tencent AI Lab, United States",,,10.1145/3077136.3080706,SCOPUS,1,,,,,,,,,,,,,SIGIR 2017 - Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,Domain adaptation; Reinforcement learning; Response generation,,2-s2.0-85029357642,,,,,,1024,1021,,,,,Scopus,,,SIGIR 2017 - Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029357642&doi=10.1145%2f3077136.3080706&partnerID=40&md5=7dbab8245c15ee222478932988b1f01b,,,,2017,,,,,,,,,,,,,,,,,,,
Investigating Deep Reinforcement Learning Techniques in Personalized Dialogue Generation,"In this paper, we propose a personalized dialogue generation system, which combines reinforcement learning techniques with an attention-based hierarchical recurrent encoderdecoder model. Firstly, we incorporate user-specific information into the decoder to …",,,,,,,0,,,Google Scholar,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,http://scholar.google.com/https://epubs.siam.org/doi/abs/10.1137/1.9781611975321.71,,,,2018,,,,,,,,,,,,0,,,,,,,
Deep Reinforcement Learning for Dynamic Treatment Regimes on Medical Registry Data,"In this paper, we propose the first deep reinforce-ment learning framework to estimate the optimal Dynamic Treat-ment Regimes from observational medical data. This framework is more flexible and adaptive for high dimensional action and state spaces than existing reinforcement learning methods to model real life complexity in heterogeneous disease progression and treatment choices, with the goal to provide doctor and patients the data-driven personalized decision recommendations. The proposed deep reinforcement learning framework contains a supervised learning step to predict the most possible expert actions; and a deep reinforcement learning step to estimate the long term value function of Dynamic Treatment Regimes. We motivated and implemented the proposed framework on a data set from the Center for International Bone Marrow Transplant Research (CIBMTR) registry database, focusing on the sequence of prevention and treatments for acute and chronic graft versus host disease. We showed results of the initial implementation that demonstrates promising accuracy in predicting human expert decisions and initial implementation for the reinforcement learning step.",,,,"Div. of Biostat., Med. Coll. of Wisconsin, Milwaukee, WI, USA",Y. Liu; B. Logan; N. Liu; Z. Xu; J. Tang; Y. Wang,,,,10.1109/ICHI.2017.45,IEEE Xplore,1,,20170914,385,,Biomedical imaging;Decision making;Diseases;Games;Learning (artificial intelligence);Machine learning,bone;data analysis;diseases;learning (artificial intelligence);patient treatment,CIBMTR;Dynamic Treatment Regimes;International Bone Marrow Transplant Research registry database;chronic graft versus host disease;deep reinforcement learning framework;deep reinforcement learning step;heterogeneous disease progression;medical registry data;observational medical data;personalized decision recommendations;reinforcement learning framework;supervised learning step,,,,23-26 Aug. 2017,,2017 IEEE International Conference on Healthcare Informatics (ICHI),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031178,,,,,,IEEE,,,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,,2017 IEEE International Conference on Healthcare Informatics (ICHI),380,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
Rapid simulation-driven reinforcement learning of multimodal dialog strategies in human-robot interaction,"In this work we propose a procedure model for rapid automatic strategy learning in multimodal dialogs. Our approach is tailored for typical task-oriented human-robot dialog interactions, with no prior knowledge about the expected user and system dynamics being present. For such scenarios, we propose the use of stochastic dialog simulation for strategy learning, where the user and system error models are solely trained through the initial execution of an inexpensive Wizard-of-Oz experiment. We argue that for the addressed dialogs, already a small data corpus combined with a low-conditioned simulation model facilitates learning of strong and complex dialog strategies. To validate our overall approach, we empirically show the supremacy of the learned strategy over a hand-crafted strategy for a concrete human-robot dialog scenario. To the authors' knowledge, this work is the first to perform strategy learning from multimodal dialog simulation.",,"Interactive Systems Labs., Universität Karlsruhe (TH), Germany",,,"Prommer T., Holzapfel H., Waibel A.","Prommer, T., Interactive Systems Labs., Universität Karlsruhe (TH), Germany; Holzapfel, H., Interactive Systems Labs., Universität Karlsruhe (TH), Germany; Waibel, A., Interactive Systems Labs., Universität Karlsruhe (TH), Germany",12,,,SCOPUS,0,,,,,,,,,,,,,"INTERSPEECH 2006 and 9th International Conference on Spoken Language Processing, INTERSPEECH 2006 - ICSLP",Multimodal human-robot dialogs; Strategy learning,,2-s2.0-44949181000,,,,,,1921,1918,,,,,Scopus,,,"INTERSPEECH 2006 and 9th International Conference on Spoken Language Processing, INTERSPEECH 2006 - ICSLP",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44949181000&partnerID=40&md5=b035fc070c6373c68c94bedfd9b4194d,,4,,2006,,,,,,,,,,,,,,,,,,,
Optimal learning control of oxygen saturation using a policy iteration algorithm and a proof-of-concept in an interconnecting three-tank system,"In this work, “policy iteration algorithm” (PIA) is applied for controlling arterial oxygen saturation that does not require mathematical models of the plant. This technique is based on nonlinear optimal control to solve the Hamilton–Jacobi–Bellman equation. The controller is synthesized using a state feedback configuration based on an unidentified model of complex pathophysiology of pulmonary system in order to control gas exchange in ventilated patients, as under some circumstances (like emergency situations), there may not be a proper and individualized model for designing and tuning controllers available in time. The simulation results demonstrate the optimal control of oxygenation based on the proposed PIA by iteratively evaluating the Hamiltonian cost functions and synthesizing the control actions until achieving the converged optimal criteria. Furthermore, as a practical example, we examined the performance of this control strategy using an interconnecting three-tank system as a real nonlinear system. © 2016 Elsevier Ltd",,"Philips Chair for Medical Information Technology, RWTH Aachen University, Aachen, Germany",,,"Pomprapa A., Leonhardt S., Misgeld B.J.E.","Pomprapa, A., Philips Chair for Medical Information Technology, RWTH Aachen University, Aachen, Germany; Leonhardt, S., Philips Chair for Medical Information Technology, RWTH Aachen University, Aachen, Germany; Misgeld, B.J.E., Philips Chair for Medical Information Technology, RWTH Aachen University, Aachen, Germany",,,10.1016/j.conengprac.2016.07.014,SCOPUS,1,,,,,,,,,,,,,Control Engineering Practice,Biomedical control system; Closed-loop ventilation; Control of oxygen saturation; Optimal control; Policy iteration algorithm; Reinforcement learning,,2-s2.0-84995900104,,,,,,203,194,,,,,Scopus,,,Control Engineering Practice,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995900104&doi=10.1016%2fj.conengprac.2016.07.014&partnerID=40&md5=8b494bd2a9f229c0bc0edde6e57e0fde,,59,,2017,,,,,,,,,,,,,,,,,,,
Incorporating prior knowledge into Q-learning for drug delivery individualization,"Individualization of drug delivery in treatment of chronic ailments is a challenge to the physician. Variability of response across patient population requires tailoring the dosing strategies to individual's needs. We have previously demonstrated the potential of reinforcement learning methods to support the physician in the management of anemia. In this paper, we propose the incorporation of prior knowledge into the learning mechanism to further improve the outcomes of the treatment.",,,,"Louisville Univ., KY, USA",A. E. Gaweda; M. K. Muezzinoglu; G. R. Aronoff; A. A. Jacobs; J. M. Zurada; M. E. Brier,,2,,10.1109/ICMLA.2005.40,IEEE Xplore,1,,20060320,,,Animals;Bones;Decision making;Drug delivery;Humans;Jacobian matrices;Learning systems;Production;Protocols;Red blood cells,diseases;drug delivery systems;drugs;learning (artificial intelligence);medical computing,Q-learning;anemia;chronic ailment treatment;drug delivery individualization;reinforcement learning,,,,15-17 Dec. 2005,,Fourth International Conference on Machine Learning and Applications (ICMLA'05),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607452,,,,,,IEEE,8,,POD:0-7695-2495-8,,Fourth International Conference on Machine Learning and Applications (ICMLA'05),6 pp.,IEEE Conferences,,,,,2005,,,,,,,,,,,,,,,,,,,
Seeking positive experiences can produce illusory correlations,"Individuals tend to select again alternatives about which they have positive impressions and to avoid alternatives about which they have negative impressions. Here we show how this sequential sampling feature of the information acquisition process leads to the emergence of an illusory correlation between estimates of the attributes of multi-attribute alternatives. The sign of the illusory correlation depends on how the decision maker combines estimates in making her sampling decisions. A positive illusory correlation emerges when evaluations are compensatory or disjunctive and a negative illusory correlation can emerge when evaluations are conjunctive. Our theory provides an alternative explanation for illusory correlations that does not rely on biased information processing nor selective attention to different pieces of information. It provides a new perspective on several well-established empirical phenomena such as the 'Halo' effect in personality perception, the relation between proximity and attitudes, and the in-group out-group bias in stereotype formation. © 2011 Elsevier B.V.",,"Universitat Pompeu Fabra, Department of Economics and Business, Ramon Trias Fargas 25-27, 08005 Barcelona, Spain; Saïd Business School, Park End Street, Oxford Oxfordshire OX1 1HP, United Kingdom",,,"Denrell J., Le Mens G.","Denrell, J., Saïd Business School, Park End Street, Oxford Oxfordshire OX1 1HP, United Kingdom; Le Mens, G., Universitat Pompeu Fabra, Department of Economics and Business, Ramon Trias Fargas 25-27, 08005 Barcelona, Spain",8,,10.1016/j.cognition.2011.01.007,SCOPUS,1,,,,,,,,,,3,,,Cognition,Adaptive behavior; Halo effect; Reinforcement learning; Sampling; Stereotype formation,,2-s2.0-79953324111,,,,,,324,313,,,,,Scopus,,,Cognition,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953324111&doi=10.1016%2fj.cognition.2011.01.007&partnerID=40&md5=0ff08a439f19bac8fc23297259212fb6,,119,,2011,,,,,,,,,,,,,,,,,,,
Usage-based web recommendations: A reinforcement learning approach,"Information overload is no longer news; the explosive growth of the Internet has made this issue increasingly serious for Web users. Users are very often overwhelmed by the huge amount of information and are faced with a big challenge to find the most relevant information in the right time. Recommender systems aim at pruning this information space and directing users toward the items that best meet their needs and interests. Web Recommendation has been an active application area in Web Mining and Machine Learning research. In this paper we propose a novel machine learning perspective toward the problem, based on reinforcement learning. Unlike other recommender systems, our system does not use the static patterns discovered from web usage data, instead it learns to make recommendations as the actions it performs in each situation. We model the problem as Q-Learning while employing concepts and techniques commonly applied in the web usage mining domain. We propose that the reinforcement learning paradigm provides an appropriate model for the recommendation problem, as well as a framework in which the system constantly interacts with the user and learns from her behavior. Our experimental evaluations support our claims and demonstrate how this approach can improve the quality of web recommendations. Copyright 2007 ACM.",,"Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran",,,"Taghipour N., Kardan A., Ghidary S.S.","Taghipour, N., Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran; Kardan, A., Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran; Ghidary, S.S., Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran",31,,10.1145/1297231.1297250,SCOPUS,1,,,,,,,,,,,,,RecSys'07: Proceedings of the 2007 ACM Conference on Recommender Systems,Machine learning; Personalization; Recommender systems; Reinforcement learning; Web usage mining,,2-s2.0-42149177829,,,,,,120,113,,,,,Scopus,,,RecSys'07: Proceedings of the 2007 ACM Conference on Recommender Systems,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149177829&doi=10.1145%2f1297231.1297250&partnerID=40&md5=0344c9539560bc03572a0e0f62a61992,,,,2007,,,,,,,,,,,,,,,,,,,
"What's in a word? How instructions, suggestions, and social information change pain and emotion","Instructions, suggestions, and other types of social information can have powerful effects on pain and emotion. Prominent examples include observational learning, social influence, placebo, and hypnosis. These different phenomena and their underlying brain mechanisms have been studied in partially separate literatures, which we discuss, compare, and integrate in this review. Converging findings from these literatures suggest that (1) instructions and social information affect brain systems associated with the generation of pain and emotion, and with reinforcement learning, and that (2) these changes are mediated by alterations in prefrontal systems responsible for top-down control and the generation of affective meaning. We argue that changes in expectation and appraisal, a process of assessing personal meaning and implications for wellbeing, are two potential key mediators of the effects of instructions and social information on affective experience. Finally, we propose a tentative model of how prefrontal regions, especially dorsolateral and ventromedial prefrontal cortex may regulate affective processing based on instructions and socially transmitted expectations more broadly. © 2017 Elsevier Ltd",,"Institute of Cognitive Science, University of Colorado Boulder, United States; Department of Psychology and Neuroscience, University of Colorado Boulder, United States; Cognitive Psychology Unit, Institute of Psychology, Leiden University, Netherlands; Leiden Institute for Brain and Cognition, Leiden University, Netherlands",,,"Koban L., Jepma M., Geuter S., Wager T.D.","Koban, L., Institute of Cognitive Science, University of Colorado Boulder, United States, Department of Psychology and Neuroscience, University of Colorado Boulder, United States; Jepma, M., Cognitive Psychology Unit, Institute of Psychology, Leiden University, Netherlands, Leiden Institute for Brain and Cognition, Leiden University, Netherlands; Geuter, S., Institute of Cognitive Science, University of Colorado Boulder, United States, Department of Psychology and Neuroscience, University of Colorado Boulder, United States; Wager, T.D., Institute of Cognitive Science, University of Colorado Boulder, United States, Department of Psychology and Neuroscience, University of Colorado Boulder, United States",7,,10.1016/j.neubiorev.2017.02.014,SCOPUS,1,,,,,,,,,,,,,Neuroscience and Biobehavioral Reviews,Appraisal; dlPFC; Emotion regulation; Expectation; Fear; fMRI; Hypnosis; Observational learning; Placebo; Social conformity; Social influence; vmPFC,,2-s2.0-85016506177,,,,,,42,29,,,,,Scopus,,,Neuroscience and Biobehavioral Reviews,,Review,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016506177&doi=10.1016%2fj.neubiorev.2017.02.014&partnerID=40&md5=f9cbdf9bc0d54ccf146f59f39b4cf521,,81,,2017,,,,,,,,,,,,,,,,,,,
Trading financial indices with reinforcement learning agents,"Intelligent agents are often used in professional portfolio management. The use of intelligent agents in personal retirement portfolio management is not investigated in the past. In this research, we consider a two-asset personal retirement portfolio and propose several reinforcement learning agents for trading portfolio assets. In particular, we design an on-policy SARSA (λ) and an off-policy Q(λ) discrete state and discrete action agents that maximize either portfolio returns or differential Sharpe ratios. Additionally, we design a temporal-difference learning, TD(λ), agent that uses a linear valuation function in discrete state and continuous action settings. Using two different two-asset portfolios, the first asset being the S&P 500 Index and the second asset being either a broad bond market index or a 10-year U.S. Treasury note (T-note), we test the performance of different agents on different holdout (test) samples. The results of our experiments indicate that the high-learning frequency (i.e., adaptive learning) TD(λ) agent consistently beats both the single asset stock and bond cumulative returns by a significant margin. © 2018 Elsevier Ltd",,"School of Business Administration, Pennsylvania State University at Harrisburg, Middletown, 777 West Harrisburg PikePA, United States",,,"Pendharkar P.C., Cusatis P.","Pendharkar, P.C., School of Business Administration, Pennsylvania State University at Harrisburg, Middletown, 777 West Harrisburg PikePA, United States; Cusatis, P., School of Business Administration, Pennsylvania State University at Harrisburg, Middletown, 777 West Harrisburg PikePA, United States",,,10.1016/j.eswa.2018.02.032,SCOPUS,1,,,,,,,,,,,,,Expert Systems with Applications,Markov decision process; Multi-agent systems; Portfolio management; Reinforcement learning,,2-s2.0-85043385087,,,,,,13,1,,,,,Scopus,,,Expert Systems with Applications,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043385087&doi=10.1016%2fj.eswa.2018.02.032&partnerID=40&md5=2fd6c8ceb9934f691c18f8f4a04b1f8f,,103,,2018,,,,,,,,,,,,,,,,,,,
A multiagent approach to managing air traffic flow,"Intelligent air traffic flow management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today. FAA estimates put weather, routing decisions and airport condition induced delays at 1,682,700 h in 2007 (FAA OPSNET Data, US Department of Transportation website, http://www.faa.gov/data_statistics/), resulting in a staggering economic loss of over $41 billion (Joint Economic Commission Majority Staff, Your flight has been delayed again, 2008). New solutions to the flow management are needed to accommodate the threefold increase in air traffic anticipated over the next two decades. Indeed, this is a complex problem where the interactions of changing conditions (e. g., weather), conflicting priorities (e. g., different airlines), limited resources (e. g., air traffic controllers) and heavy volume (e. g., over 40,000 flights over the US airspace) demand an adaptive and robust solution. In this paper we explore a multiagent algorithm where agents use reinforcement learning (RL) to reduce congestion through local actions. Each agent is associated with a fix (a specific location in 2D space) and has one of three actions: setting separation between airplanes, ordering ground delays or performing reroutes. We simulate air traffic using FACET which is an air traffic flow simulator developed at NASA and used extensively by the FAA and industry. Our FACET simulations on both artificial and real historical data from the Chicago and New York airspaces show that agents receiving personalized rewards reduce congestion by up to 80% over agents receiving a global reward and by up to 90% over a current industry approach (Monte Carlo estimation). © 2010 The Author(s).",,"University of California, Santa Cruz, CA, United States; Oregon State University, Corvallis, OR, United States",,,"Agogino A.K., Tumer K.","Agogino, A.K., University of California, Santa Cruz, CA, United States; Tumer, K., Oregon State University, Corvallis, OR, United States",31,,10.1007/s10458-010-9142-5,SCOPUS,1,,,,,,,,,,1,,,Autonomous Agents and Multi-Agent Systems,Agent coordination; Air traffic control; Multiagent learning,,2-s2.0-84855309978,,,,,,25,1,,,,,Scopus,,,Autonomous Agents and Multi-Agent Systems,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855309978&doi=10.1007%2fs10458-010-9142-5&partnerID=40&md5=2a6f989a67a241ed85211b1c24408346,,24,,2012,,,,,,,,,,,,,,,,,,,
A unified control framework of HVAC system for thermal and acoustic comforts in office building,"Intelligent building system attracts more and more attention in both academic and industrial communities. Learning human comfort requirements and incorporating it into building control system is one of the important issues. In the traditional HVAC control system, the thermal comfort and the acoustic comfort are often conflicted and we lack of a scheme to trade off them well. In this paper, we propose a unified control framework based on reinforcement learning to balance the multiple dimension comforts, including the thermal and acoustic comforts. We utilize the user's complaints in thermal and acoustic sensations as feedback and combine the current environment and devices information to learn the personalized optimal control policy using online Q-learning. The challenge caused by the complaints is coped with an incorporated perception estimation scheme in the Q-learning reward design. Both simulation results and the field experimental results demonstrate the effectiveness of the algorithm, especially in the adaptivity to the individual tradeoff between thermal and acoustic comfort.",,,,"Dept. of Autom., Tsinghua Univ., Beijing, China",Y. Zhao; Q. Zhao; L. Xia; Z. Cheng; F. Wang; F. Song,,1,,10.1109/CoASE.2013.6653964,IEEE Xplore,1,,20131107,421,,Acoustics;Buildings;Estimation;Humidity;Noise;Temperature sensors;Upper bound,HVAC;architectural acoustics;building management systems;control engineering computing;learning (artificial intelligence);optimal control,HVAC system;Q-learning reward design;acoustic comforts;acoustic sensations;office building;online Q-learning;perception estimation scheme;personalized optimal control policy;reinforcement learning;thermal comforts;thermal sensations;unified control framework,,2161-8070;21618070,,17-20 Aug. 2013,,2013 IEEE International Conference on Automation Science and Engineering (CASE),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6653964,,,,,,IEEE,17,,Electronic:978-1-4799-1515-6; POD:978-1-4799-1513-2; USB:978-1-4799-1514-9,,2013 IEEE International Conference on Automation Science and Engineering (CASE),416,IEEE Conferences,,,,,2013,,,,,,,,,,,,,,,,,,,
Machine perception and intelligent control architecture for multi-robot coordination based on biological principles,"Intelligent control, inspired by biological and AI (artificial intelligence) principles, has increased the understanding of controlling complex processes without precise mathematical model of the controlled process. Through customized applications, intelligent control has demonstrated that it is a step in the right direction. However, intelligent control has yet to provide a complete solution to the problem of integrated manufacturing systems via intelligent reconfiguration of the robotics systems. The aim of this paper is to present an intelligent control architecture and design methodology based on biological principles that govern self-organization of autonomous agents. Two key structural elements of the proposed control architecture have been tested individually on key pilot applications and shown promising results. The proposed intelligent control design is inspired by observed individual and collective biological behavior in colonies of living organisms that are capable of self-organization into groups of specialized individuals capable of collectively achieving a set of prescribed or emerging objectives. The nervous and brain system in the proposed control architecture is based on reinforcement learning principles and conditioning and modeled using adaptive neurocontrollers. Mathematical control theory (e.g. optimal control, adaptive control, and neurocontrol) is used to coordinate the interactions of multiple robotics agents. ©2005 Copyright SPIE - The International Society for Optical Engineering.",,"INTELNET Inc., 200 Innovation Blvd., State College, PA 16803, United States",,,"Thomopoulos S.C.A., Braught G.","Thomopoulos, S.C.A., INTELNET Inc., 200 Innovation Blvd., State College, PA 16803, United States; Braught, G., INTELNET Inc., 200 Innovation Blvd., State College, PA 16803, United States",2,,10.1117/12.256339,SCOPUS,1,,,,,,,,,,,,,Proceedings of SPIE - The International Society for Optical Engineering,,,2-s2.0-0008675692,,,,,,72,64,,,,,Scopus,,,Proceedings of SPIE - The International Society for Optical Engineering,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0008675692&doi=10.1117%2f12.256339&partnerID=40&md5=993b0b18ad32cba330b76d38dcba839f,,2905,,1996,,,,,,,,,,,,,,,,,,,
Hedonic value of intentional action provides reinforcement for voluntary generation but not voluntary inhibition of action,"Intentional inhibition refers to stopping oneself from performing an action at the last moment, a vital component of self-control. It has been suggested that intentional inhibition is associated with negative hedonic value, perhaps due to the frustration of cancelling an intended action. Here we investigate hedonic implications of the free choice to act or inhibit. Participants gave aesthetic ratings of arbitrary visual stimuli that immediately followed voluntary decisions to act or to inhibit action. We found that participants for whom decisions to act produced a strong positive hedonic value for the immediately following visual stimulus made more choices to act than those with weaker hedonic value for action. This finding is consistent with reinforcement learning of action decisions. However, participants who experienced inhibition as generating more positive hedonic value did not choose to inhibit more than other participants. Thus, voluntary inhibition of action did not act as reinforcement for future inhibitory behaviour. Our finding that inhibition of action lacks motivational capacity may explain why self-control is both difficult and limited. © 2013.",Open Access,"Institute of Cognitive Neuroscience, University College London, London, United Kingdom",,,"Parkinson J., Haggard P.","Parkinson, J., Institute of Cognitive Neuroscience, University College London, London, United Kingdom; Haggard, P., Institute of Cognitive Neuroscience, University College London, London, United Kingdom",2,,10.1016/j.concog.2013.08.009,SCOPUS,1,,,,,,,,,,4,,,Consciousness and Cognition,Action; Decision making; Free will; Hedonic value; Inhibition; Intention; Reinforcement; Reward; Volition,,2-s2.0-84883693383,,,,,,1261,1253,,,,,Scopus,,,Consciousness and Cognition,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883693383&doi=10.1016%2fj.concog.2013.08.009&partnerID=40&md5=e2b3ce30686d657af601b4e4bfbcc088,,22,,2013,,,,,,,,,,,,,,,,,,,
Learning interface agents,"Interface agents are computer programs that employ Artificial Intelligence techniques in order to provide assistance to a user dealing with a particular computer application. The paper discusses an interface agent which has been modelled closely after the metaphor of a personal assistant. The agent learns how to assist the user by (i) observing the user's actions and imitating them, (ii) receiving user feedback when it takes wrong actions and (iii) being trained by the user on the basis of hypothetical examples. The paper discusses how this learning agent was implemented using memory-based learning and reinforcement learning techniques. It presents actual results from two proto-type agents built using these techniques: one for a meeting scheduling application and one for electronic mail. It argues that the machine learning approach to building interface agents is a feasible one which has several advantages over other approaches: it provides a customized and adaptive solution which is less costly and ensures better user acceptability. The paper also argues what the advantages are of the particular learning techniques used.",,"MIT Media Lab, Cambridge, United States",,,"Maes Pattie, Kozierok Robyn","Maes, Pattie, MIT Media Lab, Cambridge, United States; Kozierok, Robyn, MIT Media Lab, Cambridge, United States",112,,,SCOPUS,0,,,,,,,,,,,,,Proceedings of the National Conference on Artificial Intelligence,,,2-s2.0-0027708859,,,,,,465,459,,,,,Scopus,,,Proceedings of the National Conference on Artificial Intelligence,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027708859&partnerID=40&md5=df7028dac1a1660a147b9a020dc8489d,,,,1993,,,,,,,,,,,,,,,,,,,
Inverse reinforcement learning in swarm systems,"Inverse reinforcement learning (IRL) has become a useful tool for learning behavioral models from demonstration data. However, IRL remains mostly unexplored for multi-agent systems. In this paper, we show how the principle of IRL can be extended to homogeneous large-scale problems, inspired by the collective swarming behavior of natural systems. In particular, we make the following contributions to the field: 1) We introduce the swarMDP framework, a subclass of decentralized partially observable Markov decision processes endowed with a swarm characterization. 2) Exploiting the inherent homogeneity of this framework, we reduce the resulting multi-agent IRL problem to a single-agent one by proving that the agent-specific value functions in this tnodel coincide. 3) To solve the corresponding control problem, we propose a novel heterogeneous learning scheme that is particularly tailored to the swarm setting. Results on two example systems demonstrate that our framework is able to produce meaningful local reward models from which we can replicate the observed global system dynamics. © Copyright 2017, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",,"Department of Electrical Engineering and Information Technology, Technische Universität, Darmstadt, Germany",,,"Šošic A., KhudaBukhsh W.R., Zoubir A.M., Koeppl H.","Šošic, A., Department of Electrical Engineering and Information Technology, Technische Universität, Darmstadt, Germany; KhudaBukhsh, W.R., Department of Electrical Engineering and Information Technology, Technische Universität, Darmstadt, Germany; Zoubir, A.M., Department of Electrical Engineering and Information Technology, Technische Universität, Darmstadt, Germany; Koeppl, H., Department of Electrical Engineering and Information Technology, Technische Universität, Darmstadt, Germany",1,,,SCOPUS,0,,,,,,,,,,,,,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",Inverse reinforcement learning; Multi-agent systems; Swarms,,2-s2.0-85040758666,,,,,,1420,1413,,,,,Scopus,,,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040758666&partnerID=40&md5=5c6d8eb1951628959f8c41370a525801,,3,,2017,,,,,,,,,,,,,,,,,,,
"Robot self-preservation and adaptation to user preferences in game play, a preliminary study","It is expected that in a near future, personal robots will be endowed with enough autonomy to function and live in an individual's home. This is while commercial robots are designed with default configuration and factory settings which may often be different to an individual's operating preferences. This paper presents how reinforcement learning is applied and utilised towards personalisation of a robot's behaviour. Two-level reinforcement learning has been implemented: first level is in charge of energy autonomy, i.e. how to survive, and second level is involved in adapting robot's behaviour to user's preferences. In both levels Q-learning algorithm has been applied. First level actions have been learnt in a simulated environment and then the results have been transferred to the real robot. Second level has been fully implemented in the real robot and learnt by human-robot interaction. Finally, experiments showing the performance of the system are presented.",,,,"RoboticsLab at the Carlos III University of Madrid, 28911, Legan&#x00E9;s, Madrid, Spain",Á. Castro-González; F. Amirabdollahian; D. Polani; M. Malfaz; M. A. Salichs,,2,,10.1109/ROBIO.2011.6181679,IEEE Xplore,1,,20120412,2498,,Batteries;Games;Humans;Learning;Machine learning;Machine learning algorithms;Robots,human-robot interaction;learning (artificial intelligence),Q-learning algorithm;commercial robots;game play;human-robot interaction;personal robots;reinforcement learning;robot behaviour personalisation;robot self-preservation;user preferences adaptation,,,,7-11 Dec. 2011,,2011 IEEE International Conference on Robotics and Biomimetics,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6181679,,,,,,IEEE,28,,DVD:978-1-4577-2137-3; Electronic:978-1-4577-2138-0; POD:978-1-4577-2136-6,,2011 IEEE International Conference on Robotics and Biomimetics,2491,IEEE Conferences,,,,,2011,,,,,,,,,,,,,,,,,,,
<formula><tex>$mathsf{Hap-SliceR}$</tex></formula>: A Radio Resource Slicing Framework for 5G Networks With Haptic Communications,"It is expected that the emerging 5G networks will not only support diverse use cases, but also enable unprecedented applications such as haptic communications. Therefore, network slicing will provide the required design flexibility. Radio resource slicing would be an indispensable component of any network slicing solution. This paper proposes <formula><tex>$mathsf{Hap-SliceR}$</tex></formula>, which is a novel radio resource slicing framework for 5G networks with haptic communications. First, <formula><tex>$mathsf{Hap-SliceR}$</tex></formula> derives a network-wide radio resource slicing strategy for 5G networks. The optimal slicing strategy, which is based on a reinforcement learning approach, allocates radio resources to different slices while accounting for the dynamics and utility requirements of different slices. Second, <formula><tex>$mathsf{Hap-SliceR}$</tex></formula> provides customization of radio resources for haptic communications over 5G networks. The radio resource allocation requirements of haptic communications have been translated into a unique radio resource allocation problem. A low-complexity heuristic algorithm has been developed for resource allocation. Finally, a comprehensive performance evaluation of <formula><tex>$mathsf{Hap-SliceR}$</tex></formula> has been conducted based on a recently proposed 5G air-interface design.",,,,"Telecommunications Research Laboratory, Toshiba Research Europe, Ltd., Bristol, BS1 4ND, U.K.&#x00A0;(e-mail: adnan.aijaz@toshiba-trel.com).",A. Aijaz,,,,10.1109/JSYST.2017.2647970,IEEE Xplore,1,,,12,,5G mobile communication;Base stations;Dynamic scheduling;Haptic interfaces;Resource virtualization;Wireless communication,,,,1932-8184;19328184,Early Access,,,IEEE Systems Journal,5G;LTE-A;haptic communications;radio resource allocation;radio resource slicing;virtualization,,,,,20170124,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7831356,,,,,,IEEE,,,,,IEEE Systems Journal,1,IEEE Early Access Articles,,,Early Access,,2017,,,,,,,,,,,,,,,,,,,
Modeling the violation of reward maximization and invariance in reinforcement schedules,"It is often assumed that animals and people adjust their behavior to maximize reward acquisition. In visually cued reinforcement schedules, monkeys make errors in trials that are not immediately rewarded, despite having to repeat error trials. Here we show that error rates are typically smaller in trials equally distant from reward but belonging to longer schedules (referred to as ""schedule length effect""). This violates the principles of reward maximization and invariance and cannot be predicted by the standard methods of Reinforcement Learning, such as the method of temporal differences. We develop a heuristic model that accounts for all of the properties of the behavior in the reinforcement schedule task but whose predictions are not different from those of the standard temporal difference model in choice tasks. In the modification of temporal difference learning introduced here, the effect of schedule length emerges spontaneously from the sensitivity to the immediately preceding trial. We also introduce a policy for general Markov Decision Processes, where the decision made at each node is conditioned on the motivation to perform an instrumental action, and show that the application of our model to the reinforcement schedule task and the choice task are special cases of this general theoretical framework. Within this framework, Reinforcement Learning can approach contextual learning with the mixture of empirical findings and principled assumptions that seem to coexist in the best descriptions of animal behavior. As examples, we discuss two phenomena observed in humans that often derive from the violation of the principle of invariance: ""framing,"" wherein equivalent options are treated differently depending on the context in which they are presented, and the ""sunk cost"" effect, the greater tendency to continue an endeavor once an investment in money, effort, or time has been made. The schedule length effect might be a manifestation of these phenomena in monkeys.",Open Access,"National Institute of Mental Health, National Institutes of Health, Department of Health and Human Services, Bethesda, MD, United States", e1000131,,"La Camera G., Richmond B.J.","La Camera, G., National Institute of Mental Health, National Institutes of Health, Department of Health and Human Services, Bethesda, MD, United States; Richmond, B.J., National Institute of Mental Health, National Institutes of Health, Department of Health and Human Services, Bethesda, MD, United States",15,,10.1371/journal.pcbi.1000131,SCOPUS,1,,,,,,,,,,8,,,PLoS Computational Biology,,,2-s2.0-50949100487,,,,,,,,,,,,Scopus,,,PLoS Computational Biology,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50949100487&doi=10.1371%2fjournal.pcbi.1000131&partnerID=40&md5=fb5b4a256d57aeb2818488fb299d5828,,4,,2008,,,,,,,,,,,,,,,,,,,
Recent development on statistical methods for personalized medicine discovery,"It is well documented that patients can show significant heterogeneous responses to treatments so the best treatment strategies may require adaptation over individuals and time. Recently, a number of new statistical methods have been developed to tackle the important problem of estimating personalized treatment rules using single-stage or multiple-stage clinical data. In this paper, we provide an overview of these methods and list a number of challenges. © 2013 Higher Education Press and Springer-Verlag Berlin Heidelberg.",,"Department of Biostatistics and Medical Informatics, University of Wisconsin-Madison, 600 Highland Ave., Madison, WI 53792, United States; Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States",,,"Zhao Y., Zeng D.","Zhao, Y., Department of Biostatistics and Medical Informatics, University of Wisconsin-Madison, 600 Highland Ave., Madison, WI 53792, United States; Zeng, D., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States",10,,10.1007/s11684-013-0245-7,SCOPUS,1,,,,,,,,,,1,,,Frontiers of Medicine in China,dynamic treatment regimes; personalized medicine; Q-learning; reinforcement learning,,2-s2.0-84874810512,,,,,,110,102,,,,,Scopus,,,Frontiers of Medicine in China,,Review,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874810512&doi=10.1007%2fs11684-013-0245-7&partnerID=40&md5=57299256c0f308af89fd3cc6054d33dd,,7,,2013,,,,,,,,,,,,,,,,,,,
Teaching project management using a real-world group project,"It is well established that an effective pedagogy for project management requires students to get real-world experience. The challenge in providing this when teaching undergraduate engineers is the dichotomy between achieving realism and maintaining sufficient simplicity to make the course tractable. A real-world group technology project at Victoria University of Wellington (VUW) in New Zealand establishes essential non-technical attributes required by the engineering profession while covering key elements of the project management body of knowledge (PMBOK). This paper first shows how the project covers the knowledge required in project management and then presents the results of two years of data collected from students' reflection on their own learning. We have established a pleasing congruence across the years against the specific learning topics of team working skills, communication skills and personal working skills with an improvement in project management skills. A key finding emerging from our analysis is the importance of reinforcement learning and reflective learning. We show a key link between these two learning mechanisms and the project pedagogy. Further analysis shows the link between the project pedagogy and four skill areas acquired. Finally, our research has identified specific areas for us to focus on for subsequent years.",,,,"School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand",L. Collingbourne; W. K. G. Seah,,1,,10.1109/FIE.2015.7344301,IEEE Xplore,1,,20151207,8,,Cultural differences;Education;Monitoring;Project management;Software;Software engineering,educational courses;engineering education;further education;learning (artificial intelligence);project management;team working,New Zealand;PMBOK;VUW;Victoria University of Wellington;communication skills;dichotomy;educational course;effective pedagogy;personal working skills;project management body of knowledge;real-world group project;reflective learning;reinforcement learning;teaching project management;team working skills;undergraduate engineers teaching,,,,21-24 Oct. 2015,,2015 IEEE Frontiers in Education Conference (FIE),graduate attributes;project management;real-world project,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344301,,,,,,IEEE,26,,Paper:978-1-4799-8454-1,,2015 IEEE Frontiers in Education Conference (FIE),1,IEEE Conferences,,,,,2015,,,,,,,,,,,,,,,,,,,
Intelligent feedback polarity and timing selection in the Shufti Intelligent Tutoring System,"It is well known that the training of medical students is a long and arduous process. Students master many areas of knowledge in a relatively short amount of time in order to become experts in their chosen field. The Socratic Method used in the latter stages of medical education, where a physician directly monitors a group of students, is inherently restrictive due to the limited number of cases and length of the students' rotations. Innovative Intelligent Tutoring techniques offer a solution to this problem. This paper outlines the overall structure and design of Shufti, an Intelligent Tutoring System (ITS) focused on mammography and medical imaging. Shufti's aim is to provide medical students with an improved learning environment, exposing them to a broad range of examples supported by customized feedback and hints driven by an adaptive Reinforcement Learning system and Clustering Techniques.",,"Alberta Innovates Center for Machine Learning, University of Alberta, Canada",,,"Johnson S., Zaiane O.","Johnson, S., Alberta Innovates Center for Machine Learning, University of Alberta, Canada; Zaiane, O., Alberta Innovates Center for Machine Learning, University of Alberta, Canada",1,,,SCOPUS,0,,,,,,,,,,,,,"Proceedings of the 20th International Conference on Computers in Education, ICCE 2012",Breast cancer; Data mining; Feedback; Hints; Intelligent Tutoring System; Machine learning; Reinforcement learning; Serious games,,2-s2.0-84896328874,,,,,,60,56,,,,,Scopus,,,"Proceedings of the 20th International Conference on Computers in Education, ICCE 2012",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896328874&partnerID=40&md5=2f69d64ebf1e2474954ed1c9e97c26e4,,,,2012,,,,,,,,,,,,,,,,,,,
Self-learning system for personalized e-learning,"Knowledge is a basic requirement in the era of globalization. The act of acquiring knowledge is learning and with the advent of technology, learning has become easier. E-Learning is a popular learning method which uses the web or other technologies to promote learning. A very traditional learning method is discussed and some new methods with the integration of modern technology have been discussed. Although technologies like artificial intelligence, cloud computing, ontology, fuzzy logic etc. have been used in learning systems they are still less automated, not personalized and do not pose the capability to self-learn. A method is proposed to increase the automation and self-learning in the e-learning management systems. Further, the personalization of learning for the user so that every type of learner can learn from the type and level of the content the learner seems fit is the main aim. The proposed system is feasible, economical and scalable which makes it suitable for every level.",,,,"Dept. of Computer Science & Engineering, Graphic Era University, Dehradun, India",V. Pant; S. Bhasin; S. Jain,,,,10.1109/ICETCCT.2017.8280344,IEEE Xplore,1,,20180205,6,,Crawlers;Electronic learning;Learning management systems;Semantics;Task analysis;Videos,cloud computing;computer aided instruction;fuzzy logic;ontologies (artificial intelligence),artificial intelligence;cloud computing;e-learning management systems;fuzzy logic;learning systems;ontology;personalized e-learning;self-learning system,,,,17-18 Nov. 2017,,2017 International Conference on Emerging Trends in Computing and Communication Technologies (ICETCCT),Machine learning;e-learning system;learning management system;reinforcement learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8280344,,,,,,IEEE,,,CD:978-1-5386-1146-3; Electronic:978-1-5386-1147-0; POD:978-1-5386-1148-7,,2017 International Conference on Emerging Trends in Computing and Communication Technologies (ICETCCT),1,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
Learning to dribble on a real robot by success and failure,"Learning directly on real world systems such as autonomous robots is a challenging task, especially if the training signal is given only in terms of success or failure (reinforcement learning). However, if successful, the controller has the advantage of being tailored exactly to the system it eventually has to control. Here we describe, how a neural network based RL controller learns the challenging task of ball dribbling directly on our middle-size robot. The learned behaviour was actively used throughout the RoboCup world championship tournament 2007 in Atlanta, where we won the first place. This constitutes another important step within our Brainstormers project. The goal of this project is to develop an intelligent control architecture for a soccer playing robot, that is able to learn more and more complex behaviours from scratch.",,,,"Dept. of Mathematics and Informatics, Institute of Computer Science and Institute of Cognitive Science, University of Osnabr&#252;ck, Germany",M. Riedmiller; R. Hafner; S. Lange; M. Lauer,,2,,10.1109/ROBOT.2008.4543536,IEEE Xplore,1,,20080613,2208,,Biological neural networks;Cognitive robotics;Control systems;Informatics;Intelligent robots;Learning;Mathematics;Robotics and automation;System testing;USA Councils,control engineering computing;learning (artificial intelligence);learning systems;mobile robots;multi-robot systems;neurocontrollers,Brainstormers project;RoboCup;intelligent control;middle-size robot;neural network;reinforcement learning;robot learning;soccer playing robot,,1050-4729;10504729,,19-23 May 2008,,2008 IEEE International Conference on Robotics and Automation,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4543536,,,,,,IEEE,7,,POD:978-1-4244-1646-2,,2008 IEEE International Conference on Robotics and Automation,2207,IEEE Conferences,,,,,2008,,,,,,,,,,,,,,,,,,,
Learning games for the cognitively impaired people,"Learning environments have been profoundly reshaped by pervasive technology. New educational methodologies take full advantage of ICT in a mobile customized user-friendly environment, to support learning and stimulate individuals'potential. Unfortunately, technology-enhanced learning tools are not often designed with accessibility in mind, although they can greatly benefit the personal empowerment and inclusion of special-needs people. To address this gap, a Web platform has been created for delivering accessible games to people with Down syndrome. Since personalization, orderliness and positive reinforcement are crucial to learning in these subjects, the platform offers a personalized safe environment for learning, conforming to behavioral analysis principles. Learning analytics are incorporated in the platform for easy monitoring of student progress via Web interfaces. The participatory design driving the development of the learning platform allowed the customization of the games'discriminative stimuli, difficulty levels and reinforcement, as well as the creation of a game ""engine"" to easily set up new personalized exercises. These customization features make the game platform usable by a larger audience, including individuals with learning difficulties and autism. © 2016 ACM.",,"CNR-IIT, Via Moruzzi, 1, Pisa, Italy", a30,,"Buzzi M.C., Buzzi M., Perrone E., Rapisarda B., Senette C.","Buzzi, M.C., CNR-IIT, Via Moruzzi, 1, Pisa, Italy; Buzzi, M., CNR-IIT, Via Moruzzi, 1, Pisa, Italy; Perrone, E., CNR-IIT, Via Moruzzi, 1, Pisa, Italy; Rapisarda, B., CNR-IIT, Via Moruzzi, 1, Pisa, Italy; Senette, C., CNR-IIT, Via Moruzzi, 1, Pisa, Italy",,,10.1145/2899475.2899487,SCOPUS,1,,,,,,,,,,,,,W4A 2016 - 13th Web for All Conference,Cognitive games; Computer-enhanced learning; People with special needs; Web applications,,2-s2.0-84983554779,,,,,,,,,,,,Scopus,,,W4A 2016 - 13th Web for All Conference,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983554779&doi=10.1145%2f2899475.2899487&partnerID=40&md5=a718dc59778be489c52825e92098f1e0,,,,2016,,,,,,,,,,,,,,,,,,,
A Recursive Dialogue Game for Personalized Computer-Aided Pronunciation Training,"Learning languages in addition to the native language is very important for all people in the globalized world today, and computer-aided pronunciation training (CAPT) is attractive since the software can be used anywhere at any time, and repeated as many times as desired. In this paper, we introduce the immersive interaction scenario offered by spoken dialogues to CAPT by proposing a recursive dialogue game to make CAPT personalized. A number of tree-structured sub-dialogues are linked sequentially and recursively as the script for the game. The system policy at each dialogue turn is to select in real-time along the dialogue the best training sentence for each specific individual learner within the dialogue script, considering the learner's learning status and the future possible dialogue paths in the script, such that the learner can have the scores for all pronunciation units considered reaching a predefined standard in a minimum number of turns. The purpose here is that those pronunciation units poorly produced by the specific learner can be offered with more practice opportunities in the future sentences along the dialogue, which enables the learner to improve the pronunciation without having to repeat the same training sentences many times. This makes the learning process for each learner completely personalized. The dialogue policy is modeled by Markov decision process (MDP) with high-dimensional continuous state space, and trained with fitted value iteration using a huge number of simulated learners. These simulated leaners have the behavior similar to real learners, and were generated from a corpus of real learner data. The experiments demonstrated very promising results and a real cloud-based system is also successfully implemented.",,,,"Department of Engineering, University of Cambridge, Cambridge, United Kingdom",P. h. Su; C. h. Wu; L. s. Lee,,2,,10.1109/TASLP.2014.2375572,IEEE Xplore,1,,20150114,141,,Computers;Games;Markov processes;Software;Speech;Speech processing;Training,Markov processes;cloud computing;computer based training;computer games;interactive systems;linguistics;real-time systems,MDP;Markov decision process;dialogue paths;dialogue policy;dialogue script;fitted value iteration;game script;high-dimensional continuous state space;immersive interaction scenario;language learning;learner learning status;native language;personalized CAPT;personalized computer-aided pronunciation training;pronunciation improvement;pronunciation unit scores;real cloud-based system;real learners;recursive dialogue game;sequentially-recursively linked tree-structured subdialogues;simulated leaners;spoken dialogues;system policy;training sentence,,2329-9290;23299290,1,Jan. 2015,,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",Computer-aided pronunciation training (CAPT);Markov decision process;computer-assisted language learning;dialogue game;reinforcement learning,,,,,20141202,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6971195,,,,,,IEEE,52,,,,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",127,IEEE Journals & Magazines,,,23,,2015,,,,,,,,,,,,,,,,,,,
Learn to Coordinate with Generic Non-Stationary Opponents,"Learning to coordinate with non-stationary opponents is a major challenge for adaptive agents. Most previous research investigated only restricted classes of such dynamic opponents. The main contribution of this paper is twofold: (i) A class of generic non-stationary opponents is introduced. The opponents keep mixed strategies which change with less regularity. Its showed that the independent reinforcement learners (ILs), which have neither prior knowledge nor opponent models, cannot coordinate well with this type of opponent, (ii) A new exploration strategy, the DAE (detect and explore) mechanism, is tailored for the ILs in such coordination tasks. This mechanism allows the ILs dynamically detect changes in the opponents behavior and adjust their learning rate and exploration temperature. It's showed that ILs using this strategy are still able to converge in self-play and are able to coordinate well with the non-stationary opponents",,,,"Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China, zkf03@mails.tsinghua.edu.cn",Z. Kaifu,,1,,10.1109/COGINF.2006.365546,IEEE Xplore,1,,20070910,565,,Cognitive informatics;Computer science;Decision making;Game theory;Learning;Multiagent systems;Temperature;Testing,learning (artificial intelligence);multi-agent systems,adaptive agents;detect and explore mechanism;dynamic opponents;exploration strategy;generic nonstationary opponents;independent reinforcement learner,,,,17-19 July 2006,,2006 5th IEEE International Conference on Cognitive Informatics,Coordination game;Exploration strategy;Non-stationary opponent;Reinforcement learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4216463,,,,,,IEEE,16,,POD:1-4244-0475-4,,2006 5th IEEE International Conference on Cognitive Informatics,558,IEEE Conferences,,,1,,2006,,,,,,,,,,,,,,,,,,,
Automatic navigation among mobile DTV services,"Limited number of input buttons on a mobile device, such as mobile phones and PDAs, restricts people's access to digital broadcast services. In this paper, we present a reinforcement learning approach to automatically navigating among services in mobile digital television systems. Our approach uses standard Q-learning algorithm as a theory basis to predict next button for the user by learning usage patterns from interaction experiences. We did the experiment using a modified algorithm in test system. The experimental results demonstrate that the performance is good and the method is feasible and appropriate in practice.",,"Telecom. Software Multimedia Lab., Dept. of Compter Sci. and Eng., Helsinki University of Technology, P.O. Box 5400, FIN-02015, Finland",,,"Peng C., Vuorimaa P.","Peng, C., Telecom. Software Multimedia Lab., Dept. of Compter Sci. and Eng., Helsinki University of Technology, P.O. Box 5400, FIN-02015, Finland; Vuorimaa, P., Telecom. Software Multimedia Lab., Dept. of Compter Sci. and Eng., Helsinki University of Technology, P.O. Box 5400, FIN-02015, Finland",,,,SCOPUS,0,,,,,,,,,,,,,ICEIS 2004 - Proceedings of the Sixth International Conference on Enterprise Information Systems,Automatic navigation; Button prediction; Exploration; Intelligent user interface; Reinforcement learning,,2-s2.0-8444233936,,,,,,145,140,,,,,Scopus,,,ICEIS 2004 - Proceedings of the Sixth International Conference on Enterprise Information Systems,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-8444233936&partnerID=40&md5=61d5a01b971e0282eec404d1fbe43bb2,,,,2004,,,,,,,,,,,,,,,,,,,
Learning to Ground in Spoken Dialogue Systems,"Machine learning methods such as reinforcement learning applied to dialogue strategy optimization has become a leading subject of researches since the mid 90's. Indeed, the great variability of factors to take into account makes the design of a spoken dialogue system a tailoring task and reusability of previous work is very difficult. Yet, techniques such as reinforcement learning are very demanding in training data while obtaining a substantial amount of data in the particular case of spoken dialogues is time-consuming and therefore expansive. In order to expand existing data sets, dialogue simulation techniques are becoming a standard solution. In this paper, we present a user model for realistic spoken dialogue simulation and a method for using this model so as to simulate the grounding process. This allows including grounding subdialogues as actions in the reinforcement learning process and learning adapted strategy.",,,,"&#201;cole Sup&#233;rieure d'&#201;lectricit&#233; - SUP&#201;LEC, Metz Campus - IMS Team, 2 rue &#201;douard Belin - F-57070 Metz - FRANCE. e-mail: olivier.pietquin@supelec.fr",O. Pietquin,,2,,10.1109/ICASSP.2007.367189,IEEE Xplore,1,,20070604,IV-168,,Automatic speech recognition;Grounding;Learning systems;Machine learning;Man machine systems;Optimization methods;Space exploration;Speech processing;Speech synthesis;Stochastic processes,interactive systems;speech-based user interfaces;unsupervised learning,dialogue simulation techniques;grounding process;realistic spoken dialogue simulation;reinforcement learning;spoken dialogue systems,,1520-6149;15206149,,15-20 April 2007,,"2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07",Speech Communication;Unsupervised Learning;User Modelling,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218063,,,,,,IEEE,15,,CD-ROM:1-4244-0728-1; POD:1-4244-0727-3,,"2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07",IV-165,IEEE Conferences,,,4,,2007,,,,,,,,,,,,,,,,,,,
Adaptive hierarchical resource management for satellite channel in hybrid MANET-satellite-Internet network,"MANETs are often deployed in an infrastructure-less or hostile region where the satellite provides the only link for the MANETs to communicate with the rest of the world. It faces many challenges to support multiple serviced communications between MANETs and Internet through satellite. In this paper, we propose an efficient resource management scheme called AHRM to dynamically allocate bandwidths among multiple MANET users and multiple priority and non-priority services sharing a multi-access satellite channel. It uses a flexible hierarchical structure to exploit the channel utility and resolve contention from two levels. A bandwidth adaptation algorithm is designed to adjust the allocation dynamically in response to traffic and link status changes. The algorithm turns out to be in line with reinforcement learning and is a customized version of it for the practical satellite network setting. Implementation issues are discussed. Simulation results are presented, showing that the scheme can guarantee fast delivery of critical messages in spite of channel contention, and significantly improve the performance of multiple services.",,,,"Inst. for Syst. Res., Maryland Univ., College Park, MD, USA",N. X. Liu; Xiaoming Zhou; J. S. Baras,,3,,10.1109/VETECF.2004.1404834,IEEE Xplore,1,,20050425,4031 Vol. 6,,Algorithm design and analysis;Artificial satellites;Bandwidth;Delay;Educational institutions;Intelligent networks;Mobile ad hoc networks;Resource management;Telecommunication traffic;Web and internet services,IntServ networks;Internet;ad hoc networks;channel allocation;learning (artificial intelligence);mobile satellite communication;multi-access systems,AHRM;adaptive hierarchical resource management;adaptive hierarchical scheduling;bandwidth adaptation;contention resolution;dynamic bandwidth allocation;hybrid MANET-satellite-Internet network;link status;mobile wireless ad hoc network;multiaccess satellite channel;multiple priority services;multiple service communications;reinforcement learning;traffic status,,1090-3038;10903038,,26-29 Sept. 2004,,"IEEE 60th Vehicular Technology Conference, 2004. VTC2004-Fall. 2004",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1404834,,,,,,IEEE,12,,POD:0-7803-8521-7,,"IEEE 60th Vehicular Technology Conference, 2004. VTC2004-Fall. 2004",4027,IEEE Conferences,,,6,,2004,,,,,,,,,,,,,,,,,,,
Towards the development of a soft manipulator as an assistive robot for personal care of elderly people,"Manipulators based on soft robotic technologies exhibit compliance and dexterity which ensures safe human-robot interaction. This article is a novel attempt at exploiting these desirable properties to develop a manipulator for an assistive application, in particular, a shower arm to assist the elderly in the bathing task. The overall vision for the soft manipulator is to concatenate three modules in a serial manner such that (i) the proximal segment is made up of cable-based actuation to compensate for gravitational effects and (ii) the central and distal segments are made up of hybrid actuation to autonomously reach delicate body parts to perform the main tasks related to bathing. The role of the latter modules is crucial to the application of the system in the bathing task; however, it is a nontrivial challenge to develop a robust and controllable hybrid actuated system with advanced manipulation capabilities and hence, the focus of this article. We first introduce our design and experimentally characterize its functionalities, which include elongation, shortening, omnidirectional bending. Next, we propose a control concept capable of solving the inverse kinetics problem using multiagent reinforcement learning to exploit these functionalities despite high dimensionality and redundancy. We demonstrate the effectiveness of the design and control of this module by demonstrating an open-loop task space control where it successfully moves through an asymmetric 3-D trajectory sampled at 12 points with an average reaching accuracy of 0.79 cm ± 0.18 cm. Our quantitative experimental results present a promising step toward the development of the soft manipulator eventually contributing to the advancement of soft robotics. © The Author(s) 2017.",,"BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy; Flowers Lab., Inria, France",,,"Ansari Y., Manti M., Falotico E., Mollard Y., Cianchetti M., Laschi C.","Ansari, Y., BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy; Manti, M., BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy; Falotico, E., BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy; Mollard, Y., Flowers Lab., Inria, France; Cianchetti, M., BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy; Laschi, C., BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy",,,10.1177/1729881416687132,SCOPUS,1,,,,,,,,,,2,,,International Journal of Advanced Robotic Systems,Assistive robotics; Machine learning; Robot control; Soft robotics,,2-s2.0-85018369528,,,,,,,,,,,,Scopus,,,International Journal of Advanced Robotic Systems,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018369528&doi=10.1177%2f1729881416687132&partnerID=40&md5=e3024e90f697abb148e6c74993638654,,14,,2017,,,,,,,,,,,,,,,,,,,
Illustration about a Metacognition-based learning detection system conceived to improve web-based self-regulated learning,"Many learners have found that it is difficult to complete their web-based learning plan. Once on the Internet, they can't help browsing more interesting Web pages instead of continuing to do their learning tasks. This situation we called Information Trek. To solve this problem, this study proposes an learning detection system which can discover whether the contents of a web page a student viewing is about learning or not. If a student is detected to be in the state of viewing the non-learning pages, then the alert reinforcement window will be shown. If the attentive time in learning has been reached, then encouraging reinforcement feedback is given. We must consider adequately about personalization given the different levels of Metacognition. In this system, preferring the method to let learner go back to the learning state themselves, we design some functions to guide learners regulate themselves based on the essential process of online self-regulated leaning integrating Metacognitive process.",,,,"Institute of Educational Technology, Institute of Education, Tsinghua University, Beijing, China",J. Liang,,0,,10.1109/ICEBEG.2011.5886854,IEEE Xplore,1,,20110616,4,,Artificial neural networks;Conferences;Educational technology;Filtering;Helium;Text categorization;Web pages,,,,,,6-8 May 2011,,2011 International Conference on E-Business and E-Government (ICEE),learning detection system;metacognitive strategies;metacogniton;web-based self-regulated learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5886854,,,,,,IEEE,10,,Electronic:978-1-4244-8694-6; POD:978-1-4244-8691-5,,2011 International Conference on E-Business and E-Government (ICEE),1,IEEE Conferences,,,,,2011,,,,,,,,,,,,,,,,,,,
Learning Bayesian networks probabilities from longitudinal data,"Many real applications of Bayesian networks (BN) concern problems in which several observations are collected over time on a certain number of similar plants. This situation is typical of the context of medical monitoring, in which several measurements of the relevant physiological quantities are available over time on a population of patients under treatment, and the conditional probabilities that describe the model are usually obtained from the available data through a suitable learning algorithm. In situations with small data sets for each plant, it is useful to reinforce the parameter estimation process of the BN by taking into account the observations obtained from other similar plants. On the other hand, a desirable feature to be preserved is the ability to learn individualized conditional probability tables, rather than pooling together all the available data. In this work we apply a Bayesian hierarchical model able to preserve individual parameterization, and, at the same time, to allow the conditionals of each plant to borrow strength from all the experience contained in the data-base. A testing example and an application in the context of diabetes monitoring will be shown",,,,"Lab. di Inf. Medica, Pavia Univ., Italy",R. Bellazzi; A. Riva,,2,,10.1109/3468.709608,IEEE Xplore,1,,20020806,636,,Bayesian methods;Biomedical monitoring;Condition monitoring;Context modeling;Diabetes;Medical treatment;Parameter estimation;Patient monitoring;Testing;Time measurement,Bayes methods;learning (artificial intelligence);observers;parameter estimation;patient diagnosis;probability,BN;Bayesian hierarchical model;Bayesian networks probability learning;conditional probabilities;diabetes monitoring;individualized conditional probability tables;longitudinal data;medical monitoring;parameter estimation process reinforcement,,1083-4427;10834427,5,Sep 1998,,"IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=709608,,,,,,IEEE,19,,,,"IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans",629,IEEE Journals & Magazines,,,28,,1998,,,,,,,,,,,,,,,,,,,
Imitation Learning for Dynamic VFI Control in Large-Scale Manycore Systems,"Manycore chips are widely employed in high-performance computing and large-scale data analysis. However, the design of high-performance manycore chips is dominated by power and thermal constraints. In this respect, voltage-frequency island (VFI) is a promising design paradigm to create scalable energy-efficient platforms. By dynamically tailoring the voltage and frequency of each island, we can further improve the energy savings within given performance constraints. Inspired by the recent success of imitation learning (IL) in many application domains and its significant advantages over reinforcement learning (RL), we propose the first architecture-independent IL-based methodology for dynamic VFI (DVFI) control in manycore systems. Due to its popularity in the EDA community, we consider an RL-based DVFI control methodology as a strong baseline. Our experimental results demonstrate that IL is able to obtain higher quality policies than RL (on average, 5% less energy with the same level of performance) with significantly less computation time and hardware area overheads (3.1X and 8.8X, respectively).",,,,"Carnegie Mellon University, Pittsburgh, PA, USA",R. G. Kim; W. Choi; Z. Chen; J. R. Doppa; P. P. Pande; D. Marculescu; R. Marculescu,,1,,10.1109/TVLSI.2017.2700726,IEEE Xplore,1,,20170823,2471,10.13039/100000001 - U.S. National Science Foundation; ,Control systems;Energy consumption;Energy dissipation;Hardware;Resource management;Scalability;Very large scale integration,energy conservation;learning (artificial intelligence);microprocessor chips;multiprocessing systems;parallel processing;performance evaluation;power aware computing,DVFI control;EDA community;RL-based DVFI control;architecture-independent IL-based methodology;dynamic VFI control;dynamic voltage and frequency scaling;energy savings;high-performance computing;high-performance manycore chips;imitation learning;large-scale data analysis;large-scale manycore systems;performance constraints;power constraints;reinforcement learning;scalable energy-efficient platforms;thermal constraints;voltage-frequency island,,1063-8210;10638210,9,Sept. 2017,,IEEE Transactions on Very Large Scale Integration (VLSI) Systems,Dynamic voltage and frequency scaling (DVFS);low power;machine learning (ML);manycore systems;voltage-frequency islands (VFIs),,,,,20170526,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7934357,,,,,,IEEE,,,,,IEEE Transactions on Very Large Scale Integration (VLSI) Systems,2458,IEEE Journals & Magazines,,,25,,2017,,,,,,,,,,,,,,,,,,,
Optimal medication dosing from suboptimal clinical examples: A deep reinforcement learning approach,"Misdosing medications with sensitive therapeutic windows, such as heparin, can place patients at unnecessary risk, increase length of hospital stay, and lead to wasted hospital resources. In this work, we present a clinician-in-the-loop sequential decision making framework, which provides an individualized dosing policy adapted to each patient's evolving clinical phenotype. We employed retrospective data from the publicly available MIMIC II intensive care unit database, and developed a deep reinforcement learning algorithm that learns an optimal heparin dosing policy from sample dosing trails and their associated outcomes in large electronic medical records. Using separate training and testing datasets, our model was observed to be effective in proposing heparin doses that resulted in better expected outcomes than the clinical guidelines. Our results demonstrate that a sequential modeling approach, learned from retrospective data, could potentially be used at the bedside to derive individualized patient dosing policies.",,,,"Dept. of Biomedical Informatics, Emory University, Atlanta, GA 30322",S. Nemati; M. M. Ghassemi; G. D. Clifford,,1,,10.1109/EMBC.2016.7591355,IEEE Xplore,1,,20161018,2981,,Cost function;Drugs;Feature extraction;Hospitals;Learning (artificial intelligence);Time series analysis;Training,decision making;drugs;electronic health records;learning (artificial intelligence);patient treatment,MIMIC II intensive care unit database;clinical guidelines;clinical phenotype;clinician-in-the-loop sequential decision making framework;deep reinforcement learning algorithm;electronic medical records;hospital stay length;individualized dosing policy;individualized patient dosing policies;optimal heparin dosing policy;optimal medication dosing;retrospective data;sample dosing trails;sensitive therapeutic windows;sequential modeling approach;suboptimal clinical examples;testing datasets;training datasets,,1557-170X;1557170X,,16-20 Aug. 2016,,2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),,,,,"Algorithms;Databases, Factual;Dose-Response Relationship, Drug;Heparin;Humans;Learning;Length of Stay;Markov Chains;Reinforcement (Psychology);Retrospective Studies",,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7591355,,,,,1,IEEE,,,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,,2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),2978,IEEE Conferences,,,,,2016,,,,,,,,,,,,,,,,,,,
Context-aware reinforcement learning-based mobile cloud computing for telemonitoring,"Mobile cloud computing (MCC) has been extensively studied to provide pervasive healthcare services in a more affordable manner. Through offloading computation-intensive tasks from mobile to cloud, a significant portion of energy can be saved to extend the mobile battery life, which is critical to maintaining continuous and uninterrupted healthcare services. However, given the ever-changing clinical severity, personal demands, and environmental conditions, it is essential to explore context-aware approach capable of dynamically determining the optimal task offloading strategies and algorithmic settings, with the goal of achieving a balanced trade-off among energy efficiency, diagnostic accuracy, and processing latency. To this aim, we propose a model-free reinforcement learning based task scheduling approach to adapt to the changing requirements.",,,,"Department of Electrical and Computer Engineering, Binghamton University, State University of New York, Binghamton, NY, 13902 USA",X. Wang; W. Wang; Z. Jin,,,,10.1109/BHI.2018.8333459,IEEE Xplore,1,,20180409,429,,Batteries;Cloud computing;Electrocardiography;Hidden Markov models;Medical services;Monitoring;Task analysis,cloud computing;health care;learning (artificial intelligence);mobile computing;scheduling,affordable manner;context-aware approach capable;context-aware reinforcement learning;continuous healthcare services;mobile battery life;mobile cloud;model-free reinforcement learning;offloading computation-intensive tasks;optimal task offloading strategies;pervasive healthcare services;task scheduling approach;uninterrupted healthcare services,,,,4-7 March 2018,,2018 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8333459,,,,,,IEEE,,,Electronic:978-1-5386-2405-0; POD:978-1-5386-2406-7,,2018 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI),426,IEEE Conferences,,,,,2018,,,,,,,,,,,,,,,,,,,
A Location Management Scheme with Reinforcement Learning for PCS Networks,"Mobility management, consisting of location management and handoff management, is a challenge for personal communication services (PCS) networks. Location management mainly involves two operations: location update and paging. A two-stage paging strategy with reinforcement learning is proposed. Under this scheme, each cell in a location area is given a preference. Besides, a small real number is used to decide whether to choose and page cells with higher preferences or with lower ones during the first paging stage so that the scheme can quickly adapt to the moving pattern of mobiles. Simulation results show that the proposed paging strategy can reduce the cost as much as 25%, compared with the paging strategy of the basic location management scheme used in the existing PCS networks.",,,,"College of Information Engineering, Zhejiang University of Technology, Hangzhou, China 310032, phone: +86-571-88320163; fax: +86-571-88320163; e-mail: yhzhu@zjut.edu.cn",Y. h. Zhu; G. Xiao; Z. y. Li; F. Zhu,,0,,10.1109/ICNSC.2007.372781,IEEE Xplore,1,,20070625,227,,Communication switching;Conference management;Databases;Learning;Mobile communication;Mobile radio mobility management;Paging strategies;Personal communication networks;Personal digital assistants;Wireless sensor networks,learning (artificial intelligence);mobile computing;mobility management (mobile radio);personal communication networks,handoff management;location management scheme;mobility management;paging strategy;personal communication services;reinforcement learning,,,,15-17 April 2007,,"2007 IEEE International Conference on Networking, Sensing and Control",Personal communication service (PCS) networks;location management;mobility management,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4238994,,,,,,IEEE,22,,CD-ROM:1-4244-1076-2; POD:1-4244-1075-4,,"2007 IEEE International Conference on Networking, Sensing and Control",224,IEEE Conferences,,,,,2007,,,,,,,,,,,,,,,,,,,
Knowledge transfer between speakers for personalised dialogue management,"Model-free reinforcement learning has been shown to be a promising data driven approach for automatic dialogue policy optimization, but a relatively large amount of dialogue interactions is needed before the system reaches reasonable performance. Recently, Gaussian process based reinforcement learning methods have been shown to reduce the number of dialogues needed to reach optimal performance, and pre-training the policy with data gathered from different dialogue systems has further reduced this amount. Following this idea, a dialogue system designed for a single speaker can be initialised with data from other speakers, but if the dynamics of the speakers are very different the model will have a poor performance. When data gathered from different speakers is available, selecting the data from the most similar ones might improve the performance. We propose a method which automatically selects the data to transfer by defining a similarity measure between speakers, and uses this measure to weight the influence of the data from each speaker in the policy model. The methods are tested by simulating users with different severities of dysarthria interacting with a voice enabled environmental control system.. © 2015 Association for Computational Linguistics.",,"Department of Computer Science, University of Sheffield, United Kingdom",,,"Casanueva I., Hain T., Christensen H., Marxer R., Green P.","Casanueva, I., Department of Computer Science, University of Sheffield, United Kingdom; Hain, T., Department of Computer Science, University of Sheffield, United Kingdom; Christensen, H., Department of Computer Science, University of Sheffield, United Kingdom; Marxer, R., Department of Computer Science, University of Sheffield, United Kingdom; Green, P., Department of Computer Science, University of Sheffield, United Kingdom",5,,,SCOPUS,0,,,,,,,,,,,,,"SIGDIAL 2015 - 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Proceedings of the Conference",,,2-s2.0-84988391769,,,,,,21,12,,,,,Scopus,,,"SIGDIAL 2015 - 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Proceedings of the Conference",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988391769&partnerID=40&md5=dfddd45f3d809c20ad6bebb66483451d,,,,2015,,,,,,,,,,,,,,,,,,,
Gaussian processes for fast policy optimisation of POMDP-based dialogue managers,"Modelling dialogue as a Partially Observable Markov Decision Process (POMDP) enables a dialogue policy robust to speech understanding errors to be learnt. However, a major challenge in POMDP policy learning is to maintain tractability, so the use of approximation is inevitable. We propose applying Gaussian Processes in Reinforcement learning of optimal POMDP dialogue policies, in order (1) to make the learning process faster and (2) to obtain an estimate of the uncertainty of the approximation. We first demonstrate the idea on a simple voice mail dialogue task and then apply this method to a real-world tourist information dialogue task. © 2010 Association for Computational Linguistics.",,"Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom",,,"Gasic M., Jurčíček F., Keizer S., Mairesse F., Thomson B., Yu K., Young S.","Gasic, M., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom; Jurčíček, F., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom; Keizer, S., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom; Mairesse, F., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom; Thomson, B., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom; Yu, K., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom; Young, S., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom",33,,,SCOPUS,0,,,,,,,,,,,,,Proceedings of the SIGDIAL 2010 Conference: 11th Annual Meeting of the Special Interest Group onDiscourse and Dialogue,,,2-s2.0-84857755225,,,,,,204,201,,,,,Scopus,,,Proceedings of the SIGDIAL 2010 Conference: 11th Annual Meeting of the Special Interest Group onDiscourse and Dialogue,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857755225&partnerID=40&md5=53959c32cfe4d3fe63d3cc1543d0da65,,,,2010,,,,,,,,,,,,,,,,,,,
Encouraging physical activity in patients with diabetes through automatic personalized feedback via reinforcement learning improves glycemic control,"Most patients with type 2 diabetes are sedentary despite the clear benefit of regular physical activity, including better glucose control and improvement in quality of life (1). Smartphones could potentially improve patient care by continual communication with patients and sensors …",,"Rambam Health Care Campus, Haifa, Israel; Faculty of Medicine, Technion-Israel Institute of Technology, Haifa, Israel; Faculty of Electrical Engineering, Technion-Israel Institute of Technology, Haifa, Israel; Faculty of Industrial Engineering and Management, Technion-Israel Institute of Technology, Haifa, Israel; Microsoft Research, Herzliya, Israel",,,"Hochberg I., Feraru G., Kozdoba M., Mannor S., Tennenholtz M., Yom-Tov E.","Hochberg, I., Rambam Health Care Campus, Haifa, Israel; Feraru, G., Faculty of Medicine, Technion-Israel Institute of Technology, Haifa, Israel; Kozdoba, M., Faculty of Electrical Engineering, Technion-Israel Institute of Technology, Haifa, Israel; Mannor, S., Faculty of Electrical Engineering, Technion-Israel Institute of Technology, Haifa, Israel; Tennenholtz, M., Faculty of Industrial Engineering and Management, Technion-Israel Institute of Technology, Haifa, Israel; Yom-Tov, E., Microsoft Research, Herzliya, Israel",6,,10.2337/dc15-2340,SCOPUS,1,,,,,,,,,,4,,,Diabetes Care,,,2-s2.0-84962053103,,,,,,e60,e59,,,,,Scopus,,,Diabetes Care,,Letter,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962053103&doi=10.2337%2fdc15-2340&partnerID=40&md5=6a9f3b67dee5f793093684afee87229a,,39,,2016,,,,,,,,,,,,,,,,,,,
Randomized allocation with arm elimination in a bandit problem with covariates,"Motivated by applications in personalized web services and clinical research, we consider a multi-armed bandit problem in a setting where the mean reward of each arm is associated with some covariates. A multi-stage randomized allocation with arm elimination algorithm is proposed to combine the flexibility in reward function modeling and a theoretical guarantee of a cumulative regret minimax rate. When the function smoothness parameter is unknown, the algorithm is equipped with a histogram estimation based smoothness parameter selector using Lepski’s method, and is shown to maintain the regret minimax rate up to a logarithmic factor under a “self-similarity” condition. © 2016, Institute of Mathematical Statistics. All right reserved.",,"School of Mathematical Sciences, Rochester Institute of Technology, Rochester, NY, United States; School of Statistics, University of Minnesota, Minneapolis, MN, United States",,,"Qian W., Yang Y.","Qian, W., School of Mathematical Sciences, Rochester Institute of Technology, Rochester, NY, United States; Yang, Y., School of Statistics, University of Minnesota, Minneapolis, MN, United States",,,10.1214/15-EJS1104,SCOPUS,1,,,,,,,,,,1,,,Electronic Journal of Statistics,Adaptive estimation; Contextual bandit problem; MABC; Nonparametric bandit; Regret bound,,2-s2.0-84959527338,,,,,,270,242,,,,,Scopus,,,Electronic Journal of Statistics,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959527338&doi=10.1214%2f15-EJS1104&partnerID=40&md5=562215ccad1c16d336da80cfc0856d7e,,10,,2016,,,,,,,,,,,,,,,,,,,
Extraction of a reward expectation signal from cortical units following ballistic movements generated by a brain machine interface,"Movement decoding algorithms used in today's brain-machine interface (BMI) technologies require movement-related neural activity in large quantities as training data to decode with sufficient accuracy the intended movements of the user. Because of physical disability the end users of BMI systems may be unable to readily provide such training data. Moreover, variability in the neural control of movements across patients with disability may result in individually unique training data. These issues limit the generalizability of movement decoding algorithms across BMI users. One potential method of circumventing this generalizability limitation and individualizing BMI technology is the use of reinforcement learning, a group of techniques that require minimal feedback in order to find solutions to an arbitrary problem. One promising means of providing feedback to a reinforcement learning-based BMI is via a neural reward signal found in multiple cortical and subcortical areas. Particularly attractive is the idea of parallel extraction of both the movement control signal and the reward signal from the same electrode array. We examined the neural signal underlying the expectation of reward depending on the probability of successfully reaching a target given the initial ballistic movement generated by a BMI. The real-time extraction of such signal could be used to determine if the user expects a movement generated by a BMI to succeed or fail. This information could then be used to update the control architecture of the BMI to generate an output more in line with the user's intention.",,,,"Dept. of Phys. & Pharm., State University of New York Downstate Med. Center, Brooklyn, NY",D. McNiel; M. Akanda; A. Tarigoppula; P. Y. Chhatbar; J. Francis,,0,,10.1109/SPMB.2014.7002960,IEEE Xplore,1,,20150108,1,,Accuracy;Biomedical engineering;Decoding;Educational institutions;Nervous system;Neurosurgery;Training data,biomechanics;biomedical electrodes;brain;brain-computer interfaces;decoding;feature extraction;feedback;learning (artificial intelligence);medical control systems;motion control;neurophysiology,arbitrary problem;brain-machine interface;cortical units;electrode array;generalizability limitation;individualizing BMI technology;individually unique training data;initial ballistic movement;intended user movements;minimal feedback;movement decoding algorithms;movement-related neural activity;multiple cortical areas;neural control variability;parallel extraction;physical disability;real-time extraction;reinforcement learning;reinforcement learning-based BMI;reward expectation signal extraction;subcortical areas;training data decoding,,,,13-13 Dec. 2014,,2014 IEEE Signal Processing in Medicine and Biology Symposium (SPMB),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7002960,,,,,,IEEE,,,Electronic:978-1-4799-8184-7; POD:978-1-4799-8185-4,,2014 IEEE Signal Processing in Medicine and Biology Symposium (SPMB),1,IEEE Conferences,,,,,2014,,,,,,,,,,,,,,,,,,,
Hello! Mr. Sage,"Mr. Sage, an ideal Professor, can motivate, evoke involvement, and supply rewards and reinforcement, for each individual student all within the same class period. Although his minimum standards require mastery of concepts at all levels of learning, from simple memory to creativity, he does not expect the same performance of all students. His grades are directly related to the breadth of subject matter and depth of mastery. He personalizes his instruction within the classroom and in homework assignments by a range in the difficulty of illustration, thereby challenging each student to his capacity. He uses tests and examinations as learning experiences. To achieve this he combines the excellent feature of the conventional system and the current views on personalized system of instruction.",,,,,O. E. Lancaster,,0,,10.1109/TE.1976.4321037,IEEE Xplore,1,,20071112,58,,Engineering education;Eyes;History;Testing;Vehicle dynamics;Vehicles,,,,0018-9359;00189359,2,May 1976,,IEEE Transactions on Education,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4321037,,,,,,IEEE,15,,,,IEEE Transactions on Education,54,IEEE Journals & Magazines,,,19,,1976,,,,,,,,,,,,,,,,,,,
Social learning as a way to overcome choice-induced preferences? Insights from humans and rhesus macaques,"Much theoretical attention is currently devoted to social learning. Yet, empirical studies formally comparing its effectiveness relative to individual learning are rare. Here, we focus on free choice, which is at the heart of individual reward-based learning, but absent in social learning. Choosing among two equally valued options is known to create a preference for the selected option in both humans and monkeys.We thus surmised that social learning should be more helpful when choice-induced preferences retard individual learning than when they optimize it.To test this prediction, the same task requiring to find which among two items concealed a reward was applied to rhesus macaques and humans. The initial trial was individual or social, rewarded or unrewarded. Learning was assessed on the second trial. Choice-induced preference strongly affected individual learning. Monkeys and humans performed much more poorly after an initial negative choice than after an initial positive choice. Comparison with social learning verified our prediction. For negative outcome, social learning surpassed or at least equaled individual learning in all subjects. For positive outcome, the predicted superiority of individual learning did occur in a majority of subjects (5/6 monkeys and 6/12 humans). A minority kept learning better socially though, perhaps due to a more dominant/aggressive attitude toward peers. Poor learning from errors due to over-valuation of personal choices is among the decision-making biases shared by humans and animals. The present study suggests that choice-immune social learning may help curbing this potentially harmful tendency. Learning from successes is an easier path. The present data suggest that whether one tends to walk it alone or with a peer's help might depend on the social dynamics within the actor/observer dyad. © 2012 Monfardini, Gaveau, Boussaoud, Hadj-Bouziane and Meunier.",,"INSERM, U1028, Impact Team, Lyon Neuroscience Research Center, Lyon, France; CNRS, UMR5292, Impact Team, Lyon Neuroscience Research Center, Lyon, France; University Lyon, Lyon, France; Institut de Médecine Environnementale, Paris, France; Institut de Neuroscience des Systèmes, UMR 1106, INSERM, Aix-Marseille Université, Marseille, France", Article 127,,"Monfardini E., Gaveau V., Boussaoud D., Hadj-Bouziane F., Meunier M.","Monfardini, E., INSERM, U1028, Impact Team, Lyon Neuroscience Research Center, Lyon, France, CNRS, UMR5292, Impact Team, Lyon Neuroscience Research Center, Lyon, France, University Lyon, Lyon, France, Institut de Médecine Environnementale, Paris, France; Gaveau, V., INSERM, U1028, Impact Team, Lyon Neuroscience Research Center, Lyon, France, CNRS, UMR5292, Impact Team, Lyon Neuroscience Research Center, Lyon, France, University Lyon, Lyon, France; Boussaoud, D., Institut de Neuroscience des Systèmes, UMR 1106, INSERM, Aix-Marseille Université, Marseille, France; Hadj-Bouziane, F., INSERM, U1028, Impact Team, Lyon Neuroscience Research Center, Lyon, France, CNRS, UMR5292, Impact Team, Lyon Neuroscience Research Center, Lyon, France, University Lyon, Lyon, France; Meunier, M., INSERM, U1028, Impact Team, Lyon Neuroscience Research Center, Lyon, France, CNRS, UMR5292, Impact Team, Lyon Neuroscience Research Center, Lyon, France, University Lyon, Lyon, France",10,,10.3389/fnins.2012.00127,SCOPUS,1,,,,,,,,,,SEP,,,Frontiers in Neuroscience,Choice-induced preference; Cognitive biases; Humans; Reinforcement learning; Rhesus macaques; Social learning,,2-s2.0-84870896027,,,,,,,,,,,,Scopus,,,Frontiers in Neuroscience,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870896027&doi=10.3389%2ffnins.2012.00127&partnerID=40&md5=c877f35a31016240c353c28d7736ba31,,,,2012,,,,,,,,,,,,,,,,,,,
Aligning social welfare and agent preferences to alleviate traffic congestion,"Multi-agent coordination algorithms provide unique insights into the challenging problem of alleviating traffic congestion. What is particularly interesting in this class of problem is that no individual action (e.g., leave at a given time) is intrinsically ""bad"" but that combinations of actions among agents lead to undesirable outcomes. As a consequence, agents need to learn how to coordinate their actions with those of other agents, rather than learn a particular set of ""good"" actions. In general, the traffic problem can be approached from two distinct perspectives: (i) from a city manager's point of view, where the aim is to optimize a city wide objective function (e.g., minimize total city wide delays), and (ii) from the individual driver's point of view, where each driver is aiming to optimize a personal objective function (e.g., a ""timeliness"" function that minimizes the difference desired and actual arrival times at a destination). In many cases, these two objective functions are at odds with one another, where drivers aiming to optimize their own objectives yield to congestion and poor values of city objective functions. In this paper we present an objective shaping approach to both types of problems and study the system behavior that arises from the drivers' choices. We first show a top-down approach that provides incentives to drivers and leads to good values of the city manager's objective function. We then present a bottom-up approach that shows that drivers aiming to optimize their own personal timeliness objective lead to poor performance with respect to a city manager's objective function. Finally, we present the intriguing result that drivers that aim to optimize a modified version of their own timeliness function not only perform well in terms of the city manager's objective function, but also perform better with respect to their own original timeliness functions. Copyright © 2008, International Foundation for Autonomous Agents and Multi-agent Systems (www.ifaamas.org). All rights reserved.",,"Oregon State University, 204 Rogers Hall, Corvailis, OR 97331, United States; UCSC, NASA Ames Res. Ctr., Mailstop 269-3, Moffett Field, CA 94035, United States",,,"Turner K., Welch Z.T., Agogino A.","Turner, K., Oregon State University, 204 Rogers Hall, Corvailis, OR 97331, United States; Welch, Z.T., Oregon State University, 204 Rogers Hall, Corvailis, OR 97331, United States; Agogino, A., UCSC, NASA Ames Res. Ctr., Mailstop 269-3, Moffett Field, CA 94035, United States",17,,,SCOPUS,0,,,,,,,,,,,,,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",Multi-agent systems; Optimization; Reinforcement learning; Traffic management; Transportation,,2-s2.0-84899955497,,,,,,653,646,,,,,Scopus,,,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899955497&partnerID=40&md5=9b77acafd443bb62e6e336d3822aa81c,,2,,2008,,,,,,,,,,,,,,,,,,,
Multiagent reinforcement learning method with an improved ant colony system,"Multiagent reinforcement learning has gained increasing attention in recent years. The authors discuss coordination means for sharing episodes and sharing policies in the field of multiagent reinforcement learning. From the point of the view of reinforcement learning, we analyse the performance of indirect media communication among multi-agents on an ant colony system which is an efficient method that uses pheromones to solve optimization problems. Based on the above, we propose the Q-ACS method, modifying the global updating rule in ACS for learning agents to share better episodes benefited from the exploitation of accumulated knowledge. Meanwhile, taking the visited times into account, we propose T-ACS by presenting a state transition policy for learning agents to share better policies, benefiting from biased exploration. To demonstrate the coordination performance of learning agents in our methods, we conducted experiments on an optimization problem, the traveling salesman problem. Comparison of results with ACS, Q-ACS and T-ACS show that the improved methods are efficient for solving the optimization problem",,,,"Fac. of Eng., Osaka City Univ., Japan",Ruoying Sun; S. Tatsumi; Gang Zhao,,3,,10.1109/ICSMC.2001.973515,IEEE Xplore,1,,20020806,1617 vol.3,,Ant colony optimization;Distributed computing;Explosives;Internet;Learning;Optimization methods;Personal digital assistants;Sun;Telephony;Traveling salesman problems,learning (artificial intelligence);multi-agent systems;travelling salesman problems,ACS;Q-ACS method;T-ACS;accumulated knowledge;ant colony system;biased exploration;coordination means;coordination performance;global updating rule;indirect media communication;learning agents;multiagent reinforcement learning;optimization problems;pheromones;state transition policy,,1062-922X;1062922X,,2001,,"2001 IEEE International Conference on Systems, Man and Cybernetics. e-Systems and e-Man for Cybernetics in Cyberspace (Cat.No.01CH37236)",,,,07 Oct 2001-10 Oct 2001,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=973515,,,,,,IEEE,18,,POD:0-7803-7087-2,,"2001 IEEE International Conference on Systems, Man and Cybernetics. e-Systems and e-Man for Cybernetics in Cyberspace (Cat.No.01CH37236)",1612,IEEE Conferences,,,3,,2001,,,,,,,,,,,,,,,,,,,
Dynamic thermal management for multimedia applications using machine learning,"Multimedia applications are expected to form the largest portion of workload in general purpose PC and portable devices. The ever-increasing computation intensity of multimedia applications elevates the processor temperature and consequently impairs the reliability and performance of the system. In this paper, we propose to perform dynamic thermal management using reinforcement learning algorithm for multimedia applications. The proposed learning model does not need any prior knowledge of the workload information or the system thermal and power characteristics. It learns the temperature change and workload switching patterns by observing the temperature sensor and event counters on the processor, and finds the management policy that provides good performance-thermal tradeoff during the runtime. We validated our model on a Dell personal computer with Intel Core 2 processor. Experimental results show that our approach provides considerable performance improvements with marginal increase in the percentage of thermal hotspot comparing to existing workload phase detection approach.",,,,"Department of Electrical and Computer Engineering, Binghamton University, USA",Y. Ge; Q. Qiu,,2,,,IEEE Xplore,0,,20110811,100,,Decoding;Heuristic algorithms;Learning;Multimedia communication;Radiation detectors;Temperature sensors;Thermal management,learning (artificial intelligence);multimedia computing;thermal management (packaging),Dell personal computer;Intel Core 2 processor;dynamic thermal management;machine learning;multimedia applications;reinforcement learning algorithm;workload phase detection approach,,0738-100x;0738100x,,5-9 June 2011,,2011 48th ACM/EDAC/IEEE Design Automation Conference (DAC),Dynamic thermal management;multimedia application;reinforcement learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5981924,,,,,,IEEE,17,,DVD:978-1-4503-0636-2; Electronic:978-1-4503-0636-2; POD:978-1-4503-0636-2,,2011 48th ACM/EDAC/IEEE Design Automation Conference (DAC),95,IEEE Conferences,,,,,2011,,,,,,,,,,,,,,,,,,,
Reinforcement Learning of User Preferences for a Ubiquitous Personal Assistant,"New technologies bring a multiplicity of new possibilities for users to work with computers. Not only are spaces more and more equipped with stationary computers or notebooks, but more and more users carry mobile devices with them (smart-phones, personal digital …",,,,,,,2,,,Google Scholar,0,,,,,,,,,,,,,,,,,,,,"http://scholar.google.com/scholar?cluster=8925224382166895311&hl=en&as_sdt=0,5",,,,,,,,,,,,,,http://scholar.google.com/https://www.intechopen.com/download/pdf/13053,,,,2011,,,8.92522438216689E+018,,,,,,,,,18,,,,,,"http://scholar.google.com/scholar?cites=8925224382166895311&as_sdt=2005&sciodt=0,5&hl=en",
On personalizing Web content through reinforcement learning,"Nowadays, a large use of personalization techniques is used to adapt Web content to users’ habits, mainly with the aim of offering appropriate products and services. This paper presents a system that uses personalization and adaptation techniques, in order to transcode or modify contents (e.g., adapt text fonts) so as to meet preferences and needs of elderly users and users with disabilities. Such an adaptation can have a positive effect, in particular for users with some reading-related disabilities (i.e., people with dyslexia, users with low vision, users with color blindness, elderly people.). To avoid issues arising from applying transformations to the whole content, the proposed system uses Web intelligence to perform automatic adaptations on single elements composing a Web page. The transformation is applied on the basis of a reinforcement learning algorithm which manages a user profile. The system is evaluated through simulations and a real assessment, where elderly users where asked to use the system prototype for a time period and to perform some specific tasks. Results of the qualitative evaluation confirm the feasibility of the proposal, showing its validity and the users’ appreciation. © 2016, Springer-Verlag Berlin Heidelberg.",,"Department of Computer Science and Engineering, Università di Bologna, Via Mura Anteo Zamboni 7, Bologna, Italy",,,"Ferretti S., Mirri S., Prandi C., Salomoni P.","Ferretti, S., Department of Computer Science and Engineering, Università di Bologna, Via Mura Anteo Zamboni 7, Bologna, Italy; Mirri, S., Department of Computer Science and Engineering, Università di Bologna, Via Mura Anteo Zamboni 7, Bologna, Italy; Prandi, C., Department of Computer Science and Engineering, Università di Bologna, Via Mura Anteo Zamboni 7, Bologna, Italy; Salomoni, P., Department of Computer Science and Engineering, Università di Bologna, Via Mura Anteo Zamboni 7, Bologna, Italy",1,,10.1007/s10209-016-0463-2,SCOPUS,1,,,,,,,,,,2,,,Universal Access in the Information Society,Content adaptation; Legibility; Reinforcement learning; User profiling; Web personalization,,2-s2.0-84960114893,,,,,,410,395,,,,,Scopus,,,Universal Access in the Information Society,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960114893&doi=10.1007%2fs10209-016-0463-2&partnerID=40&md5=5261867eed4eafe7188ba50052e7410c,,16,,2017,,,,,,,,,,,,,,,,,,,
Adaptive interventions treatment modelling and regimen optimization using Sequential Multiple Assignment Randomized Trials (SMART) and Q-learning,"Nowadays, pharmacological practices are focused on a single best treatment to treat a disease which sounds impractical as the same treatment may not work the same way for every patient. Thus, there is a need of shift towards more patient-centric rather than disease-centric approach, in which personal characteristics of a patient or biomarkers are used to determine the tailored optimal treatment. The ""one size fits all"" concept is contradicted by research area of personalized medicine. The Sequential Multiple Assignment Randomized Trial (SMART) is a multi-stage trials to inform the development of dynamic treatment regimens (DTR's). In SMART, a subject is randomized through different stages of treatment where each stage corresponds to a treatment decision. These types of adaptive interventions are individualized and are repeatedly adjusted across time based on patient's individual clinical characteristics and ongoing performance. The reinforcement learning (Q-learning), a computational algorithm for optimization of treatment regimens to maximize desired clinical outcome is used in optimizing the sequence of treatments. This statistical model contains regression analysis for function approximation of data from clinical trials. The model will predict a series of regimens across time, depending on the biomarkers of a new participant for optimizing the weight management decision rules.",,"Department of Electrical Engineering and Computer Science, United States; Sanford Health Research SD, United States; Department of Construction and Operations Management, South Dakota State University, United States",,,"Baniya A., Herrmann S., Qiao Q., Lu H.","Baniya, A., Department of Electrical Engineering and Computer Science, United States, Department of Construction and Operations Management, South Dakota State University, United States; Herrmann, S., Sanford Health Research SD, United States; Qiao, Q., Department of Electrical Engineering and Computer Science, United States; Lu, H., Department of Construction and Operations Management, South Dakota State University, United States",,,,SCOPUS,0,,,,,,,,,,,,,67th Annual Conference and Expo of the Institute of Industrial Engineers 2017,Dynamic treatment regimens (DTR); Personalized medicine; Q-learning algorithm; Regression analysis; Sequential Multiple Assignment Randomized Trial (SMART),,2-s2.0-85031025743,,,,,,1192,1187,,,,,Scopus,,,67th Annual Conference and Expo of the Institute of Industrial Engineers 2017,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031025743&partnerID=40&md5=80f6536d12d6f4028e4e66ec08b8128d,,,,2017,,,,,,,,,,,,,,,,,,,
A self-adaptive system for improving autonomy and public spaces accessibility for elderly,"Nowadays, there is an increasing need to provide a safe and independent living for cognitively deficient population. Notably, we have to improve seniors’ autonomy and their public spaces accessibility. Giving these observations, the aim of this paper is to provide a personalized adaptive assisting system for elderly. More precisely, this paper presents the specification and implementation of a self-organizing multi-agent system able to abstract the different distributed components involved in user’s environment. This system is able to detect different possible situations that a user could face in his daily outdoors activities and propose accordingly appropriate actions. This system not only learns user’s habits from its perceptions but also improves its recommendations thanks to feedbacks provided by stakeholders (family, doctors …) following a reinforcement learning reasoning. Finally, we present our system evaluation specially its learning capabilities through different scenarios that have been generated automatically. © Springer International Publishing AG 2018.",,"118 Route de Narbonne, Toulouse, France; 2 Rue du Doyen-Gabriel-Marty, Toulouse, France",,,"Triki S., Hanachi C.","Triki, S., 118 Route de Narbonne, Toulouse, France; Hanachi, C., 2 Rue du Doyen-Gabriel-Marty, Toulouse, France",,,10.1007/978-3-319-59394-4_6,SCOPUS,1,,,,,,,,,,,,,"Smart Innovation, Systems and Technologies",AMAS theory; Assisted living system; Multi-agent system; Reinforcement learning,,2-s2.0-85020375682,,,,,,66,53,,,,,Scopus,,,"Smart Innovation, Systems and Technologies",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020375682&doi=10.1007%2f978-3-319-59394-4_6&partnerID=40&md5=e06d5cd071339abef3f12d85ab89af53,,74,,2018,,,,,,,,,,,,,,,,,,,
A hierarchical reinforcement learning based artificial intelligence for non-player characters in video games,"Nowadays, video games conforms a huge industry that is always developing new technology. In particular, artificial intelligence techniques have been used broadly in the well-known non-player characters (NPC) given the opportunity to users to feel video games more real. This paper proposes the usage of the MaxQ-Q hierarchical reinforcement learning algorithm in non-player characters in order to increase the experience of the user in terms of naturalness. A case study of an NPC with the proposed artificial intelligence based algorithm in a first personal shooter video game was developed. Experimental results show that this implementation improves naturalness from the user’s point of view. In addition, the proposed MaxQ-Q based algorithm in NPCs allow to programmers a robust way to give artificial intelligence to them. © Springer International Publishing Switzerland 2014.",,"Graduate School of Engineering, Tecnologico de Monterrey, Campus Ciudad de Mexico, Mexico City, Mexico; Universidad Panamericana, Mexico City, Mexico; Department of Computer Science, Tecnologico de Monterrey, Campus Ciudad de Mexico, Mexico City, Mexico",,,"Ponce H., Padilla R.","Ponce, H., Graduate School of Engineering, Tecnologico de Monterrey, Campus Ciudad de Mexico, Mexico City, Mexico, Universidad Panamericana, Mexico City, Mexico; Padilla, R., Department of Computer Science, Tecnologico de Monterrey, Campus Ciudad de Mexico, Mexico City, Mexico",1,,,SCOPUS,0,,,,,,,,,,,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Hierarchical reinforcement learning; Human assessment; Naturalness; Non-player characters; Video games,,2-s2.0-84914125261,,,,,,183,172,,,,,Scopus,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914125261&partnerID=40&md5=52586b31e9733b68ff9d157927322735,,8857,,2014,,,,,,,,,,,,,,,,,,,
Automated Ingestion Detection for a Health Monitoring System,"Obesity is a global epidemic that imposes a financial burden and increased risk for a myriad of chronic diseases. Presented here is an overview of a prototype automated ingestion detection (AID) process implemented in a health monitoring system (HMS). The automated detection of ingestion supports personal record keeping which is essential during obesity management. Personal record keeping allows the care provider to monitor the therapeutic progress of a patient. The AID-HMS determines the levels of ingestion activity from sounds captured by an external throat microphone. Features are extracted from the sound recording and presented to machine learning classifiers, where a simple voting procedure is employed to determine instances of ingestion. Using a dataset acquired from seven individuals consisting of consumption of liquid and solid, speech, and miscellaneous sounds, > 94% of ingestion sounds are correctly identified with false positive rates around 9% based on 10-fold cross validation. The detected levels of ingestion activity are transmitted and stored on a remote web server, where information is displayed through a web application operating in a web browser. This information allows remote users (health provider) determine meal lengths and levels of ingestion activity during the meal. The AID-HMS also provides a basis for automated reinforcement for the patient.",,,,"Embedded and Adaptive Computing Group, the University of Texas at Dallas, Richardson, TX, USA",W. P. Walker; D. K. Bhatia,,4,,10.1109/JBHI.2013.2279193,IEEE Xplore,1,,20140303,692,,Detectors;Feature extraction;Liquids;Microphones;Monitoring;Solids,Internet;data acquisition;diseases;epidemics;feature extraction;learning (artificial intelligence);medical signal processing;microphones;online front-ends;patient care;patient monitoring;pattern classification;signal classification;speech processing,AID-HMS;Web application;Web browser;automated ingestion detection;chronic diseases;dataset acquisition;external throat microphone;false positive rates;feature extraction;health monitoring system;ingestion activity;machine learning classifiers;miscellaneous sounds;myriad;obesity management;patient care;patient monitoring;personal record;prototype automated ingestion detection;remote Web server;sound recording;speech;therapeutic progress,,2168-2194;21682194,2,March 2014,,IEEE Journal of Biomedical and Health Informatics,Health monitoring;obesity management;patient empowerment;patient monitoring,,,,0,20130909,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6594833,,,,,,IEEE,65,,,,IEEE Journal of Biomedical and Health Informatics,682,IEEE Journals & Magazines,,,18,,2014,,,,,,,,,,,,,,,,,,,
Online learning to rank: Absolute vs. relative,"Online learning to rank holds great promise for learning personalized search result rankings. First algorithms have been proposed, namely absolute feedback approaches, based on contextual bandits learning; and relative feedback approaches, based on gradient methods and inferred preferences between complete result rankings. Both types of approaches have shown promise, but they have not previously been compared to each other. It is therefore unclear which type of approach is the most suitable for which online learning to rank problems. In this work we present the first empirical comparison of absolute and relative online learning to rank approaches.",,"University College London, United Kingdom; Microsoft Research, United States",,,"Chen Y., Hofmann K.","Chen, Y., University College London, United Kingdom; Hofmann, K., Microsoft Research, United States",5,,10.1145/2740908.2742718,SCOPUS,1,,,,,,,,,,,,,WWW 2015 Companion - Proceedings of the 24th International Conference on World Wide Web,Information retrieval; Learning to rank; Online learning,,2-s2.0-84968542091,,,,,,20,19,,,,,Scopus,,,WWW 2015 Companion - Proceedings of the 24th International Conference on World Wide Web,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968542091&doi=10.1145%2f2740908.2742718&partnerID=40&md5=a5678748f6782e9a44832ddac4643955,,,,2015,,,,,,,,,,,,,,,,,,,
Supervisory output prediction for bilinear systems by reinforcement learning,"Online output prediction is an indispensable part of any model predictive control implementation. For several application scenarios, operating conditions may change quite often, while designing the data collection process may not be an option. To this end, this study introduces a supervisory output prediction scheme, tailored specifically for input-output stable bilinear systems, that intends on automating the process of selecting the most appropriate prediction model during runtime. The selection process is based upon a reinforcement-learning scheme, where prediction models are selected according to their prior prediction performance. An additional selection process is concerned with appropriately partitioning the control-inputs' domain also to allow for switched-system approximations of the original bilinear dynamics. The authors show analytically that the proposed scheme converges (in probability) to the best model and partition. They also demonstrate these properties through simulations of temperature prediction in residential buildings.",,,,"Software Competence Center Hagenberg GmbH, Austria",G. C. Chasparis; T. Natschläger,,,,10.1049/iet-cta.2016.1400,IEEE Xplore,1,,20170608,1521,,,bilinear systems;buildings (structures);digital simulation;learning (artificial intelligence);predictive control;switching systems (control);temperature control,control-input domain;data collection process;input-output stable bilinear systems;model predictive control implementation;online output prediction;original bilinear dynamics;prior prediction performance;reinforcement learning;residential buildings;supervisory output prediction;switched-system approximations;temperature prediction simulation,,1751-8644;17518644,10,6 23 2017,,IET Control Theory & Applications,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7942299,,,,,,IET,,,,,IET Control Theory & Applications,1514,IET Journals & Magazines,,,11,,2017,,,,,,,,,,,,,,,,,,,
Trust and privacy correlations in social networks: A deep learning framework,"Online Social Networks (OSNs) remain the focal point of Internet usage. Since the beginning, networking sites tried best to have right privacy mechanisms in place for users, enabling them to share the right content with the right audience. With all these efforts, privacy customizations remain hard for users across the sites. Existing research that address this problem mainly focus on semi-supervised strategies that introduce extra complexity by requiring the user to manually specify initial privacy preferences for their friends. In this work, we suggest an adaptive solution that can dynamically generate privacy labels for users in OSNs. To this end, we introduce a deep reinforcement learning framework that targets two key problems in OSNs like Facebook: the exposure of users' interactions through the network to less trusted direct friends, and the possibility of propagating user updates through direct friends' interactions to indirect friends. By implementing this framework, we aim at understanding how social trust and privacy could be correlated, specifically in a dynamic fashion. We report the ranked dependence between the generated privacy labels and the estimated user trust values, which indicate the ability of the framework to identify the highly trusted users and share with them higher percentages of data. © 2016 IEEE.",,"KTH, Royal Institute of Technology, Sweden; University of Insubria, Italy",7752236,,"Jaradat S., Dokoohaki N., Matskin M., Ferrari E.","Jaradat, S., KTH, Royal Institute of Technology, Sweden; Dokoohaki, N., KTH, Royal Institute of Technology, Sweden; Matskin, M., KTH, Royal Institute of Technology, Sweden; Ferrari, E., University of Insubria, Italy",1,,10.1109/ASONAM.2016.7752236,SCOPUS,1,,,,,,,,,,,,,"Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2016",,,2-s2.0-85006765626,,,,,,206,203,,,,,Scopus,,,"Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2016",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006765626&doi=10.1109%2fASONAM.2016.7752236&partnerID=40&md5=fb08bc9dfc0d971eb728bd1856cff9d7,,,,2016,,,,,,,,,,,,,,,,,,,
Personalized response generation by Dual-learning based domain adaptation,"Open-domain conversation is one of the most challenging artificial intelligence problems, which involves language understanding, reasoning, and the utilization of common sense knowledge. The goal of this paper is to further improve the response generation, using personalization criteria. We propose a novel method called PRGDDA (Personalized Response Generation by Dual-learning based Domain Adaptation) which is a personalized response generation model based on theories of domain adaptation and dual learning. During the training procedure, PRGDDA first learns the human responding style from large general data (without user-specific information), and then fine-tunes the model on a small size of personalized data to generate personalized conversations with a dual learning mechanism. We conduct experiments to verify the effectiveness of the proposed model on two real-world datasets in both English and Chinese. Experimental results show that our model can generate better personalized responses for different users. © 2018 Elsevier Ltd",,"Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; School of Information Management and Engineering, Shanghai University of Finance and Economics, Shanghai, China; School of Computing Science, Zhejiang University, Hangzhou, China; College of Computer Science and Software, Shenzhen University, Shenzhen, China; School of Computer Science, South China Normal University, Guangzhou, China",,,"Yang M., Tu W., Qu Q., Zhao Z., Chen X., Zhu J.","Yang, M., Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Tu, W., School of Information Management and Engineering, Shanghai University of Finance and Economics, Shanghai, China; Qu, Q., Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Zhao, Z., School of Computing Science, Zhejiang University, Hangzhou, China; Chen, X., College of Computer Science and Software, Shenzhen University, Shenzhen, China; Zhu, J., School of Computer Science, South China Normal University, Guangzhou, China",,,10.1016/j.neunet.2018.03.009,SCOPUS,1,,,,,,,,,,,,,Neural Networks,Deep reinforcement learning; Domain adaptation; Dual learning; Personalized response generation,,2-s2.0-85045705590,,,,,,82,72,,,,,Scopus,,,Neural Networks,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045705590&doi=10.1016%2fj.neunet.2018.03.009&partnerID=40&md5=62c0e7468a872e17eeb031613314833a,,103,,2018,,,,,,,,,,,,,,,,,,,
Latent state models of primary user behavior for opportunistic spectrum access,"Opportunistic spectrum access, where cognitive radio devices detect available unused radio channels and exploit them for communication, avoiding collisions with existing users of the channels, is a central topic of research for future wireless communication. When each device has limited resources to sense which channels are available, the task becomes a reinforcement learning problem that has been studied with partially observable Markov decision processes (POMDPs). However, current POMDP solutions are based on simplistic representations where channels are simply on/off (transmitting or idle). We show that more complicated Markov models where on/off states are part of complicated behavior of the channel owner (primary user) yield better POMDPs achieving more successful transmissions and less collisions.",,,,"Department of Information and Computer Science, Helsinki University of Technology, P.O. Box 5400, FI-02015 TKK, Finland",J. Pajarinen; J. Peltonen; M. A. Uusitalo; A. Hottinen,,3,,10.1109/PIMRC.2009.5450037,IEEE Xplore,1,,20100415,1271,,Chromium;Cognitive radio;Computer science;Interference;Internet telephony;Learning;Road accidents;Telecommunication traffic;Wireless communication;Wireless sensor networks,Markov processes;cognitive radio;learning (artificial intelligence);wireless channels,cognitive radio devices;latent state models;opportunistic spectrum access;partially observable Markov decision processes;primary user behavior;radio channels;reinforcement learning;wireless communication,,2166-9570;21669570,,13-16 Sept. 2009,,"2009 IEEE 20th International Symposium on Personal, Indoor and Mobile Radio Communications",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5450037,,,,,4,IEEE,16,,Electronic:978-1-4244-5123-4; POD:978-1-4244-5122-7,,"2009 IEEE 20th International Symposium on Personal, Indoor and Mobile Radio Communications",1267,IEEE Conferences,,,,,2009,,,,,,,,,,,,,,,,,,,
Contextual multi-armed bandit algorithms for personalized learning action selection,"Optimizing the selection of learning resources and practice questions to address each individual student's needs has the potential to improve students' learning efficiency. In this paper, we study the problem of selecting a personalized learning action for each student (e.g. watching a lecture video, working on a practice question, etc.), based on their prior performance, in order to maximize their learning outcome. We formulate this problem using the contextual multi-armed bandits framework, where students' prior concept knowledge states (estimated from their responses to questions in previous assessments) correspond to contexts, the personalized learning actions correspond to arms, and their performance on future assessments correspond to rewards. We propose three new Bayesian policies to select personalized learning actions for students that each exhibits advantages over prior work, and experimentally validate them using real-world datasets.",,,,"Rice University, United States of America",I. Manickam; A. S. Lan; R. G. Baraniuk,,,,10.1109/ICASSP.2017.7953377,IEEE Xplore,1,,20170619,6348,,Approximation algorithms;Bayes methods;Context;Programmable logic arrays;Random variables;Schedules;Uncertainty,educational institutions,Bayesian policies;contextual multi-armed bandit algorithms;learning resources;personalized learning action selection;prior concept knowledge states;student learning efficiency,,,,5-9 March 2017,,"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",contextual bandits;personalized learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7953377,,,,,,IEEE,,,Electronic:978-1-5090-4117-6; POD:978-1-5090-4118-3; USB:978-1-5090-4116-9,,"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",6344,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
Artist agent: A reinforcement learning approach to automatic stroke generation in oriental ink painting,"Oriental ink painting, called Sumi-e, is one of the most distinctive painting styles and has attracted artists around the world. Major challenges in Sumi-e simulation are to abstract complex scene information and reproduce smooth and natural brush strokes. To automatically generate such strokes, we propose to model the brush as a reinforcement learning agent, and let the agent learn the desired brush-trajectories by maximizing the sum of rewards in the policy search framework. To achieve better performance, we provide elaborate design of actions, states, and rewards specifically tailored for a Sumi-e agent. The effectiveness of our proposed approach is demonstrated through experiments on Sumi-e simulation. Copyright © 2013 The Institute of Electronics, Information and Communication Engineers.",,"Tokyo Institute of Technology, Tokyo, 152-8550, Japan",,,"Xie N., Hachiya H., Sugiyama M.","Xie, N., Tokyo Institute of Technology, Tokyo, 152-8550, Japan; Hachiya, H., Tokyo Institute of Technology, Tokyo, 152-8550, Japan; Sugiyama, M., Tokyo Institute of Technology, Tokyo, 152-8550, Japan",2,,10.1587/transinf.E96.D.1134,SCOPUS,1,,,,,,,,,,5,,,IEICE Transactions on Information and Systems,Painterly rendering; Policy gradient; Reinforcement learning; Stroke-based rendering,,2-s2.0-84878232055,,,,,,1144,1134,,,,,Scopus,,,IEICE Transactions on Information and Systems,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878232055&doi=10.1587%2ftransinf.E96.D.1134&partnerID=40&md5=42a89e33d745f1d465ad15ab23290171,,E96-D,,2013,,,,,,,,,,,,,,,,,,,
The necessity of a secondary system in machine consciousness,"Our research purpose is to realize a consciousness system on computers. In this paper, we focus on the relation between a primary system that directly interacts and learns the input-output relation within an environment, and a secondary system that continuously monitors and is able to direct the primary system. We hold that consciousness is not composed of a primary system alone, but that the employment of a secondary system is essential. The purpose of this paper is therefore to clarify the importance of a secondary system. We show by numerical experiments that the addition of a secondary system provides adaptability to a wider range of environmental changes than a primary system alone. Alternatively, we present an extraordinary case where a customized primary system fully adapts to an environment undergoing a particular type of change. Far from refuting the value of a secondary system, this special case serves to point out the importance of the effective design of a secondary system. Therefore, we confirm the value and usefulness of a secondary system in a machine consciousness system through these numerical experiments. © The Authors. Published by Elsevier B.V.",Open Access,"National Institution for Academic Degrees, University Evaluation, Kodaira, Tokyo, Japan; Meiji University, Kawasaki, Kanagawa, Japan",,,"Miyazaki K., Takeno J.","Miyazaki, K., National Institution for Academic Degrees, University Evaluation, Kodaira, Tokyo, Japan; Takeno, J., Meiji University, Kawasaki, Kanagawa, Japan",3,,10.1016/j.procs.2014.11.079,SCOPUS,1,,,,,,,,,,,,,Procedia Computer Science,Consciousness system; Exploitation-oriented learning (XoL); Primary system; Reinforcement learning; Secondary system,,2-s2.0-84939220757,,,,,,22,15,,,,,Scopus,,,Procedia Computer Science,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939220757&doi=10.1016%2fj.procs.2014.11.079&partnerID=40&md5=3ce53d88cb483fa5999f27e16b540688,,41,,2014,,,,,,,,,,,,,,,,,,,
Using Options to Accelerate Learning of New Tasks According to Human Preferences,"Over the years, people need to incorporate a wider range of information and multiple objectives for their decision making. Nowadays, humans are dependent on computer systems to interpret and take profit from the huge amount of available data on the Internet. Hence, varied services, such as location- based systems, must combine a huge quantity of raw data to give the desired response to the user. However, as humans have different preferences, the optimal answer is different for each user profile, and few systems offer the service of solving tasks in a customized manner for each user. Reinforcement Learning (RL) has been used to autonomously train systems to solve (or assist on) decision-making tasks according to user preferences. However, the learning process is very slow and require many interactions with the environment. Therefore, we here propose to reuse knowledge from previous tasks to accelerate the learning process in a new task. Our proposal, called Multiobjective Options, accelerates learning while providing a customized solution according to the current user preferences. Our experiments in the Tourist World Domain show that our proposal learns faster and better than regular learning, and that the achieved solutions follow user preferences. © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Escola Politdcnica da Universidade de São Paulo, São Paulo, Brazil",,,"Bonini R.C., Da Silva F.L., Spina E., Costa A.H.R.","Bonini, R.C., Escola Politdcnica da Universidade de São Paulo, São Paulo, Brazil; Da Silva, F.L., Escola Politdcnica da Universidade de São Paulo, São Paulo, Brazil; Spina, E., Escola Politdcnica da Universidade de São Paulo, São Paulo, Brazil; Costa, A.H.R., Escola Politdcnica da Universidade de São Paulo, São Paulo, Brazil",,,,SCOPUS,0,,,,,,,,,,,,,AAAI Workshop - Technical Report,,,2-s2.0-85046100452,,,,,,650,643,,,,,Scopus,,,AAAI Workshop - Technical Report,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046100452&partnerID=40&md5=5301426c1524d266fb75276e43518ac7,,WS-17-01 - WS-17-15,,2017,,,,,,,,,,,,,,,,,,,
Comparing generic parameter controllers for EAs,"Parameter controllers for Evolutionary Algorithms (EAs) deal with adjusting parameter values during an evolutionary run. Many ad hoc approaches have been presented for parameter control, but few generic parameter controllers exist and, additionally, no comparisons or in depth analyses of these generic controllers are available in literature. This paper presents an extensive comparison of such generic parameter control methods, including a number of novel controllers based on reinforcement learning which are introduced here. We conducted experiments with different EAs and test problems in an one-off setting, i.e. relatively long runs with controllers used out-of-the-box with no tailoring to the problem at hand. Results reveal several interesting insights regarding the effectiveness of parameter control, the niche applications/EAs, the effect of continuous treatment of parameters and the influence of noise and randomness on control.",,,,"Computational Intelligence Group, VU University Amsterdam, The Netherlands",G. Karafotias; M. Hoogendoorn; B. Weel,,0,,10.1109/FOCI.2014.7007806,IEEE Xplore,1,,20150115,53,,Aerospace electronics;Computational intelligence;Estimation;Interpolation;Learning (artificial intelligence);Phase change random access memory;Silicon,control engineering computing;evolutionary computation;learning (artificial intelligence),ad hoc approaches;depth analyses;evolutionary algorithms;generic parameter control methods;niche applications;reinforcement learning,,,,9-12 Dec. 2014,,2014 IEEE Symposium on Foundations of Computational Intelligence (FOCI),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7007806,,,,,,IEEE,24,,Electronic:978-1-4799-4491-0; POD:978-1-4799-4490-3,,2014 IEEE Symposium on Foundations of Computational Intelligence (FOCI),46,IEEE Conferences,,,,,2014,,,,,,,,,,,,,,,,,,,
High-order feature-based mixture models of classification learning predict individual learning curves and enable personalized teaching,"Pattern classification learning tasks are commonly used to explore learning strategies in human subjects. The universal and individual traits of learning such tasks reflect our cognitive abilities and have been of interest both psychophysically and clinically. From a computational perspective, these tasks are hard, because the number of patterns and rules one could consider even in simple cases is exponentially large. Thus, when we learn to classify we must use simplifying assumptions and generalize. Studies of human behavior in probabilistic learning tasks have focused on rules in which pattern cues are independent, and also described individual behavior in terms of simple, single-cue, feature-based models. Here, we conducted psychophysical experiments in which people learned to classify binary sequences according to deterministic rules of different complexity, including high-order, multicue-dependent rules. We show that human performance on such tasks is very diverse, but that a class of reinforcement learning-like models that use a mixture of features captures individual learning behavior surprisinglywell. Thesemodels reflect the important role of subjects' priors, and their reliance on high-order features even when learning a low-order rule. Further, we show that these models predict future individual answers to a high degree of accuracy. We then use these models to build personally optimized teaching sessions and boost learning.",,"Department of Neurobiology, Weizmann Institute of Science, Rehovot 76100, Israel",,,"Cohen Y., Schneidman E.","Cohen, Y., Department of Neurobiology, Weizmann Institute of Science, Rehovot 76100, Israel; Schneidman, E., Department of Neurobiology, Weizmann Institute of Science, Rehovot 76100, Israel",4,,10.1073/pnas.1211606110,SCOPUS,1,,,,,,,,,,2,,,Proceedings of the National Academy of Sciences of the United States of America,Inference; Information; Maximum entropy,,2-s2.0-84872186192,,,,,,689,684,,,,,Scopus,,,Proceedings of the National Academy of Sciences of the United States of America,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872186192&doi=10.1073%2fpnas.1211606110&partnerID=40&md5=6691bc98916c6238b952fd11589c72bd,,110,,2013,,,,,,,,,,,,,,,,,,,
Individual differences in the influence of task-irrelevant Pavlovian cues on human behavior,"Pavlovian-to-instrumental transfer (PIT) refers to the process of a Pavlovian rewardpaired cue acquiring incentive motivational proprieties that drive choices. It represents a crucial phenomenon for understanding cue-controlled behavior, and it has both adaptive and maladaptive implications (i.e., drug-taking). In animals, individual differences in the degree to which such cues bias performance have been identified in two types of individuals that exhibit distinct Conditioned Responses (CR) during Pavlovian conditioning: Sign-Trackers (ST) and Goal-Trackers (GT). Using an appetitive PIT procedure with a monetary reward, the present study investigated, for the first time, the extent to which such individual differences might affect the influence of rewardpaired cues in humans. In a first task, participants learned an instrumental response leading to reward; then, in a second task, a visual Pavlovian cue was associated with the same reward; finally, in a third task, PIT was tested by measuring the preference for the reward-paired instrumental response when the task-irrelevant reward-paired cue was presented, in the absence of the reward itself. In ST individuals, but not in GT individuals, reward-related cues biased behavior, resulting in an increased likelihood to perform the instrumental response independently paired with the same reward when presented with the task-irrelevant reward-paired cue, even if the reward itself was no longer available (i.e., stronger PIT effect). This finding has important implications for developing individualized treatment for maladaptive behaviors, such as addiction. © 2015 Garofalo and di Pellegrino.",,"Center for Studies and Research in Cognitive Neuroscience, Department of Psychology, University of Bologna, Cesena, Italy; Department of Psychiatry, University of Cambridge, Cambridge, United Kingdom; Behavioural and Clinical Neuroscience Institute, Department of Psychology, University of Cambridge, Cambridge, United Kingdom",163,,"Garofalo S., di Pellegrino G.","Garofalo, S., Center for Studies and Research in Cognitive Neuroscience, Department of Psychology, University of Bologna, Cesena, Italy, Department of Psychiatry, University of Cambridge, Cambridge, United Kingdom, Behavioural and Clinical Neuroscience Institute, Department of Psychology, University of Cambridge, Cambridge, United Kingdom; di Pellegrino, G., Center for Studies and Research in Cognitive Neuroscience, Department of Psychology, University of Bologna, Cesena, Italy",19,,10.3389/fnbeh.2015.00163,SCOPUS,1,,,,,,,,,,JUNE,,,Frontiers in Behavioral Neuroscience,Cue-controlled behavior; Goal-Tracker; Pavlovian-to-instrumental transfer; Reinforcement learning; Sign-Tracker,,2-s2.0-84933039584,,,,,11,,,,,,,Scopus,,,Frontiers in Behavioral Neuroscience,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933039584&doi=10.3389%2ffnbeh.2015.00163&partnerID=40&md5=90df2c854310a8deae8890e93288872d,,9,,2015,,,,,,,,,,,,,,,,,,,
Personalizing robot behavior for interruption in social human-robot interaction,"People engaging in an activity usually has individual tolerance to be interrupted [1], [2]. Humans subconsciously adapt their behaviors to draw other one's attention and to get into a conversation based on their historical experiences, but robots often fail to be aware of humans' feeling and thus interrupt their users repeatedly. To endow service robots with such socially acceptable ability, we propose an online human-aware interactive learning framework in this paper, under which the robot personalizes its behaviors according to both observed user's attention and its conjecture about user's awareness of itself. To this purpose, the correlation between the robot's theory of awareness, user's attention and robot behavior are explored through reinforcement learning techniques. The conducted experiment shows that the robot can personalize its interruption strategy, and the optimal policies converged for at least 26 episodes.",,,,"Department of Computer Science and Information Engineering, National Taiwan University, Taipei 10617, Taiwan",Y. S. Chiang; T. S. Chu; C. D. Lim; T. Y. Wu; S. H. Tseng; L. C. Fu,,0,,10.1109/ARSO.2014.7020978,IEEE Xplore,1,,20150126,49,,Face;Hidden Markov models;Interrupters;Learning (artificial intelligence);Markov processes;Robot sensing systems,human-robot interaction;learning (artificial intelligence);service robots;social sciences,interruption strategy;online human-aware interactive learning framework;reinforcement learning techniques;robot behavior personalization;robot theory of awareness;service robots;social human-robot interaction;user attention;user awareness,,2162-7568;21627568,,11-13 Sept. 2014,,2014 IEEE International Workshop on Advanced Robotics and its Social Impacts,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7020978,,,,,,IEEE,23,,Electronic:978-1-4799-6968-5; POD:978-1-4799-6969-2; USB:978-1-4799-6967-8,,2014 IEEE International Workshop on Advanced Robotics and its Social Impacts,44,IEEE Conferences,,,,,2014,,,,,,,,,,,,,,,,,,,
Valence-dependent belief updating: Computational validation,"People tend to update beliefs about their future outcomes in a valence-dependent way: they are likely to incorporate good news and to neglect bad news. However, belief formation is a complex process which depends not only on motivational factors such as the desire for favorable conclusions, but also on multiple cognitive variables such as prior beliefs, knowledge about personal vulnerabilities and resources, and the size of the probabilities and estimation errors. Thus, we applied computational modeling in order to test for valence-induced biases in updating while formally controlling for relevant cognitive factors. We compared biased and unbiased Bayesian models of belief updating, and specified alternative models based on reinforcement learning. The experiment consisted of 80 trials with 80 different adverse future life events. In each trial, participants estimated the base rate of one of these events and estimated their own risk of experiencing the event before and after being confronted with the actual base rate. Belief updates corresponded to the difference between the two self-risk estimates. Valence-dependent updating was assessed by comparing trials with good news (better-than-expected base rates) with trials with bad news (worse-than-expected base rates). After receiving bad relative to good news, participants' updates were smaller and deviated more strongly from rational Bayesian predictions, indicating a valence-induced bias. Model comparison revealed that the biased (i.e., optimistic) Bayesian model of belief updating better accounted for data than the unbiased (i.e., rational) Bayesian model, confirming that the valence of the new information influenced the amount of updating. Moreover, alternative computational modeling based on reinforcement learning demonstrated higher learning rates for good than for bad news, as well as a moderating role of personal knowledge. Finally, in this specific experimental context, the approach based on reinforcement learning was superior to the Bayesian approach. The computational validation of valence-dependent belief updating represents a novel support for a genuine optimism bias in human belief formation. Moreover, the precise control of relevant cognitive variables justifies the conclusion that the motivation to adopt the most favorable self-referential conclusions biases human judgments. © 2017 Kuzmanovic and Rigoux.",,"Translational Neurocircuitry Group, Max Planck Institute for Metabolism Research, Cologne, Germany; Translational Neuromodeling Unit, Institute for Biomedical Engineering, University of Zurich and ETH Zurich, Zurich, Switzerland",1087,,"Kuzmanovic B., Rigoux L.","Kuzmanovic, B., Translational Neurocircuitry Group, Max Planck Institute for Metabolism Research, Cologne, Germany; Rigoux, L., Translational Neurocircuitry Group, Max Planck Institute for Metabolism Research, Cologne, Germany, Translational Neuromodeling Unit, Institute for Biomedical Engineering, University of Zurich and ETH Zurich, Zurich, Switzerland",1,,10.3389/fpsyg.2017.01087,SCOPUS,1,,,,,,,,,,JUN,,,Frontiers in Psychology,"Bayesian theorem; Belief updating; Computational modeling; Desirability; Motivation, probability; Optimism bias; Risk judgments",,2-s2.0-85021326063,,,,,,,,,,,,Scopus,,,Frontiers in Psychology,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021326063&doi=10.3389%2ffpsyg.2017.01087&partnerID=40&md5=3fe7f419a8d8399d3369b3a1f11610a2,,8,,2017,,,,,,,,,,,,,,,,,,,
A Bayesian reinforcement learning approach for customizing human-robot interfaces,"Personal robots are becoming increasingly prevalent, which raises a number of interesting issues regarding the design and customization of interfaces to such platforms. The particular problem addressed by this paper is the use of learning methods to improve the quality and effectiveness of human-machine interaction onboard a robotic wheelchair. In support of this, we present a method for learning and adapting probabilistic models with the aid of a human operator. We use a Bayesian reinforcement learning framework, that allows us to mix learning and execution, as well as take advantage of prior information about the world. We address the problems of learning, handling a partially observable environment, and limiting the number of action requests. We demonstrate empirical feasibility of our approach on an interface for an autonomous wheelchair. Copyright 2009 ACM.",,"School of Computer Science, McGill University, Montreal, QC H3A 2A7, Canada",,,"Atrash A., Pineau J.","Atrash, A., School of Computer Science, McGill University, Montreal, QC H3A 2A7, Canada; Pineau, J., School of Computer Science, McGill University, Montreal, QC H3A 2A7, Canada",10,,10.1145/1502650.1502700,SCOPUS,1,,,,,,,,,,,,,"International Conference on Intelligent User Interfaces, Proceedings IUI",Activity & plan recognition; Intelligent assistants; Intelligent interfaces for ubiquitous computing,,2-s2.0-77953871643,,,,,,359,355,,,,,Scopus,,,"International Conference on Intelligent User Interfaces, Proceedings IUI",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953871643&doi=10.1145%2f1502650.1502700&partnerID=40&md5=265764ecab4726718eea0e4605b2a094,,,,2009,,,,,,,,,,,,,,,,,,,
Offline and online adaptation in prosocial games,"Personalization and maintenance of high levels of engagement still remain two of the main challenges in the design of serious games. Towards this end, in this paper we propose a novel adaptation approach for both online and offline adaptation in prosocial games. In this paper, we describe the implementation of an artificial intelligence driven adaptation manager, whose purpose is to direct players towards game content the players are most likely to enjoy (measured in their engagement responses). As a consequence, we demonstrate how the adaptation manager can be used to increase the chances of players attaining the game's specific prosocial learning objectives. Each mechanism (offline and online) processes different information about the player and concerns different types of factors affecting engagement and prosocial behavior. More specifically, the online mechanism maintains a player engagement profile for game elements related to the provision of Corrective Feedback and Positive Reinforcement, in order to adapt existing game content in real time. On the other hand, off-line adaptation matches players to game scenarios according to the players' prosocial ability and the game scenarios' ranking. The efficiency of the proposed adaptation manger as a tool for enhancing students' prosocial skills development is demonstrated through a small scale experiment, under real-conditions in a school environment, using the prosocial game of Path of Trust.",,,,"Information Technologies Institute - ITI, Centre for Research and Technology Hellas - CERTH, Thessaloniki, Greece",K. C. Apostolakis; K. Stefanidis; A. Psaltis; K. Dimitropoulos; P. Daras,,,,10.1109/VS-GAMES.2017.8056602,IEEE Xplore,1,,20171005,208,,,artificial intelligence;computer aided instruction;serious games (computing),Path of Trust;artificial intelligence driven adaptation manager;corrective feedback;direct players;engagement responses;existing game content;game elements;game scenarios;maintenance;off-line adaptation;offline adaptation;online adaptation;online mechanism;player engagement profile;positive reinforcement;prosocial behavior;prosocial games;prosocial skills development;school environment;serious games;students,,,,6-8 Sept. 2017,,2017 9th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games),adaptation;engagement;prosociality;serious games,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8056602,,,,,,IEEE,,,Electronic:978-1-5090-5812-9; POD:978-1-5386-1203-3; USB:978-1-5090-5811-2,,2017 9th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games),201,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
A self-taught artificial agent for multi-physics computational model personalization,"Personalization is the process of fitting a model to patient data, a critical step towards application of multi-physics computational models in clinical practice. Designing robust personalization algorithms is often a tedious, time-consuming, model- and data-specific process. We propose to use artificial intelligence concepts to learn this task, inspired by how human experts manually perform it. The problem is reformulated in terms of reinforcement learning. In an off-line phase, Vito, our self-taught artificial agent, learns a representative decision process model through exploration of the computational model: it learns how the model behaves under change of parameters. The agent then automatically learns an optimal strategy for on-line personalization. The algorithm is model-independent; applying it to a new model requires only adjusting few hyper-parameters of the agent and defining the observations to match. The full knowledge of the model itself is not required. Vito was tested in a synthetic scenario, showing that it could learn how to optimize cost functions generically. Then Vito was applied to the inverse problem of cardiac electrophysiology and the personalization of a whole-body circulation model. The obtained results suggested that Vito could achieve equivalent, if not better goodness of fit than standard methods, while being more robust (up to 11% higher success rates) and with faster (up to seven times) convergence rate. Our artificial intelligence approach could thus make personalization algorithms generalizable and self-adaptable to any patient and any model. © 2016",,"Medical Imaging Technologies, Siemens Healthcare GmbH, Erlangen, Germany; Medical Imaging Technologies, Siemens Healthcare, Princeton, United States; Pattern Recognition Lab, FAU Erlangen-Nürnberg, Erlangen, Germany; Siemens Corporate Technology, Siemens SRL, Brasov, Romania; Transilvania University of Brasov, Brasov, Romania; Department of Internal Medicine III, University Hospital Heidelberg, Germany",,,"Neumann D., Mansi T., Itu L., Georgescu B., Kayvanpour E., Sedaghat-Hamedani F., Amr A., Haas J., Katus H., Meder B., Steidl S., Hornegger J., Comaniciu D.","Neumann, D., Medical Imaging Technologies, Siemens Healthcare GmbH, Erlangen, Germany, Pattern Recognition Lab, FAU Erlangen-Nürnberg, Erlangen, Germany; Mansi, T., Medical Imaging Technologies, Siemens Healthcare, Princeton, United States; Itu, L., Siemens Corporate Technology, Siemens SRL, Brasov, Romania, Transilvania University of Brasov, Brasov, Romania; Georgescu, B., Medical Imaging Technologies, Siemens Healthcare, Princeton, United States; Kayvanpour, E., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Sedaghat-Hamedani, F., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Amr, A., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Haas, J., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Katus, H., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Meder, B., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Steidl, S., Pattern Recognition Lab, FAU Erlangen-Nürnberg, Erlangen, Germany; Hornegger, J., Pattern Recognition Lab, FAU Erlangen-Nürnberg, Erlangen, Germany; Comaniciu, D., Medical Imaging Technologies, Siemens Healthcare, Princeton, United States",5,,10.1016/j.media.2016.04.003,SCOPUS,1,,,,,,,,,,,,,Medical Image Analysis,Artificial intelligence; Computational modeling; Model personalization; Reinforcement learning,,2-s2.0-84964690557,,,,,,64,52,,,,,Scopus,,,Medical Image Analysis,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964690557&doi=10.1016%2fj.media.2016.04.003&partnerID=40&md5=b1d426957966100eec637cc992aa1ff5,,34,,2016,,,,,,,,,,,,,,,,,,,
Adaptive treatment allocation using sub-sampled Gaussian processes,"Personalized medicine targets the customization of treatment strategies to patients' individual characteristics. Here we consider the problem of optimizing personalized pharmacological treatment strategies for cancer. We focus primarily on developing effective strategies to collect the data necessary for the construction of personalized treatments. We formulate this problem as a contextual bandit and present a new algorithm based on repeated sub-sampling for robust data collection in this framework. We present a case study showing experiments on a simulation setting, built from real data collected in a previous animal experiments. Promising results in this case study have since lead us to deploy this strategy in a partner wet lab to allocate treatments for the next phase of animal experiments. Copyright © 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Department of Electrical Engineering and Computer Engineering Universite, Laval, QC, Canada; School of Computer Science, McGill University, Montreal, Canada",,,"Durand A., Pineau J.","Durand, A., Department of Electrical Engineering and Computer Engineering Universite, Laval, QC, Canada; Pineau, J., School of Computer Science, McGill University, Montreal, Canada",,,,SCOPUS,0,,,,,,,,,,,,,AAAI Fall Symposium - Technical Report,,,2-s2.0-84964573656,,,,,,11,9,,,,,Scopus,,,AAAI Fall Symposium - Technical Report,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964573656&partnerID=40&md5=534281ed1c527f1c0b6fa9b0c046b90e,,FS-15-04,,2015,,,,,,,,,,,,,,,,,,,
Personalized recommendation via parameter-free contextual bandits,"Personalized recommendation services have gained increasing popularity and attention in recent years as most useful information can be accessed online in real-time. Most online recommender systems try to address the information needs of users by virtue of both user and content information. Despite extensive recent advances, the problem of personalized recommendation remains challenging for at least two reasons. First, the user and item repositories undergo frequent changes, which makes traditional recommendation algorithms ineffective. Second, the so-called cold-start problem is difficult to address, as the information for learning a recommendation model is limited for new items or new users. Both challenges are formed by the dilemma of exploration and exploitation. In this paper, we formulate personalized recommendation as a contextual bandit problem to solve the exploration/exploitation dilemma. Specifically in our work, we propose a parameter-free bandit strategy, which employs a principled resampling approach called online bootstrap, to derive the distribution of estimated models in an online manner. Under the paradigm of probability matching, the proposed algorithm randomly samples a model from the derived distribution for every recommendation. Extensive empirical experiments on two real-world collections of web data (including online advertising and news recommendation) demonstrate the effectiveness of the proposed algorithm in terms of the click-through rate. The experimental results also show that this proposed algorithm is robust in the cold-start situation, in which there is no sufficient data or knowledge to tune the parameters. © 2015 ACM.",,"School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States",,,"Tang L., Jiang Y., Li L., Zeng C., Li T.","Tang, L., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Jiang, Y., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Li, L., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Zeng, C., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Li, T., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States",8,,10.1145/2766462.2767707,SCOPUS,1,,,,,,,,,,,,,SIGIR 2015 - Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,Bootstrapping; Contextual bandit; Personalization; Probability matching; Recommender systems,,2-s2.0-84953732355,,,,,,332,323,,,,,Scopus,,,SIGIR 2015 - Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953732355&doi=10.1145%2f2766462.2767707&partnerID=40&md5=10663d72e54cf9a059b4485b18011be2,,,,2015,,,,,,,,,,,,,,,,,,,
Latent contextual bandits and their application to personalized recommendations for new users,"Personalized recommendations for new users, also known as the cold-start problem, can be formulated as a contextual bandit problem. Existing contextual bandit algorithms generally rely on features alone to capture user variability. Such methods are inefficient in learning new users' interests. In this paper we propose Latent Contextual Bandits. We consider both the benefit of leveraging a set of learned latent user classes for new users, and how we can learn such latent classes from prior users. We show that our approach achieves a better regret bound than existing algorithms. We also demonstrate the benefit of our approach using a large real world dataset and a preliminary user study.",,"Computer Science Department, Carnegie Mellon University, United States",,,"Zhou L., Brunskill E.","Zhou, L., Computer Science Department, Carnegie Mellon University, United States; Brunskill, E., Computer Science Department, Carnegie Mellon University, United States",1,,,SCOPUS,0,,,,,,,,,,,,,IJCAI International Joint Conference on Artificial Intelligence,,,2-s2.0-85006132706,,,,,,3653,3646,,,,,Scopus,,,IJCAI International Joint Conference on Artificial Intelligence,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006132706&partnerID=40&md5=85ce893966ccab257cc8d48bc9141540,,2016-January,,2016,,,,,,,,,,,,,,,,,,,
A contextual-bandit approach to personalized news article recommendation,"Personalized web services strive to adapt their services (advertisements, news articles, etc.) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation. In this work, we model personalized recommendation of news articles as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks. The contributions of this work are three-fold. First, we propose a new, general contextual bandit algorithm that is computationally efficient and well motivated from learning theory. Second, we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic. Finally, using this offline evaluation method, we successfully applied our new algorithm to a Yahoo Front Page Today Module dataset containing over 33 million events. Results showed a 12.5% click lift compared to a standard context-free bandit algorithm, and the advantage becomes even greater when data gets more scarce. © 2010 International World Wide Web Conference Committee (IW3C2).",,"Yahoo Labs., United States; Dept. of Computer Science, Princeton University, United States",,,"Li L., Chu W., Langford J., Schapire R.E.","Li, L., Yahoo Labs., United States; Chu, W., Yahoo Labs., United States; Langford, J., Yahoo Labs., United States; Schapire, R.E., Dept. of Computer Science, Princeton University, United States",350,,10.1145/1772690.1772758,SCOPUS,1,,,,,,,,,,,,,"Proceedings of the 19th International Conference on World Wide Web, WWW '10",contextual bandit; exploration/exploitation dilemma; personalization; recommender systems; web service,,2-s2.0-77954641643,,,,,,670,661,,,,,Scopus,,,"Proceedings of the 19th International Conference on World Wide Web, WWW '10",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954641643&doi=10.1145%2f1772690.1772758&partnerID=40&md5=0deba3e304c09a3db001bfeeca14cd3f,,,,2010,,,,,,,,,,,,,,,,,,,
Towards an agent-based Approach for Service Emergence in Pervasive Computing,"Pervasive computing is a new paradigm with a goal to provide computing and communication services all the time and everywhere. In this paper, a service emergence model for the implementation of pervasive computing applications is presented. Inspired by natural immune system concepts, the model allows the emergence of ad hoc services on the fly according to dynamically changing context environments such as computing context and user context. In this model, ad hoc or composite services are represented by an organization or group of autonomous agents. Agents establish relationships based on affinities to form groups of agents to provide the composite services. Affinities are parameters that represent priorities between agents. They help to distinguish between agents that can satisfy certain conditions or criteria. Affinity adjustments are based on two level of satisfaction. The first level is a local satisfaction depending on available services offered by neighboring agents together with respect to dynamic changes in network resources. The second level is a global satisfaction determined by the user satisfaction regarding the end service provided.",,,,"Institut National des Telecommunications, France",A. Hassnaoui; M. Bakhouya; J. Gaber,,0,,10.1109/AICT-ICIW.2006.196,IEEE Xplore,1,,20060403,15,,Autonomous agents;Computer vision;Context modeling;Context-aware services;Hospitals;Immune system;Personal digital assistants;Pervasive computing;Protocols;Ubiquitous computing,,Agent-based system;Anytime/anywhere elearning;Immune system.;Pervasive computing;Reinforcement learning;Service composition and emergence;collaborative mobile learning,,,,19-25 Feb. 2006,,Advanced Int'l Conference on Telecommunications and Int'l Conference on Internet and Web Applications and Services (AICT-ICIW'06),Agent-based system;Anytime/anywhere elearning;Immune system.;Pervasive computing;Reinforcement learning;Service composition and emergence;collaborative mobile learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1602147,,,,,,IEEE,14,,POD:0-7695-2522-9,,Advanced Int'l Conference on Telecommunications and Int'l Conference on Internet and Web Applications and Services (AICT-ICIW'06),15,IEEE Conferences,,,,,2006,,,,,,,,,,,,,,,,,,,
A multimodal adaptive session manager for physical rehabilitation exercising,"Physical exercising is an essential part of any rehabilitation plan. The subject must be committed to a daily exercising routine, as well as to a frequent contact with the therapist. Rehabilitation plans can be quite expensive and time-consuming. On the other hand, tele-rehabilitation systems can be really helpful and efficient for both subjects and therapists. In this paper, we present ReAdapt, an adaptive module for a tele-rehabilitation system that takes into consideration the progress and performance of the exercising utilizing multisensing data and adjusts the session difficulty resulting to a personalized session. Multimodal data such as speech, facial expressions and body motion are being collected during the exercising and feed the system to decide on the exercise and session difficulty. We formulate the problem as a Markov Decision Process and apply a Reinforcement Learning algorithm to train and evaluate the system on simulated data. © 2015 ACM.",,"HERACLEIA Lab, Computer Science and Engineering Department, University of Texas, Arlington, United States; Computer Science and Engineering Department, University of Texas, Arlington, United States", a33,,"Tsiakas K., Huber M., Makedon F.","Tsiakas, K., HERACLEIA Lab, Computer Science and Engineering Department, University of Texas, Arlington, United States; Huber, M., Computer Science and Engineering Department, University of Texas, Arlington, United States; Makedon, F., HERACLEIA Lab, Computer Science and Engineering Department, University of Texas, Arlington, United States",6,,10.1145/2769493.2769507,SCOPUS,1,,,,,,,,,,,,,"8th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2015 - Proceedings",Markov Decision Process; Multimodal adaptive systems; Personalized rehabilitation systems; Reinforcement Learning,,2-s2.0-84956977207,,,,,,,,,,,,Scopus,,,"8th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2015 - Proceedings",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956977207&doi=10.1145%2f2769493.2769507&partnerID=40&md5=0f798bcebf296279e4bbce0786ed18c2,,,,2015,,,,,,,,,,,,,,,,,,,
Vito – a generic agent for multi-physics model personalization: Application to heart modeling,"Precise estimation of computational physiological model parameters from patient data is one of the main hurdles towards their clinical applicability. Designing robust estimation algorithms is often a tedious and model-specific process. We propose to use, for the first time to our knowledge, artificial intelligence (AI) concepts to learn how to personalize a computational model, inspired by how an expert manually personalizes. We reformulate the parameter estimation problem in terms of Markov decision process and reinforcement learning. In an off-line phase, the artificial agent, called Vito, automatically learns a representative state-action-state model through data-driven exploration of the computational model under consideration. In other words, Vito learns how the model behaves under change of parameters and how to personalize it. Vito then controls the on-line personalization by exploiting its automatically derived action policy. Because the algorithm is model-independent, personalizing a completely new model would require only adjusting some simple parameters of the agent and defining the observations to match, without the full knowledge of the model itself. Vito was evaluated on two challenging problems: the inverse problem of cardiac electrophysiology and the personalization of a lumped-parameter whole-body circulation model. Obtained results suggested that Vito could achieve equivalent goodness of fit than standard methods, while being more robust (up to 25% higher success rates) and with faster (up to three times) convergence rate. Our AI approach could thus make model personalization algorithms generalizable and self-adaptable to any patient, like a human operator. © Springer International Publishing Switzerland 2015.",,"Imaging and Computer Vision, Siemens Corporate Technology, Princeton, NJ, Romania; Pattern Recognition Lab, FAU Erlangen-Nürnberg, Germany; Imaging and Computer Vision, Siemens Corporate Technology, Romania; Department of Internal Medicine III, University Hospital Heidelberg, Germany",,,"Neumann D., Mansi T., Itu L., Georgescu B., Kayvanpour E., Sedaghat-Hamedani F., Haas J., Katus H., Meder B., Steidl S., Hornegger J., Comaniciu D.","Neumann, D., Imaging and Computer Vision, Siemens Corporate Technology, Princeton, NJ, Romania, Pattern Recognition Lab, FAU Erlangen-Nürnberg, Germany; Mansi, T., Imaging and Computer Vision, Siemens Corporate Technology, Princeton, NJ, Romania; Itu, L., Imaging and Computer Vision, Siemens Corporate Technology, Romania; Georgescu, B., Imaging and Computer Vision, Siemens Corporate Technology, Princeton, NJ, Romania; Kayvanpour, E., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Sedaghat-Hamedani, F., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Haas, J., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Katus, H., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Meder, B., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Steidl, S., Pattern Recognition Lab, FAU Erlangen-Nürnberg, Germany; Hornegger, J., Pattern Recognition Lab, FAU Erlangen-Nürnberg, Germany; Comaniciu, D., Imaging and Computer Vision, Siemens Corporate Technology, Princeton, NJ, Romania",1,,10.1007/978-3-319-24571-3_53,SCOPUS,1,,,,,,,,,,,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,,2-s2.0-84950986600,,,,,,449,442,,,,,Scopus,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950986600&doi=10.1007%2f978-3-319-24571-3_53&partnerID=40&md5=c5628b61c47dad9007b6e8161dc760f4,,9350,,2015,,,,,,,,,,,,,,,,,,,
Emotion-driven learning agent for setting rich presence in mobile telephony,"Presence or personal status information is going to be an integral part of human life in the near future. With the possibility of personalizing user preferences in a fine grained way, mobile presence will appeal to most users. Among all the advantages, one of the most annoying problems is to set the presence status manually each time. This paper discusses the development of an intelligent agent based presence client that will learn and make decisions on behalf of the user about his or her presence status. The decision is emotion driven and the learning depends on real world experience. The proposed system utilizes a neural network (NN) based emotion-driven agent to learn user preferences. As a NN learning algorithm, two approaches based on Differential Evolution and Reinforcement have been proposed, of which either one can be used. Rich presence status is set using a scripting language named Call Processing Language; and SIP is used for publishing the presence to others.",,,,"The Royal Institute of Technology (KTH), Sweden and University of Dhaka, Bangladesh",S. Saha; R. Quazi,,0,,10.1109/ICCITECHN.2008.4803023,IEEE Xplore,1,,20090321,126,,Artificial neural networks;Context awareness;Humans;Information technology;Intelligent agent;Mobile computing;Mobile radio mobility management;Neural networks;Publishing;Telephony,authoring languages;learning (artificial intelligence);mobile computing;neural nets;software agents;telephony,call processing language;differential evolution;emotion-driven agent;emotion-driven learning agent;intelligent agent;mobile telephony;neural network;personalizing user preferences;reinforcement;rich presence;scripting language,,,,24-27 Dec. 2008,,2008 11th International Conference on Computer and Information Technology,ANN;CPL;DE;Presence;SIP;context aware;learning agent;neural network;reinforcement,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4803023,,,,,,IEEE,21,,CD-ROM:978-1-4244-2136-7; POD:978-1-4244-2135-0,,2008 11th International Conference on Computer and Information Technology,121,IEEE Conferences,,,,,2008,,,,,,,,,,,,,,,,,,,
Optimizing vs. Matching: Response Strategy in a Probabilistic Learning Task is associated with Negative Symptoms of Schizophrenia,"Previous research indicates that behavioral performance in simple probability learning tasks can be organized into response strategy classifications that are thought to predict important personal characteristics and individual differences. Typically, relatively small proportion of subjects can be identified as optimizers for effectively exploiting the environment and choosing the more rewarding stimulus nearly all of the time. In contrast, the vast majority of subjects behaves sub-optimally and adopts the matching or super-matching strategy, apportioning their responses in a way that matches or slightly exceeds the probabilities of reinforcement. In the present study, we administered a two-choice probability learning paradigm to 51 individuals with schizophrenia (SZ) and 29 healthy controls (NC) to examine whether there are differences in the proportion of subjects falling into these response strategy classifications, and to determine whether task performance is differentially associated with symptom severity and neuropsychological functioning. Although the sample of SZ patients did not differ from NC in overall rate of learning or end performance, significant clinical differences emerged when patients were divided into optimizing, super-matching and matching subgroups based upon task performance. Patients classified as optimizers, who adopted the most advantageous learning strategy, exhibited higher levels of positive and negative symptoms than their matching and super-matching counterparts. Importantly, when both positive and negative symptoms were considered together, only negative symptom severity was a significant predictor of whether a subject would behave optimally, with each one standard deviation increase in negative symptoms increasing the odds of a patient being an optimizer by as much as 80%. These data provide a rare example of a greater clinical impairment being associated with better behavioral performance. © 2010 Elsevier B.V.",,"Department of Psychiatry, Maryland Psychiatric Research Center, University of Maryland School of Medicine, PO Box 21247, Baltimore, MD 21228, United States; Department of Cognitive, Linguistic and Psychological Sciences, Providence, RI, United States; Department of Psychiatry and Human Behavior, Brown University, Providence, RI, United States",,,"Kasanova Z., Waltz J.A., Strauss G.P., Frank M.J., Gold J.M.","Kasanova, Z., Department of Psychiatry, Maryland Psychiatric Research Center, University of Maryland School of Medicine, PO Box 21247, Baltimore, MD 21228, United States; Waltz, J.A., Department of Psychiatry, Maryland Psychiatric Research Center, University of Maryland School of Medicine, PO Box 21247, Baltimore, MD 21228, United States; Strauss, G.P., Department of Psychiatry, Maryland Psychiatric Research Center, University of Maryland School of Medicine, PO Box 21247, Baltimore, MD 21228, United States; Frank, M.J., Department of Cognitive, Linguistic and Psychological Sciences, Providence, RI, United States, Department of Psychiatry and Human Behavior, Brown University, Providence, RI, United States; Gold, J.M., Department of Psychiatry, Maryland Psychiatric Research Center, University of Maryland School of Medicine, PO Box 21247, Baltimore, MD 21228, United States",10,,10.1016/j.schres.2010.12.003,SCOPUS,1,,,,,,,,,,1-3,,,Schizophrenia Research,Exploration; Matching; Negative Symptoms; Probability Learning Task; Reinforcement Learning; Schizophrenia,,2-s2.0-79952316333,,,,,,222,215,,,,,Scopus,,,Schizophrenia Research,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952316333&doi=10.1016%2fj.schres.2010.12.003&partnerID=40&md5=f0df20a2c8bca795481cdcbab687e5bb,,127,,2011,,,,,,,,,,,,,,,,,,,
Learning from demonstration: Teaching a myoelectric prosthesis with an intact limb via reinforcement learning,"Prosthetic arms should restore and extend the capabilities of someone with an amputation. They should move naturally and be able to perform elegant, coordinated movements that approximate those of a biological arm. Despite these objectives, the control of modern-day prostheses is often nonintuitive and taxing. Existing devices and control approaches do not yet give users the ability to effect highly synergistic movements during their daily-life control of a prosthetic device. As a step towards improving the control of prosthetic arms and hands, we introduce an intuitive approach to training a prosthetic control system that helps a user achieve hard-to-engineer control behaviours. Specifically, we present an actor-critic reinforcement learning method that for the first time promises to allow someone with an amputation to use their non-amputated arm to teach their prosthetic arm how to move through a wide range of coordinated motions and grasp patterns. We evaluate our method during the myoelectric control of a multi-joint robot arm by non-amputee users, and demonstrate that by using our approach a user can train their arm to perform simultaneous gestures and movements in all three degrees of freedom in the robot's hand and wrist based only on information sampled from the robot and the user's above-elbow myoelectric signals. Our results indicate that this learning-from-demonstration paradigm may be well suited to use by both patients and clinicians with minimal technical knowledge, as it allows a user to personalize the control of his or her prosthesis without having to know the underlying mechanics of the prosthetic limb. These preliminary results also suggest that our approach may extend in a straightforward way to next-generation prostheses with precise finger and wrist control, such that these devices may someday allow users to perform fluid and intuitive movements like playing the piano, catching a ball, and comfortably shaking hands.",,,,"Department of Computing Science and the Department of Medicine, University of Alberta, Edmonton, AB T6G 2E1, Canada",G. Vasan; P. M. Pilarski,,,,10.1109/ICORR.2017.8009453,IEEE Xplore,1,,20170814,1464,,Elbow;Manipulators;Muscles;Prosthetics;Training;Wrist,electromyography;gait analysis;medical robotics;medical signal processing;prosthetics;signal sampling;student experiments,actor-critic reinforcement learning method;amputation;biological arm;control approaches;coordinated motions;daily-life control;effect highly synergistic movements;elegant coordinated movements;fluid movements;grasp patterns;hard-to-engineer control behaviours;information sampling;intact limb;intuitive movements;learning-from-demonstration paradigm;minimal technical knowledge;modern-day prostheses;multijoint robot arm;myoelectric prosthesis teaching;next-generation prostheses;nonamputated arm;prosthetic arms;prosthetic control system;prosthetic device;prosthetic limb;reinforcement learning;robot hand;three degrees-of-freedom;user above-elbow myoelectric signals;wrist,,,,17-20 July 2017,,2017 International Conference on Rehabilitation Robotics (ICORR),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8009453,,,,,,IEEE,,,Electronic:978-1-5386-2296-4; POD:978-1-5386-2297-1; USB:978-1-5386-2295-7,,2017 International Conference on Rehabilitation Robotics (ICORR),1457,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
An MARL-Based Distributed Learning Scheme for Capturing User Preferences in a Smart Environment,"Providing a personalized service to a user in a smart environment has been one of the key goals in the area of pervasive computing. The proliferation of individually developed smart devices in the name of Internet of Things opens up a possibility of providing personalized services to a user in an autonomous and distributed manner. As a user's task often involves services supported by multiple devices, capturing a device-specific service preference is not enough to maximize a user's comfort. In this paper, we propose a distributed learning scheme for capturing multiple device service preferences in a smart environment. We exploit multi-agent reinforcement learning (MARL) method where each smart device acts as a reinforcement learning agent to incrementally and cooperatively capture a user specific preference of a task. Experiments confirm that smart devices with the proposed scheme are able to capture multiple device service preferences from a small number of interactions with a user and an environment. Also, the proposed transfer learning method improves learning performance for a new task.",,,,"Sch. of Comput., KAIST, Daejeon, South Korea",J. Lim; H. Son; D. Lee; D. Lee,,,,10.1109/SCC.2017.24,IEEE Xplore,1,,20170914,139,,Brightness;Learning (artificial intelligence);Ontologies;Performance evaluation;Servers;Smart devices;TV,learning (artificial intelligence);multi-agent systems;ubiquitous computing,Internet of Things;MARL;autonomous distributed manner;capturing user preferences;device-specific service preference;distributed learning scheme;individually developed smart devices;multiagent reinforcement learning method;multiple device service preferences;personalized service;pervasive computing;reinforcement learning agent;smart environment;transfer learning method;user specific preference,,,,25-30 June 2017,,2017 IEEE International Conference on Services Computing (SCC),context aware;distributed learning;personalization;smart device;user preference,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8034977,,,,,,IEEE,,,Electronic:978-1-5386-2005-2; POD:978-1-5386-2006-9; USB:978-1-5386-2004-5,,2017 IEEE International Conference on Services Computing (SCC),132,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
Deep reinforcement learning for automated radiation adaptation in lung cancer:,"Purpose: To investigate deep reinforcement learning (DRL) based on historical treatment plans for developing automated radiation adaptation protocols for nonsmall cell lung cancer (NSCLC) patients that aim to maximize tumor local control at reduced rates of radiation pneumonitis grade 2 (RP2). Methods: In a retrospective population of 114 NSCLC patients who received radiotherapy, a three-component neural networks framework was developed for deep reinforcement learning (DRL) of dose fractionation adaptation. Large-scale patient characteristics included clinical, genetic, and imaging radiomics features in addition to tumor and lung dosimetric variables. First, a generative adversarial network (GAN) was employed to learn patient population characteristics necessary for DRL training from a relatively limited sample size. Second, a radiotherapy artificial environment (RAE) was reconstructed by a deep neural network (DNN) utilizing both original and synthetic data (by GAN) to estimate the transition probabilities for adaptation of personalized radiotherapy patients' treatment courses. Third, a deep Q-network (DQN) was applied to the RAE for choosing the optimal dose in a response-adapted treatment setting. This multicomponent reinforcement learning approach was benchmarked against real clinical decisions that were applied in an adaptive dose escalation clinical protocol. In which, 34 patients were treated based on avid PET signal in the tumor and constrained by a 17.2% normal tissue complication probability (NTCP) limit for RP2. The uncomplicated cure probability (P+) was used as a baseline reward function in the DRL. Results: Taking our adaptive dose escalation protocol as a blueprint for the proposed DRL (GAN + RAE + DQN) architecture, we obtained an automated dose adaptation estimate for use at ∼2/3 of the way into the radiotherapy treatment course. By letting the DQN component freely control the estimated adaptive dose per fraction (ranging from 1-5 Gy), the DRL automatically favored dose escalation/de-escalation between 1.5 and 3.8 Gy, a range similar to that used in the clinical protocol. The same DQN yielded two patterns of dose escalation for the 34 test patients, but with different reward variants. First, using the baseline P+ reward function, individual adaptive fraction doses of the DQN had similar tendencies to the clinical data with an RMSE = 0.76 Gy; but adaptations suggested by the DQN were generally lower in magnitude (less aggressive). Second, by adjusting the P+ reward function with higher emphasis on mitigating local failure, better matching of doses between the DQN and the clinical protocol was achieved with an RMSE = 0.5 Gy. Moreover, the decisions selected by the DQN seemed to have better concordance with patients eventual outcomes. In comparison, the traditional temporal difference (TD) algorithm for reinforcement learning yielded an RMSE = 3.3 Gy due to numerical instabilities and lack of sufficient learning. Conclusion: We demonstrated that automated dose adaptation by DRL is a feasible and a promising approach for achieving similar results to those chosen by clinicians. The process may require customization of the reward function if individual cases were to be considered. However, development of this framework into a fully credible autonomous system for clinical decision support would require further validation on larger multi-institutional datasets. © 2017 American Association of Physicists in Medicine.",,"Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States; Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan",,,"Tseng H.-H., Luo Y., Cui S., Chien J.-T., Ten Haken R.K., Naqa I.E.","Tseng, H.-H., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States; Luo, Y., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States; Cui, S., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States; Chien, J.-T., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States, Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan; Ten Haken, R.K., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States; Naqa, I.E., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States",2,,10.1002/mp.12625,SCOPUS,1,,,,,,,,,,12,,,Medical Physics,adaptive radiotherapy; deep learning; lung cancer; reinforcement learning,,2-s2.0-85037829216,,,,,,6705,6690,,,,,Scopus,,,Medical Physics,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037829216&doi=10.1002%2fmp.12625&partnerID=40&md5=64aadbb97a2f33ad182430a659f43b97,,44,,2017,,,,,,,,,,,,,,,,,,,
Tools for the Precision Medicine Era: How to Develop Highly Personalized Treatment Recommendations from Cohort and Registry Data Using Q-Learning,"Q-Learning is a method of reinforcement learning that employs backwards stagewise estimation to identify sequences of actions that maximize some long-term reward. The method can be applied to sequential multipleassignment randomized trials to develop personalized adaptive treatment strategies (ATSs)-longitudinal practice guidelines highly tailored to time-varying attributes of individual patients. Sometimes, the basis for choosing which ATSs to include in a sequential multiple-assignment randomized trial (or randomized controlled trial) may be inadequate. Nonrandomized data sources may inform the initial design of ATSs, which could later be prospectively validated. In this paper, we illustrate challenges involved in using nonrandomized data for this purpose with a case study from the Center for International Blood and Marrow Transplant Research registry (1995-2007) aimed at 1) determining whether the sequence of therapeutic classes used in graft-versus-host disease prophylaxis and in refractory graft-versus-host disease is associated with improved survival and 2) identifying donor and patient factors with which to guide individualized immunosuppressant selections over time. We discuss how to communicate the potential benefit derived from following an ATS at the population and subgroup levels and how to evaluate its robustness to modeling assumptions. This worked example may serve as a model for developing ATSs from registries and cohorts in oncology and other fields requiring sequential treatment decisions. © 2017 The Author(s).",,"Department of Epidemiology Biostatistics and Occupational Health, Faculty of Medicine, McGill University, 1020 Pine Avenue West, Montreal, QC, Canada; Division of Hematology and Oncology, Department of Medicine, Hopital MaisonneuveRosemont Research Center, University of Montreal, Montreal, QC, Canada; Center for International Blood and Marrow Transplant Research, Department of Medicine, Medical College of Wisconsin, Milwaukee, WI, United States; Division of Biostatistics, Institute for Health and Society, Medical College of Wisconsin, Milwaukee, WI, United States; Division of Hematology, Oncology, and Transplantation, Department of Medicine, University of Minnesota Medical Center, Minneapolis, MN, United States; Center for International Blood and Marrow Transplant Research, National Marrow Donor Program/Be the Match, Minneapolis, MN, United States; Division of Hematology/BMT, Department of Internal Medicine, University of Utah Health Care and Huntsman Cancer Center, University of Utah, Salt Lake City, UT, United States; Division of Cancer Medicine, Department of Stem Cell Transplantation, University of Texas M.D. Anderson Cancer Center, Houston, TX, United States; Department of Blood and Marrow Transplantation, Moffitt Cancer Center, Tampa, FL, United States; US Department of Defense, Fort Meade, MD, United States",,,"Krakow E.F., Hemmer M., Wang T., Logan B., Arora M., Spellman S., Couriel D., Alousi A., Pidala J., Last M., Lachance S., Moodie E.E.M.","Krakow, E.F., Department of Epidemiology Biostatistics and Occupational Health, Faculty of Medicine, McGill University, 1020 Pine Avenue West, Montreal, QC, Canada, Division of Hematology and Oncology, Department of Medicine, Hopital MaisonneuveRosemont Research Center, University of Montreal, Montreal, QC, Canada; Hemmer, M., Department of Epidemiology Biostatistics and Occupational Health, Faculty of Medicine, McGill University, 1020 Pine Avenue West, Montreal, QC, Canada; Wang, T., Center for International Blood and Marrow Transplant Research, Department of Medicine, Medical College of Wisconsin, Milwaukee, WI, United States, Division of Biostatistics, Institute for Health and Society, Medical College of Wisconsin, Milwaukee, WI, United States; Logan, B., Center for International Blood and Marrow Transplant Research, Department of Medicine, Medical College of Wisconsin, Milwaukee, WI, United States, Division of Biostatistics, Institute for Health and Society, Medical College of Wisconsin, Milwaukee, WI, United States; Arora, M., Division of Hematology, Oncology, and Transplantation, Department of Medicine, University of Minnesota Medical Center, Minneapolis, MN, United States; Spellman, S., Center for International Blood and Marrow Transplant Research, National Marrow Donor Program/Be the Match, Minneapolis, MN, United States; Couriel, D., Division of Hematology/BMT, Department of Internal Medicine, University of Utah Health Care and Huntsman Cancer Center, University of Utah, Salt Lake City, UT, United States; Alousi, A., Division of Cancer Medicine, Department of Stem Cell Transplantation, University of Texas M.D. Anderson Cancer Center, Houston, TX, United States; Pidala, J., Department of Blood and Marrow Transplantation, Moffitt Cancer Center, Tampa, FL, United States; Last, M., Center for International Blood and Marrow Transplant Research, Department of Medicine, Medical College of Wisconsin, Milwaukee, WI, United States, US Department of Defense, Fort Meade, MD, United States; Lachance, S., Division of Hematology and Oncology, Department of Medicine, Hopital MaisonneuveRosemont Research Center, University of Montreal, Montreal, QC, Canada; Moodie, E.E.M., Department of Epidemiology Biostatistics and Occupational Health, Faculty of Medicine, McGill University, 1020 Pine Avenue West, Montreal, QC, Canada",1,,10.1093/aje/kwx027,SCOPUS,1,,,,,,,,,,2,,,American Journal of Epidemiology,Adaptive treatment strategies; Dynamic treatment regimes; Graft-versus-host disease; Machine learning; Personalized medicine; Prediction; Q-learning; Registry data,,2-s2.0-85029668317,,,,,,172,160,,,,,Scopus,,,American Journal of Epidemiology,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029668317&doi=10.1093%2faje%2fkwx027&partnerID=40&md5=1c867a9b4398186b0bd9c0698bd7be80,,186,,2017,,,,,,,,,,,,,,,,,,,
MOOClets: A framework for dynamic experimentation and personalization,"Randomized experiments in online educational environments are ubiquitous as a scientific method for investigating learning and motivation, but they rarely improve educational resources and produce practical benefits for learners. We suggest that tools for experimentally comparing resources are designed primarily through the lens of experiments as a scientific methodology, and therefore miss a tremendous opportunity for online experiments to serve as engines for dynamic improvement and personalization. We present the MOOClet requirements specification to guide the implementation of software tools for experiments to ensure that whenever alternative versions of a resource can be experimentally compared (by randomly assigning versions), the resource can also be dynamically improved (by changing which versions are presented), and personalized (by presenting different versions to different people). The MOOClet specification was used to implement DEXPER, a proof-of-concept web service backend that enables dynamic experimentation and personalization of resources embedded in frontend educational platforms. We describe three use cases of MOOClets for dynamic experimentation and personalization of motivational emails, explanations, and problems. © 2017 ACM.",,"Harvard University, Cambridge, MA, United States; Carleton College, Northfield, MN, United States; San Jose State University, San Jose, CA, United States; KAIST, Daejeon, South Korea",,,"Jaywilliams J., Rafferty A.N., Maldonado S., Ang A., Tingley D., Kim J.","Jaywilliams, J., Harvard University, Cambridge, MA, United States; Rafferty, A.N., Carleton College, Northfield, MN, United States; Maldonado, S., San Jose State University, San Jose, CA, United States; Ang, A., Harvard University, Cambridge, MA, United States; Tingley, D., Harvard University, Cambridge, MA, United States; Kim, J., KAIST, Daejeon, South Korea",,,10.1145/3051457.3054006,SCOPUS,1,,,,,,,,,,,,,L@S 2017 - Proceedings of the 4th (2017) ACM Conference on Learning at Scale,A/B experiment; Adaptive learning; Dynamic experimentation; MOOCLet; Multi-Armed bandit; Personalization; Reinforcement learning; Statistical machine learning,,2-s2.0-85018407094,,,,,,290,287,,,,,Scopus,,,L@S 2017 - Proceedings of the 4th (2017) ACM Conference on Learning at Scale,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018407094&doi=10.1145%2f3051457.3054006&partnerID=40&md5=e7315bb93f2d39de7391ea6a6ad5617a,,,,2017,,,,,,,,,,,,,,,,,,,
Framework for control and deep reinforcement learning in traffic,"Recent advances in deep reinforcement learning (RL) offer an opportunity to revisit complex traffic control problems at the level of vehicle dynamics, with the aim of learning locally optimal policies (with respect to the policy parameterization) for a variety of objectives such as matching a target velocity or minimizing fuel consumption. In this article, we present a framework called CISTAR (Customized Interface for SUMO, TraCI, and RLLab) that integrates the widely used traffic simulator SUMO with a standard deep reinforcement learning library RLLab. We create an interface allowing for easy customization of SUMO, allowing users to easily implement new controllers, heterogeneous experiments, and user-defined cost functions that depend on arbitrary state variables. We demonstrate the usage of CISTAR with several benchmark control and RL examples.",,,,"UC Berkeley, Electrical Engineering and Computer Science",C. Wu; K. Parvate; N. Kheterpal; L. Dickstein; A. Mehta; E. Vinitsky; A. M. Bayen,,,,10.1109/ITSC.2017.8317694,IEEE Xplore,1,,20180315,8,,Acceleration;Automobiles;Learning (artificial intelligence);Libraries;Machine learning;Roads;Vehicle dynamics,control engineering computing;learning (artificial intelligence);optimisation;road traffic control;road vehicles;traffic engineering computing;vehicle dynamics,Customized Interface;SUMO;benchmark control;fuel consumption minimization;locally optimal policies;policy parameterization;standard deep reinforcement learning library RLLab;traffic control;vehicle dynamics,,,,16-19 Oct. 2017,,2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC),Simulation;control;deep reinforcement learning;vehicle dynamics,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8317694,,,,,,IEEE,,,Electronic:978-1-5386-1526-3; POD:978-1-5386-1527-0; USB:978-1-5386-1525-6,,2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC),1,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
A personalized QoE-aware handover decision based on distributed reinforcement learning,"Recent developments in heterogeneous mobile networks and growing demands for variety of real-time and multimedia applications have emphasized the necessity of more intelligent handover decisions. Addressing the context knowledge of mobile devices, users, applications, and networks is the subject of context-aware handoff decision as a recent effort to this aim. However, user perception has not been attended adequately in the area of context-aware handover decision making. Mobile users may have different judgments about the Quality of Service (QoS) depending on their environmental conditions, and personal and psychological characteristics. This reality has been exploited in this paper to introduce a personalized user-centric handoff decision method to decide about the time and target of handover based on User Perceived Quality (UPQ) feedbacks. The UPQ degradations are mainly for the sake of (1) exiting the coverage of the serving Point of Attachment (PoA) or (2) QoS degradation of serving access network. Using UPQ metric, the proposed method obviates the necessity of being aware about rapidly varying network QoS parameters and overcomes the complexity and overhead of gathering and managing some other context information. Moreover, considering the underlying network and geographical map, the proposed method is able to inherently exploit the trajectory information of mobile users for handover decision. UPQ degradation is not only due to the user behaviour, but also due to the behaviours of others users. As such, multi-agent reinforcement learning paradigm has been considered for target PoA selection. The employed decision algorithm is based on WoLF-PHC learning method where UPQ is used as a delayed reward for training. The proposed handoff decision has been implemented under IEEE 802.21 framework using NS2 network simulator. The results have shown better performance of the proposed method comparing to conventional methods assuming regular movement of mobile users. © 2013 Springer Science+Business Media New York.",,"Department of Information Technology Engineering, University of Isfahan, Isfahan, Iran; Department of Computer Engineering, University of Isfahan, Isfahan, Iran",,,"Ghahfarokhi B.S., Movahhedinia N.","Ghahfarokhi, B.S., Department of Information Technology Engineering, University of Isfahan, Isfahan, Iran; Movahhedinia, N., Department of Computer Engineering, University of Isfahan, Isfahan, Iran",5,,10.1007/s11276-013-0572-2,SCOPUS,1,,,,,,,,,,8,,,Wireless Networks,Context-aware handover; Distributed reinforcement learning; QoE-aware handover; User perceived quality,,2-s2.0-84886583003,,,,,,1828,1807,,,,,Scopus,,,Wireless Networks,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886583003&doi=10.1007%2fs11276-013-0572-2&partnerID=40&md5=163b8778ea5599a29885ace91718c438,,19,,2013,,,,,,,,,,,,,,,,,,,
Computational heterogeneity in the human mesencephalic dopamine system,"Recent evidence in animals has indicated that the mesencephalic dopamine system is heterogeneous anatomically, molecularly, and functionally, and it has been suggested that the dopamine system comprises distinct functional systems. Identifying and characterizing these systems in humans will have widespread ramifications for understanding drug addiction and mental health disorders. Model-based studies in humans have suggested an analogous computational heterogeneity, in which dopaminergic targets in striatum encode both experience-based learning signals and counterfactual learning signals that are based on hypothetical information. We used brainstem-tailored fMRI to identify mesencephalic sources of experiential and counterfactual learning signals. Participants completed a decision-making task based on investing in markets. This sequential investment task generated experience-based learning signals, in the form of temporal difference (TD) reward prediction errors, and counterfactual learning signals, in the form of ""fictive errors."" Fictive errors are reinforcement learning signals based on hypothetical information about ""what could have been."" An additional learning signal was constructed to be relatable to a motivational salience signal. Blood oxygenation level dependent responses in regions of substantia nigra (SN) and ventral tegmental area (VTA), where dopamine neurons are located, coded for TD and fictive errors, and additionally were related to the motivational salience signal these results are highly consistent with animal electrophysiology and provide direct evidence that human SN and VTA heterogeneously handle important reward-harvesting computations. © 2013 The Author(s).",,"Virginia Tech Carilion Research Institute, Roanoke, VA, United States; Pediatric Emergency Medicine, Baylor College of Medicine, Houston, TX, United States; Department of Physics, Virginia Tech, Blacksburg, VA, United States; Wellcome Centre for Neuroimaging, University College London, 12 Queen Square, London WC1N 3BG, United Kingdom; Human Neuroimaging Laboratory, Virginia Tech Carilion Research Institute, 2 Riverside Circle, Roanoke, VA 24016, United States",,,"D'Ardenne K., Lohrenz T., Bartley K.A., Montague P.R.","D'Ardenne, K., Virginia Tech Carilion Research Institute, Roanoke, VA, United States; Lohrenz, T., Virginia Tech Carilion Research Institute, Roanoke, VA, United States; Bartley, K.A., Pediatric Emergency Medicine, Baylor College of Medicine, Houston, TX, United States; Montague, P.R., Virginia Tech Carilion Research Institute, Roanoke, VA, United States, Department of Physics, Virginia Tech, Blacksburg, VA, United States, Wellcome Centre for Neuroimaging, University College London, 12 Queen Square, London WC1N 3BG, United Kingdom, Human Neuroimaging Laboratory, Virginia Tech Carilion Research Institute, 2 Riverside Circle, Roanoke, VA 24016, United States",18,,10.3758/s13415-013-0191-5,SCOPUS,1,,,,,,,,,,4,,,"Cognitive, Affective and Behavioral Neuroscience",Brainstem fMRI; Decision making; Dopamine; Reinforcement learning; Reward; SN; VTA,,2-s2.0-84891421186,,,,,,756,747,,,,,Scopus,,,"Cognitive, Affective and Behavioral Neuroscience",,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891421186&doi=10.3758%2fs13415-013-0191-5&partnerID=40&md5=88015b9b776289751a9ec8a31ec8fe8f,,13,,2013,,,,,,,,,,,,,,,,,,,
A learning-based control architecture for an assistive robot providing social engagement during cognitively stimulating activities,"Recent studies have shown that sustained engagement in cognitively stimulating activities has had positive effects on the cognitive functioning of humans. The objective of our work is to develop an intelligent socially assistive robot that can engage individuals in person-centered cognitively stimulating activities. In this paper, we present the design of a novel learning-based control architecture that enables the robot to act as a social motivator by providing assistance, encouragement and celebration during the course of an activity. A hierarchical reinforcement learning (HRL) approach is used to provide the robot with the ability to: (i) learn appropriate assistive behaviors based on the structure of the activity and (ii) personalize the interaction based on the person's affective state during the activity. Preliminary experiments show that the proposed learning-based control architecture is effective in determining the optimal assistive behaviors of the robot during a memory game interaction.",,,,"Autonomous Systems and Biomechatronics Laboratory in the Department of Mechanical and Industrial Engineering at the University of Toronto, 5 King's College Road, ON, M5S 3G8 Canada",J. Chan; G. Nejat,,4,,10.1109/ICRA.2011.5980426,IEEE Xplore,1,,20110818,3933,,Games;Heart rate;Humans;Robot sensing systems;Speech recognition;Training,cognition;intelligent robots;learning (artificial intelligence);service robots;social sciences,cognitively stimulating activity;hierarchical reinforcement learning;human cognitive function;intelligent socially assistive robot;learning-based control architecture;memory game interaction,,1050-4729;10504729,,9-13 May 2011,,2011 IEEE International Conference on Robotics and Automation,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5980426,,,,,,IEEE,18,,CD:978-1-61284-380-3; Electronic:978-1-61284-385-8; Paper:978-1-61284-386-5,,2011 IEEE International Conference on Robotics and Automation,3928,IEEE Conferences,,,,,2011,,,,,,,,,,,,,,,,,,,
"Individual differences in substance dependence: At the intersection of brain, behaviour and cognition","Recent theories of drug dependence propose that the transition from occasional recreational substance use to harmful use and dependence results from the impact of disrupted midbrain dopamine signals for reinforcement learning on frontal brain areas that implement cognitive control and decision-making.We investigated this hypothesis in humans using electrophysiological and behavioral measures believed to assay the integrity of midbrain dopamine system and its neural targets. Our investigation revealed two groups of dependent individuals, one characterized by disrupted dopamine-dependent reward learning and the other by disrupted error learning associated with depression-proneness. These results highlight important neurobiological and behavioral differences between two classes of dependent users that can inform the development of individually tailored treatment programs. © 2010 The Authors, Addiction Biology © 2010 Society for the Study of Addiction.",,"Department of Psychology, University of Victoria, P.O. Box 3050 STN CSC, Victoria, BC V8W 3P5, Canada; Center of Addiction Research of British Columbia, University of Victoria, Canada; Child and Youth Care, University of Victoria, Canada",,,"Baker T.E., Stockwell T., Barnes G., Holroyd C.B.","Baker, T.E., Department of Psychology, University of Victoria, P.O. Box 3050 STN CSC, Victoria, BC V8W 3P5, Canada; Stockwell, T., Department of Psychology, University of Victoria, P.O. Box 3050 STN CSC, Victoria, BC V8W 3P5, Canada, Center of Addiction Research of British Columbia, University of Victoria, Canada; Barnes, G., Child and Youth Care, University of Victoria, Canada; Holroyd, C.B., Department of Psychology, University of Victoria, P.O. Box 3050 STN CSC, Victoria, BC V8W 3P5, Canada",21,,10.1111/j.1369-1600.2010.00243.x,SCOPUS,1,,,,,,,,,,3,,,Addiction Biology,Addiction; Cognitive control; Event-related brain potentials; Feedback error-related negativity; Midbrain dopamine system; Reinforcement learning,,2-s2.0-79959823204,,,,,,466,458,,,,,Scopus,,,Addiction Biology,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959823204&doi=10.1111%2fj.1369-1600.2010.00243.x&partnerID=40&md5=e52c842a99576e3ddaf9e327c0554916,,16,,2011,,,,,,,,,,,,,,,,,,,
Decomposing drama management in educational interactive narrative: A modular reinforcement learning approach,"Recent years have seen growing interest in data-driven approaches to personalized interactive narrative generation and drama management. Reinforcement learning (RL) shows particular promise for training policies to dynamically shape interactive narratives based on corpora of player-interaction data. An important open question is how to design reinforcement learning-based drama managers in order to make effective use of player interaction data, which is often expensive to gather and sparse relative to the vast state and action spaces required by drama management. We investigate an offline optimization framework for training modular reinforcement learning-based drama managers in an educational interactive narrative, CRYSTAL ISLAND. We leverage importance sampling to evaluate drama manager policies derived from different decompositional representations of the interactive narrative. Empirical results show significant improvements in drama manager quality from adopting an optimized modular RL decomposition compared to competing representations. © Springer International Publishing AG 2016.",,"North Carolina State University, Raleigh, NC, United States",,,"Wang P., Rowe J., Mott B., Lester J.","Wang, P., North Carolina State University, Raleigh, NC, United States; Rowe, J., North Carolina State University, Raleigh, NC, United States; Mott, B., North Carolina State University, Raleigh, NC, United States; Lester, J., North Carolina State University, Raleigh, NC, United States",1,,10.1007/978-3-319-48279-8_24,SCOPUS,1,,,,,,,,,,,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Drama management; Educational interactive narrative; Intelligent narrative technologies; Modular reinforcement learning,,2-s2.0-84996879693,,,,,,282,270,,,,,Scopus,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996879693&doi=10.1007%2f978-3-319-48279-8_24&partnerID=40&md5=3e9d8ada18e9ba06d96af278df3be0e5,,10045 LNCS,,2016,,,,,,,,,,,,,,,,,,,
Design of semi-decentralized control laws for distributed-air-jet micromanipulators by reinforcement learning,"Recently, a great deal of interest has been developed in learning in multi-agent systems to achieve decentralized control. Machine learning is a popular approach to find controllers that are tailored exactly to the system without any prior model. In this paper, we propose a semi-decentralized reinforcement learning control approach in order to position and convey an object on a contact-free MEMS-based distributed-manipulation system. The experimental results validate the semi-decentralized reinforcement learning method as a way to design control laws for such distributed systems.",,,,"FEMTOST/UFC-ENSMM-UTBM-CNRS, Universit&#233; de Franche-Comt&#233;, Besan&#231;on, France",L. Matignon; G. J. Laurent; N. Le Fort-Piat,,1,,10.1109/IROS.2009.5353902,IEEE Xplore,1,,20091215,3283,,Actuators;Control systems;Distributed control;Electrodes;Machine learning;Micromanipulators;Microvalves;Multiagent systems;Open loop systems;Sorting,distributed control;jets;learning (artificial intelligence);micromanipulators;micromechanical devices;multi-agent systems,MEMS based distributed manipulation system;distributed air jet micromanipulators;distributed systems;machine learning;multi agent systems;reinforcement learning;semi decentralized control laws;semi decentralized reinforcement learning control,,2153-0858;21530858,,10-15 Oct. 2009,,2009 IEEE/RSJ International Conference on Intelligent Robots and Systems,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5353902,,,,,,IEEE,21,,POD:978-1-4244-3803-7,,2009 IEEE/RSJ International Conference on Intelligent Robots and Systems,3277,IEEE Conferences,,,,,2009,,,,,,,,,,,,,,,,,,,
A design proposal of a game-based professional training system for highly dangerous professions,"Recently, modern society frequently faces local wars, terrorism, earthquakes, fire accidents, epidemics and coal mining accidents. Members of highly dangerous professions must obtain rigorous training so that they could bear the great historical mission. Generally speaking, these professions include armed forces, special police force, fire department, astronauts and mine disaster rescue troop. The game-based professional training systems for highly dangerous professions have their own distinct requirements. The aim of game-based learning systems is not only the study of declarative knowledge, but also entraining procedural knowledge through repeated practice until it becomes an automatic skill. The result of highly dangerous professional training is extremely important, since if a trainee dose not master the basic knowledge and skill, they could be in grave danger; the trainee's mental qualities should be continuously prompted by the training system so that they could be act intuitively under the most execrable circumstance. Based on requirements analysis and taking the case of mining rescue into account, we divide the whole training system into three parts: machine learning subsystem, brain information subsystem and credit-assignment subsystem. The machine learning subsystem (as know as serious game subsystem), contains the audio-visual coherency analysis, semantic annotation of a scene based on association memory, cooperating management of audio-visual cross-modal signals, personalization rendering of a scene. The brain information subsystem includes functions for receiving, storing and analyzing trainee's trial data based on visual and auditory signals from EEG, sEMG and psychological tests. The credit-assignment subsystem involves trainee's profiles and effect evaluation which are sent from brain information subsystem to machine learning subsystem, while the plan of knowledge learning, the result of skills training and consequence of the desensitization trial are sent as the feedback to brain information subsystem. Therefore, the whole framework works as a reinforcement learning system. The kernel of this system is the cooperating learning schema of audio-visual cross-modal signals. Furthermore, in this system the main visual signals contain scene textures, 3D character animation, 3D scene animation, while the main auditory signals contain the realistic sound, the on-the-spot orders, the on-the-spot yells and background music. In the light of cognitive principles, the following factors should be considered when a game-based leaning system is designed: (1)The working memory including phonological loop and visuo-spatial sketchpad act as two slave systems, play the role of dual sensory channels so that semantic coherency of the visual and the auditory data could be combined with the prior knowledge to be formed as long-term memory; (2) A goal of cooperative learning for audio-visual cross-modal signals is to create an approach which can process verbal information(like the realistic sound and the on-the-spot orders) and non-verbal information(such as 3D character animation as well as 3D scene animation) from the two separate subsystems; (3) Schema acquisition (based on Theory of Cognitive Load -TCL) should be a primary means of learning, and the automation of cognitive process (including declarative knowledge procedural knowledge) will be used to reduce working memory load.",,"College of Computer and Software, Taiyuan University of Technology, China",,,"Xueli y., Zhi L., Changneng Z., Guangping Z., Zengrong L.","Xueli, y., College of Computer and Software, Taiyuan University of Technology, China; Zhi, L., College of Computer and Software, Taiyuan University of Technology, China; Changneng, Z., College of Computer and Software, Taiyuan University of Technology, China; Guangping, Z., College of Computer and Software, Taiyuan University of Technology, China; Zengrong, L., College of Computer and Software, Taiyuan University of Technology, China",,,,SCOPUS,0,,,,,,,,,,,,,Proceedings of the European Conference on Games-based Learning,Audio-visual coherency; Game-based professional training; Highly dangerous professions; Theory of cognitive load; Working memory,,2-s2.0-84938575896,,,,,,394,388,,,,,Scopus,,,Proceedings of the European Conference on Games-based Learning,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938575896&partnerID=40&md5=6cd3db1e9a9c7a1b9429e5e93b4e8cab,,2009-January,,2009,,,,,,,,,,,,,,,,,,,
Reinforcement learning approach to dynamic activation of base station resources in wireless networks,"Recently, the issue of energy efficiency in wireless networks has attracted much research attention due to the growing concern on global warming and operator's profitability. We focus on energy efficiency of base stations because they account for 80% of total energy consumed in a wireless network. In this paper, we intend to reduce energy consumption of a base station by dynamically activating and deactivating the modular resources at the base station depending on the instantaneous network traffic. We propose an online reinforcement learning algorithm that will continuously adapt to the changing network traffic in deciding which action to take to maximize energy saving. As an online algorithm, the proposed scheme does not require a separate training phase and can be deployed immediately. Simulation results have confirmed that the proposed algorithm can achieve more than 50% energy saving without compromising network service quality which is measured in terms of user blocking probability.",,,,"Khalifa Univ. of Sci., Technol. & Res. (KUSTAR), Abu Dhabi, United Arab Emirates",P. Y. Kong; D. Panaitopol,,4,,10.1109/PIMRC.2013.6666710,IEEE Xplore,1,,20131125,3268,,Base stations;Dynamic scheduling;Energy consumption;Heuristic algorithms;Learning (artificial intelligence);Q-factor;Wireless networks,learning (artificial intelligence);probability;radio networks;telecommunication computing;telecommunication traffic,base station resource dynamic activation;blocking probability;energy efficiency;energy saving;global warming;network traffic;operator profitability;reinforcement learning approach;wireless networks,,2166-9570;21669570,,8-11 Sept. 2013,,"2013 IEEE 24th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",Green wireless networks;energy efficient base station;online Q-Learning;reinforcement learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6666710,,,,,,IEEE,10,,Electronic:978-1-4673-6235-1; POD:978-1-4673-6233-7; USB:978-1-4673-6234-4,,"2013 IEEE 24th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",3264,IEEE Conferences,,,,,2013,,,,,,,,,,,,,,,,,,,
Learning to rank for recommender systems,"Recommender system aim at providing a personalized list of items ranked according to the preferences of the user, as such ranking methods are at the core of many recommendation algorithms. The topic of this tutorial focuses on the cutting-edge algorithmic development in the area of recommender systems. This tutorial will provide an in depth picture of the progress of ranking models in the field, summarizing the strengths and weaknesses of existing methods, and discussing open issues that could be promising for future research in the community. A qualitative and quantitative comparison between different models will be provided while we will also highlight recent developments in the areas of Reinforcement Learning. © 2013 ACM.",,"Telefonica Research, Spain; Delft University of Technology, Netherlands",,,"Karatzoglou A., Baltrunas L., Shi Y.","Karatzoglou, A., Telefonica Research, Spain; Baltrunas, L., Telefonica Research, Spain; Shi, Y., Delft University of Technology, Netherlands",18,,10.1145/2507157.2508063,SCOPUS,1,,,,,,,,,,,,,RecSys 2013 - Proceedings of the 7th ACM Conference on Recommender Systems,Collaborative filtering; Learning to rank; Ranking; Recommender systems,,2-s2.0-84887600751,,,,,,494,493,,,,,Scopus,,,RecSys 2013 - Proceedings of the 7th ACM Conference on Recommender Systems,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887600751&doi=10.1145%2f2507157.2508063&partnerID=40&md5=6fb877b463ecc336e9a285a543b4ae18,,,,2013,,,,,,,,,,,,,,,,,,,
Learning and adaptivity in interactive recommender systems,"Recommender systems are intelligent E-commerce applications that assist users in a decision-making process by offering personalized product recommendations during an interaction session. Quite recently, conversational approaches have been introduced in order to support more interactive recommendation sessions. Notwithstanding the increased interactivity offered by these approaches, the system employs an interaction strategy that is specified apriori (at design time) and followed quite rigidly during the interaction. In this paper, we present a new type of recommender system which is capable of learning autonomously an adaptive interaction strategy for assisting the users in acquiring their interaction goals. We view the recommendation process as a sequential decision problem and we model it as a Markov Decision Process (MDP). We learn a model of the user behavior, and use it to acquire the adaptive strategy using Reinforcement Learning (RL) techniques. In this context, the system learns the optimal strategy by observing the consequences of its actions on the users and also on the final outcome of the recommendation session. We apply our approach within an existing travel recommender system which uses a rigid, non-adaptive support strategy for advising a user in refining a query to a travel product catalogue. The initial results demonstrate the value of our approach and show that our system is able to improve the non-adaptive strategy in order to learn an optimal (adaptive) recommendation strategy. Copyright 2007 ACM.",,"University of Trento, Trento, Italy; Free University of Bozen-Bolzano, Bolzano, Italy",,,"Mahmood T., Ricci F.","Mahmood, T., University of Trento, Trento, Italy; Ricci, F., Free University of Bozen-Bolzano, Bolzano, Italy",25,,10.1145/1282100.1282114,SCOPUS,1,,,,,,,,,,,,,ACM International Conference Proceeding Series,Adaptivity; Conversational recommender systems; Markov decision process; Reinforcement learning,,2-s2.0-36849060222,,,,,,84,75,,,,,Scopus,,,ACM International Conference Proceeding Series,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36849060222&doi=10.1145%2f1282100.1282114&partnerID=40&md5=3eb06672fc6e015b5b7949df5d904559,,258,,2007,,,,,,,,,,,,,,,,,,,
Locality-sensitive linear bandit model for online social recommendation,"Recommender systems provide personalized suggestions by learning users’ preference based on their historical feedback. To alleviate the heavy relying on historical data, several online recommendation methods are recently proposed and have shown the effectiveness in solving data sparsity and cold start problems in recommender systems. However, existing online recommendation methods neglect the use of social connections among users, which has been proven as an effective way to improve recommendation accuracy in offline settings. In this paper, we investigate how to leverage social connections to improve online recommendation performance. In particular, we formulate the online social recommendation task as a contextual bandit problem and propose a Locality-sensitive Linear Bandit (LS.Lin) method to solve it. The proposed model incorporates users’ local social relations into a linear contextual bandit model and is capable to deal with the dynamic changes of user preference and the network structure. We provide a theoretical analysis to the proposed LS.Lin method and then demonstrate its improved performance for online social recommendation in empirical studies compared with baseline methods. © Springer International Publishing AG 2016.",,"Shenzhen Key Laboratory of Rich Media Big Data Analytics and Applications, Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong",,,"Zhao T., King I.","Zhao, T., Shenzhen Key Laboratory of Rich Media Big Data Analytics and Applications, Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China, Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong; King, I., Shenzhen Key Laboratory of Rich Media Big Data Analytics and Applications, Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China, Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong",1,,10.1007/978-3-319-46687-3_9,SCOPUS,1,,,,,,,,,,,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Linear bandits; Online learning; Social recommendation,,2-s2.0-84992623509,,,,,,90,80,,,,,Scopus,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992623509&doi=10.1007%2f978-3-319-46687-3_9&partnerID=40&md5=9050adb209e548fd851f01c9385bc3ba,,9947 LNCS,,2016,,,,,,,,,,,,,,,,,,,
Design of recurrent neural networks for solving constrained least absolute deviation problems,"Recurrent neural networks for solving constrained least absolute deviation (LAD) problems or L1-norm optimization problems have attracted much interest in recent years. But so far most neural networks can only deal with some special linear constraints efficiently. In this paper, two neural networks are proposed for solving LAD problems with various linear constraints including equality, two-sided inequality and bound constraints. When tailored to solve some special cases of LAD problems in which not all types of constraints are present, the two networks can yield simpler architectures than most existing ones in the literature. In particular, for solving problems with both equality and one-sided inequality constraints, another network is invented. All of the networks proposed in this paper are rigorously shown to be capable of solving the corresponding problems. The different networks designed for solving the same types of problems possess the same structural complexity, which is due to the fact these architectures share the same computing blocks and only differ in connections between some blocks. By this means, some flexibility for circuits realization is provided. Numerical simulations are carried out to illustrate the theoretical results and compare the convergence rates of the networks. © 2006 IEEE.",,"State Key Laboratory of Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China; School of Automation, Southeast University, Nanjing 210096, China",5487384,,"Hu X., Sun C., Zhang B.","Hu, X., State Key Laboratory of Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China; Sun, C., State Key Laboratory of Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China; Zhang, B., School of Automation, Southeast University, Nanjing 210096, China",18,,10.1109/TNN.2010.2048123,SCOPUS,1,,,,,,,,,,7,,,IEEE Transactions on Neural Networks,L1-norm optimization; least absolute deviation (LAD); minimax optimization; recurrent neural network (RNN); stability analysis,,2-s2.0-77954563179,,,,,,1086,1073,,,,,Scopus,,,IEEE Transactions on Neural Networks,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954563179&doi=10.1109%2fTNN.2010.2048123&partnerID=40&md5=89b6a9456374fa8c76f6ffd6d0839616,,21,,2010,,,,,,,,,,,,,,,,,,,
AgentX: Using Reinforcement Learning to Improve the Effectiveness of Intelligent Tutoring Systems,"Reinforcement Learning (RL) can be used to train an agent to comply with the needs of a student using an intelligent tutoring system. In this paper, we introduce a method of increasing efficiency by way of customization of the hints provided by a tutoring system, by applying techniques from RL to gain knowledge about the usefulness of hints leading to the exclusion or introduction of other helpful hints. Students are clustered into learning levels and can influence the agents method of selecting actions in each state in their cluster of affect. In addition, students can change learning levels based on their performance within the tutoring system and continue to affect the entire student population. The RL agent, AgentX, then uses the cluster information to create one optimal policy for all students in the cluster and begin to customize the help given to the cluster based on that optimal policy. © Springer-Verlag 2004 References.",,"Department of Computer Science, University of Massachusetts, 140 Governors Drive, Amherst, MA 01003, United States",,,"Martin K.N., Arroyo I.","Martin, K.N., Department of Computer Science, University of Massachusetts, 140 Governors Drive, Amherst, MA 01003, United States; Arroyo, I., Department of Computer Science, University of Massachusetts, 140 Governors Drive, Amherst, MA 01003, United States",17,,,SCOPUS,0,,,,,,,,,,,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,,2-s2.0-35048874714,,,,,,572,564,,,,,Scopus,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048874714&partnerID=40&md5=c80fd6957e1ff113fd93a7e0ef37fab6,,3220,,2004,,,,,,,,,,,,,,,,,,,
MOSAIC for Multiple-Reward Environments,"Reinforcement learning (RL) can provide a basic framework for autonomous robots to learn to control and maximize future cumulative rewards in complex environments. To achieve high performance, RL controllers must consider the complex external dynamics for movements and task (reward function) and optimize control commands. For example, a robot playing tennis and squash needs to cope with the different dynamics of a tennis or squash racket and such dynamic environmental factors as the wind. In addition, this robot has to tailor its tactics simultaneously under the rules of either game. This double complexity of the external dynamics and reward function sometimes becomes more complex when both the multiple dynamics and multiple reward functions switch implicitly, as in the situation of a real (multi-agent) game of tennis where one player cannot observe the intention of her opponents or her partner. The robot must consider its opponent's and its partner's unobservable behavioral goals (reward function). In this article, we address how an RL agent should be designed to handle such double complexity of dynamics and reward. We have previously proposed modular selection and identification for control (MOSAIC) to cope with nonstationary dynamics where appropriate controllers are selected and learned among many candidates based on the error of its paired dynamics predictor: the forward model. Here we extend this framework for RL and propose MOSAIC-MR architecture. It resembles MOSAIC in spirit and selects and learns an appropriate RL controller based on the RL controller's TD error using the errors of the dynamics (the forward model) and the reward predictors. Furthermore, unlike other MOSAIC variants for RL, RL controllers are not a priori paired with the fixed predictors of dynamics and rewards. The simulation results demonstrate that MOSAIC-MR outperforms other counterparts because of this flexible association ability among RL controllers, forward models, and reward pr- dictors.",,,,"Center for Information and Neural Networks, National Institute of Information and Communications Technology, Kyoto 619-0288, Japan, and Department of Brain Robot Interface, Brain Information Communication Research Laboratory Group, ATR, Kyoto 619-0288, Japan xsugi@nict.go.jp",N. Sugimoto; M. Haruno; K. Doya; M. Kawato,,2,,10.1162/NECO_a_00246,IEEE Xplore,1,,20140519,606,,,,,,0899-7667;08997667,3,March 2012,,Neural Computation,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6797532,,,,,,MIT Press,,,,,Neural Computation,577,MIT Press Journals,,,24,,2012,,,,,,,,,,,,,,,,,,,
Customised pearlmutter propagation: A hardware architecture for trust region policy optimisation,"Reinforcement Learning (RL) is an area of machine learning in which an agent interacts with the environment by making sequential decisions. The agent receives reward from the environment to find an optimal policy that maximises the reward. Trust Region Policy Optimisation (TRPO) is a recent policy optimisation algorithm that achieves superior results in various RL benchmarks, but is computationally expensive. This paper proposes Customised Pearlmutter Propagation (CPP), a novel hardware architecture that accelerates TRPO on FPGA. We use the Pearlmutter Algorithm to address the key computational bottleneck of TRPO in a hardware efficient manner, avoiding symbolic differentiation with change of variables. Experimental evaluation using robotic locomotion benchmarks demonstrates that the proposed CPP architecture implemented on Stratix-V FPGA can achieve up to 20 times speed-up against 6-threaded Keras deep learning library with Theano backend running on a Core i7-5930K CPU.",,,,"Department of Computing, Imperial College London",S. Shao; W. Luk,,,,10.23919/FPL.2017.8056789,IEEE Xplore,1,,20171005,6,,Acceleration;Benchmark testing;Computer architecture;Field programmable gate arrays;Hardware;Machine learning;Optimization,computer architecture;field programmable gate arrays;learning (artificial intelligence);multi-agent systems;optimisation,CPP architecture;Customised Pearlmutter Propagation;Customised pearlmutter propagation;Pearlmutter Algorithm;Stratix-V FPGA;TRPO;hardware architecture;machine learning;reinforcement learning;robotic locomotion benchmarks;sequential decision making;trust region policy optimisation,,,,4-8 Sept. 2017,,2017 27th International Conference on Field Programmable Logic and Applications (FPL),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8056789,,,,,,IEEE,,,Electronic:978-9-0903-0428-1; POD:978-1-5386-2040-3,,2017 27th International Conference on Field Programmable Logic and Applications (FPL),1,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
Neural control of a tracking task via attention-gated reinforcement learning for brain-machine interfaces,"Reinforcement learning (RL)-based brain machine interfaces (BMIs) enable the user to learn from the environment through interactions to complete the task without desired signals, which is promising for clinical applications. Previous studies exploited Q-learning techniques to discriminate neural states into simple directional actions providing the trial initial timing. However, the movements in BMI applications can be quite complicated, and the action timing explicitly shows the intention when to move. The rich actions and the corresponding neural states form a large state-action space, imposing generalization difficulty on Q-learning. In this paper, we propose to adopt attention-gated reinforcement learning (AGREL) as a new learning scheme for BMIs to adaptively decode high-dimensional neural activities into seven distinct movements (directional moves, holdings and resting) due to the efficient weight-updating. We apply AGREL on neural data recorded from M1 of a monkey to directly predict a seven-action set in a time sequence to reconstruct the trajectory of a center-out task. Compared to Q-learning techniques, AGREL could improve the target acquisition rate to 90.16% in average with faster convergence and more stability to follow neural activity over multiple days, indicating the potential to achieve better online decoding performance for more complicated BMI tasks. © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",,"Qiushi Academy for Advanced Studies, Key Laboratory of Biomedical Engineering of Ministry of Education, Zhejiang University, Hangzhou, China; Qiushi Academy for Advanced Studies, Department of Biomedical Engineering, Zhejiang University, Hangzhou, China",6863657,,"Wang Y., Wang F., Xu K., Zhang Q., Zhang S., Zheng X.","Wang, Y., Qiushi Academy for Advanced Studies, Key Laboratory of Biomedical Engineering of Ministry of Education, Zhejiang University, Hangzhou, China; Wang, F., Qiushi Academy for Advanced Studies, Department of Biomedical Engineering, Zhejiang University, Hangzhou, China; Xu, K., Qiushi Academy for Advanced Studies, Department of Biomedical Engineering, Zhejiang University, Hangzhou, China; Zhang, Q., Qiushi Academy for Advanced Studies, Department of Biomedical Engineering, Zhejiang University, Hangzhou, China; Zhang, S., Qiushi Academy for Advanced Studies, Key Laboratory of Biomedical Engineering of Ministry of Education, Zhejiang University, Hangzhou, China; Zheng, X., Qiushi Academy for Advanced Studies, Key Laboratory of Biomedical Engineering of Ministry of Education, Zhejiang University, Hangzhou, China",5,,10.1109/TNSRE.2014.2341275,SCOPUS,1,,,,,,,,,,3,,,IEEE Transactions on Neural Systems and Rehabilitation Engineering,Attention-gated reinforcement learning (AGREL); Brain-machine interfaces (BMIS); Neural control; Trajectory tracking,,2-s2.0-84929330639,,,,,,467,458,,,,,Scopus,,,IEEE Transactions on Neural Systems and Rehabilitation Engineering,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929330639&doi=10.1109%2fTNSRE.2014.2341275&partnerID=40&md5=272bbcb07328e30c279d66aedce4eb4f,,23,,2015,,,,,,,,,,,,,,,,,,,
Implications of decentralized Q-learning resource allocation in wireless networks,"Reinforcement Learning is gaining attention by the wireless networking community due to its potential to learn good-performing configurations only from the observed results. In this work we propose a stateless variation of Q-learning, which we apply to exploit spatial reuse in a wireless network. In particular, we allow networks to modify both their transmission power and the channel used solely based on the experienced throughput. We concentrate in a completely decentralized scenario in which no information about neighbouring nodes is available to the learners. Our results show that although the algorithm is able to find the best-performing actions to enhance aggregate throughput, there is high variability in the throughput experienced by the individual networks. We identify the cause of this variability as the adversarial setting of our setup, in which the most played actions provide intermittent good/poor performance depending on the neighbouring decisions. We also evaluate the effect of the intrinsic learning parameters of the algorithm on this variability.",,,,"Wireless Networking (WN-UPF), Universitat Pompeu Fabra, Barcelona, Spain",F. Wilhelmi; B. Bellalta; C. Cano; A. Jonsson,,,,10.1109/PIMRC.2017.8292321,IEEE Xplore,1,,20180215,5,,Aggregates;Interference;Meters;Resource management;Signal to noise ratio;Throughput;Wireless networks,learning (artificial intelligence);radio networks;resource allocation,Reinforcement Learning;decentralized Q-learning resource allocation;wireless network;wireless networking community,,,,8-13 Oct. 2017,,"2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8292321,,,,,,IEEE,,,Electronic:978-1-5386-3531-5; POD:978-1-5386-3532-2; Paper:978-1-5386-3529-2; USB:978-1-5386-3530-8,,"2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",1,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
Learning classifier systems for adaptive learning of intrusion detection system,"Relational databases contain information that must be protected such as personal information, the problem of intrusion detection of relational database is considered important. Also, the pattern of attacks evolves and it is difficult to grasp by rule-based method or general machine learning, so adaptive learning is needed. Learning classifier systems are system that combines supervised learning, reinforcement learning and evolutionary computation. It creates and updates classifiers according to data input. Learning classifier systems can learn adaptive because they generate and evaluate classifiers in real time. In this paper, we apply accuracy based learning classifier systems to relational database and confirm that adaptive learning is possible. Also, we confirmed their practical usability that they close to the best accuracy, though were not the best. © 2018, Springer International Publishing AG.",,"Department of Computer Science, Yonsei University, 50Yonsei-Ro, Seodaemun-Gu, Seoul, South Korea",,,"Lee C.S., Cho S.B.","Lee, C.S., Department of Computer Science, Yonsei University, 50Yonsei-Ro, Seodaemun-Gu, Seoul, South Korea; Cho, S.B., Department of Computer Science, Yonsei University, 50Yonsei-Ro, Seodaemun-Gu, Seoul, South Korea",,,10.1007/978-3-319-67180-2_54,SCOPUS,1,,,,,,,,,,,,,Advances in Intelligent Systems and Computing,Anomaly detection; Database; Learning classifier systems; SQL query,,2-s2.0-85028680808,,,,,,566,557,,,,,Scopus,,,Advances in Intelligent Systems and Computing,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028680808&doi=10.1007%2f978-3-319-67180-2_54&partnerID=40&md5=c0423f92b3d8090bc33934e9e4184701,,649,,2018,,,,,,,,,,,,,,,,,,,
Learning algorithms For intelligent agents based e-learning system,"Requirements are the basic entity which makes the project to be successfully implemented. In the development of the software, theses requirements must be measured accurately and correctly. The requirements of the end users may be changed with respect to different individual personality. As different requirements are given by different customers, all of them cannot be processed by single software system. In such a case, it is essential to make the changes in the software systems automatically which fulfills all the requirements of the user. Such condition is met by the systems with intelligent agents. In this paper, algorithms of intelligent agents adviser agent, personalization agent and content managing agent are proposed to automatically gather the requirements of the students and fulfill them. The algorithms are structured using reinforcement learning. Theses algorithm makes the agents to sense the needs of the students and evolve in the course of their operation. These intelligent agents are applied after the deployment of the e-learning software. All the algorithms have been implemented successfully.",,,,"Department of Computer Science & Engineering, Ajay Kumar Garg Engineering College, Ghaziabad. India",N. Pandey; R. K. Tyagi; S. Sahu; A. Dwivedi,,1,,10.1109/IAdCC.2013.6514369,IEEE Xplore,1,,20130513,1039,,Databases;Electronic learning;Intelligent agents;Negative feedback;Software algorithms;Standards,computer aided instruction;learning (artificial intelligence);multi-agent systems,adviser agent;content managing agent;e-learning system;electronic learning;intelligent agent;learning algorithm;personalization agent;reinforcement learning;software development;student requirement;user requirement,,,,22-23 Feb. 2013,,2013 3rd IEEE International Advance Computing Conference (IACC),E-Learning;Intelligent Agent;Reinforcement Learnin;Requirement Engineering,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514369,,,,,,IEEE,8,,Electronic:978-1-4673-4529-3; POD:978-1-4673-4527-9,,2013 3rd IEEE International Advance Computing Conference (IACC),1034,IEEE Conferences,,,,,2013,,,,,,,,,,,,,,,,,,,
Autonomous resource allocation for dense LTE networks: A Multi Armed Bandit formulation,"Resource allocation is an important prerequisite for the effective deployment of Pico Cells (PCs). This topic becomes even more challenging in the case of heterogeneous networks, where autonomous interference management mechanisms are necessary. In this article, we propose a resource sharing method inspired from the reinforcement learning theory and particularly the methods used to solve the Multi Armed Bandit (MAB) problem. The main goal resides in giving the ability for each cell to make its decision autonomously while dynamically taking into account the resource occupation of each surrounding cell. We set up the global framework for the MAB based resource allocation strategies in the case of total frequency overlapping PCs. The performances of the proposed method are evaluated in the case of Long Term Evolution (LTE) Pico Cells deployment and compared to static allocation schemes. The results demonstrate the efficiency of our method.",,,,"Alcatel-Lucent Bell Labs, France",A. Feki; V. Capdevielle,,6,,10.1109/PIMRC.2011.6140047,IEEE Xplore,1,,20120126,70,,Indexes;Interference;OFDM;Resource management;Signal to noise ratio;Throughput;Time frequency analysis,Long Term Evolution;interference suppression;learning (artificial intelligence);picocellular radio;telecommunication computing;telecommunication network management,Long Term Evolution;MAB based resource allocation;autonomous interference management mechanisms;autonomous resource allocation;dense LTE networks;heterogeneous networks;multiarmed bandit formulation;pico cells;reinforcement learning theory;resource sharing method,,2166-9570;21669570,,11-14 Sept. 2011,,"2011 IEEE 22nd International Symposium on Personal, Indoor and Mobile Radio Communications",Inter-Cell Interference (ICI);LTE;MAB;Pico Cell (PC);Reinforcement Learning theory;Resource allocation,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140047,,,,,1,IEEE,12,,Electronic:978-1-4577-1348-4; POD:978-1-4577-1346-0; USB:978-1-4577-1347-7,,"2011 IEEE 22nd International Symposium on Personal, Indoor and Mobile Radio Communications",66,IEEE Conferences,,,,,2011,,,,,,,,,,,,,,,,,,,
A comparison of two algorithms for robot learning from demonstration,"Robot learning from demonstration focuses on algorithms that enable a robot to learn a policy from demonstrations performed by a teacher, typically a human expert. This paper presents an experimental evaluation of two learning from demonstration algorithms, Interactive Reinforcement Learning and Behavior Networks. We evaluate the performance of these algorithms using a humanoid robot and discuss the relative advantages and drawbacks of these methods with respect to learning time, number of demonstrations, ease of implementation and other metrics. Our results show that Behavior Networks rely on a greater degree of domain knowledge and programmer expertise, requiring very precise definitions for behavior pre- and post-conditions. By contrast Interactive RL requires a relatively simple implementation based only on the robot's sensor data and actions. However, Behavior Networks leverage the pre-coded knowledge to effectively reduce learning time and the required number of human interactions to learn the task.",,,,"Robotics Engineering Program, Worcester Polytechnic Institute, MA 01609, USA",H. B. Suay; S. Chernova,,0,,10.1109/ICSMC.2011.6084052,IEEE Xplore,1,,20111121,2500,,Actuators;Humans;Knowledge engineering;Learning;Robot sensing systems;Strontium,human-robot interaction;humanoid robots;learning by example,behavior networks;human interactions;humanoid robot;interactive reinforcement learning;robot learning from demonstration,,1062-922X;1062922X,,9-12 Oct. 2011,,"2011 IEEE International Conference on Systems, Man, and Cybernetics",Learning and Adaptive Systems;Personal Robots,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6084052,,,,,,IEEE,15,,Electronic:978-1-4577-0653-0; POD:978-1-4577-0652-3,,"2011 IEEE International Conference on Systems, Man, and Cybernetics",2495,IEEE Conferences,,,,,2011,,,,,,,,,,,,,,,,,,,
Reference signal power control for load balancing in downlink LTE-A self-organizing networks,"Self-organizing network (SON) is considered as a driving technology for the deployment of next generation radio access networks. This paper addresses the problem of load balancing (LB) for multi-hop cellular network (MCN) with fixed relays such as LTE-A network in the context of SON. The designed SON algorithm, namely RSPC-RL, is based on two ideas: relay node reference signal power control (RSPC) and multi-agent reinforcement learning (RL). In the proposed RSPC-RL algorithm, the relay node is modeled as an agent that learns an optimal policy of reference signal power control from its interaction with environment to balance the load distribution of the network through dynamically changing its coverage area. Numerical results show the significant performance gain brought about by the proposed algorithm RSPC-RL.",,,,"Institute of information and communication engineering, Zhejiang University, Hangzhou, China",C. Ma; R. Yin; G. Yu; J. Zhang,,7,,10.1109/PIMRC.2012.6362829,IEEE Xplore,1,,20121129,464,,Algorithm design and analysis;Heuristic algorithms;Learning;Load management;Power control;Relays;Throughput,Long Term Evolution;cellular radio;control engineering computing;learning (artificial intelligence);mobile computing;multi-agent systems;next generation networks;power control;radio access networks;resource allocation;telecommunication control,MCN;RSPC-RL;RSPC-RL algorithm;SON;downlink LTE-A self-organizing networks;load balancing;load distribution;multiagent reinforcement learning;multihop cellular network;next generation radio access networks;relay node reference signal power control,,2166-9570;21669570,,9-12 Sept. 2012,,"2012 IEEE 23rd International Symposium on Personal, Indoor and Mobile Radio Communications - (PIMRC)",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6362829,,,,,,IEEE,18,,Electronic:978-1-4673-2569-1; POD:978-1-4673-2566-0; USB:978-1-4673-2568-4,,"2012 IEEE 23rd International Symposium on Personal, Indoor and Mobile Radio Communications - (PIMRC)",460,IEEE Conferences,,,,,2012,,,,,,,,,,,,,,,,,,,
Relational sequence learning,"Sequential behavior and sequence learning are essential to intelligence. Often the elements of sequences exhibit an internal structure that can elegantly be represented using relational atoms. Applying traditional sequential learning techniques to such relational sequences requires one either to ignore the internal structure or to live with a combinatorial explosion of the model complexity. This chapter briefly reviews relational sequence learning and describes several techniques tailored towards realizing this, such as local pattern mining techniques, (hidden) Markov models, conditional random fields, dynamic programming and reinforcement learning. © 2008 Springer-Verlag Berlin Heidelberg.",,"CSAIL, Massachusetts Institute of Technology, 32 Vassar Street, Cambridge, MA 02139-4307, United States; Departement Computerwetenschappen, K.U. Leuven, Celestijnenlaan 200A - bus 2402, Heverlee B-3001, Belgium; Machine Learning Lab., Institute for Computer Science, University of Freiburg, Georges-Koehler Allee, Freiburg 79110, Germany",,,"Kersting K., De Raedt L., Gutmann B., Karwath A., Landwehr N.","Kersting, K., CSAIL, Massachusetts Institute of Technology, 32 Vassar Street, Cambridge, MA 02139-4307, United States; De Raedt, L., Departement Computerwetenschappen, K.U. Leuven, Celestijnenlaan 200A - bus 2402, Heverlee B-3001, Belgium; Gutmann, B., Departement Computerwetenschappen, K.U. Leuven, Celestijnenlaan 200A - bus 2402, Heverlee B-3001, Belgium; Karwath, A., Machine Learning Lab., Institute for Computer Science, University of Freiburg, Georges-Koehler Allee, Freiburg 79110, Germany; Landwehr, N., Machine Learning Lab., Institute for Computer Science, University of Freiburg, Georges-Koehler Allee, Freiburg 79110, Germany",8,,10.1007/978-3-540-78652-8_2,SCOPUS,1,,,,,,,,,,,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,,2-s2.0-40249105911,,,,,,55,28,,,,,Scopus,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40249105911&doi=10.1007%2f978-3-540-78652-8_2&partnerID=40&md5=fec366043b3d7dcd69b6241a78a19d35,,4911 LNAI,,2008,,,,,,,,,,,,,,,,,,,
Routing an autonomous taxi with reinforcement learning,"Singapore's vision of a Smart Nation encompasses the development of effective and efficient means of transportation. The government's target is to leverage new technologies to create services for a demand-driven intelligent transportation model including personal vehicles, public transport, and taxis. Singapore's government is strongly encouraging and supporting research and development of technologies for autonomous vehicles in general and autonomous taxis in particular. The design and implementation of intelligent routing algorithms is one of the keys to the deployment of autonomous taxis. In this paper we demonstrate that a reinforcement learning algorithm of the Q-learning family, based on a customized exploration and exploitation strategy, is able to learn optimal actions for the routing autonomous taxis in a real scenario at the scale of the city of Singapore with pick-up and drop-off events for a fleet of one thousand taxis. © 2016 Copyright held by the owner/author(s).",,"Télécom ParisTech, Paris, France; IPAL, I2R, ASTAR, Singapore, Singapore; IPAL, NUS, Singapore, Singapore",,,"Han M., Senellart P., Bressan S., Wu H.","Han, M., Télécom ParisTech, Paris, France, IPAL, I2R, ASTAR, Singapore, Singapore; Senellart, P., Télécom ParisTech, Paris, France, IPAL, NUS, Singapore, Singapore; Bressan, S., IPAL, NUS, Singapore, Singapore; Wu, H., IPAL, I2R, ASTAR, Singapore, Singapore",1,,10.1145/2983323.2983379,SCOPUS,1,,,,,,,,,,,,,"International Conference on Information and Knowledge Management, Proceedings",,,2-s2.0-84996565915,,,,,,2424,2421,,,,,Scopus,,,"International Conference on Information and Knowledge Management, Proceedings",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996565915&doi=10.1145%2f2983323.2983379&partnerID=40&md5=68050ff7024ebcda3277136c6f8d0512,,24-28-October-2016,,2016,,,,,,,,,,,,,,,,,,,
Adaptive Behavior Generation for Child-Robot Interaction,"Social robots are increasingly applied in assistive settings where they interact with human users to support them in their daily life. There, abilities for a robust and reliable social interaction are required, especially for robots that interact autonomously with humans. Apart from challenges regarding safety and trust, the complexity and difficulty of attaining mutual understanding, engagement or assistance in social interactions that comprise spoken languages and non-verbal behaviors need to be taken into account. In addition, different users or user groups have inter-individual differences with respect to their personal preferences, skills and limitations. This makes it more difficult to develop reliable and understandable robots that work well in different situations or for different users. © 2018 Authors.",,"CITEC, Bielefeld University, Bielefeld, Germany",,,"Hemminghaus J., Kopp S.","Hemminghaus, J., CITEC, Bielefeld University, Bielefeld, Germany; Kopp, S., CITEC, Bielefeld University, Bielefeld, Germany",,,10.1145/3173386.3176916,SCOPUS,1,,,,,,,,,,,,,ACM/IEEE International Conference on Human-Robot Interaction,Assistive Robotics; Child-Robot-Interaction; Multimodal Social Behavior; Reinforcement Learning,,2-s2.0-85045246276,,,,,,296,295,,,,,Scopus,,,ACM/IEEE International Conference on Human-Robot Interaction,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045246276&doi=10.1145%2f3173386.3176916&partnerID=40&md5=5d60132774050efaff259e83b239d95d,,,,2018,,,,,,,,,,,,,,,,,,,
OWLS: Observational wireless life-enhancing system,"Socially assistive robotics technologies for individuals, who have been affected by age-related disabilities and similar types of disorders, have become popular options for facilitating natural independence and uninterrupted mobility. Wireless wearable sensor systems enable proactive personal health management and the ubiquitous monitoring of vital signs to keep an active watch on immediate health conditions. In this paper, we develop a system, called OWLS, where multiple wearable sensors, software agents, robots and health analysis technology, have been integrated into a single personal therapy solution (SPTS). Our system uses a reinforcement learning algorithm to make decisions about the user's current health conditions, and to take appropriate actions, as necessary (i.e, contacting outside parties). We show that the approach of non-invasive monitoring, when combined with an alert system, makes this a desirable SPTS in future health care. Copyright © 2016, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",,"Allegheny College, 520 North Main Street, Allegheny, PA, United States",,,"Zheng H., Jumadinova J.","Zheng, H., Allegheny College, 520 North Main Street, Allegheny, PA, United States; Jumadinova, J., Allegheny College, 520 North Main Street, Allegheny, PA, United States",,,,SCOPUS,0,,,,,,,,,,,,,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,,2-s2.0-85014134480,,,,,,1426,1425,,,,,Scopus,,,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014134480&partnerID=40&md5=26e364abd3feeb0a7316192d9b9e02f8,,,,2016,,,,,,,,,,,,,,,,,,,
Dialogue manager domain adaptation using Gaussian process reinforcement learning,"Spoken dialogue systems allow humans to interact with machines using natural speech. As such, they have many benefits. By using speech as the primary communication medium, a computer interface can facilitate swift, human-like acquisition of information. In recent years, speech interfaces have become ever more popular, as is evident from the rise of personal assistants such as Siri, Google Now, Cortana and Amazon Alexa. Recently, data-driven machine learning methods have been applied to dialogue modelling and the results achieved for limited-domain applications are comparable to or out-perform traditional approaches. Methods based on Gaussian processes are particularly effective as they enable good models to be estimated from limited training data. Furthermore, they provide an explicit estimate of the uncertainty which is particularly useful for reinforcement learning. This article explores the additional steps that are necessary to extend these methods to model multiple dialogue domains. We show that Gaussian process reinforcement learning is an elegant framework that naturally supports a range of methods, including prior knowledge, Bayesian committee machines and multi-agent learning, for facilitating extensible and adaptable dialogue systems. © 2016 The Authors",Open Access,"University of Cambridge, Trumpington Street, Cambridge, United Kingdom",,,"Gašić M., Mrkšić N., Rojas-Barahona L.M., Su P.-H., Ultes S., Vandyke D., Wen T.-H., Young S.","Gašić, M., University of Cambridge, Trumpington Street, Cambridge, United Kingdom; Mrkšić, N., University of Cambridge, Trumpington Street, Cambridge, United Kingdom; Rojas-Barahona, L.M., University of Cambridge, Trumpington Street, Cambridge, United Kingdom; Su, P.-H., University of Cambridge, Trumpington Street, Cambridge, United Kingdom; Ultes, S., University of Cambridge, Trumpington Street, Cambridge, United Kingdom; Vandyke, D., University of Cambridge, Trumpington Street, Cambridge, United Kingdom; Wen, T.-H., University of Cambridge, Trumpington Street, Cambridge, United Kingdom; Young, S., University of Cambridge, Trumpington Street, Cambridge, United Kingdom",,,10.1016/j.csl.2016.09.003,SCOPUS,1,,,,,,,,,,,,,Computer Speech and Language,Dialogue systems; Gaussian process; Reinforcement learning,,2-s2.0-85007550419,,,,,,569,552,,,,,Scopus,,,Computer Speech and Language,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007550419&doi=10.1016%2fj.csl.2016.09.003&partnerID=40&md5=d21df44714e6a813bc0ce40cbb80b74f,,45,,2017,,,,,,,,,,,,,,,,,,,
Neural networks for incremental dimensionality reduced reinforcement learning,"State-of-the-art personal robots must perform complex manipulation tasks to be viable in assistive scenarios. However, many of these robots, like the PR2, use manipulators with high degrees-of-freedom. The complexity of these robots lead to large dimensional state spaces, which are difficult to fully explore. Our previous work introduced the IDRRL algorithm, which compresses the learning space by transforming a high-dimensional learning space onto a lower-dimensional manifold while preserving expressivity. In this work we formally prove that IDRRL maintains PAC-MDP guarantees. We then improve upon our previous formulation of IDRRL by introducing cascading autoencoders (CAE) for dimensionality reduction, producing the new algorithm IDRRL-CAE. We demonstrate the improvement of this extension over our previous formulation, IDRRL-PCA, in the Mountain Car and Swimmers domains.",,,,"Oregon State University, Corvallis, United States",W. Curran; R. Pocius; W. D. Smart,,,,10.1109/IROS.2017.8205962,IEEE Xplore,1,,20171214,1565,,Algorithm design and analysis;Computational complexity;Learning (artificial intelligence);Principal component analysis;Robots,learning (artificial intelligence);manipulators;neurocontrollers;principal component analysis,IDRRL algorithm;IDRRL-CAE;IDRRL-PCA;PAC-MDP;PR2;cascading autoencoders;dimensionality reduction;high-dimensional learning space;incremental dimensionality;lower-dimensional manifold;manipulation tasks;manipulators;neural networks;personal robots;reinforcement learning,,,,24-28 Sept. 2017,,2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8205962,,,,,,IEEE,,,Electronic:978-1-5386-2682-5; POD:978-1-5386-2683-2; USB:978-1-5386-2681-8,,2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1559,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
Dimensionality reduced reinforcement learning for assistive robots,"State-of-the-art personal robots need to perform complex manipulation tasks to be viable in assistive scenarios. However, many of these robots, like the PR2, use manipulators with high degrees-of-freedom, and the problem is made worse in bimanual manipulation tasks. The complexity of these robots lead to large dimensional state spaces, which are difficult to learn in. We reduce the state space by using demonstrations to discover a representative low-dimensional hyperplane in which to learn. This allows the agent to converge quickly to a good policy. We call this Dimensionality Reduced Reinforcement Learning (DRRL). However, when performing dimensionality reduction, not all dimensions can be fully represented. We extend this work by first learning in a single dimension, and then transferring that knowledge to a higher-dimensional hyperplane. By using our Iterative DRRL (IDRRL) framework with an existing learning algorithm, the agent converges quickly to a better policy by iterating to increasingly higher dimensions. IDRRL is robust to demonstration quality and can learn efficiently using few demonstrations. We show that adding IDRRL to the Q-Learning algorithm leads to faster learning on a set of mountain car tasks and the robot swimmers problem. Copyright © 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Oregon State University, Corvallis, United States; Vrije Universiteit Brussel, Belgium; Navy Center for Applied Research in AI, United States; Washington State University, Pullman, Washington, United States",,,"Curran W., Brys T., Aha D., Taylor M., Smart W.D.","Curran, W., Oregon State University, Corvallis, United States; Brys, T., Vrije Universiteit Brussel, Belgium; Aha, D., Navy Center for Applied Research in AI, United States; Taylor, M., Washington State University, Pullman, Washington, United States; Smart, W.D., Oregon State University, Corvallis, United States",1,,,SCOPUS,0,,,,,,,,,,,,,AAAI Fall Symposium - Technical Report,,,2-s2.0-85025818100,,,,,,31,25,,,,,Scopus,,,AAAI Fall Symposium - Technical Report,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025818100&partnerID=40&md5=03d1c87d8c5fe85436796dd60feb5b9d,,FS-16-01 - FS-16-05,,2016,,,,,,,,,,,,,,,,,,,
A unified learning paradigm for large-scale personalized information management,"Statistical-learning approaches such as unsupervised learning, supervised learning, active learning, and reinforcement learning have generally been separately studied and applied to solve application problems. In this paper, we provide an overview of our newly proposed unified learning paradigm (ULP), which combines these approaches into one synergistic framework. We outline the architecture and the algorithm of ULP, and explain benefits of employing this unified learning paradigm on personalizing information management.",,,,"Electr. & Comput. Eng., California Univ., Santa Barbara, CA, USA",E. Y. Chang; S. C. H. Hop; Xinjing Wang; Wei-Ying Max; M. R. Lyu,,1,,10.1109/EITC.2005.1544372,IEEE Xplore,1,,20051205,,,Clustering algorithms;Convergence;Humans;Information management;Kernel;Large-scale systems;Machine learning;Stability;Supervised learning;Unsupervised learning,information management;learning (artificial intelligence),large-scale personalized information management;statistical-learning approach;unified learning paradigm,,,,15-16 Aug. 2005,,"Conference, Emerging Information Technology 2005.",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544372,,,,,,IEEE,9,,POD:0-7803-9328-7,,"Conference, Emerging Information Technology 2005.",4 pp.,IEEE Conferences,,,,,2005,,,,,,,,,,,,,,,,,,,
Closed-loop task difficulty adaptation during virtual reality reach-to-grasp training assisted with an exoskeleton for stroke rehabilitation,"Stroke patients with severe motor deficits of the upper extremity may practice rehabilitation exercises with the assistance of a multi-joint exoskeleton. Although this technology enables intensive task-oriented training, it may also lead to slacking when the assistance is too supportive. Preserving the engagement of the patients while providing ""assistance-as-needed"" during the exercises, therefore remains an ongoing challenge. We applied a commercially available seven degree-of-freedom arm exoskeleton to provide passive gravity compensation during task-oriented training in a virtual environment. During this 4-week pilot study, five severely affected chronic stroke patients performed reach-to-grasp exercises resembling activities of daily living. The subjects received virtual reality feedback from their three-dimensional movements. The level of difficulty for the exercise was adjusted by a performance-dependent real-time adaptation algorithm. The goal of this algorithm was the automated improvement of the range of motion. In the course of 20 training and feedback sessions, this unsupervised adaptive training concept led to a progressive increase of the virtual training space (p < 0.001) in accordance with the subjects' abilities. This learning curve was paralleled by a concurrent improvement of real world kinematic parameters, i.e., range of motion (p = 0.008), accuracy of movement (p = 0.01), and movement velocity (p < 0.001). Notably, these kinematic gains were paralleled by motor improvements such as increased elbow movement (p = 0.001), grip force (p < 0.001), and upper extremity Fugl-Meyer-Assessment score from 14.3 ï¿½ 5 to 16.9 ï¿½ 6.1 (p = 0.026). Combining gravity-compensating assistance with adaptive closed-loop feedback in virtual reality provides customized rehabilitation environments for severely affected stroke patients. This approach may facilitate motor learning by progressively challenging the subject in accordance with the individual capacity for functional restoration. It might be necessary to apply concurrent restorative interventions to translate these improvements into relevant functional gains of severely motor impaired patients in activities of daily living. ï¿½ 2016 Grimm, Naros and Gharabaghi.",,"Division of Functional and Restorative Neurosurgery, Centre for Integrative Neuroscience, Eberhard Karls University of Tuebingen, Tuebingen, Germany",518,,"Grimm F., Naros G., Gharabaghi A.","Grimm, F., Division of Functional and Restorative Neurosurgery, Centre for Integrative Neuroscience, Eberhard Karls University of Tuebingen, Tuebingen, Germany; Naros, G., Division of Functional and Restorative Neurosurgery, Centre for Integrative Neuroscience, Eberhard Karls University of Tuebingen, Tuebingen, Germany; Gharabaghi, A., Division of Functional and Restorative Neurosurgery, Centre for Integrative Neuroscience, Eberhard Karls University of Tuebingen, Tuebingen, Germany",8,,10.3389/fnins.2016.00518,SCOPUS,1,,,,,,,,,,NOV,,,Frontiers in Neuroscience,Hemiparesis; Individualized therapy; Motor recovery; Reinforcement learning; Robot-assisted rehabilitation; Robotic rehabilitation; Upper-limb assistance,,2-s2.0-85009810403,,,,,,,,,,,,Scopus,,,Frontiers in Neuroscience,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009810403&doi=10.3389%2ffnins.2016.00518&partnerID=40&md5=81fb7c532c89357ddb73a579d4d5fd91,,10,,2016,,,,,,,,,,,,,,,,,,,
New software for learner-centered circuits instruction,"Summary form only given. Today's classroom and technologies offer solutions to the challenges that face students and instructors in circuits. Software tools now support several areas of the curriculum (e.g. circuits and electromagnetic fields), and many students learn to use them early in their studies. The availability of powerful personal computers linked to classroom video projection systems creates an opportunity for faculty to broaden the scope of their instruction on-the-fly, with a high level of audience interaction and exploration. With software tools, examples can be explored freely, and students can address ""what-if"" questions immediately. Students gain valuable reinforcement for their understanding of abstract concepts by seeing physical, practical effects on the screen, under their control. Here, the authors discuss such software for learner-centered circuits instruction.",,,,"Dept. of Electr. & Comput. Eng., Colorado Univ., Colorado Springs, CO, USA",M. D. Ciletti,,0,,10.1109/FIE.1998.738572,IEEE Xplore,1,,20020806,,,Circuit analysis;Circuit theory;Electrical engineering education;Fourier series;Fourier transforms;Linear circuits;Software tools;Springs;Time domain analysis;User-generated content,computer aided instruction;educational courses;electronic engineering computing;electronic engineering education;microcomputer applications,classroom video projection systems;curriculum;instructors;learner-centered circuits instruction software;personal computers;software tools;students,,0190-5848;01905848,,4-7 Nov. 1998,,"Frontiers in Education Conference, 1998. FIE '98. 28th Annual",,,,04 Nov 1998-07 Nov 1998,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=738572,,,,,,IEEE,,,POD:0-7803-4762-5,,"Frontiers in Education Conference, 1998. FIE '98. 28th Annual",1100 vol.3,IEEE Conferences,,,3,,1998,,,,,,,,,,,,,,,,,,,
Learning conversational systems that interleave task and non-task content,"Task-oriented dialog systems have been applied in various tasks, such as automated personal assistants, customer service providers and tutors. These systems work well when users have clear and explicit intentions that are well-aligned to the systems' capabilities. However, they fail if users intentions are not explicit. To address this shortcoming, we propose a framework to interleave nontask content (i.e. everyday social conversation) into task conversations. When the task content fails, the system can still keep the user engaged with the non-task content. We trained a policy using reinforcement learning algorithms to promote long-turn conversation coherence and consistency, so that the system can have smooth transitions between task and non-task content. To test the effectiveness of the proposed framework, we developed a movie promotion dialog system. Experiments with human users indicate that a system that interleaves social and task content achieves a better task success rate and is also rated as more engaging compared to a pure task-oriented system.",,"Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA, United States",,,"Yu Z., Black A.W., Rudnicky A.I.","Yu, Z., Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA, United States; Black, A.W., Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA, United States; Rudnicky, A.I., Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA, United States",,,,SCOPUS,0,,,,,,,,,,,,,IJCAI International Joint Conference on Artificial Intelligence,,,2-s2.0-85031939839,,,,,,4220,4214,,,,,Scopus,,,IJCAI International Joint Conference on Artificial Intelligence,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031939839&partnerID=40&md5=35012a2acb00bcb4b13740192de24e3f,,,,2017,,,,,,,,,,,,,,,,,,,
Analyzing and visualizing multiagent rewards in dynamic and stochastic domains,"The ability to analyze the effectiveness of agent reward structures is critical to the successful design of multiagent learning algorithms. Though final system performance is the best indicator of the suitability of a given reward structure, it is often preferable to analyze the reward properties that lead to good system behavior (i.e., properties promoting coordination among the agents and providing agents with strong signal to noise ratios). This step is particularly helpful in continuous, dynamic, stochastic domains ill-suited to simple table backup schemes commonly used in TD(λ)/Q-learning where the effectiveness of the reward structure is difficult to distinguish from the effectiveness of the chosen learning algorithm. In this paper, we present a new reward evaluation method that provides a visualization of the tradeoff between the level of coordination among the agents and the difficulty of the learning problem each agent faces. This method is independent of the learning algorithm and is only a function of the problem domain and the agents' reward structure. We use this reward property visualization method to determine an effective reward without performing extensive simulations. We then test this method in both a static and a dynamic multi-rover learning domain where the agents have continuous state spaces and take noisy actions (e.g., the agents' movement decisions are not always carried out properly). Our results show that in the more difficult dynamic domain, the reward efficiency visualization method provides a two order of magnitude speedup in selecting good rewards, compared to running a full simulation. In addition, this method facilitates the design and analysis of new rewards tailored to the observational limitations of the domain, providing rewards that combine the best properties of traditional rewards. © 2008 Springer Science+Business Media, LLC.",,"University of California, Santa Cruz, Santa Cruz, CA, United States; Oregon State University, 204 Rogers Hall, Corvallis, OR 97330, United States",,,"Agogino A.K., Tumer K.","Agogino, A.K., University of California, Santa Cruz, Santa Cruz, CA, United States; Tumer, K., Oregon State University, 204 Rogers Hall, Corvallis, OR 97330, United States",50,,10.1007/s10458-008-9046-9,SCOPUS,1,,,,,,,,,,2,,,Autonomous Agents and Multi-Agent Systems,Multiagent learning; Reinforcement learning; Reward analysis; Visualization,,2-s2.0-51649111408,,,,,,338,320,,,,,Scopus,,,Autonomous Agents and Multi-Agent Systems,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51649111408&doi=10.1007%2fs10458-008-9046-9&partnerID=40&md5=8c7580869c4516766cd5b5f8b2d46fd2,,17,,2008,,,,,,,,,,,,,,,,,,,
Dynamic optimization of the level of operational effectiveness of a CSOC under adverse conditions,"The analysts at a cybersecurity operations center (CSOC) analyze the alerts that are generated by intrusion detection systems (IDSs). Under normal operating conditions, sufficient numbers of analysts are available to analyze the alert workload. For the purpose of this article, this means that the cybersecurity analysts in each shift can fully investigate each and every alert that is generated by the IDSs in a reasonable amount of time and perform their normal tasks in a shift. Normal tasks include analysis time, time to attend training programs, report writing time, personal break time, and time to update the signatures on new patterns in alerts as detected by the IDS. There are several disruptive factors that occur randomly and can adversely impact the normal operating condition of a CSOC, such as (1) higher alert generation rates from a few IDSs, (2) new alert patterns that decrease the throughput of the alert analysis process, and (3) analyst absenteeism. The impact of the preceding factors is that the alerts wait for a long duration before being analyzed, which impacts the level of operational effectiveness (LOE) of the CSOC. To return the CSOC to normal operating conditions, the manager of a CSOC can take several actions, such as increasing the alert analysis time spent by analysts in a shift by canceling a training program, spending some of his own time to assist the analysts in alert investigation, and calling upon the on-call analyst workforce to boost the service rate of alerts. However, additional resources are limited in quantity over a 14-day work cycle, and the CSOC manager must determine when and how much action to take in the face of uncertainty, which arises from both the intensity and the random occurrences of the disruptive factors. The preceding decision by the CSOC manager is nontrivial and is often made in an ad hoc manner using prior experiences. This work develops a reinforcement learning (RL) model for optimizing the LOE throughout the entire 14-day work cycle of a CSOC in the face of uncertainties due to disruptive events. Results indicate that the RL model is able to assist the CSOCmanager with a decision support tool to make better decisions than current practices in determining when and how much resource to allocate when the LOE of a CSOC deviates from the normal operating condition. © 2018 ACM.",,"Center for Secure Information Systems, George Mason University, 4400 University Dr., Fairfax, VA, United States; Army Research Laboratory, 2800 Powder Mill Road, Adelphi, MD, United States",51,,"Shah A., Ganesan R., Jajodia S., Cam H.","Shah, A., Center for Secure Information Systems, George Mason University, 4400 University Dr., Fairfax, VA, United States; Ganesan, R., Center for Secure Information Systems, George Mason University, 4400 University Dr., Fairfax, VA, United States; Jajodia, S., Center for Secure Information Systems, George Mason University, 4400 University Dr., Fairfax, VA, United States; Cam, H., Army Research Laboratory, 2800 Powder Mill Road, Adelphi, MD, United States",,,10.1145/3173457,SCOPUS,1,,,,,,,,,,5,,,ACM Transactions on Intelligent Systems and Technology,Absenteeism in shift; Allocate resources; Analysts; Average time to analyze alerts; Cybersecurity; Level of operational effectiveness; Oncall analysts; Reinforcement learning; Resource allocation; Stochastic optimization,,2-s2.0-85047116204,,,,,,,,,,,,Scopus,,,ACM Transactions on Intelligent Systems and Technology,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047116204&doi=10.1145%2f3173457&partnerID=40&md5=a83cea6dd84e9af765943a9f8f535d02,,9,,2018,,,,,,,,,,,,,,,,,,,
Ensemble contextual bandits for personalized recommendation,"The cold-start problem has attracted extensive attention among various online services that provide personalized recommendation. Many online vendors employ contextual bandit strategies to tackle the so-called exploration/exploitation dilemma rooted from the cold-start problem. However, due to high-dimensional user/item features and the underlying characteristics of bandit policies, it is often difficult for service providers to obtain and deploy an appropriate algorithm to achieve acceptable and robust economic profit. In this paper, we explore ensemble strategies of contextual bandit algorithms to obtain robust predicted click-through rate (CTR) of web objects. The ensemble is acquired by aggregating different pulling policies of bandit algorithms, rather than forcing the agreement of prediction results or learning a unified predictive model. To this end, we employ a meta-bandit paradigm that places a hyper bandit over the base bandits, to explicitly explore/exploit the relative importance of base bandits based on user feedbacks. Extensive empirical experiments on two real-world data sets (news recommendation and online advertising) demonstrate the effectiveness of our proposed approach in terms of CTR. Copyright © 2014 ACM.",,"School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States",,,"Tang L., Jiang Y., Li L., Li T.","Tang, L., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Jiang, Y., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Li, L., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Li, T., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States",17,,10.1145/2645710.2645732,SCOPUS,1,,,,,,,,,,,,,RecSys 2014 - Proceedings of the 8th ACM Conference on Recommender Systems,Contextual bandit; CTR prediction; Ensemble recommendation; Meta learning; Personalized recommendation,,2-s2.0-84908877655,,,,,,80,73,,,,,Scopus,,,RecSys 2014 - Proceedings of the 8th ACM Conference on Recommender Systems,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908877655&doi=10.1145%2f2645710.2645732&partnerID=40&md5=ba570d5a0e504e972c6b955345f4d614,,,,2014,,,,,,,,,,,,,,,,,,,
Contextual recommender problems,"The contextual recommender task is the problem of making useful offers, e.g., placing ads or related links on a web page, based on the context information, e.g., contents of the page and information about the user visiting, and information on the available alternatives, i.e., the advertisements or relevant links. In the case of ads for example, the goal is to select ads that result in high click rates, where the (ad) click rate is some unknown function of the attributes of the context and ad. We describe the task and make connections to related problems including recommender and multi-armed bandit problems. Copyright 2005 ACM.",,"Yahoo Research, 74 N. Pasadena Ave, Pasadena, CA 91103, United States",,,"Madani O., DeCoste D.","Madani, O., Yahoo Research, 74 N. Pasadena Ave, Pasadena, CA 91103, United States; DeCoste, D., Yahoo Research, 74 N. Pasadena Ave, Pasadena, CA 91103, United States",9,,10.1145/1089827.1089838,SCOPUS,1,,,,,,,,,,,,,"Proceedings of the 1st International Workshop on Utility-Based Data Mining, UBDM '05",data mining; exploration-exploitation; multi-armed bandit; personalization; recommenders; regression; reinforcement learning; utility,,2-s2.0-33751025354,,,,,,89,86,,,,,Scopus,,,"Proceedings of the 1st International Workshop on Utility-Based Data Mining, UBDM '05",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751025354&doi=10.1145%2f1089827.1089838&partnerID=40&md5=02bb7c496352dcb4e276b940c4c3ea5f,,,,2005,,,,,,,,,,,,,,,,,,,
Adaptive reservoir computing through evolution and learning,"The development of real-world, fully autonomous agents would require mechanisms that would offer generalization capabilities from experience, suitable for a large range of machine learning tasks, like those from the areas of supervised and reinforcement learning. Such capacities could be offered by parametric function approximators that could either model the environment or the agent's policy. To promote autonomy, these structures should be adapted to the problem at hand with no or little human expert input. Towards this goal, we propose an adaptive function approximator method for developing appropriate neural networks in the form of reservoir computing systems through evolution and learning. Our neuro-evolution of augmenting reservoirs approach comprises of several ideas, successful on their own, in an effort to develop an algorithm that could handle a large range of problems, more efficiently. In particular, we use the neuro-evolution of augmented topologies algorithm as a meta-search method for the adaptation of echo state networks for handling problems to be encountered by autonomous entities. We test our approach on several test-beds from the realms of time series prediction and reinforcement learning. We compare our methodology against similar state-of-the-art algorithms with promising results. © 2012 Elsevier B.V.",,"Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki 54124, Greece",,,"Chatzidimitriou K.C., Mitkas P.A.","Chatzidimitriou, K.C., Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki 54124, Greece; Mitkas, P.A., Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki 54124, Greece",5,,10.1016/j.neucom.2012.09.022,SCOPUS,1,,,,,,,,,,,,,Neurocomputing,Echo state networks; Evolutionary computation; Neuroevolution; Reinforcement learning; Reservoir computing,,2-s2.0-84870392112,,,,,,209,198,,,,,Scopus,,,Neurocomputing,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870392112&doi=10.1016%2fj.neucom.2012.09.022&partnerID=40&md5=3754cbca67fac087b95af3b50d04c59b,,103,,2013,,,,,,,,,,,,,,,,,,,
On-line policy learning and adaptation for real-time personalization of an artificial pancreas,"The dynamic complexity of the glucose-insulin metabolism in diabetic patients is the main obstacle towards widespread use of an artificial pancreas. The significant level of subject-specific glycemic variability requires continuously adapting the control policy to successfully face daily changes in patient's metabolism and lifestyle. In this paper, an on-line selective reinforcement learning algorithm that enables real-time adaptation of a control policy based on ongoing interactions with the patient so as to tailor the artificial pancreas is proposed. Adaptation includes two online procedures: on-line sparsification and parameter updating of the Gaussian process used to approximate the control policy. With the proposed sparsification method, the support data dictionary for on-line learning is modified by checking if in the arriving data stream there exists novel information to be added to the dictionary in order to personalize the policy. Results obtained in silico experiments demonstrate that on-line policy learning is both safe and efficient for maintaining blood glucose variability within the normoglycemic range. © 2014 Elsevier Ltd. All rights reserved.",,"INTELYMEC, Centro de Investigaciones en Física e Ingeniería del Centro - CIFICEN, CONICET, Av. del Valle 5737, Olavarría, Argentina; UNCPBA-Facultad de Ingeniería, Universidad Nacional del Centro de la Provincia de Buenos Aires, Av. del Valle 5737, Olavarría, Argentina; INGAR (CONICET-UTN), Avellaneda 3657, Santa Fe, Argentina",,,"De Paula M., Acosta G.G., Martínez E.C.","De Paula, M., INTELYMEC, Centro de Investigaciones en Física e Ingeniería del Centro - CIFICEN, CONICET, Av. del Valle 5737, Olavarría, Argentina; Acosta, G.G., UNCPBA-Facultad de Ingeniería, Universidad Nacional del Centro de la Provincia de Buenos Aires, Av. del Valle 5737, Olavarría, Argentina; Martínez, E.C., INGAR (CONICET-UTN), Avellaneda 3657, Santa Fe, Argentina",4,,10.1016/j.eswa.2014.10.038,SCOPUS,1,,,,,,,,,,4,,,Expert Systems with Applications,Diabetes; Gaussian processes; Glycemic variability; On-line sparsification; Policy learning; Reinforcement learning,,2-s2.0-84910646834,,,,,,2255,2234,,,,,Scopus,,,Expert Systems with Applications,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910646834&doi=10.1016%2fj.eswa.2014.10.038&partnerID=40&md5=0b2a6928c88bfd78c305b765e1fa2e2b,,42,,2015,,,,,,,,,,,,,,,,,,,
A Novel Scheduling Algorithm for Video Traffic in High-Rate WPANs,"The emerging high-rate wireless personal area network (WPAN) technology is capable of supporting high-speed and high-quality real-time multimedia applications. In particular, MPEG-4 video streams are deemed to be a widespread traffic type. However, in the current IEEE 802.15.3 standard for media access control (MAC) of high-rate WPANs, the implementation details of some key issues such as scheduling and quality of service (QoS) provisioning have not been addressed. In this paper, we first propose a mathematical model for the optimal scheduling scheme for MPEG-4 flows in high-rate WPANs. We also propose an RL scheduler based on the reinforcement learning (RL) technique. Simulation results show that our proposed RL scheduler achieves nearly optimal performance and performs better than F-SRPT (Mangharam et al., 2004), EDD+SRPT (Torok et al., 2005), and PAP (Kim and Cho, 2005) scheduling algorithms in terms of a lower decoding failure rate.",,,,"Univ. of British Columbia, Vancouver",S. Moradi; A. H. Mohsenian Rad; V. W. S. Wong,,3,,10.1109/GLOCOM.2007.144,IEEE Xplore,1,,20071226,747,,Communication system traffic control;MPEG 4 Standard;Mathematical model;Media Access Protocol;Optimal scheduling;Quality of service;Scheduling algorithm;Streaming media;Traffic control;Wireless personal area networks,personal area networks;scheduling;telecommunication traffic;video coding;video streaming;wireless sensor networks,MPEG-4 video streams;RL scheduler;high-rate wireless personal area network;real-time multimedia;reinforcement learning;scheduling algorithm;video traffic,,1930-529X;1930529X,,26-30 Nov. 2007,,IEEE GLOBECOM 2007 - IEEE Global Telecommunications Conference,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4411054,,,,,,IEEE,22,,CD-ROM:978-1-4244-1043-9; POD:978-1-4244-1042-2,,IEEE GLOBECOM 2007 - IEEE Global Telecommunications Conference,742,IEEE Conferences,,,,,2007,,,,,,,,,,,,,,,,,,,
An RL-based scheduling algorithm for video traffic in high-rate wireless personal area networks,"The emerging high-rate wireless personal area network (WPAN) technology is capable of supporting high-speed and high-quality real-time multimedia applications. In particular, video streams are deemed to be a dominant traffic type, and require quality of service (QoS) support. However, in the current IEEE 802.15.3 standard for MAC (media access control) of high-rate WPANs, the implementation details of some key issues such as scheduling and QoS provisioning have not been addressed. In this paper, we first propose a Markov decision process (MDP) model for optimal scheduling for video flows in high-rate WPANs. Using this model, we also propose a scheduler that incorporates compact state space representation, function approximation, and reinforcement learning (RL). Simulation results show that our proposed RL scheduler achieves nearly optimal performance and performs better than F-SRPT, EDD + SRPT, and PAP scheduling algorithms in terms of a lower decoding failure rate. © 2009 Elsevier B.V. All rights reserved.",,"Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, Canada",,,"Moradi S., Mohsenian-Rad A.H., Wong V.W.S.","Moradi, S., Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, Canada; Mohsenian-Rad, A.H., Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, Canada; Wong, V.W.S., Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, Canada",3,,10.1016/j.comnet.2009.07.012,SCOPUS,1,,,,,,,,,,18,,,Computer Networks,Markov decision process (MDP); QoS; Reinforcement learning; Scheduling; Ultra-wide band (UWB); Wireless personal area networks,,2-s2.0-70449529532,,,,,,3010,2997,,,,,Scopus,,,Computer Networks,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449529532&doi=10.1016%2fj.comnet.2009.07.012&partnerID=40&md5=f2f8311900ac740326aa5a6c4a386bab,,53,,2009,,,,,,,,,,,,,,,,,,,
A learning strategy for paging in mobile environments,"The essence of designing a good paging strategy is to incorporate user mobility characteristics in a predictive mechanism that reduces the average paging cost with as little computational effort as possible. We introduce a novel paging scheme based on the concept of reinforcement learning. Learning endows the paging mechanism with the predictive power necessary to determine a mobile terminal's position, without having to extract a location probability distribution for each specific user. The proposed algorithm is compared against a heuristic randomized learning strategy akin to reinforcement learning, that we invented for this purpose and performs better than the case where no learning is used at all. It is shown that if the user normally moves only among a fraction of cells in the location area, significant savings can be achieved over the randomized strategy, without excessive time to train the network.",,,,"Sch. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Greece",I. Koukoutsidis; P. Demestichas; M. Theologou,,0,,10.1049/cp:20030322,IEEE Xplore,1,,20041101,590,,,cellular radio;learning (artificial intelligence);paging communication;software agents;telecommunication computing,heuristic randomized learning strategy;intelligent agent;learning strategy;location probability distribution;mobile environments;predictive mechanism;reinforcement learning;terminal paging;user mobility characteristics,,0537-9989;05379989,,22-25 April 2003,,2003 5th European Personal Mobile Communications Conference (Conf. Publ. No. 492),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1350260,,,,,,IET,,,Paper:0-85296-753-5,,2003 5th European Personal Mobile Communications Conference (Conf. Publ. No. 492),585,IET Conferences,,,,,2003,,,,,,,,,,,,,,,,,,,
Genomics and artificial intelligence working together in drug discovery and repositioning: The advent of adaptive pharmacogenomics in glioblastoma and chronic arterial inflammation therapies,"The field of pharmacogenomics investigates how genomics may modulate pathological trends using information on both genotype and phenotype, with the aim of designing personalised healthcare. Homoeostasis is partially regulated through the expression of core protein groups whose functionality is determined at gene level and modulated by environmental factors. Harmful changes in physiology may promote several dis-functionalities. In prior work gene expression was used as a biomarker to assess both pathological propensity and disease progression. A growing body of pharmacogenomics research has developed new compounds, on one hand, and on the other, it has proposed novel therapeutic applications for the existing ones. Over the past decades, collective efforts have significantly increased the number of omics information available. However, efficient and deterministic in silico mechanisms that efficiently analyse and detect trends on the basis of often unknown and limited physiological information responding to challenging clinical questions are still lacking. In this context, computational automation via artificial intelligence methodologies has proven to be accurate, robust to noise, cost efficient, and dynamic dealing with massive databases and forecasting on the basis of the available information. Moreover, this set of computational techniques, based on well-established mathematical models, provide efficient ways of determining trends based on both a priori knowledge and dynamically acquired information, working successfully on incomplete datasets. Therefore, in this chapter we assess developmental similarities between two major causes of worldwide death: glioblastoma and chronic arterial inflammation; and discuss the potential applicability of two artificial intelligence approaches for drug discovery and repositioning. According to the World Health Organization (WHO) a glioblastoma multiform is the most malignant glial-type tumour (graded level IV in the WHO scale); and inflammatory diseases affecting the cardiovascular network are the cause of high mortality. As suggested, these two pathologies have several developmental similarities and share common genetic variants. Therefore, we additionally seek to discuss the main promoters presented in the current literature, aiming at benefiting from their similarities in drug discovery and repositioning, via automatic artificial intelligence pattern recognition, forecasting, and computational design. © Springer International Publishing AG 2017.",,"Biotechnology, Icelandic Institute for Intelligent Machines, Reykjavik, Iceland; Department of Computer Sciences, Polytechnic Institute, University Autonoma of Madrid, Madrid, Spain; Department of Bioengineering, Imperial College London, London, United Kingdom",,,Pereira G.C.,"Pereira, G.C., Biotechnology, Icelandic Institute for Intelligent Machines, Reykjavik, Iceland, Department of Computer Sciences, Polytechnic Institute, University Autonoma of Madrid, Madrid, Spain, Department of Bioengineering, Imperial College London, London, United Kingdom",,,10.1007/978-3-319-53880-8_11,SCOPUS,1,,,,,,,,,,,,,Biotechnology and Production of Anti-Cancer Compounds,Adaptive pharmacogenomics; Artificial intelligence; Chronic arterial inflammation; Cytokines; Deep neural networks; Drug discovery; Drug repositioning; Genetic fingerprints; Genomics; Glioblastoma; Inflammatory signalling cascades; Reinforcement learning; Transcription factors,,2-s2.0-85034400377,,,,,,281,253,,,,,Scopus,,,Biotechnology and Production of Anti-Cancer Compounds,,Book Chapter,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034400377&doi=10.1007%2f978-3-319-53880-8_11&partnerID=40&md5=fde62df000191d3078169ff2d8da4bd5,,,,2017,,,,,,,,,,,,,,,,,,,
Table of contents,The following topics are dealt with: data mining; Japanese WordNet synonym misplacement detection; social network; recommender system; sentiment analysis; workshop-based instruction; Japanese public libraries; machine learning methods; collaborative Web presentation support system; SMS4 ultracompact hardware implementation; wireless sensor networks; personalized public transportation recommendation system; adaptive user interface; NIS-Apriori algorithm; GetRNIA software tool; rough set-based rule generation; tree-Ga bump hunting; neural network model; weighted citation network analysis; sound proofing ventilation unit; touch interaction; mutually dependent Markov decision processes; ozone treatment; dynamic query optimization; big data; learner activity recognition; IoT-security approach; nutrition-based vegetable production; farm product cultivation; polynomial time mat learning; C-deterministic regular formal graph system; article abstract key expression extraction; English text comprehension; online social games; knowledge creation; knowledge utilization; online stock trading; customer behavior analysis; project-based collaborative learning; in-field mobile game-based learning activities; e-portfolio system design; self-regulated learning ontological model; mobile augmented reality based scaffolding platform; context-aware mobile Japanese conversation learning system; English writing error classification; image processing; outside-class learning; exercise-centric teaching materials; UML modeling; online historical document reading literacy; MMORPG-based learning environment; computer courses; undergraduate education; energy management system; higher education; decentralised auction-based bandwidth allocation; wireless networked control systems; resource scheduling algorithm; embedded cloud computing; Poisson distribution; Japanese seismic activity; suspect vehicle detection; 3D network traffic visualization; Web information retrieval; agent based disaster evacua- ion assist system; electroencephalogram; random number generator; multiagent simulations; multicore environment; CPU scheduler; multithreaded processes; reserve-price biddings; real-time traffic signal control; evolutionary computation; robot-assisted rehabilitation system; hybrid automata; Batik motif classification; color-texture-based feature extraction; backpropagation; multimedia storytelling; e-tourism service; Web mining; search engine; simulation-based e-learning mobile application software; library classification training system; WebQuest learning strategy; context-aware ubiquitous English learning; support vector machine; RFID tag ownership transfer protocol; cognitive linguistics; collaborative software engineering learning; write-access reduction method; NVM-DRAM hybrid memory; garbage collection; parallel indexing scheme; lazy-updating snoop cache protocol; distributed storage system; ITS application; software engineering education; ophthalmic multimodal imaging system; injected bug classification; secure live virtual machine migration; flash memory management; genetic programming; heterogeneous databases; time series similarity search; concurrency control program generation; incremental data migration; multidatabase system; software release time decision making; analytic hierarchy process; interactive genetic algorithm; biometric intelligence; talking robots; archaeological ruin analysis; GIS; optical wireless pedestrian-support systems; visual impairment; extreme programming; Japanese e-commerce Web sites; Chinese sign language animation; hearing-impaired people mammography inspection; geographical maps; electroculogram; XML element retrieval technique; image recognition; reinforcement learning; ECU formal verification; gasoline direct injection engines; earthquake disaster simulation; smart devices for autistic children; RoboCup rescue simulation; inductive logic programming; master-slave asynchronous evolutionary hybrid algorithm; VANET routing opt,,,,,,,0,,10.1109/IIAI-AAI.2014.4,IEEE Xplore,1,,20141201,xix,,,Big Data;DRAM chips;Internet of Things;Markov processes;Poisson distribution;Unified Modeling Language;XML;agriculture;analytic hierarchy process;archaeology;augmented reality;automata theory;backpropagation;bandwidth allocation;biometrics (access control);cache storage;citation analysis;cloud computing;computational complexity;computer animation;computer games;computer science education;concurrency control;consumer behaviour;data mining;data visualisation;distributed databases;educational courses;electro-oculography;electroencephalography;electronic commerce;emergency management;energy management systems;engines;feature extraction;flash memories;formal verification;further education;genetic algorithms;geographic information systems;groupware;handicapped aids;human computer interaction;humanoid robots;image classification;image colour analysis;image texture;inductive logic programming;intelligent tutoring systems;investment;library automation;linguistics;mammography;medical robotics;mobile computing;multi-agent systems;multi-threading;multimedia computing;multiprocessing systems;natural language processing;networked control systems;neural nets;object detection;ozonation (materials processing);patient rehabilitation;pedestrians;processor scheduling;public transport;query processing;random number generation;recommender systems;rescue robots;resource allocation;rough set theory;search engines;security of data;seismology;social networking (online);software prototyping;software tools;stock markets;storage management;support vector machines;teaching;telecommunication network routing;text analysis;time series;traffic control;traffic engineering computing;travel industry;trees (mathematics);unsupervised learning;user interfaces;vehicular ad hoc networks;ventilation;virtual machines;wireless sensor networks,3D network traffic visualization;Batik motif classification;C-deterministic regular formal graph system;CPU scheduler;Chinese sign language animation;ECU formal verification;English text comprehension;English writing error classification;GIS;GetRNIA software tool;ITS application;IoT-security approach;Japanese WordNet synonym misplacement detection;Japanese e-commerce Web sites;Japanese public libraries;Japanese seismic activity;MMORPG-based learning environment;NIS-Apriori algorithm;NVM-DRAM hybrid memory;Poisson distribution;RFID tag ownership transfer protocol;RoboCup rescue simulation;SMS4 ultracompact hardware implementation;UML modeling;VANET routing optimization;Web image sharing services;Web information retrieval;Web mining;WebQuest learning strategy;XML element retrieval technique;adaptive user interface;agent based disaster evacuation assist system;analytic hierarchy process;archaeological ruin analysis;article abstract key expression extraction;autistic children;backpropagation;big data;biometric intelligence;cognitive linguistics;collaborative Web presentation support system;collaborative software engineering learning;color-texture-based feature extraction;computer courses;concurrency control program generation;context-aware mobile Japanese conversation learning system;context-aware ubiquitous English learning;customer behavior analysis;data mining;decentralised auction-based bandwidth allocation;distributed storage system;dynamic query optimization;e-portfolio system design;e-tourism service;earthquake disaster simulation;electroencephalogram;electrooculogram;embedded cloud computing;energy management system;evolutionary computation;exercise-centric teaching materials;extreme programming;farm product cultivation;flash memory management;garbage collection;gasoline direct injection engines;genetic programming;geographical maps;hearing-impaired people mammography inspection;heterogeneous databases;higher education;hybrid automata;image processing;image recognition;in-field mobile game-based learning activities;incremental data migration;inductive logic programming;injected bug classification;interactive genetic algorithm;knowledge creation;knowledge utilization;lazy-updating snoop cache protocol;learner activity recognition;library classification training system;machine learning methods;master-slave asynchronous evolutionary hybrid algorithm;mobile augmented reality based scaffolding platform;multiagent simulations;multicore environment;multidatabase system;multimedia storytelling;multithreaded processes;mutually dependent Markov decision processes;neural network model;nutrition-based vegetable production;online historical document reading literacy;online social games;online stock trading;ophthalmic multimodal imaging system;optical wireless pedestrian-support systems;outside-class learning;ozone treatment;parallel indexing scheme;personalized public transportation recommendation system;polynomial time mat learning;project-based collaborative learning;random number generator;real-time traffic signal control;recommender system;reinforcement learning;reserve-price biddings;resource scheduling algorithm;robot-assisted rehabilitation system;rough set-based rule generation;search engine;secure live virtual machine migration;self-regulated learning ontological model;sentiment analysis;simulation-based e-learning mobile application software;social network;software engineering education;software release time decision making;sound proofing ventilation unit;support vector machine;suspect vehicle detection;talking robots;time series similarity search;touch interaction;tree-Ga bump hunting;undergraduate education;visual impairment;weighted citation network analysis;wireless networked control systems;wireless sensor networks;workshop-based instruction;write-access reduction method,,,,Aug. 31 2014-Sept. 4 2014,,2014 IIAI 3rd International Conference on Advanced Applied Informatics,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6913248,,,,,,IEEE,,,CD-ROM:978-1-4799-4175-9; Electronic:978-1-4799-4173-5; POD:978-1-4799-1679-5,,2014 IIAI 3rd International Conference on Advanced Applied Informatics,v,IEEE Conferences,,,,,2014,,,,,,,,,,,,,,,,,,,
Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453),The following topics are dealt with: tactile sensing; aerial vehicles; legged robots; motion and path planning; learning systems; simultaneous localization and mapping; visual tracking; cooperative sensing; outdoor vehicles; biped walking; collision avoidance; reinforcement learning; visual servoing; sensor applications; underwater robots; legged locomotion; learning control; sensor-based planning; computational intelligence; mobile robot localization; robot vision; Internet robots; humanoid robots; fuzzy and neural control; sensing for mobile platforms; biologically inspired robots; trajectory planning; architecture and programming; vision-based monitoring; 3D sensing; cellular and modular robots; planning algorithms; mobiligence; multi-robot control; intelligent environment; sensor fusion; micro and nano robotic systems; task allocation; actuator systems; multi-robot systems; manufacturing systems; mechanism design; integrated MEMS sensors and actuators; force-responsive mechatronics in industry; medical robots and haptics; service robots; dexterous hands; sensing and navigation; telerobotics; personal robots; rescue and security robots; spaced robots; human/robot cooperation; compliant motion control; robot assisted surgery; grasping; human-robot interaction; intelligent robots; and virtual reality.,,,,,,,0,,10.1109/IROS.2003.1249176,IEEE Xplore,1,,20040107,,,Fuzzy control;Learning control systems;Mechatronics;Motion control;Multisensor systems;Neurocontrollers;Robots;Tactile sensors;Tracking,collision avoidance;control engineering computing;fuzzy control;learning systems;mechatronics;motion control;neurocontrollers;robots;sensor fusion;tactile sensors;tracking,Internet robots;MEMS sensors;actuator systems;aerial vehicles;biped walking;collision avoidance;compliant motion control;computational intelligence;cooperative sensing;force-responsive mechatronics;fuzzy control;human-robot interaction;humanoid robots;intelligent robots;learning control;learning systems;legged locomotion;legged robots;manufacturing systems;medical robots;microrobotic systems;mobile robot localization;motion planning;multirobot control;nanorobotic systems;neural control;outdoor vehicles;path planning;reinforcement learning;robot assisted surgery;robot vision;sensor fusion;sensor-based planning;simultaneous localization;simultaneous mapping;spaced robots;tactile sensing;task allocation;telerobotics;trajectory planning;underwater robots;vision-based monitoring;visual servoing;visual tracking,,,,27-31 Oct. 2003,,Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1249176,,,,,,IEEE,,,POD:0-7803-7860-1,,Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453),,IEEE Conferences,,,3,,2003,,,,,,,,,,,,,,,,,,,
Smart Lifelong Learning System Based on Q-Learning,"The majority of current web-based learning systems are closed learning environments where courses and learning materials are fixed and the only dynamic aspect is the organization of the material that can be adapted to allow a relatively individualized learning environment. In this paper, we propose an evolving web-based learning system which can adapt itself to its learners. More specifically, the novelty with respect to the system lies in its ability to find relevant content on the web, and its ability to personalize and adapt this content based on the system's observation of its learners and the accumulated ratings given by the learners. Hence, although learners do not have direct interaction with the open Web, the system can retrieve relevant information related to them and their situated learning characteristics. Lifelong learning scenarios have particular differences in their need for personalized recommendations that make not possible reusing existing general approaches of recommender systems. The paper describes those challenges and we propose a hybrid technique based on machine learning to recognize learner preferences and predict theirs required contents with high accuracy.",,,,"Adv. E-Learning Technol. Lab., AmirKabir Univ. of Technol., Tehran, Iran",A. A. Kardan; O. R. B. Speily,,0,,10.1109/ITNG.2010.140,IEEE Xplore,1,,20100701,1091,,Adaptive systems;Electronic learning;Information filtering;Information retrieval;Information technology;Learning systems;Least squares approximation;Machine learning;Multitasking;Recommender systems,Internet;computer aided instruction;continuing professional development;information filters;learning (artificial intelligence),Q-Learning;Web-based learning systems;learning materials;machine learning;personalized recommendations;recommender systems;smart lifelong learning system,,,,12-14 April 2010,,2010 Seventh International Conference on Information Technology: New Generations,Learning Promotion;Lifelong Learning;Machine Learning;Q Learning;Recommender Systems;Reinforcement Learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501486,,,,,,IEEE,12,,Electronic:978-1-4244-6271-1; POD:978-1-4244-6270-4,,2010 Seventh International Conference on Information Technology: New Generations,1086,IEEE Conferences,,,,,2010,,,,,,,,,,,,,,,,,,,
Using personality models as prior knowledge to accelerate learning about stress-coping preferences (demonstration),"The management of (dis)stress is an important factor for a long and healthy life. Yet, stress affects people differently and everyone manages stress in different ways. In this paper we introduce PeSA, the Personality-enabled Stress Assistant, an agent-based application that accounts for this individualism. PeSA merges several agent techniques: Reinforcement learning is used to learn about preferences of the users, prior knowledge and knowledge transfer is applied to accelerate the learning process, agent mirroring helps to enable communication and offline functionalities. Based on these mechanisms, PeSA guides through stressful phases by proposing coping strategies that are tailored to the personality of each individual user. Users can assess these advices and thus provide a reward or punishment signal that helps PeSA to improve its suggestions. Copyright © 2016, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",,"DAI-Labor, Technische Universität Berlin, Ernst-Reuter-Platz 7, Berlin, Germany; German Turkish Advanced Research Centre for ICT, Ernst-Reuter-Platz 7, Berlin, Germany",,,"Ahrndt S., Lützenberger M., Prochnow S.M.","Ahrndt, S., DAI-Labor, Technische Universität Berlin, Ernst-Reuter-Platz 7, Berlin, Germany; Lützenberger, M., DAI-Labor, Technische Universität Berlin, Ernst-Reuter-Platz 7, Berlin, Germany; Prochnow, S.M., German Turkish Advanced Research Centre for ICT, Ernst-Reuter-Platz 7, Berlin, Germany",,,,SCOPUS,0,,,,,,,,,,,,,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",Human-agent teamwork; Human-behaviour models; Reinforcement learning,,2-s2.0-85014298076,,,,,,1487,1485,,,,,Scopus,,,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014298076&partnerID=40&md5=ae6bb87e160503ce6a9421e9baa40182,,,,2016,,,,,,,,,,,,,,,,,,,
A reinforcement learning approach to weaning of mechanical ventilation in intensive care units,"The management of invasive mechanical ventilation, and the regulation of sedation and analgesia during ventilation, constitutes a major part of the care of patients admitted to intensive care units. Both prolonged dependence on mechanical ventilation and premature extubation are associated with increased risk of complications and higher hospital costs, but clinical opinion on the best protocol for weaning patients off of a ventilator varies. This work aims to develop a decision support tool that uses available patient information to predict time-to-extubation readiness and to recommend a personalized regime of sedation dosage and ventilator support. To this end, we use off-policy reinforcement learning algorithms to determine the best action at a given patient state from sub-optimal historical ICU data. We compare treatment policies from fitted Qiteration with extremely randomized trees and with feedforward neural networks, and demonstrate that the policies learnt show promise in recommending weaning protocols with improved outcomes, in terms of minimizing rates of reintubation and regulating physiological stability.",,"Computer Science, Princeton University, United States; Electrical Engineering, Princeton University, United States; Penn Medicine, United States",,,"Prasad N., Cheng L.-F., Chivers C., Draugelis M., Engelhardt B.E.","Prasad, N., Computer Science, Princeton University, United States; Cheng, L.-F., Electrical Engineering, Princeton University, United States; Chivers, C., Penn Medicine, United States; Draugelis, M., Penn Medicine, United States; Engelhardt, B.E., Computer Science, Princeton University, United States",,,,SCOPUS,0,,,,,,,,,,,,,"Uncertainty in Artificial Intelligence - Proceedings of the 33rd Conference, UAI 2017",,,2-s2.0-85031120880,,,,,,,,,,,,Scopus,,,"Uncertainty in Artificial Intelligence - Proceedings of the 33rd Conference, UAI 2017",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031120880&partnerID=40&md5=e33338ea1b60efc1a559dcfffb00b4f9,,,,2017,,,,,,,,,,,,,,,,,,,
Reducing Delay during Vertical Handover,"The next generation of wireless networks will provide heterogeneous wireless technologies to a mobile user, in which they can move using terminals with multiple access interfaces and non-real-time or real-time services. The most important issue in such environment is the Always Best Connected (ABC) concept allowing the best connectivity to applications anywhere at any time. With the growing of communication world lot of research is carried out in the field of wireless networks specifically next generation networks. The integration and interoperability of various wireless networks will lead to number of challenges. One of the challenges is the handovers across various/heterogeneous wireless networks as well as reduction of call drop probability. Number of researches has proposed solutions for this challenge. This paper proposes a solution to this problem for the improvement of end-to-end Quality of Service supporting high data rates, real time transmission over wide area and enabling users to specify their personal preferences using Markov Decision Process (MDP). The proposed method uses delay as its basic parameter to select a network during handovers. The selection of the network amongst the existing wireless network considers the ongoing application of the user during the handover. Through simulations we show that our algorithm reduces call drop probability and satisfies user's requirements.",,,,"Dept. of Electron. Eng., Pad. Dr. D.Y.P.I.E.T., Pune, India",N. Bagdure; B. Ambudkar,,0,,10.1109/ICCUBEA.2015.44,IEEE Xplore,1,,20150716,204,,Bandwidth;Delays;Handover;Mobile communication;Quality of service;Wireless networks,delays;mobility management (mobile radio);next generation networks;probability;quality of service,call drop probability reduction;delay reduction;end-to-end quality of service;handover;heterogeneous wireless networks;mobile user;next generation networks;wireless network integration;wireless network interoperability,,,,26-27 Feb. 2015,,2015 International Conference on Computing Communication Control and Automation,Markov Decision process;Reinforcement Learning;Reward;Transition Probability;Vertical Handover,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155834,,,,,,IEEE,13,,Electronic:978-1-4799-6892-3; POD:978-1-4799-6893-0,,2015 International Conference on Computing Communication Control and Automation,200,IEEE Conferences,,,,,2015,,,,,,,,,,,,,,,,,,,
Personalized Web recommendations: supporting epistemic information about end-users,The online recommendations are a popular presence in the Web sites world due to their potential to increase the customers' satisfaction. The ability to represent epistemic information about the clients' beliefs is important to understand their needs. This paper presents a recommender system based on reinforcement learning. The system represents concepts presented on a Web site by epistemic logical programs and uses a similarity measure between programs in order to facilitate generalization. A prototype of this system and experiments are presented.,,,,"Dept. of Comput. Sci., Craiova Univ., Romania",M. Preda; D. Popescu,,0,,10.1109/WI.2005.115,IEEE Xplore,1,,20051017,695,,Automation;Computer science;Feedback;Function approximation;Humans;Learning;Logic;Ontologies;Prototypes;Recommender systems,Web sites;customer satisfaction;information filters;learning (artificial intelligence);logic programming,Web site;customer satisfaction;end-user;epistemic logical program;online recommendation;personalized Web recommendation;program similarity measure;reinforcement learning,,,,19-22 Sept. 2005,,The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1517935,,,,,,IEEE,8,,POD:0-7695-2415-X,,The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05),692,IEEE Conferences,,,,,2005,,,,,,,,,,,,,,,,,,,
Personalization Information Technology of Multi Roles Cooperative Reinforcement Learning [J],"The paper concerns the present application situation that the network resources inflating frequently make the current search engine index tens of thousands of results, the good and evil intermingled. Referring social science and economics and psychology and distributed …",,,,,,,0,,,Google Scholar,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,http://en.cnki.com.cn/Article_en/CJFDTOTAL-JSGG200631053.htm,,,,2006,,,,,,,,,,,,0,,,,,,,
Learning rate control for downlink shared channel in WCDMA,"The paper considers bit rate control algorithms for the downlink shared channel in WCDMA (wideband code division multiple access) network. The goal of the rate control is to maximise the data throughput while satisfying quality constraints for the voice users. A learning algorithm, based on Reinforcement learning, is proposed. The algorithm facilitates frame acknowledgments to make decisions on the rate change. The control channel power is used as the state variable and a probabilistic decision policy is applied. Performance of the algorithm is evaluated and compared with performance of a simple algorithm without learning. Data throughput and voice quality cost function are used as performance measures. The learning algorithm achieves higher throughput and lower voice quality cost than the reference algorithm.",,,,,B. Makarevitch,,1,,10.1109/PIMRC.2003.1259284,IEEE Xplore,1,,20040114,2922 vol.3,,Control systems;Cost function;Downlink;Intelligent networks;Interference;Learning;Multiaccess communication;Propagation losses;Throughput;Wideband,broadband networks;code division multiple access;learning (artificial intelligence);probability;telecommunication channels;telecommunication links,Reinforcement learning;WCDMA;bit rate control algorithm;downlink shared channel;learning algorithm;probabilistic decision policy;state variable;voice user;wideband code division multiple access,,,,7-10 Sept. 2003,,"14th IEEE Proceedings on Personal, Indoor and Mobile Radio Communications, 2003. PIMRC 2003.",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1259284,,,,,,IEEE,5,,POD:0-7803-7822-9,,"14th IEEE Proceedings on Personal, Indoor and Mobile Radio Communications, 2003. PIMRC 2003.",2919,IEEE Conferences,,,3,,2003,,,,,,,,,,,,,,,,,,,
Adaptive Learning Based on Exercises Fitness Degree,"The paper considers the e-learning systems that provide personalized content to their users and that permanently adapt to the evolution of the users during their learning stages. Such systems help the students to consolidate their knowledge faster than other methods. The main contribution is the proposal of a mathematical model of an adaptive learning system with the mentioned characteristics. The model involves a multi step process where, at each stage, the performances of the student are measured and the system is adapting accordingly.",,,,,A. M. Mirea; M. C. Preda,,0,,10.1109/WI-IAT.2009.266,IEEE Xplore,1,,20091009,218,,Adaptive systems;Computer science;Electronic learning;Intelligent agent;Learning systems;Mathematical model;Paper technology;Performance evaluation;Proposals;Testing,,,,,,15-18 Sept. 2009,,2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology,adaptive control;adaptive learning environments;learning systems;personalized learning;reinforcement learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5284965,,,,,,IEEE,5,,POD:978-0-7695-3801-3,,2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology,215,IEEE Conferences,,,3,,2009,,,,,,,,,,,,,,,,,,,
Application of reinforcement learning to admission control in CDMA network,The paper describes an admission control algorithm for the CDMA networks which is able to adapt to the operating environment. The algorithm is based on the principle of reinforcement learning and it achieves near-optimal performance for various radio propagation conditions and network operator's objectives. The performance evaluation results for different state space alternatives and algorithm parameters are presented and compared with the conventional admission control based on the power thresholds,,,,"Commun. Lab., Helsinki Univ. of Technol., Espoo, Finland",B. Makarevitch,,3,,10.1109/PIMRC.2000.881639,IEEE Xplore,1,,20020806,1357 vol.2,,Admission control;Base stations;Communication system control;Degradation;Downlink;Intelligent networks;Interference;Learning;Multiaccess communication;Paper technology,Markov processes;cellular radio;code division multiple access;learning (artificial intelligence);multiuser channels;radio networks;telecommunication congestion control,CDMA network;Markov decision process;admission control algorithm;algorithm parameters;cellular radio;near-optimal performance;network operator objectives;performance evaluation results;power thresholds;radio propagation conditions;reinforcement learning;state space alternatives,,,,2000,,11th IEEE International Symposium on Personal Indoor and Mobile Radio Communications. PIMRC 2000. Proceedings (Cat. No.00TH8525),,,,18 Sep 2000-21 Sep 2000,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=881639,,,,,1,IEEE,4,,POD:0-7803-6463-5,,11th IEEE International Symposium on Personal Indoor and Mobile Radio Communications. PIMRC 2000. Proceedings (Cat. No.00TH8525),1353,IEEE Conferences,,,2,,2000,,,,,,,,,,,,,,,,,,,
Moral learning: Psychological and philosophical perspectives,"The past 15 years occasioned an extraordinary blossoming of research into the cognitive and affective mechanisms that support moral judgment and behavior. This growth in our understanding of moral mechanisms overshadowed a crucial and complementary question, however: How are they learned? As this special issue of the journal Cognition attests, a new crop of research into moral learning has now firmly taken root. This new literature draws on recent advances in formal methods developed in other domains, such as Bayesian inference, reinforcement learning and other machine learning techniques. Meanwhile, it also demonstrates how learning and deciding in a social domain—and especially in the moral domain—sometimes involves specialized cognitive systems. We review the contributions to this special issue and situate them within the broader contemporary literature. Our review focuses on how we learn moral values and moral rules, how we learn about personal moral character and relationships, and the philosophical implications of these emerging models. © 2017 Elsevier B.V.",,"Department of Psychology, Harvard University, United States; Department of Philosophy, Boston University, United States; Department of Philosophy, University of Michigan, United States",,,"Cushman F., Kumar V., Railton P.","Cushman, F., Department of Psychology, Harvard University, United States; Kumar, V., Department of Philosophy, Boston University, United States; Railton, P., Department of Philosophy, University of Michigan, United States",,,10.1016/j.cognition.2017.06.008,SCOPUS,1,,,,,,,,,,,,,Cognition,,,2-s2.0-85020828418,,,,,,10,1,,,,,Scopus,,,Cognition,,Editorial,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020828418&doi=10.1016%2fj.cognition.2017.06.008&partnerID=40&md5=443f3960e177478132939cdf99032dd8,,167,,2017,,,,,,,,,,,,,,,,,,,
Emergency Navigation in Confined Spaces Using Dynamic Grouping,"The performance of Emergency Management Systems (EMS) in confined spaces is highly dependent on the decision algorithm employed for the safe navigation of the evacuees to the available exits. In the algorithm proposed in this paper, we have considered evacuees under two groups, based on their age and physical condition, and we tailor two routing metrics, one for each group, in finding suitable paths for the evacuees. A dynamic grouping mechanism that can adjust an evacuee's group, and therefore routing metric, according to its on-going health condition is employed during the evacuation. To implement the routing metrics, we have used the Cognitive Packet Network (CPN) with random neural networks (RNN) and reinforcement learning. The CPN is an adaptive routing protocol that is loop-free at all times and easily handles multiple quality of service (QoS) metrics. Simulation results show that allowing the navigation system to be sensitive to the on-going health conditions and mobility of the evacuees, using our proposed dynamic grouping, can achieve higher survival rates.",,,,"Dept. of Electr. & Electron. Eng., Imperial Coll. London, London, UK",H. Bi; O. J. Akinwande; E. Gelenbe,,,,10.1109/NGMAST.2015.12,IEEE Xplore,1,,20160107,125,,Buildings;Hazards;Heuristic algorithms;Measurement;Navigation;Quality of service;Routing,cognitive radio;emergency management;learning (artificial intelligence);navigation;neural nets;packet radio networks;quality of service;routing protocols;safety,CPN;EMS;QoS metrics;RNN;adaptive routing protocol;cognitive packet network;confined spaces;decision algorithm;dynamic grouping mechanism;emergency management systems;emergency navigation;health conditions;navigation safety;navigation system;physical condition;quality of service metrics;random neural networks;reinforcement learning;routing metrics,,,,9-11 Sept. 2015,,"2015 9th International Conference on Next Generation Mobile Applications, Services and Technologies",Cognitive Packet Network;Dynamic Grouping;Emergency navigation;QoS driven protocol,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7373229,,,,,,IEEE,22,,CD-ROM:978-1-4799-8659-0; Electronic:978-1-4799-8660-6; POD:978-1-4799-8661-3,,"2015 9th International Conference on Next Generation Mobile Applications, Services and Technologies",120,IEEE Conferences,,,,,2015,,,,,,,,,,,,,,,,,,,
A markov-based decision process model for wireless access agent,"The Personal Router is a mobile personal user agent whose task is to dynamically model the user, update its knowledge of a market of wireless service providers and select providers that satisfies the user's expected preferences. In this paper, we show how the user modeling problem can be represented as a Markov-based Decision Process (MDP) and suggest reinforcement learning and collaborative filtering as two candidate solution mechanisms for the information problem in the user modeling. © 2009 Binary Information Press.",,"Business School, Renmin University of China, Beijing 100872, China",,,Chen H.,"Chen, H., Business School, Renmin University of China, Beijing 100872, China",,,,SCOPUS,0,,,,,,,,,,1,,,Journal of Information and Computational Science,Agent; Markov-based decision process; Personal router; Wireless access,,2-s2.0-66149128446,,,,,,422,415,,,,,Scopus,,,Journal of Information and Computational Science,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66149128446&partnerID=40&md5=ad9493ff0bfd400ce2d780a8a277aa4f,,6,,2009,,,,,,,,,,,,,,,,,,,
Active learning for personalizing treatment,"The personalization of treatment via genetic biomarkers and other risk categories has drawn increasing interest among clinical researchers and scientists. A major challenge here is to construct individualized treatment rules (ITR), which recommend the best treatment for each of the different categories of individuals. In general, ITRs can be constructed using data from clinical trials, however these are generally very costly to run. In order to reduce the cost of learning an ITR, we explore active learning techniques designed to carefully decide whom to recruit, and which treatment to assign, throughout the online conduct of the clinical trial. As an initial investigation, we focus on simple ITRs that utilize a small number of subpopulation categories to personalize treatment. To minimize the maximal uncertainty regarding the treatment effects for each subpopulation, we propose the use of a minimax bandit model and provide an active learning policy for solving it. We evaluate our active learning policy using simulated data and data modeled after a clinical trial involving treatments for depressed individuals. We contrast this policy with other plausible active learning policies. The techniques presented in the paper may be generalized to tackle problems of efficient exploration in other domains.",,,,"Department of Statistics, University of Michigan, USA",K. Deng; J. Pineau; S. Murphy,,1,,10.1109/ADPRL.2011.5967348,IEEE Xplore,1,,20110728,39,,Clinical trials;Learning systems;Loss measurement;Machine learning;Recruitment;Resource management;Uncertainty,learning (artificial intelligence);medical computing;minimax techniques;patient treatment,active learning;clinical research;genetic biomarkers;individualized treatment rules;minimax bandit model;risk category;treatment personalization,,2325-1824;23251824,,11-15 April 2011,,2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967348,,,,,,IEEE,32,,Electronic:978-1-4244-9888-8; POD:978-1-4244-9887-1,,2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL),32,IEEE Conferences,,,,,2011,,,,,,,,,,,,,,,,,,,
Personalized galaxies of information,"The Personalized Galaxies of Information demonstration presents a new interface approach for visualizing, navigating and accessing information objects in a large body of unstructured information, such as on-line new stories, photographs and video clips available via Clarinews; electronic mail; and World Wide Web documents. The system provides mechanisms to analyze the relationships between information objects and builds a representation of the underlying structure of the entire body of information. This relational structure is used to construct a visual information space with which the user interacts to explore the contents of the information base. The system also uses a learning algorithm to adaptively customize the presentation of information to a particular user's interests. This dynamic, personalized structuring of information helps users perform directed searches while simultaneously affording general browsing in a fluid and seamless environment.",,"MIT Media Lab, Cambridge, United States",,,Rennison Earl,"Rennison, Earl, MIT Media Lab, Cambridge, United States",7,,,SCOPUS,0,,,,,,,,,,,,,Conference on Human Factors in Computing Systems - Proceedings,,,2-s2.0-0029204727,,,,,,32,31,,,,,Scopus,,,Conference on Human Factors in Computing Systems - Proceedings,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029204727&partnerID=40&md5=46e64663194ee5b069580c8532f337c5,,2,,1995,,,,,,,,,,,,,,,,,,,
User centered and context dependent personalization through experiential transcoding,"The pervasive presence of devices exploited to use and deliver entertainment text-based content can make reading easier to get but more difficult to enjoy, in particular for people with reading-related disabilities. The main solutions that allow overcoming some of the difficulties experienced by users with specific and special needs are based on content adaptation and user profiling. This paper presents a system that aims to improve content legibility by exploiting experiential transcoding techniques. The system we propose tracks users' behaviors so as to provide a tailored adaptation of textual content, in order to fit the needs of a specific user on the several different devices she/he actually uses.",,,,"Department of Computer Science and Engineering University of Bologna Bologna, Italy",S. Ferretti; S. Mirri; C. Prandi; P. Salomoni,,4,,10.1109/CCNC.2014.6940520,IEEE Xplore,1,,20141103,491,,Adaptation models;Context;Entertainment industry;Multimedia communication;Prototypes;Transcoding;Web pages,Web sites;assisted living;human factors;text analysis;transcoding;ubiquitous computing,content adaptation;content legibility improvement;context dependent personalization;entertainment text-based content;transcoding techniques;user centered personalization;user profiling,,2331-9852;23319852,,10-13 Jan. 2014,,2014 IEEE 11th Consumer Communications and Networking Conference (CCNC),content adaptation;device capabilities;legibility;reinforcement learning;user profiling,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6940520,,,,,,IEEE,23,,Electronic:978-1-4799-2355-7; POD:978-1-4799-2357-1,,2014 IEEE 11th Consumer Communications and Networking Conference (CCNC),486,IEEE Conferences,,,,,2014,,,,,,,,,,,,,,,,,,,
Dynamic Class of Service mapping for Quality of Experience control in future networks,"The present work introduces a novel approach to cope with some key limitations of the present communication networks. In particular, the need for effectively managing heterogeneous resources over heterogeneous networks while guaranteeing personalized Quality of Experience (QoE) requirements to the applications, claims for a full cognitive approach which is realized through the introduction of an appropriate Future Internet architecture. The paper, taking this architecture in mind, introduces the innovative concept of dynamic association between applications and Classes of Services, performed by means of proper Reinforcement Learning algorithms. The resulting procedure guarantees QoE personalization, requires low processing capabilities and entails limited signalling overhead.",,,,,F. D. Priscoli; L. Fogliati; A. Palo; A. Pietrabissa,,0,,,IEEE Xplore,0,,20140624,6,,,,,,,,1-3 June 2014,,WTC 2014; World Telecommunications Congress 2014,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6840010,,,,,,VDE,,,Paper:978-3-8007-3602-7,,WTC 2014; World Telecommunications Congress 2014,1,VDE Conferences,,,,,2014,,,,,,,,,,,,,,,,,,,
Parallel implementation of instinctual and learning neural mechanisms in a simulated mobile robot,"The question of how biological learning and instinctive neural mechanisms interact with each other in the course of development to produce novel, adaptive behaviors was explored via a robotic simulation. Instinctive behavior in the agent was implemented in a hard-wired network which produced obstacle avoidance. Phototactic behavior was produced in two serially connected plastic layers. A self-organizing feature map was combined with a reinforcement learning layer to produce a learning network. The reinforcement came from an internally generated signal. Both the adaptive and fixed networks supplied motor control signals to the robot motors. The sizes of the self-organizing layer, reinforcement layer, and the complexity of the environment were varied and effects on robot phototactic efficiency and accuracy in the mature networks were measured. A significant interaction of the three independent variables was found, supporting the idea that organisms evolve distinct combinations of instinctive and plastic neural mechanisms which are tailored to the demands of the environment in which their species evolved. © 2012 Springer-Verlag.",,"BioMimetic and Cognitive Robotics Laboratory, Brooklyn College, Brooklyn, NY, United States; Department of Psychology, Brooklyn College, Brooklyn, NY, United States; Centre for the Study of Cultural Evolution, Stockholm University, Sweden",,,"Young B., Ghirlanda S., Grasso F.W.","Young, B., BioMimetic and Cognitive Robotics Laboratory, Brooklyn College, Brooklyn, NY, United States; Ghirlanda, S., Department of Psychology, Brooklyn College, Brooklyn, NY, United States, Centre for the Study of Cultural Evolution, Stockholm University, Sweden; Grasso, F.W., BioMimetic and Cognitive Robotics Laboratory, Brooklyn College, Brooklyn, NY, United States, Department of Psychology, Brooklyn College, Brooklyn, NY, United States",,,10.1007/978-3-642-31525-1_26,SCOPUS,1,,,,,,,,,,,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),evolution; instinct; learning neural network; robots,,2-s2.0-84864023295,,,,,,308,298,,,,,Scopus,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864023295&doi=10.1007%2f978-3-642-31525-1_26&partnerID=40&md5=e3e1a751618d3dcae7806ade3a230c20,,7375 LNAI,,2012,,,,,,,,,,,,,,,,,,,
"Maximization, learning, and economic behavior","The rationality assumption that underlies mainstream economic theory has proved to be a useful approximation, despite the fact that systematic violations to its predictions can be found. That is, the assumption of rational behavior is useful in understanding the ways in which many successful economic institutions function, although it is also true that actual human behavior falls systematically short of perfect rationality. We consider a possible explanation of this apparent inconsistency, suggesting that mechanisms that rest on the rationality assumption are likely to be successful when they create an environment in which the behavior they try to facilitate leads to the best payoff for all agents on average, and most of the time. Review of basic learning research suggests that, under these conditions, people quickly learn to maximize expected return. This review also shows that there are many situations in which experience does not increase maximization. In many cases, experience leads people to underweight rare events. In addition, the current paper suggests that it is convenient to distinguish between two behavioral approaches to improve economic analyses. The first, and more conventional approach among behavioral economists and psychologists interested in judgment and decision making, highlights violations of the rational model and proposes descriptive models that capture these violations. The second approach studies human learning to clarify the conditions under which people quickly learn to maximize expected return. The current review highlights one set of conditions of this type and shows how the understanding of these conditions can facilitate market design.",,"Industrial Engineering and Management, Technion, Haifa 32000, Israel; Warwick Business School, University of Warwick, Coventry CV4 7AL, United Kingdom; Department of Economics, Stanford University, Stanford, CA 94305-6072, United States",,,"Erev I., Roth A.E.","Erev, I., Industrial Engineering and Management, Technion, Haifa 32000, Israel, Warwick Business School, University of Warwick, Coventry CV4 7AL, United Kingdom; Roth, A.E., Department of Economics, Stanford University, Stanford, CA 94305-6072, United States",34,,10.1073/pnas.1402846111,SCOPUS,1,,,,,,,,,,SUPPL.3,,,Proceedings of the National Academy of Sciences of the United States of America,Contingencies of reinforcements; Decisions from experience; Experience-description gap; Mechanism design; Reinforcement learning,,2-s2.0-84904623675,,,,,,10825,10818,,,,,Scopus,,,Proceedings of the National Academy of Sciences of the United States of America,,Review,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904623675&doi=10.1073%2fpnas.1402846111&partnerID=40&md5=69d7ba87bf82a1c7d278af694eecaf43,,111,,2014,,,,,,,,,,,,,,,,,,,
GongBroker: A Broker Model for Power Trading in Smart Grid Markets,"The Smart Grid market is dynamic and complex, and brokers are widely introduced to better manage this market. This paper proposes an intelligent broker model - GongBroker, with smart trading strategies to cope with the dynamics and complexity. GongBroker first predicts the short-term demands of various consumers, and then buys energy from the wholesale market through auctions, and sells energy to various consumers in the retail market. In order to predict the customer demand, a data-driven method is proposed. All the consumers are hierarchically clustered according to their historical energy consumptions, and different prediction methods are tailored for different customer clusters to predict one-day-ahead hourly energy demand. Based on the predicted demand, the GongBroker employs a Markov Decision Process for the one-day-ahead auction in the wholesale market. To compete with other brokers, GongBroker uses independent reinforcement learning processes to optimize prices for different types of consumers. Experimental results demonstrate that GongBroker not only is competitive in making profit, but also keeps a good supply-demand balance.",,,,"Sch. of Comput. & Inf. Technol., Univ. of Wollongong, Wollongong, NSW, Australia",X. Wang; M. Zhang; F. Ren; T. Ito,,1,,10.1109/WI-IAT.2015.108,IEEE Xplore,1,,20160204,24,,Adaptation models;Electronic mail;Energy consumption;Mathematical model;Prediction algorithms;Prediction methods;Smart grids,Markov processes;demand side management;learning (artificial intelligence);power markets;smart power grids;supply and demand,GongBroker;Markov decision process;consumers short-term demands;data-driven method;day-ahead hourly energy demand;independent reinforcement learning processes;intelligent broker model;power trading;prediction methods;smart grid markets;smart trading strategies;supply-demand balance,,,,6-9 Dec. 2015,,2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT),Broker Model;Data-driven;Reinforcement Learning;Smart Grid Market,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7397309,,,,,,IEEE,6,,Electronic:978-1-4673-9618-9; POD:978-1-4673-9619-6,,2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT),21,IEEE Conferences,,,2,,2015,,,,,,,,,,,,,,,,,,,
Recipe tuning by reinforcement learning in the SandS ecosystem,"The Social and Smart (SandS) project ecosystem is compounded of household appliance users sharing recipes for the used of appliances, an intermediate control layer, and an intelligent social layer which aims to optimize the appliance recipes maximizing user satisfaction. We consider two aspects of the social intelligence, the innovation producing new recipes for unkown user tasks, and the adaptation to personalize the recipe to an individual user on the basis of his/her specific feedback. The second aspect is proposed to be dealt with by Reinforcement Learning approach, thus user feedback becomes the system reward. In this paper we discuss such an architecture based on the actor-critic approach, providing some experimental results on synthetic datasets that demonstrate the feasibility of the approach, previous to real life implementations.",,,,"Computational Intelligence Group, University of the Basque Country, UPV/EHU, San Sebastian, Spain",B. Fernandez-Gauna; M. Graña,,0,,10.1109/CASoN.2014.6920422,IEEE Xplore,1,,20141013,60,,Biological system modeling;Computational modeling;Computer architecture;Robots;Service-oriented architecture,domestic appliances;learning (artificial intelligence);social sciences computing;user interfaces,SandS ecosystem;actor-critic approach;household appliance;intelligent social layer;recipe tuning;reinforcement learning;social and smart project ecosystem;social intelligence;user satisfaction,,,,July 30 2014-Aug. 1 2014,,2014 6th International Conference on Computational Aspects of Social Networks,Reinforcement Learning;Social computing;Social networks;subconscious social intelligence,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6920422,,,,,,IEEE,15,,Electronic:978-1-4799-5940-2; POD:978-1-4799-5941-9,,2014 6th International Conference on Computational Aspects of Social Networks,55,IEEE Conferences,,,,,2014,,,,,,,,,,,,,,,,,,,
Software service selection by multi-level matching and reinforcement learning,"The software realization of distributed systems is typically achieved as loose coalitions of independently created services. The selection of such services, to act as building blocks of a distributed system, is a critical task that requires discovery and matching activities. This selection task is generally based on simple matching techniques and without any notion of customization. This paper presents a method to achieve the service discovery process using the principles of multilevel matching based on multi-level specifications and customization based on reinforcement learning techniques. In this method, services are selected dynamically using an on-line performance-based reinforcement feedback. In contrast to methods which require the services to actually carry out a task before being selected, in the method proposed in this paper, service selection is carried out using only specification matching, thereby eliminating a large amount of redundant computation. Experimental results are presented in the context of a information classification system. These experiments demonstrate that a high degree of performance can be achieved at a much reduced computational cost using only multi-level specification-matching based reinforcement feedback signals. © 2012 ICST Institute for Computer Science, Social Informatics and Telecommunications Engineering.",,"Indiana University Purdue University Indianapolis, Indianapolis, IN 46202, United States",,,"Raje R.R., Mukhopadhyay S., Phatak S., Shastri R., Gallege L.S.","Raje, R.R., Indiana University Purdue University Indianapolis, Indianapolis, IN 46202, United States; Mukhopadhyay, S., Indiana University Purdue University Indianapolis, Indianapolis, IN 46202, United States; Phatak, S., Indiana University Purdue University Indianapolis, Indianapolis, IN 46202, United States; Shastri, R., Indiana University Purdue University Indianapolis, Indianapolis, IN 46202, United States; Gallege, L.S., Indiana University Purdue University Indianapolis, Indianapolis, IN 46202, United States",,,10.1007/978-3-642-32615-8_31,SCOPUS,1,,,,,,,,,,,,,"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering",acquaintances; classification; discovery; multi-level specifications; reinforcement learning; software services,,2-s2.0-84869594563,,,,,,324,310,,,,,Scopus,,,"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869594563&doi=10.1007%2f978-3-642-32615-8_31&partnerID=40&md5=0cd9062b73a1835a5e2626173a36dbc9,,87 LNICST,,2012,,,,,,,,,,,,,,,,,,,
Evaluation of trust in robots: A cognitive approach,"The study of Human-Robot interaction faces one of the biggest challenges in measuring the trustworthiness of the robots. The enhancement and the augmentation of the human capabilities using the human robot integration are dependent on the reliability and dependability of the robots. These factors become more significant when the participation of the robot is the human robot integration is active and the cohesion between humans and robots is high. In order to measure the trust and other cognitive parameters of the robot, we have designed trust model in this research paper. This paper evaluates the trust of a customized robot while performing a task using three different algorithms. The algorithms used for the path planning task in this paper are simple artificial neural network; reinforcement based artificial neural network and Situation-Operator Model. The trust model proposed in this paper has been simulated using the results obtained while the robot performed its tasks using the three algorithms. The results show that the trust of the robot increases with each learning cycle thereby indicating that the training of the robot enhances the trust parameter of the robot.",,,,"Department of CS/IS SMCS-CEST, Fiji National University, Fiji",B. Kumar; A. D. Dubey,,,,10.1109/ICCCI.2017.8117701,IEEE Xplore,1,,20171123,6,,Artificial neural networks;Indexes;Informatics;Path planning;Robots,human-robot interaction;learning (artificial intelligence);man-machine systems;neural nets;path planning;robots,Situation-Operator Model;customized robot;human capabilities;human robot integration;robot increases;robot trustworthiness;trust model;trust parameter,,,,5-7 Jan. 2017,,2017 International Conference on Computer Communication and Informatics (ICCCI),Capability;Cognitive;Human Computer Interaction;Robot;Trust Model,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8117701,,,,,,IEEE,,,CD:978-1-4673-8853-5; Electronic:978-1-4673-8855-9; POD:978-1-4673-8856-6,,2017 International Conference on Computer Communication and Informatics (ICCCI),1,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
Multi-timescale nexting in a reinforcement learning robot,"The term 'nexting' has been used by psychologists to refer to the propensity of people and many other animals to continually predict what will happen next in an immediate, local, and personal sense. The ability to 'next' constitutes a basic kind of awareness and knowledge of one's environment. In this paper we present results with a robot that learns to next in real time, making thousands of predictions about sensory input signals at timescales from 0.1 to 8 seconds. Our predictions are formulated as a generalization of the value functions commonly used in reinforcement learning, where now an arbitrary function of the sensory input signals is used as a pseudo reward, and the discount rate determines the timescale. We show that six thousand predictions, each computed as a function of six thousand features of the state, can be learned and updated online ten times per second on a laptop computer, using the standard temporal-difference(λ) algorithm with linear function approximation. This approach is sufficiently computationally efficient to be used for real-time learning on the robot and sufficiently data efficient to achieve substantial accuracy within 30 minutes. Moreover, a single tile-coded feature representation suffices to accurately predict many different signals over a significant range of timescales. We also extend nexting beyond simple timescales by letting the discount rate be a function of the state and show that nexting predictions of this more general form can also be learned with substantial accuracy. General nexting provides a simple yet powerful mechanism for a robot to acquire predictive knowledge of the dynamics of its environment. © The Author(s) 2014.",,"Reinforcement Learning and Artificial Intelligence Laboratory, University of Alberta, Canada",,,"Modayil J., White A., Sutton R.S.","Modayil, J., Reinforcement Learning and Artificial Intelligence Laboratory, University of Alberta, Canada; White, A., Reinforcement Learning and Artificial Intelligence Laboratory, University of Alberta, Canada; Sutton, R.S., Reinforcement Learning and Artificial Intelligence Laboratory, University of Alberta, Canada",16,,10.1177/1059712313511648,SCOPUS,1,,,,,,,,,,2,,,Adaptive Behavior,predictive knowledge; Reinforcement learning; robotics; temporal difference learning,,2-s2.0-84896357393,,,,,,160,146,,,,,Scopus,,,Adaptive Behavior,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896357393&doi=10.1177%2f1059712313511648&partnerID=40&md5=bb7136a9a1b35419ad7b0a5bd5a80987,,22,,2014,,,,,,,,,,,,,,,,,,,
Optimal radio channel recommendations with explicit and implicit feedback,"The very large majority of recommender systems are running as server-side applications, and they are controlled by the content provider, i.e., who provides the recommended items. This paper focuses on a different scenario: the user is supposed to be able to access content from multiple providers, in our application they offer radio channels, and it is up to a personal recommender installed on the clients' side to decide which channel to select and recommend to the user. We exploit the implicit feedback derived from the user's listening behavior, and we model channel recommendation as a sequential decision making problem. We have implemented a personal RS that integrates reinforcement learning techniques to decide what channel to play every time the user asks for a new music track or the current track finishes playing. In a live user study we show that the proposed system can sequentially select the next channel to play such that the users listen to the streamed tracks for a larger fraction, and for more time, compared to a baseline system not exploiting implicit feedback. Copyright © 2012 by the Association for Computing Machinery, Inc. (ACM).",,"Free University of Bozen-Bolzano, Piazza Domenicani 3, Bolzano, Italy; Telefonica Research, Plaza se E. Lluchi Martin 5, Barcelona, Spain",,,"Moling O., Baltrunas L., Ricci F.","Moling, O., Free University of Bozen-Bolzano, Piazza Domenicani 3, Bolzano, Italy; Baltrunas, L., Telefonica Research, Plaza se E. Lluchi Martin 5, Barcelona, Spain; Ricci, F., Free University of Bozen-Bolzano, Piazza Domenicani 3, Bolzano, Italy",6,,10.1145/2365952.2365971,SCOPUS,1,,,,,,,,,,,,,RecSys'12 - Proceedings of the 6th ACM Conference on Recommender Systems,Implicit feedback; Reinforcement learning; Sequential music recommendations,,2-s2.0-84867388659,,,,,,82,75,,,,,Scopus,,,RecSys'12 - Proceedings of the 6th ACM Conference on Recommender Systems,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867388659&doi=10.1145%2f2365952.2365971&partnerID=40&md5=5a2334482844dda8d0e8d5a453984015,,,,2012,,,,,,,,,,,,,,,,,,,
Hybrid-ε-greedy for mobile context-aware recommender system,"The wide development of mobile applications provides a considerable amount of data of all types. In this sense, Mobile Context-aware Recommender Systems (MCRS) suggest the user suitable information depending on her/his situation and interests. Our work consists in applying machine learning techniques and reasoning process in order to adapt dynamically the MCRS to the evolution of the user's interest. To achieve this goal, we propose to combine bandit algorithm and case-based reasoning in order to define a contextual recommendation process based on different context dimensions (social, temporal and location). This paper describes our ongoing work on the implementation of a MCRS based on a hybrid-ε-greedy algorithm. It also presents preliminary results by comparing the hybrid-ε-greedy and the standard ε-greedy algorithm. © 2012 Springer-Verlag.",,"Department of Computer Science, Télécom SudParis, UMR CNRS Samovar, 91011 Evry Cedex, France",,,"Bouneffouf D., Bouzeghoub A., Gançarski A.L.","Bouneffouf, D., Department of Computer Science, Télécom SudParis, UMR CNRS Samovar, 91011 Evry Cedex, France; Bouzeghoub, A., Department of Computer Science, Télécom SudParis, UMR CNRS Samovar, 91011 Evry Cedex, France; Gançarski, A.L., Department of Computer Science, Télécom SudParis, UMR CNRS Samovar, 91011 Evry Cedex, France",11,,10.1007/978-3-642-30217-6_39,SCOPUS,1,,,,,,,,,,PART 1,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),contextual bandit; exploration/exploitation dilemma; Machine learning; personalization; recommender systems,,2-s2.0-84861444817,,,,,,479,468,,,,,Scopus,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861444817&doi=10.1007%2f978-3-642-30217-6_39&partnerID=40&md5=ad6ea70f5fbea80eaaa0f3864a7fd509,,7301 LNAI,,2012,,,,,,,,,,,,,,,,,,,
Customized learning algorithms for episodic tasks with acyclic state spaces,"The work presented in this paper provides a practical, customized learning algorithm for reinforcement learning tasks that evolve episodically over acyclic state spaces. The presented results are motivated by the optimal disassembly planning (ODP) problem described in, and they complement and enhance some earlier developments on this problem that were presented in. In particular, the proposed algorithm is shown to be a substantial improvement of the original algorithm developed in, in terms of, both, the involved computational effort and the attained performance, where the latter is measured by the accumulated reward. The new algorithm also leads to a robust performance gain over the typical Q-learning implementations for the considered problem context.",,,,"School of Industrial & Systems Engineering, Georgia Institute of Technology, USA",T. Bountourelis; S. Reveliotis,,0,,10.1109/COASE.2009.5234189,IEEE Xplore,1,,20090909,634,,Aerospace industry;Algorithm design and analysis;Automation;Convergence;Data mining;Learning;Q factor;Space technology;State-space methods;Systems engineering and theory,learning (artificial intelligence),Q-learning implementation;acyclic state space;customized learning algorithm;episodic task;optimal disassembly planning;reinforcement learning;robust performance gain,,2161-8070;21618070,,22-25 Aug. 2009,,2009 IEEE International Conference on Automation Science and Engineering,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5234189,,,,,,IEEE,18,,POD:978-1-4244-4578-3,,2009 IEEE International Conference on Automation Science and Engineering,627,IEEE Conferences,,,,,2009,,,,,,,,,,,,,,,,,,,
An interactive multisensing framework for personalized human robot collaboration and assistive training using reinforcement learning,"There is a recent trend of research and applications of Cyber- Physical Systems (CPS) in manufacturing to enhance humanrobot collaboration and production. In this paper, we propose a CPS framework for personalized Human-Robot Collaboration and Training to promote safe human-robot collaboration in manufacturing environments. We propose a human-centric CPS approach that focuses on multimodal human behavior monitoring and assessment, to promote human worker safety and enable human training in Human- Robot Collaboration tasks. We present the architecture of our proposed system, our experimental testbed and our proposed methods for multimodal physiological sensing, human state monitoring and interactive robot adaptation, to enable personalized interaction. © 2017 ACM.",,"HERACLEIA Human-Centered Computing Laboratory, CSE Dept., University of Texas, Arlington, United States; Industrial Engineering Dept., University of Texas, Arlington, United States; Dept. of Psychiatry, Yale School of Medicine, United States; CSE Dept., University of Michigan, United States; Mechanical Engineering Dept., University of Michigan-Flint, United States",,,"Tsiakas K., Papakostas M., Papakostas M., Bell M., Mihalcea R., Wang S., Burzo M., Makedon F.","Tsiakas, K., HERACLEIA Human-Centered Computing Laboratory, CSE Dept., University of Texas, Arlington, United States; Papakostas, M., HERACLEIA Human-Centered Computing Laboratory, CSE Dept., University of Texas, Arlington, United States; Papakostas, M., HERACLEIA Human-Centered Computing Laboratory, CSE Dept., University of Texas, Arlington, United States; Bell, M., Dept. of Psychiatry, Yale School of Medicine, United States; Mihalcea, R., CSE Dept., University of Michigan, United States; Wang, S., Industrial Engineering Dept., University of Texas, Arlington, United States; Burzo, M., Mechanical Engineering Dept., University of Michigan-Flint, United States; Makedon, F., HERACLEIA Human-Centered Computing Laboratory, CSE Dept., University of Texas, Arlington, United States",,,10.1145/3056540.3076191,SCOPUS,1,,,,,,,,,,,,,ACM International Conference Proceeding Series,Cyber Physical Systems; Human Robot Collaboration; Intelligent Manufacturing; Vocational Assessment and Training,,2-s2.0-85025128294,,,,,,427,423,,,,,Scopus,,,ACM International Conference Proceeding Series,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025128294&doi=10.1145%2f3056540.3076191&partnerID=40&md5=c676d60143fb0a043e53e429754aa90b,,Part F128530,,2017,,,,,,,,,,,,,,,,,,,
Budgeted Learning for Developing Personalized Treatment,"There is increased interest in using patient-specific information to personalize treatment. Personalized treatment decision rules can be learned using data from standard clinical trials, but such trials are very costly to run. This paper explores the use of budgeted learning techniques to design more efficient clinical trials, by effectively determining which type of patients to recruit, at each time, throughout the duration of the trial. We propose a Bayesian bandit model and discuss the computational challenges and issues pertaining to this approach. We compare our budgeted learning algorithm, which approximately minimizes the Bayes risk, using both simulated data and data modeled after a clinical trial for treating depressed individuals, with other plausible algorithms. We show that our budgeted learning algorithm demonstrated excellent performance across a wide variety of situations.",,,,"Dept. of Stat., Univ. of Michigan, Ann Arbor, MI, USA",K. Deng; R. Greiner; S. Murphy,,0,,10.1109/ICMLA.2014.8,IEEE Xplore,1,,20150209,14,,Algorithm design and analysis;Approximation algorithms;Approximation methods;Bayes methods;Clinical trials;Fasteners;Resource management,Bayes methods;learning (artificial intelligence);medical information systems;minimisation;patient treatment,Bayes risk;Bayesian bandit model;budgeted learning;clinical trial;patient-specific information;personalized treatment decision rule,,,,3-6 Dec. 2014,,2014 13th International Conference on Machine Learning and Applications,Active Learning;Bayesian;Budgeted Learning;Personalized Treatment;Reinforcement Learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7033084,,,,,,IEEE,24,,Electronic:978-1-4799-7415-3; POD:978-1-4799-7416-0; USB:978-1-4799-7414-6,,2014 13th International Conference on Machine Learning and Applications,7,IEEE Conferences,,,,,2014,,,,,,,,,,,,,,,,,,,
Adaptive joint call admission control and access network selection for multimedia wireless systems,"Third generation wireless networks and beyond will solicit the cooperation of heterogeneous access networks, so as to provide multimedia traffic to different classes of users, with varying quality requisites over regions and time zones. We address the problem of how to partition the traffic demand efficiently onto the underlying radio access networks. The design objective is a resource allocation strategy, which provides a maximal resource utilization across all access networks. At the same time, the allocation should respect quality levels related to handover dropping performance; these levels can be predefined per service and per region. We propose a solution based on reinforcement learning, which runs independently at each of the cells of every access system, and report results. In the case where network revenue does not depend solely on resource utilization, but on parameters such as the type of service and/or the service duration, the method is readily extensible to include these factors.",,,,"INT, Motorola Labs., Paris, France",E. Alexandri; G. Martinez; D. Zeghlache,,7,,10.1109/WPMC.2002.1088408,IEEE Xplore,1,,20021216,1394 vol.3,,Adaptive control;Call admission control;Communication system traffic control;Learning;Multimedia systems;Programmable control;Radio access networks;Resource management;Telecommunication traffic;Wireless networks,3G mobile communication;cellular radio;learning (artificial intelligence);multimedia communication;optimisation;quality of service;radio access networks;resource allocation;telecommunication congestion control;telecommunication traffic,adaptive access network selection;adaptive call admission control;handover dropping;multimedia traffic;network revenue;quality levels;radio access networks;reinforcement learning;resource allocation strategy;resource utilization optimization;third generation wireless networks,,1347-6890;13476890,,27-30 Oct. 2002,,The 5th International Symposium on Wireless Personal Multimedia Communications,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1088408,,,,,,IEEE,5,,POD:0-7803-7442-8,,The 5th International Symposium on Wireless Personal Multimedia Communications,1390,IEEE Conferences,,,3,,2002,,,,,,,,,,,,,,,,,,,
Implementation of a learning fuzzy controller,"This article describes our efforts at designing and implementing a practical learning fuzzy controller using inexpensive hardware. The controller engages basic control concepts and system-independent learning rules to enable it to adapt in real time to unknown plants even when it starts with a vacuous initial control policy. The controller remains dormant when the plant is operating satisfactorily and autonomously initiates online adaptation in real time when adverse performance is observed. The Intel-8031-based hardware implementation is geared for extensibility, robustness, and fault tolerance. Limited plant-dependent information is incorporated to tailor the hardware to applications. The design produces learning rates exceeding 200 reinforcements per second. The controller thus is able to learn to control unknown plants in real time even while it is controlling them. Physical experiments indicate that the learning fuzzy controller can rapidly and effectively deal with variations in plant characteristics, compensate for wear and tear, and handle disturbances and noise.<<ETX>>",,,,"Center for Intelligent Syst., Tulsa Univ., OK, USA",S. Shenoi; K. Ashenayi; M. Timmerman,,11,,10.1109/37.387620,IEEE Xplore,1,,20020806,80,,Control systems;Delay;Error correction;Fault tolerance;Fuzzy control;Hardware;Real time systems;Robustness;Shape control;Table lookup,control system synthesis;fuzzy control;learning systems;microcontrollers;robust control,Intel-8031-based hardware;basic control concepts;extensibility;fault tolerance;learning fuzzy controller;limited plant-dependent information;robustness;system-independent learning rules;vacuous initial control policy,,1066-033X;1066033X,3,June 1995,,IEEE Control Systems,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=387620,,,,,,IEEE,24,,,,IEEE Control Systems,73,IEEE Journals & Magazines,,,15,,1995,,,,,,,,,,,,,,,,,,,
Reinforcement learning for resource provisioning in the vehicular cloud,"This article presents a concise view of vehicular clouds that incorporates various vehicular cloud models that have been proposed to date. Essentially, they all extend the traditional cloud and its utility computing functionalities across the entities in the vehicular ad hoc network. These entities include fixed roadside units, onboard units embedded in the vehicle, and personal smart devices of drivers and passengers. Cumulatively, these entities yield abundant processing, storage, sensing, and communication resources. However, vehicular clouds require novel resource provisioning techniques that can address the intrinsic challenges of dynamic demands for the resources and stringent QoS requirements. In this article, we show the benefits of reinforcement-learning-based techniques for resource provisioning in the vehicular cloud. The learning techniques can perceive long-term benefits and are ideal for minimizing the overhead of resource provisioning for vehicular clouds.",,,,Universite du Quebec a Montreal,M. A. Salahuddin; A. Al-Fuqaha; M. Guizani,,1,,10.1109/MWC.2016.7553036,IEEE Xplore,1,,20160826,135,,Cloud computing;Dynamic scheduling;Intelligent vehicles;Quality of service;Resource management;Software as a service;Vehicle dynamics,cloud computing;learning (artificial intelligence);minimisation;quality of service;resource allocation;traffic engineering computing;vehicular ad hoc networks,QoS requirements;communication resources;dynamic resource demands;fixed roadside units;onboard units;overhead minimization;personal smart devices;processing resources;reinforcement learning;resource provisioning;sensing resources;storage resources;utility computing functionalities;vehicular ad hoc network;vehicular cloud models,,1536-1284;15361284,4,August 2016,,IEEE Wireless Communications,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7553036,,,,,,IEEE,,,,,IEEE Wireless Communications,128,IEEE Journals & Magazines,,,23,,2016,,,,,,,,,,,,,,,,,,,
A review of reward processing and motivational impairment in schizophrenia,"This article reviews and synthesizes research on reward processing in schizophrenia, which has begun to provide important insights into the cognitive and neural mechanisms associated with motivational impairments. Aberrant cortical-striatal interactions may be involved with multiple reward processing abnormalities, including: (1) dopamine-mediated basal ganglia systems that support reinforcement learning and the ability to predict cues that lead to rewarding outcomes; (2) orbitofrontal cortex-driven deficits in generating, updating, and maintaining value representations; (3) aberrant effort-value computations, which may be mediated by disrupted anterior cingulate cortex and midbrain dopamine functioning; and (4) altered activation of the prefrontal cortex, which is important for generating exploratory behaviors in environments where reward outcomes are uncertain. It will be important for psychosocial interventions targeting negative symptoms to account for abnormalities in each of these reward processes, which may also have important interactions; suggestions for novel behavioral intervention strategies that make use of external cues, reinforcers, and mobile technology are discussed. © 2013 © The Author 2014. Published by Oxford University Press on behalf of the Maryland Psychiatric Research Center. All rights reserved.",,"Department of Psychology, State University of New York at Binghamton, PO Box 6000, Binghamton, NY 13902, United States; Department of Psychiatry, University of Maryland School of Medicine, Maryland Psychiatric Research Center, Baltimore, MD, United States",,,"Strauss G.P., Waltz J.A., Gold J.M.","Strauss, G.P., Department of Psychology, State University of New York at Binghamton, PO Box 6000, Binghamton, NY 13902, United States; Waltz, J.A., Department of Psychiatry, University of Maryland School of Medicine, Maryland Psychiatric Research Center, Baltimore, MD, United States; Gold, J.M., Department of Psychiatry, University of Maryland School of Medicine, Maryland Psychiatric Research Center, Baltimore, MD, United States",109,,10.1093/schbul/sbt197,SCOPUS,1,,,,,,,,,,SUPPL. 2,,,Schizophrenia Bulletin,anhedonia; avolition; motivation; negative symptoms; psychosis; reward,,2-s2.0-84895746086,,,,,,S116,S107,,,,,Scopus,,,Schizophrenia Bulletin,,Review,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895746086&doi=10.1093%2fschbul%2fsbt197&partnerID=40&md5=f5b2699983cc16f3d2cd92e69f3d0f38,,40,,2014,,,,,,,,,,,,,,,,,,,
Machine learning in personalized anemia treatment,"This chapter presents application of reinforcement learning to drug dosing personalization in treatment of chronic conditions. Reinforcement learning is a machine learning paradigm that mimics the trialand-error skill acquisition typical for humans and animals. In treatment of chronic illnesses, finding the optimal dose amount for an individual is also a process that is usually based on trial-and-error. In this chapter, the author focuses on the challenge of personalized anemia treatment with recombinant human erythropoietin. The author demonstrates the application of a standard reinforcement learning method, called Q-learning, to guide the physician in selecting the optimal erythropoietin dose. The author further addresses the issue of random exploration in Q-learning from the drug dosing perspective and proposes a ""smart"" exploration method. Finally, the author performs computer simulations to compare the outcomes from reinforcement learning-based anemia treatment to those achieved by a standard dosing protocol used at a dialysis unit. © 2010, IGI Global.",,"Department of Medicine, Division of Nephrology, University of Louisville, United States",,,Gaweda A.E.,"Gaweda, A.E., Department of Medicine, Division of Nephrology, University of Louisville, United States",,,10.4018/978-1-60566-766-9.ch012,SCOPUS,1,,,,,,,,,,,,,"Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques",,,2-s2.0-84898381465,,,,,,276,265,,,,,Scopus,,,"Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques",,Book Chapter,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898381465&doi=10.4018%2f978-1-60566-766-9.ch012&partnerID=40&md5=25913f90e423c1b3501bf0b18d7056c2,,,,2009,,,,,,,,,,,,,,,,,,,
The route not taken: Driver-centric estimation of electric vehicle range,"This paper addresses the challenge of efficiently and accurately predicting an electric vehicle's attainable range. Specifically, our approach accounts for a driver's generalised route preferences to provide up-to-date, personalised information based on estimates of the energy required to reach every possible destination in a map. We frame this task in the context of sequential decision making and show that energy consumption in reaching a particular destination can be formulated as policy evaluation in a Markov Decision Process. In particular, we exploit the properties of the model adopted for predicting likely energy consumption to every possible destination in a realistically sized map in real-time. The policy to be evaluated is learned and, over time, refined using Inverse Reinforcement Learning to provide for a life-long adaptive system. Our approach is evaluated using a publicly available dataset providing real trajectory data of 50 individuals spanning approximately 10,000 miles of travel. We show that by accounting for driver specific route preferences our system significantly reduces the relative error in energy prediction compared to more common, driver-agnostic heuristics such as shortest-path or shortest-time routes. Copyright © 2014, Association for the Advancement of Artificial Intelligence.",,"Mobile Robotics Group, University of Oxford, United Kingdom",,,"Ondrúška P., Posner I.","Ondrúška, P., Mobile Robotics Group, University of Oxford, United Kingdom; Posner, I., Mobile Robotics Group, University of Oxford, United Kingdom",11,,,SCOPUS,0,,,,,,,,,,January,,,"Proceedings International Conference on Automated Planning and Scheduling, ICAPS",,,2-s2.0-84933050809,,,,,,420,413,,,,,Scopus,,,"Proceedings International Conference on Automated Planning and Scheduling, ICAPS",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933050809&partnerID=40&md5=46b4fa6e96f048605ef4708bb7d219f6,,2014-January,,2014,,,,,,,,,,,,,,,,,,,
A computational model for distributed knowledge systems with learning mechanisms,"This paper addresses the issues of machine learning in distributed knowledge systems, which will consist of distributed software agents with problem solving, communication and learning functions. To develop such systems, we must analyze the roles of problem-solving and communication capabilities among knowledge systems. To facilitate the analyses, we propose a computational model: LPC. The model consists of a set of agents with (a) a knowledge base for learned concepts, (b) a knowledge base for problem solving, (c) prolog-based inference mechanisms and (d) a set of beliefs on the reliability of the other agents. Each agent can improve its own problem-solving capabilities by deductive learning from the given problems, by memory-based learning from communications between the agents and by reinforcement learning from the reliability of communications between the other agents. An experimental system of the model has been implemented in Prolog language on a Window-based personal computer. Intensive experiments have been carried out to examine the feasibility of the machine learning mechanisms of agents for problem-solving and communication capabilities. The experimental results have shown that the multiagent system improves the performance of the whole system in problem solving, when each agent has a higher learning ability or when an agent with a very high ability for problem solving joins the organization to cooperate with the other agents in problem solving. These results suggest that the proposed model is useful in analyzing the learning mechanisms applicable to distributed knowledge systems. Copyright © 1996 Elsevier Science Ltd.",,"Grad. School of Systems Management, University of Tsukuba, 3-29-1 Otsuka, Bunkyo-ku, Tokyo 112, Japan; Engineering Systems Department, Mitsubishi Research Institute Inc., 2-3-6 Otemachi, Chiyoda-ku, Toyko 100, Japan",,,"Aiba H., Terano T.","Aiba, H., Grad. School of Systems Management, University of Tsukuba, 3-29-1 Otsuka, Bunkyo-ku, Tokyo 112, Japan, Engineering Systems Department, Mitsubishi Research Institute Inc., 2-3-6 Otemachi, Chiyoda-ku, Toyko 100, Japan; Terano, T., Grad. School of Systems Management, University of Tsukuba, 3-29-1 Otsuka, Bunkyo-ku, Tokyo 112, Japan",9,,,SCOPUS,0,,,,,,,,,,3-4 SPEC. ISS.,,,Expert Systems with Applications,,,2-s2.0-0029724561,,,,,,427,417,,,,,Scopus,,,Expert Systems with Applications,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029724561&partnerID=40&md5=94aef0749f79d93a223768fd0a3a5948,,10,,1996,,,,,,,,,,,,,,,,,,,
Automatic web content personalization through reinforcement learning,"This paper deals with the automatic adaptation of Web contents. It is recognized that quite often users need some personalized adaptations to access Web contents. This is more evident when we focus on people with some accessibility needs. Based on the user profile, it is possible to transcode or modify contents (e.g., adapt text fonts) so as to meet the user preferences. The problem is that applying such a kind of transformations to the whole content might significantly alter Web pages that might become unreadable, hence making matters worse. We present a system that employs Web intelligence to perform automatic adaptations on single elements composing a Web page. A reinforcement learning algorithm is utilized to manage user profiles. We evaluate our system through simulation and a real assessment where elderly users where asked to use for a time period our system prototype. Results confirm the feasibility of the proposal. © 2016 Elsevier Inc.",,"Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy",,,"Ferretti S., Mirri S., Prandi C., Salomoni P.","Ferretti, S., Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy; Mirri, S., Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy; Prandi, C., Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy; Salomoni, P., Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy",9,,10.1016/j.jss.2016.02.008,SCOPUS,1,,,,,,,,,,,,,Journal of Systems and Software,Reinforcement learning; User profiling; Web personalization,,2-s2.0-84975757906,,,,,,169,157,,,,,Scopus,,,Journal of Systems and Software,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975757906&doi=10.1016%2fj.jss.2016.02.008&partnerID=40&md5=8d7c3661bc0d939a042f1b25211981a7,,121,,2016,,,,,,,,,,,,,,,,,,,
A Learning interface agent for scheduling meetings,"This paper describes a Learning Interface Agent for a meeting scheduling application. The agent employs Machine Learning techniques to customize itself to the user's personal scheduling rules and preferences by observing the user's actions and receiving direct user-feedback. Our approach provides the user with sophisticated control over the gradual delegation of scheduling tasks to the agent, as a trust relationship is built. We report upon an experiment in which a collection of such assistants became gradually more helpful to their users through the use of memory-based and reinforcement learning. The experimental data reported upon demonstrate that the learning approach to building intelligent interface agents is a very promising one which has several advantages over more standard approaches. © 1992 ACM.",,"MIT Media-Lab, 20 Ames Street, Cambridge, MA, United States",,,"Kozierok R., Maes P.","Kozierok, R., MIT Media-Lab, 20 Ames Street, Cambridge, MA, United States; Maes, P., MIT Media-Lab, 20 Ames Street, Cambridge, MA, United States",57,,,SCOPUS,0,,,,,,,,,,,,,"International Conference on Intelligent User Interfaces, Proceedings IUI",Interface agents; Learning interface agents; Machine learning; Personal assistants; Software agents,,2-s2.0-84943177303,,,,,,88,81,,,,,Scopus,,,"International Conference on Intelligent User Interfaces, Proceedings IUI",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943177303&partnerID=40&md5=7e3e84b3f621b3d76bbe6069ae98fb28,,Part F127502,,1993,,,,,,,,,,,,,,,,,,,
Learning user's preferences by analyzing Web-browsing behaviors,"This paper describes a method for an information filtering agent to learn user's preferences. The proposed method observes user's reactions to the filtered documents and learns from them the profiles for the individual users. Reinforcement learning is used to adapt the most significant terms that best represent user's interests. In contrast to conventional relevance feedback methods which require explicit user feedbacks, our approach learns user preferences implicitly from direct observations of browsing behaviors during interaction. Field tests have been made which involved 10 users reading a total of 18,750 HTML documents during 45 days. The proposed method showed superior performance in personalized information filtering compared to the existing relevance feedback methods.",,"Seoul Natl Univ, Seoul, South Korea",,,"Seo Young-Woo, Zhang Byoung-Tak","Seo, Young-Woo, Seoul Natl Univ, Seoul, South Korea; Zhang, Byoung-Tak, Seoul Natl Univ, Seoul, South Korea",47,,,SCOPUS,0,,,,,,,,,,,,,Proceedings of the International Conference on Autonomous Agents,,,2-s2.0-0033700745,,,,,,387,381,,,,,Scopus,,,Proceedings of the International Conference on Autonomous Agents,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033700745&partnerID=40&md5=e804803772de377c8c621ff60bb12c35,,,,2000,,,,,,,,,,,,,,,,,,,
A reinforcement learning agent for personalized information filtering,This paper describes a method for learning user's interests in the Web-based personalized information filtering system called WAIR. The proposed method analyzes user's reactions to the presented documents and learns from them the profiles for the individual users …,,,,,,,109,,,Google Scholar,0,,,,,,,,,,,,,,,,,,,,"http://scholar.google.com/scholar?cluster=12777029087535817507&hl=en&as_sdt=0,5",,,,,,,,,,,,,,http://scholar.google.com/https://dl.acm.org/citation.cfm?id=325859,,,,2000,,,1.27770290875358E+019,,,,,,,,,15,,,,,,"http://scholar.google.com/scholar?cites=12777029087535817507&as_sdt=2005&sciodt=0,5&hl=en",
Reinforcement learning agent for personalized information filtering,"This paper describes a method for learning user's interests in the Web-based personalized information filtering system called WAIR. The proposed method analyzes user's reactions to the presented documents and learns from them the profiles for the individual users. Reinforcement learning is used to adapt the term weights in the user profile so that user's preferences are best represented. In contrast to conventional relevance feedback methods which require explicit user feedbacks, our approach learns user preferences implicitly from direct observations of user behaviors during interaction. Field tests have been made which involved 7 users reading a total of 7,700 HTML documents during 4 weeks. The proposed method showed superior performance in personalized information filtering compared to the existing relevance feedback methods.",,"Seoul Natl Univ, Seoul, South Korea",,,"Seo Young-Woo, Zhang Byoung-Tak","Seo, Young-Woo, Seoul Natl Univ, Seoul, South Korea; Zhang, Byoung-Tak, Seoul Natl Univ, Seoul, South Korea",61,,,SCOPUS,0,,,,,,,,,,,,,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,2-s2.0-0033688671,,,,,,251,248,,,,,Scopus,,,"International Conference on Intelligent User Interfaces, Proceedings IUI",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033688671&partnerID=40&md5=0bbfc79c2083cfd6cde5235144850154,,,,2000,,,,,,,,,,,,,,,,,,,
Integration of Semantic and Episodic Memories,"This paper describes the integration of semantic and episodic memory (EM) models and the benefits of such integration. Semantic memory (SM) is used as a foundation of knowledge and concept learning, and is needed for the operation of any cognitive system. EM retains personal experiences stored based on their significance-it is supported by the SM, and in return, it supports SM operations. Integrated declarative memories are critical for cognitive system development, yet very little research has been done to develop their computational models. We considered structural self-organization of both semantic and episodic memories with a symbolic representation of input events. Sequences of events are stored in EM and are used to build associations in SM. We demonstrated that integration of semantic and episodic memories improves the native operation of both types of memories. Experimental results are presented to illustrate how the two memories complement each other by improving recognition, prediction, and context-based generalization of individual memories.",,,,"Department of Automatics and Biomedical Engineering, AGH University of Science and Technology, Krakow, Poland",A. Horzyk; J. A. Starzyk; J. Graham,,,,10.1109/TNNLS.2017.2728203,IEEE Xplore,1,,20171116,3095,National Science Centre of Poland; ,Biological system modeling;Computational modeling;Learning systems;Neurons;Semantics;Training,cognitive systems;generalisation (artificial intelligence);learning (artificial intelligence),SM operations;cognitive system development;computational models;concept learning;context-based generalization;episodic memories;integrated declarative memories;reinforcement learning;semantic memories;semantic memory models,,2162-237X;2162237X,12,Dec. 2017,,IEEE Transactions on Neural Networks and Learning Systems,Cognitive system;episodic memory (EM);event significance;motivated and reinforcement learning;semantic memory (SM),,,,,20170811,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8008846,,,,,,IEEE,,,,,IEEE Transactions on Neural Networks and Learning Systems,3084,IEEE Journals & Magazines,,,28,,2017,,,,,,,,,,,,,,,,,,,
Smart Cable-Driven Camera Robotic Assistant,"This paper describes the mechanical design and a cognition system for a novel concept-of-camera robotic assistant. The system combines the advantages of intra-abdominal devices and autonomous camera navigation. The robotic assistant is composed of a magnetic intra-abdominal camera robot with two internal cable-driven degrees of freedom and an external robot that handles an external magnet. The intelligence of the robot is implemented in a cognitive architecture based on a long-term memory that stores the robot's knowledge and learning capabilities to improve the robot's behavior. The navigation strategy combines a reactive control based on instrument tracking and a proactive control based on predefined behaviors, depending on the actual state of the task. The robot's learning capabilities include a semantic customization, to adapt the camera's behavior to the surgeon's preferences, and reinforcement learning, to improve the camera navigation strategy. This paper details both the hardware implementation of the system and the software implementation in a robotic operating system architecture. The cognition system and the performance of the cable-driven mechanism have been validated with a set of in vitro experiments. Moreover, the camera robot has been evaluated through an in-vivo experiment in a pig.",,,,"Department of System Engineering and Automation, University of Malaga, Andaluc&#x00ED;a Tech, M&#x00E1;laga, Spain",I. Rivas-Blanco; C. López-Casado; C. J. Pérez-del-Pulgar; F. García-Vacas; J. C. Fraile; V. F. Muñoz,,,,10.1109/THMS.2017.2767286,IEEE Xplore,1,,20180313,196,Spanish national projects; ,Cameras;Navigation;Robot vision systems;Surgery;Tools,cameras;control engineering computing;learning (artificial intelligence);medical computing;medical robotics;robot programming;robot vision;surgery,autonomous camera navigation;camera navigation strategy;cognition system;cognitive architecture;concept-of-camera robotic assistant;external magnet;external robot;intra-abdominal devices;learning capabilities;long-term memory;magnetic intra-abdominal camera robot;mechanical design;reinforcement learning;robotic operating system architecture;smart cable,,2168-2291;21682291,2,April 2018,,IEEE Transactions on Human-Machine Systems,Cognitive robotics;mechatronics;medical robotics;robot control;robot motion,,,,,20171114,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8107576,,,,,,IEEE,,,,,IEEE Transactions on Human-Machine Systems,183,IEEE Journals & Magazines,,,48,,2018,,,,,,,,,,,,,,,,,,,
Constructing evidence-based treatment strategies using methods from computer science,"This paper details a new methodology, instance-based reinforcement learning, for constructing adaptive treatment strategies from randomized trials. Adaptive treatment strategies are operationalized clinical guidelines which recommend the next best treatment for an individual based on his/her personal characteristics and response to earlier treatments. The instance-based reinforcement learning methodology comes from the computer science literature, where it was developed to optimize sequences of actions in an evolving, time varying system. When applied in the context of treatment design, this method provides the means to evaluate both the therapeutic and diagnostic effects of treatments in constructing an adaptive treatment strategy. The methodology is illustrated with data from the STAR*D trial, a multi-step randomized study of treatment alternatives for individuals with treatment-resistant major depressive disorder. © 2007 Elsevier Ireland Ltd. All rights reserved.",,"McGill University, School of Computer Science, 3480 University St., Montreal, Que. H3A 2A7, Canada; University of Texas, Southwestern Medical Center, 323 Harry Hines Blvd, Dallas, TX 75390, United States; University of Michigan, Institute for Social Research, 426 Thompson St, Ann Arbor, MI 48106, United States",,,"Pineau J., Bellemare M.G., Rush A.J., Ghizaru A., Murphy S.A.","Pineau, J., McGill University, School of Computer Science, 3480 University St., Montreal, Que. H3A 2A7, Canada; Bellemare, M.G., McGill University, School of Computer Science, 3480 University St., Montreal, Que. H3A 2A7, Canada; Rush, A.J., University of Texas, Southwestern Medical Center, 323 Harry Hines Blvd, Dallas, TX 75390, United States; Ghizaru, A., McGill University, School of Computer Science, 3480 University St., Montreal, Que. H3A 2A7, Canada; Murphy, S.A., University of Michigan, Institute for Social Research, 426 Thompson St, Ann Arbor, MI 48106, United States",25,,10.1016/j.drugalcdep.2007.01.005,SCOPUS,1,,,,,,,,,,SUPPL. 2,,,Drug and Alcohol Dependence,Clinical decision-making; Learning; Methodology; Sequential decisions; Treatment,,2-s2.0-34047273906,,,,,,S60,S52,,,,,Scopus,,,Drug and Alcohol Dependence,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047273906&doi=10.1016%2fj.drugalcdep.2007.01.005&partnerID=40&md5=636602c8d15aa0f10a7e2b9c0a82f70d,,88,,2007,,,,,,,,,,,,,,,,,,,
A learning model for personalized adaptive cruise control,"This paper develops a learning model for personalized adaptive cruise control that can learn from human demonstration online and mimic a human driver's driving strategies in the dynamic traffic environment. Under the framework of the proposed model, reinforcement learning is used to capture the human-desired driving strategy, and the proportion-integration-differentiation controller is adopted to convert the learning strategy to low-level control commands. The performance of the learning model is tested in the simulation environment built in a driving simulator using PreScan. Experimental results show that the learning model can duplicate human driving strategies with acceptable errors. Moreover, compared with the traditional adaptive cruise control, the proposed model can provide better driving comfort and smoothness in the dynamic situation.",,,,"Intelligent Vehicle Research Center, Beijing Institute of Technology, Beijing 100081 China",X. Chen; Y. Zhai; C. Lu; J. Gong; G. Wang,,,,10.1109/IVS.2017.7995748,IEEE Xplore,1,,20170731,384,,Acceleration;Adaptation models;Cruise control;Data models;Learning (artificial intelligence);Vehicle dynamics;Vehicles,adaptive control;intelligent transportation systems;learning (artificial intelligence);three-term control,PreScan;driving simulator;human demonstration;human driver;human-desired driving strategy;intelligent driving systems;learning model;personalized adaptive cruise control;proportion-integration-differentiation controller;reinforcement learning,,,,11-14 June 2017,,2017 IEEE Intelligent Vehicles Symposium (IV),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7995748,,,,,,IEEE,,,Electronic:978-1-5090-4804-5; POD:978-1-5090-4805-2; USB:978-1-5090-4803-8,,2017 IEEE Intelligent Vehicles Symposium (IV),379,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
Learning motor skills with non-rigid materials by reinforcement learning,"This paper focuses on learning motor skills for anthropomorphic robots which must interact with non-rigid materials to perform tasks, such as wearing clothes, turning socks inside out, and bandaging. To learn such a motor skill, the task to be performed needs to be quantitatively defined using not only the state of the robot, but also the state of the non-rigid material. However, the non-rigid material is generally represented in a high dimensional configuration space (e.g., [1]) and obtaining such information in a real environment is difficult. In this paper we propose a novel learning framework for learning motor skills interacting with non-rigid materials by reinforcement learning that avoids these difficulties. Our learning framework focuses on the topological relationship between the configuration of the robot and the non-rigid material based on the consideration that most details of the material (e.g., wrinkles) are not important for performing the motor tasks. This focus allows us to define the task performance and provide reward signals based on a low-dimensional variable and to measure task performance in a real environment using reliable sensors. We constructed an experimental setting with an anthropomorphic dual-arm robot and a tailor-made T-shirt for the robot. To demonstrate the feasibility of the proposed method, we applied the method to have the robot perform the motor task of putting on the T-shirt. As a result of our learning framework, through trial and error the robot was able to acquire sequential movements that performed the goal of putting both arms into the corresponding sleeves of the T-shirt.",,,,"Graduate School of Information Science, Nara Institute of Science and Technology, Japan",D. Shinohara; T. Matsubara; M. Kidode,,2,,10.1109/ROBIO.2011.6181709,IEEE Xplore,1,,20120412,2681,,Joints;Manipulators;Materials;Robot kinematics;Topology;Trajectory,humanoid robots;learning (artificial intelligence);motion control,anthropomorphic robot;dual-arm robot;high dimensional configuration space;low-dimensional variable;motor skill;nonrigid material;reinforcement learning;sequential movement;tailor-made T-shirt;topological relationship,,,,7-11 Dec. 2011,,2011 IEEE International Conference on Robotics and Biomimetics,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6181709,,,,,,IEEE,18,,DVD:978-1-4577-2137-3; Electronic:978-1-4577-2138-0; POD:978-1-4577-2136-6,,2011 IEEE International Conference on Robotics and Biomimetics,2676,IEEE Conferences,,,,,2011,,,,,,,,,,,,,,,,,,,
Efficient Dynamic Pinning of Parallelized Applications by Distributed Reinforcement Learning,"This paper introduces a resource allocation framework specifically tailored for addressing the problem of dynamic placement (or pinning) of parallelized applications to processing units. Under the proposed setup each thread of the parallelized application constitutes an independent decision maker (or agent), which (based on its own prior performance measurements and its own prior CPU-affinities) decides on which processing unit to run next. Decisions are updated recursively for each thread by a resource manager/scheduler which runs in parallel to the application’s threads and periodically records their performances and assigns to them new CPU affinities. For updating the CPU-affinities, the scheduler uses a distributed reinforcement-learning algorithm, each branch of which is responsible for assigning a new placement strategy to each thread. The proposed framework is flexible enough to address alternative optimization criteria, such as maximum average processing speed and minimum speed variance among threads. We demonstrate analytically that convergence to locally-optimal placements is achieved asymptotically. Finally, we validate these results through experiments in Linux platforms. © 2017 Springer Science+Business Media, LLC, part of Springer Nature",,"Software Competence Center Hagenberg GmbH, Softwarepark 21, Hagenberg, Austria",,,"Chasparis G.C., Rossbory M.","Chasparis, G.C., Software Competence Center Hagenberg GmbH, Softwarepark 21, Hagenberg, Austria; Rossbory, M., Software Competence Center Hagenberg GmbH, Softwarepark 21, Hagenberg, Austria",,,10.1007/s10766-017-0541-y,SCOPUS,1,,,,,,,,,,,,,International Journal of Parallel Programming,Dynamic pinning; Parallel applications; Reinforcement learning,,2-s2.0-85035118833,,,,,,15,1,,,,,Scopus,,,International Journal of Parallel Programming,,Article in Press,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035118833&doi=10.1007%2fs10766-017-0541-y&partnerID=40&md5=37f483cb6f54fd0fcddba092af0a2eec,,,,2017,,,,,,,,,,,,,,,,,,,
Self-optimization of capacity and coverage in LTE networks using a fuzzy reinforcement learning approach,This paper introduces a solution to enable self-optimization of coverage and capacity in LTE networks through base stations' downtilt angle adjustment. The proposed method is based on fuzzy reinforcement learning techniques and operates in a fully distributed and autonomous fashion without any need for a priori information or human interventions. The solution is shown to be capable of handling extremely noisy feedback information from mobile users as well as being responsive to the changes in the environment including self-healing properties. The simulation results confirm the convergence of the solution to the global optimal settings and that the proposed scheme provides up to 20% performance improvement when compared with an existing fuzzy logic based reinforcement learning approach.,,,,"Bell Laboratories, Alcatel-Lucent, Blanchardtown Industrial Park, Dublin 15, Republic of Ireland",R. Razavi; S. Klein; H. Claussen,,30,,10.1109/PIMRC.2010.5671622,IEEE Xplore,1,,20101217,1870,,Fuzzy logic;Geophysical measurement techniques;Ground penetrating radar;Learning;Measurement;Mobile communication;Optimization,Long Term Evolution;feedback;fuzzy logic;learning (artificial intelligence);optimisation;telecommunication computing,LTE networks;base station;fuzzy logic;fuzzy reinforcement learning;mobile users;noisy feedback information;self-healing;self-optimization,,2166-9570;21669570,,26-30 Sept. 2010,,"21st Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications",Downtilt Adjustment;Fuzzy Logic;LTE;Reinforcement Learning;Self-x Networks;component,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5671622,,,,,,IEEE,14,,Electronic:978-1-4244-8016-6; POD:978-1-4244-8017-3,,"21st Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications",1865,IEEE Conferences,,,,,2010,,,,,,,,,,,,,,,,,,,
Self-configuring Switched Multi-Element Antenna system for interference mitigation in femtocell networks,"This paper introduces a Switched Multi-Element Antenna (SMEA) solution for interference mitigation in femtocell networks. While the main objective is to protect the femtocell users against uplink interference, the downlink interference from femtocell base stations to the other users is simultaneously reduced as a by-product of this technique. A tailored form of reinforcement learning is used to allow for self-configuration of the femtocell base station and to adaptively select the optimal antenna configuration in a time varying environment. Compared to the traditional Omni-directional antenna systems, the results show an average of 2.5dB gain in uplink direction in terms of reduced transmission power and approximately 1dB of gain in the downlink channel.",,,,"Bell Laboratories, Alcatel-Lucent, Blanchardtown Industrial Park, Dublin 15, Republic of Ireland",R. Razavi; H. Claussen,,3,,10.1109/PIMRC.2011.6139947,IEEE Xplore,1,,20120126,242,,Antenna measurements;Base stations;Interference;Macrocell networks;Signal to noise ratio;Transmitting antennas,antenna arrays;femtocellular radio;radiofrequency interference,SMEA;femtocell networks;interference mitigation;omnidirectional antenna systems;reinforcement learning;self-configuring switched multielement antenna system;transmission power reduction,,2166-9570;21669570,,11-14 Sept. 2011,,"2011 IEEE 22nd International Symposium on Personal, Indoor and Mobile Radio Communications",Femtocell networks;Interference management;Multi-element antenna;Q-learning;Reinforcement learning;Self-configuration;WCDMA Femtocells,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139947,,,,,,IEEE,19,,Electronic:978-1-4577-1348-4; POD:978-1-4577-1346-0; USB:978-1-4577-1347-7,,"2011 IEEE 22nd International Symposium on Personal, Indoor and Mobile Radio Communications",237,IEEE Conferences,,,,,2011,,,,,,,,,,,,,,,,,,,
A Comparative Study of Parallel Reinforcement Learning Methods with a PC Cluster System,"This paper presents a comparative study of three parallel implementation models for reinforcement learning. Two of them utilize Q-learning, and the other one utilizes fuzzy Q-learning for agent learning. In order to evaluate performance and validity of the three method, a PC (personal computer) cluster system consisting of 16 PCs connected via Gigabit ethernet has been built. For communications to deliver data among PCs, MPI (Message Passing Interface) is employed. Experimental results are compared with one another to show the performance and characteristics of the three methods.",,,,"Hiroshima City University, Japan",M. Kushida; K. Takahashi; H. Ueda; T. Miyahara,,2,,10.1109/IAT.2006.3,IEEE Xplore,1,,20070108,419,,Computer simulation;Ethernet networks;Learning systems;Master-slave;Message passing;Personal communication networks;Process control,fuzzy set theory;learning (artificial intelligence);message passing;parallel algorithms;workstation clusters,PC cluster system;agent learning;fuzzy Q-learning;gigabit Ethernet;message passing interface;parallel implementation model;parallel reinforcement learning,,,,18-22 Dec. 2006,,2006 IEEE/WIC/ACM International Conference on Intelligent Agent Technology,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4052954,,,,,,IEEE,7,,POD:0-7695-2748-5,,2006 IEEE/WIC/ACM International Conference on Intelligent Agent Technology,416,IEEE Conferences,,,,,2006,,,,,,,,,,,,,,,,,,,
A digital circuit design of state-space recurrent neural networks,"This paper presents a digital circuit design of a state-space recurrent neural network (RNN). The proposed digital circuit design separates the datapath of the state-space RNN into a linear subcircuit and a nonlinear subcircuit. The linear subcircuit is realized by a matrix-vector multiplier while the nonlinear subcircuit by a customized nonlinear function computing unit. The throughput rate of the proposed RNN circuit is 36060.5 times faster than that of the software simulation using MATLAB®. The proposed state-space RNN digital design methodology not only possesses the advantages including high computing speed, small area and portability, but also increases the possibility of using the digital RNN circuit in real-world dynamic problems. © 2008 IEEE.",,"Department of Electrical Engineering, National Cheng Kung University, Tainan 701, Taiwan",4811556,,"Lin C.-W., Wang J.-S.","Lin, C.-W., Department of Electrical Engineering, National Cheng Kung University, Tainan 701, Taiwan; Wang, J.-S., Department of Electrical Engineering, National Cheng Kung University, Tainan 701, Taiwan",,,10.1109/ICSMC.2008.4811556,SCOPUS,1,,,,,,,,,,,,,"Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics",,,2-s2.0-69949122521,,,,,,1842,1838,,,,,Scopus,,,"Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69949122521&doi=10.1109%2fICSMC.2008.4811556&partnerID=40&md5=c2b5b56323768f5e7dc643fba041608f,,,,2008,,,,,,,,,,,,,,,,,,,
Investigation of Q-learning applied to DVFS management of a System-on-Chip,"This paper presents a new Q-learning based strategy to manage Dynamic Voltage Frequency Scaling (DVFS) on a system on chip (SoC) such that the energy consumption is reduced. We address software applications with throughput constraints. The proposed Q-learning formulation has two main advantages: it has a reduced state space to limit the overhead and it embeds a new reward function tailored for throughput-constrained applications. The DVFS manager is evaluated on a test chip executing an HMAX object recognition application. We perform an experimental investigation of the main Q-learning parameters. The results suggest that the proposed method reduces the energy consumed with up to 44% at the cost of occasionally increasing the number of throughput violations, when compared to two state-of-the-art feedback controllers that address the same application domain. © 2016",,"Univ. Grenoble Alpes, F-38000 Grenoble, France, CEA, LETI, MINATEC Campus, Grenoble, France",,,"Molnos A., Lesecq S., Mottin J., Puschini D.","Molnos, A., Univ. Grenoble Alpes, F-38000 Grenoble, France, CEA, LETI, MINATEC Campus, Grenoble, France; Lesecq, S., Univ. Grenoble Alpes, F-38000 Grenoble, France, CEA, LETI, MINATEC Campus, Grenoble, France; Mottin, J., Univ. Grenoble Alpes, F-38000 Grenoble, France, CEA, LETI, MINATEC Campus, Grenoble, France; Puschini, D., Univ. Grenoble Alpes, F-38000 Grenoble, France, CEA, LETI, MINATEC Campus, Grenoble, France",1,,10.1016/j.ifacol.2016.07.126,SCOPUS,1,,,,,,,,,,5,,,IFAC-PapersOnLine,DVFS; Reinforcement learning; system-of-chip energy management,,2-s2.0-84991111239,,,,,,284,278,,,,,Scopus,,,IFAC-PapersOnLine,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991111239&doi=10.1016%2fj.ifacol.2016.07.126&partnerID=40&md5=56c52e02deb63ed6679f3dc7913f9969,,49,,2016,,,,,,,,,,,,,,,,,,,
Angle of arrival estimation in dynamic indoor THz channels with Bayesian filter and reinforcement learning,"This paper presents a novel algorithm to estimate the Angle of Arrival (AoA) in a dynamic indoor Terahertz channel. In a realistic application, the user equipment is often moved by the user during the data transmission and the AoA must be estimated periodically, such that the adaptive directional antenna can be adjusted to realize a high antenna gain. The Bayesian filter is applied to exploit continuity and smoothness of the channel dynamics for the AoA estimation. Reinforcement learning is introduced to adapt the prior transition probabilities between system states, in order to fit the variation of application scenarios and personal habits. The algorithm is validated using the ray launching channel simulator and realistic human movement models.",,,,"Technische Universit&#x00E4;t Braunschweig Schleinitzstra&#x00DF;e 22, 38106 Braunschweig, Germany",B. Peng; Q. Jiao; T. Kürner,,1,,10.1109/EUSIPCO.2016.7760594,IEEE Xplore,1,,20161201,1979,,Azimuth;Bayes methods;Directive antennas;Estimation;Gain;Learning (artificial intelligence),belief networks;directive antennas;filtering theory;learning (artificial intelligence);probability,AoA estimation;Bayesian filter;adaptive directional antenna;angle of arrival estimation;dynamic indoor THz channels;high antenna gain;prior transition probabilities;ray launching channel simulator;reinforcement learning,,,,Aug. 29 2016-Sept. 2 2016,,2016 24th European Signal Processing Conference (EUSIPCO),Bayesian filter;Terahertz communication;angle of arrival estimation;dynamic channel;reinforcement learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7760594,,,,,,IEEE,,,Electronic:978-0-9928-6265-7; POD:978-1-5090-1891-8,,2016 24th European Signal Processing Conference (EUSIPCO),1975,IEEE Conferences,,,,,2016,,,,,,,,,,,,,,,,,,,
Parallel distributed profit sharing for PC cluster,"This paper presents a parallel reinforcement learning method considered communication cost. In our method, each agent communicates only action sequences with a constant episode interval. As the communication interval is longer, communication cost is smaller, but parallelism is lower. Implementing our method on PC cluster, we investigate such trade-off characteristics. We show that computation time to learning can be reduced by properly adjusting the communication interval. © Springer-Verlag Berlin Heidelberg 2006.",,"Musashi Institute of Technology, 1-28-1 Tamazutsumi, Setagayaku, Tokyo, 158-8557, Japan",,,"Fujishiro T., Nakano H., Miyauchi A.","Fujishiro, T., Musashi Institute of Technology, 1-28-1 Tamazutsumi, Setagayaku, Tokyo, 158-8557, Japan; Nakano, H., Musashi Institute of Technology, 1-28-1 Tamazutsumi, Setagayaku, Tokyo, 158-8557, Japan; Miyauchi, A., Musashi Institute of Technology, 1-28-1 Tamazutsumi, Setagayaku, Tokyo, 158-8557, Japan",1,,,SCOPUS,0,,,,,,,,,,,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,,2-s2.0-33749828133,,,,,,819,811,,,,,Scopus,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749828133&partnerID=40&md5=ffbbc47e327e70287984d490431bc14d,,4131 LNCS - I,,2006,,,,,,,,,,,,,,,,,,,
A reinforcement learning approach for individualizing erythropoietin dosages in hemodialysis patients,"This paper presents a reinforcement learning (RL) approach for anemia management in patients undergoing chronic renal failure. Erythropoietin (EPO) is the treatment of choice for this kind of anemia but it is an expensive drug and with some dangerous side-effects that should be considered especially for patients who do not respond to the treatment. Therefore, an individualized treatment appears to be necessary. RL is a suitable approach to tackle this problem. Moreover, resulting policies are similar to medical protocols, and hence, they can easily be transferred to daily practice. A cohort of 64 patients are included in the study. An implementation of the Q-learning algorithm based on a state-aggregation table and another implementation using the multi-layer perceptron as a function approximator (Q-MLP) are compared with the protocols followed in the Nephrology Unit. The policy obtained by the Q-MLP approach outperforms the hospital policy in terms of the ratio of patients that are within the targeted range of hemoglobin (11.5-12.5 g/dl) at the end of the analyzed period, since an increase of 25% is observed. It ensures an improvement in patients' quality-of-life and considerable economic savings for the health care system due to both the expensiveness of EPO treatment and the costs incurred by the health care system in order to alleviate problems related to EPO over-dosing. It should be pointed out that the approach presented here is completely general, and therefore, it can be applied to any problem of drug dosage optimization. © 2009 Elsevier Ltd. All rights reserved.",,"Department of Electronic Engineering, University of Valencia, CL. Dr. Moliner, 50, 46100 Burjassot, Valencia, Spain; Istituto Dalle Molle di Studi sull'Intelligenza Artificiale (IDSIA), Lugano, Switzerland; Pharmacy Unit, University Hospital, Dr. Peset, Valencia, Spain; Pharmacy and Pharmaceutical Technology Department, University of Valencia, Spain",,,"Martín-Guerrero J.D., Gomez F., Soria-Olivas E., Schmidhuber J., Climente-Martí M., Jiménez-Torres N.V.","Martín-Guerrero, J.D., Department of Electronic Engineering, University of Valencia, CL. Dr. Moliner, 50, 46100 Burjassot, Valencia, Spain; Gomez, F., Istituto Dalle Molle di Studi sull'Intelligenza Artificiale (IDSIA), Lugano, Switzerland; Soria-Olivas, E., Department of Electronic Engineering, University of Valencia, CL. Dr. Moliner, 50, 46100 Burjassot, Valencia, Spain; Schmidhuber, J., Istituto Dalle Molle di Studi sull'Intelligenza Artificiale (IDSIA), Lugano, Switzerland; Climente-Martí, M., Pharmacy Unit, University Hospital, Dr. Peset, Valencia, Spain; Jiménez-Torres, N.V., Pharmacy Unit, University Hospital, Dr. Peset, Valencia, Spain, Pharmacy and Pharmaceutical Technology Department, University of Valencia, Spain",17,,10.1016/j.eswa.2009.02.041,SCOPUS,1,,,,,,,,,,6,,,Expert Systems with Applications,Anemia; Chronic renal failure; Clinical pharmacokinetics; Erythropoietin; Reinforcement learning,,2-s2.0-64049101720,,,,,,9742,9737,,,,,Scopus,,,Expert Systems with Applications,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-64049101720&doi=10.1016%2fj.eswa.2009.02.041&partnerID=40&md5=3f10a7edf6be7a31b5baf5d6be7218ef,,36,,2009,,,,,,,,,,,,,,,,,,,
Perturbed learning automata in potential games,"This paper presents a reinforcement learning algorithm and provides conditions for global convergence to Nash equilibria. For several reinforcement learning schemes, including the ones proposed here, excluding convergence to action profiles which are not Nash equilibria may not be trivial, unless the step-size sequence is appropriately tailored to the specifics of the game. In this paper, we sidestep these issues by introducing a new class of reinforcement learning schemes where the strategy of each agent is perturbed by a state-dependent perturbation function. Contrary to prior work on equilibrium selection in games, where perturbation functions are globally state dependent, the perturbation function here is assumed to be local, i.e., it only depends on the strategy of each agent. We provide conditions under which the strategies of the agents will converge to an arbitrarily small neighborhood of the set of Nash equilibria almost surely. We further specialize the results to a class of potential games.",,,,"Department of Automatic Control, Lund University, 221 00-SE, Sweden",G. C. Chasparis; J. S. Shamma; A. Rantzer,,5,,10.1109/CDC.2011.6161294,IEEE Xplore,1,,20120301,2458,,Convergence;Games;Learning;Learning systems;Nash equilibrium;Sensitivity;Vectors,functions;game theory;learning (artificial intelligence);learning automata;perturbation techniques,Nash equilibria;equilibrium selection;global convergence;perturbed learning automata;potential games;reinforcement learning scheme;state-dependent perturbation function;step-size sequence,,0191-2216;01912216,,12-15 Dec. 2011,,2011 50th IEEE Conference on Decision and Control and European Control Conference,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6161294,,,,,,IEEE,16,,Electronic:978-1-61284-801-3; POD:978-1-61284-800-6; USB:978-1-61284-799-3,,2011 50th IEEE Conference on Decision and Control and European Control Conference,2453,IEEE Conferences,,,,,2011,,,,,,,,,,,,,,,,,,,
An Advanced Software Tool to Simulate Service Restoration Problems: A case study on Power Distribution Systems,"This paper presents a software tool to simulate a practical problem in smart grid systems. A feature of the smart grid is a system self-recovery capability in the occurrence of anomalies, such as a recovery of a power distribution network after an occurrence of a fault. When this system has a capacity for self-recovery, it is called self-healing. The intersection among areas as computer science, telecommunication, automation and electrical engineering, has allowed power systems to gain new technologies. However, because it is a multi-area domain, self-recovery simulation tools in smart grids are often highly complex as well as presenting low fidelity by using approximation algorithms. The main contribution of this paper is a simulator with high fidelity and low complexity in terms of programming, usability and semantics. In this simulator, a computational intelligence technique and a derivative method for calculating the power flow were encapsulated. The result is a software tool with high abstraction and easy customization, aimed at a self-healing system for a reconfiguration of an electric power distribution network. © 2017 The Authors. Published by Elsevier B.V.",Open Access,"Federal University of Paraná, Curitiba, Parana, Brazil; Department of Informatics, Federal University of Technology of Paraná, Pato Branco, Brazil; Federal University of Technology of Paraná, Ponta Grossa, Brazil; Pontifical Catholic University of Paraná, Curitiba, Brazil",,,"Ribeiro R., Enembreck F., Guisi D.M., Casanova D., Teixeira M., De Souza F.A., Borges A.P.","Ribeiro, R., Federal University of Paraná, Curitiba, Parana, Brazil; Enembreck, F., Pontifical Catholic University of Paraná, Curitiba, Brazil; Guisi, D.M., Department of Informatics, Federal University of Technology of Paraná, Pato Branco, Brazil; Casanova, D., Department of Informatics, Federal University of Technology of Paraná, Pato Branco, Brazil; Teixeira, M., Department of Informatics, Federal University of Technology of Paraná, Pato Branco, Brazil; De Souza, F.A., Federal University of Paraná, Curitiba, Parana, Brazil; Borges, A.P., Federal University of Technology of Paraná, Ponta Grossa, Brazil",1,,10.1016/j.procs.2017.05.248,SCOPUS,1,,,,,,,,,,,,,Procedia Computer Science,Reinforcement Learning; Restoration Problem; Self-Healing Smart Grid; Software Tool,,2-s2.0-85027365477,,,,,,684,675,,,,,Scopus,,,Procedia Computer Science,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027365477&doi=10.1016%2fj.procs.2017.05.248&partnerID=40&md5=d2adc95a68ffa6a01f2a02e19646d737,,108,,2017,,,,,,,,,,,,,,,,,,,
Performance improvements in schools with Adaptive Learning and Assessment,"This paper presents Amrita Learning, a web-based, multimedia-enabled, Adaptive Assessment and Learning System for schools. Computer-based adaptive assessments aim to use an optimal and individualized assessment path to determine the knowledge level of students. The new goal for adaptive assessment is based on educational outcomes, which describe what learners must be able to do as a result of items studied. Assessment based on outcomes creates the initial roadmap for the educational model, ensuring that students are not learning items that are already mastered. Learners and instructors can accurately determine their areas of strengths and weaknesses, and use this to determine future instruction. This paper explains the underlying principles used in the initial adaptive assessment followed by evaluation that is closely interwoven with learning. An expert module continuously adjusts the content and method of presentation based on the sequence of learner's recent responses and prior knowledge. The system maintains and updates both the individual learner profile and group profiles. Amrita Learning, targeted to school students, is built upon the principles of spiral learning with mixed presentation from multiple skill areas, thus providing continuous reinforcement in all skill-areas. The proposed competency model has been pilot tested in both city and rural area schools. In the majority of cases where students used it consistently, there were quantifiable improvements in learning levels and performance in schools. Summaries of the results and recommendations are included in this paper.",,,,"School of Engineering, Amrita Vishwa Vidyapeetham, India",R. Raman; P. Nedungadi,,1,,10.1109/ICDLE.2010.5606052,IEEE Xplore,1,,20101021,14,,Adaptation model;Browsers;Equations;Fires;Geometry;Mathematical model;XML,Internet;computer aided instruction;educational institutions;learning (artificial intelligence);multimedia systems,Amrita learning;Schools;Web based multimedia enabled adaptive assessment;adaptive learning;city area school;learner response;performance improvement;rural area school;spiral learning;student knowledge level,,2169-1428;21691428,,3-5 Oct. 2010,,2010 4th International Conference on Distance Learning and Education,Adaptive Learning;Assessment;Continuous Evaluation;Flash Animations;ICT;Intelligent Tutoring Systems Spiral Learning;Interactive;Mastery Learning;Mixed Presentation;individualised instruction;special needs,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5606052,,,,,,IEEE,10,,Electronic:978-1-4244-8752-3; POD:978-1-4244-8751-6,,2010 4th International Conference on Distance Learning and Education,10,IEEE Conferences,,,,,2010,,,,,,,,,,,,,,,,,,,
Research on intelligent home network model based on multi-level agents,"This paper presents an intelligent home network model based on multi-level agents to solve the problem that the existing intelligent devices in home networks can not study users' habits efficiently so that they can not satisfy users' personalized QoS. A formal description for devices in intelligent home networks is given before the model is established. The overall agent in an intelligent home network summarizes the rules of service through the study of the long-term data of the family, which guide the work of equipment agents according to the life habits of the family. The equipment agents learn and detect the change of the family environments automatically and make the optimal choices using the reinforcement learning algorithms. An application scene of the multi-level agent model and the simulation result show that, by applying this model, the devices can study the habits of the user and provide personalized service to the user.",,"Graduate University of Chinese Academy of Sciences, Beijing 100049, China; Institute of Acoustics, Chinese Acad. of Sci., Beijing 100190, China",,,"Yao G., Zhang W., Wang J.","Yao, G., Graduate University of Chinese Academy of Sciences, Beijing 100049, China, Institute of Acoustics, Chinese Acad. of Sci., Beijing 100190, China; Zhang, W., Institute of Acoustics, Chinese Acad. of Sci., Beijing 100190, China; Wang, J., Institute of Acoustics, Chinese Acad. of Sci., Beijing 100190, China",,,10.3772/j.issn.1002-0470.2009.09.008,SCOPUS,1,,,,,,,,,,9,,,Gaojishu Tongxin/Chinese High Technology Letters,Intelligent home network; Multi-level agents; Reinforcement learning,,2-s2.0-70350540861,,,,,,925,919,,,,,Scopus,,,Gaojishu Tongxin/Chinese High Technology Letters,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350540861&doi=10.3772%2fj.issn.1002-0470.2009.09.008&partnerID=40&md5=297fa71683a5373e29a2b4d2b92e7aa1,,19,,2009,,,,,,,,,,,,,,,,,,,
Forecasting sunspot numbers with Recurrent Neural Networks (RNN) using 'sunspot neural forecaster' system,"This paper presents the investigations of forecasting performance of different type of Recurrent Neural Networks (RNN) in forecasting the sunspot numbers. Recurrent Neural Network will be used in this investigation by using different learning algorithms, sunspot data models and RNN transfer functions. Simulations are done using Matlab 7 where customized Graphic User Interface (GUI) called 'Sunspot Neural Forecaster' have been developed for analysis. A complete analysis for different learning algorithms, sunspot data models and RNN transfer functions are examined in terms of Mean Square Error (MSE) and correlation analysis. Finally, the best optimized RNN parameters will be used to forecast the sunspot numbers. © 2010 IEEE.",,"Faculty of Electrical and Electronics Engineering, Universiti Malaysia Pahang, Kuantan, Pahang, Malaysia; Faculty of Science, Arts and Heritage, Universiti Tun Hussein Onn Malaysia, Batu Pahat, Johor, Malaysia",5675853,,"Samin R.E., Khamis A., Kasmani R.Md., Isa S.","Samin, R.E., Faculty of Electrical and Electronics Engineering, Universiti Malaysia Pahang, Kuantan, Pahang, Malaysia; Khamis, A., Faculty of Science, Arts and Heritage, Universiti Tun Hussein Onn Malaysia, Batu Pahat, Johor, Malaysia; Kasmani, R.Md., Faculty of Science, Arts and Heritage, Universiti Tun Hussein Onn Malaysia, Batu Pahat, Johor, Malaysia; Isa, S., Faculty of Science, Arts and Heritage, Universiti Tun Hussein Onn Malaysia, Batu Pahat, Johor, Malaysia",,,10.1109/ACT.2010.50,SCOPUS,1,,,,,,,,,,,,,"Proceedings - 2010 2nd International Conference on Advances in Computing, Control and Telecommunication Technologies, ACT 2010",Mean square error (mse); Recurrent neural networks (rnn); Sunspot numbers,,2-s2.0-78751638681,,,,,,14,10,,,,,Scopus,,,"Proceedings - 2010 2nd International Conference on Advances in Computing, Control and Telecommunication Technologies, ACT 2010",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751638681&doi=10.1109%2fACT.2010.50&partnerID=40&md5=ce015317eb2780ecdf92030923ec3af2,,,,2010,,,,,,,,,,,,,,,,,,,
Computer-aided process control laboratory systems,"This paper presents the use of personal computers in the process control laboratory at the Department of Chemical Engineering, Universiti Teknologi Malaysia. A description of four control systems interfaced to computers-a flow control system, a level control system, a heated tank control system and a fermenter/batch reactor control system are given. Being versatile controllers and data loggers, the computers also provide a friendly and appealing environment for students to perform experiments on the systems. The different characteristic of each system provides a variety of hands-on experience which incorporates the application of process control and reinforcement of the theories learnt in class. Finally, a sample experiment is included to illustrate the application and experience gained by students",,,,"Dept. of Chem. Eng., Univ. Teknologi Malaysia, Malaysia",K. M. Yusof; T. K. Liong; A. K. Baderon,,0,,10.1109/MMEE.1994.383204,IEEE Xplore,1,,20020806,280,,Application software;Chemical engineering;Computer interfaces;Control systems;Inductors;Laboratories;Level control;Microcomputers;Process control;Temperature control,computer aided instruction;control engineering computing;control engineering education;flow control;level control;process control,computer-aided process control laboratory systems;data loggers;fermenter/batch reactor control system;flow control system;heated tank control system;level control system;versatile controllers,,,,6-8 Jul 1994,,Proceedings IEEE 1st International Conference on Multi Media Engineering Education,,,,06 Jul 1994-08 Jul 1994,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=383204,,,,,,IEEE,,,POD:0-7803-1963-X,,Proceedings IEEE 1st International Conference on Multi Media Engineering Education,276,IEEE Conferences,,,,,1994,,,,,,,,,,,,,,,,,,,
Multi-objective reinforcement learning algorithm and its improved convergency method,"This paper proposes a multi-objective reinforcement learning algorithm (MORLA) and uses simultaneous perturbation stochastic approximation (SPSA) to improve the convergence of it. Usually, reinforcement learning (RL) is used to design neurocontroller for control system with single objective. When facing multi-objective system, it is necessary to design the neurocontroller according to the personal preference. The MORLA can transform the multi-objective into synthetical objective and applies parallel genetic algorithm (PGA) to evolve the neurocontroller according to the synthetical objective. To establish the synthetical objective, the objective weight which represents the personal preference is calculated by solving the constrained optimization problem (COP) at the end of each generation. The COP requires not only the biggest variance of the synthetical objective in the population, but also requires the weight to fit the designer's preference. After acquiring the weights, the PGA can select the elitists from the population according to the designer's preference and design a satisfying neurocontroller by evolutionary operations. In addition, although GA has good global search ability, it descends slowly at local area. This paper applies SPSA algorithm to search optimal solution when GA is vibrating at local area. The SPSA converges fast by efficient gradient approximation that relies on measurements of the objective function. The hybrid algorithm accelerates the learning speed of reinforcement learning. At last, the MORLA is used to design neurocontroller for a speed-controlled induction motor drive with indirect vector control. With different personal preferences for the drive system, the simulation results show the feasibility and validity of the MORLA.",,,,"Department of Control Science and Engineering, Huazhong University of Science and Technology, China",Z. Jin; Z. Huajun,,0,,10.1109/ICIEA.2011.5976002,IEEE Xplore,1,,20110804,2445,,Algorithm design and analysis;Approximation methods;Control systems;Convergence;Genetic algorithms;Learning;Neurocontrollers,genetic algorithms;learning (artificial intelligence);neurocontrollers,constrained optimization problem;indirect vector control;multiobjective reinforcement learning;multiobjective system;neurocontroller;objective function;parallel genetic algorithm;simultaneous perturbation stochastic approximation;speed-controlled induction motor drive,,2156-2318;21562318,,21-23 June 2011,,2011 6th IEEE Conference on Industrial Electronics and Applications,SPSA;multi-objective reinforcement learning;speed-controlled,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976002,,,,,,IEEE,47,,Electronic:978-1-4244-8756-1; POD:978-1-4244-8754-7,,2011 6th IEEE Conference on Industrial Electronics and Applications,2438,IEEE Conferences,,,,,2011,,,,,,,,,,,,,,,,,,,
Learning ontology for personalized video retrieval,"This paper proposes a new method for using implicit user feedback from clickthrough data to provide personalized ranking of results in a video retrieval system. The annotation based search is complemented with a feature based ranking in our approach. The ranking algorithm uses belief revision in a Bayesian Network, which is derived from a multimedia ontology that captures the probabilistic association of a concept with expected video features. We have developed a content model for videos using discrete feature states to enable Bayesian reasoning and to alleviate on-line feature processing overheads. We propose a reinforcement learning algorithm for the parameters of the Bayesian Network with the implicit feedback obtained from the clickthrough data. Copyright 2007 ACM.",,"Innovation Labs, Delhi, TCS Limited, 249 D and E Udyog Vihar Phase 4, Gurgaon 122016, India; Electrical Engineering Deptt., Indian Institute of Technology, Delhi, New Delhi 110016, India",,,"Ghosh H., Poornachander P., Mallik A., Chaudhury S.","Ghosh, H., Innovation Labs, Delhi, TCS Limited, 249 D and E Udyog Vihar Phase 4, Gurgaon 122016, India; Poornachander, P., Electrical Engineering Deptt., Indian Institute of Technology, Delhi, New Delhi 110016, India; Mallik, A., Electrical Engineering Deptt., Indian Institute of Technology, Delhi, New Delhi 110016, India; Chaudhury, S., Electrical Engineering Deptt., Indian Institute of Technology, Delhi, New Delhi 110016, India",5,,10.1145/1290067.1290075,SCOPUS,1,,,,,,,,,,,,,Proceedings of the ACM International Multimedia Conference and Exhibition,Bayesian network; Content modeling; Reinforcement learning; Video retrieval,,2-s2.0-37849030482,,,,,,46,39,,,,,Scopus,,,Proceedings of the ACM International Multimedia Conference and Exhibition,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37849030482&doi=10.1145%2f1290067.1290075&partnerID=40&md5=f20e9386c1b589ba6b1a0af3b6d9de83,,,,2007,,,,,,,,,,,,,,,,,,,
Towards a virtual personal assistant based on a user-defined portfolio of multi-domain vocal applications,"This paper proposes a novel approach to defining and simulating a new generation of virtual personal assistants as multi-application multi-domain distributed dialogue systems. The first contribution is the assistant architecture, composed of independent third-party applications handled by a Dispatcher. In this view, applications are black-boxes responding with a self-scored answer to user requests. Next, the Dispatcher distributes the current request to the most relevant application, based on these scores and the context (history of interaction etc.), and conveys its answer to the user. To address variations in the user-defined portfolio of applications, the second contribution, a stochastic model automates the online optimisation of the Dispatcher's behaviour. To evaluate the learnability of the Dispatcher's policy, several parametrisations of the user and application simulators are enabled, in such a way that they cover variations of realistic situations. Results confirm in all considered configurations of interest, that reinforcement learning can learn adapted strategies.",,,,"Orange Labs, France",T. Ekeinhor-Komi; J. L. Bouraoui; R. Laroche; F. Lefèvre,,,,10.1109/SLT.2016.7846252,IEEE Xplore,1,,20170209,113,,Gold;History;Learning (artificial intelligence);Meteorology;Portfolios;Semantics;Symmetric matrices,computational linguistics;human computer interaction;interactive systems;learning (artificial intelligence);optimisation;stochastic processes,Dispatcher policy learnability evaluation;application simulator parametrisation;black-boxes;multiapplication multidomain distributed dialogue systems;multidomain vocal applications;online optimisation;reinforcement learning;stochastic model;third-party applications;user parametrisation;user-defined portfolio;virtual personal assistant architecture,,,,13-16 Dec. 2016,,2016 IEEE Spoken Language Technology Workshop (SLT),dialogue strategy;multi-application spoken dialogue systems;multi-domain;reinforcement learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846252,,,,,,IEEE,,,Electronic:978-1-5090-4903-5; POD:978-1-5090-4904-2; USB:978-1-5090-4902-8,,2016 IEEE Spoken Language Technology Workshop (SLT),106,IEEE Conferences,,,,,2016,,,,,,,,,,,,,,,,,,,
Towards scalable and privacy preserving commercial content dissemination in social wireless networks,"This paper proposes a Q-learning based Device-to-Device multicast routing framework for Social Wireless Networks. The goal of the proposed Scalable Q-learning based Gain-aware Routing (SQGR) content dissemination algorithm is to maximize a predefined economic gain for commercial content generators. This economic gain is defined as the revenue from delivery of a coupon minus the forwarding cost associated with that delivery. SQGR, with its embedding learning abilities, is expected to be robust in dynamic mobility environments. It also preserves scalability and privacy since it does not require storage of per-individual consuming interest and interaction profiles within the network. Using the DTN simulator software ONE, we evaluate functional validity and compare gain performance of SQGR with few existing protocols under various commercial, network and protocol parameters.",,,,"Electrical and Computer Engineering, Michigan State University, East Lansing, USA",F. Hajiaghajani; S. Biswas,,,,10.1109/PIMRC.2017.8292416,IEEE Xplore,1,,20180215,7,,Biological system modeling;Economics;Peer-to-peer computing;Privacy;Probabilistic logic;Routing;Scalability,data privacy;delay tolerant networks;learning (artificial intelligence);mobile computing;mobile radio;multicast communication;routing protocols,Device-to-Device multicast;Gain-aware Routing content dissemination algorithm;SQGR;Scalable Q-learning;commercial content dissemination;dynamic mobility environments;social wireless networks,,,,8-13 Oct. 2017,,"2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",Economic Gain of Dissemination;Q-Learning;Reinforcement Learning;Social Wireless Networks,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8292416,,,,,,IEEE,,,Electronic:978-1-5386-3531-5; POD:978-1-5386-3532-2; Paper:978-1-5386-3529-2; USB:978-1-5386-3530-8,,"2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",1,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
"Stochastic Optimal Relaxed Automatic Generation Control in Non-Markov Environment Based on Multi-Step <formula formulatype=""inline""> <tex Notation=""TeX"">$Q(lambda)$</tex></formula> Learning","This paper proposes a stochastic optimal relaxed control methodology based on reinforcement learning (RL) for solving the automatic generation control (AGC) under NERC's control performance standards (CPS). The multi-step <i>Q</i>(λ) learning algorithm is introduced to effectively tackle the long time-delay control loop for AGC thermal plants in non-Markov environment. The moving averages of CPS1/ACE are adopted as the state feedback input, and the CPS control and relaxed control objectives are formulated as multi-criteria reward function via linear weighted aggregate method. This optimal AGC strategy provides a customized platform for interactive self-learning rules to maximize the long-run discounted reward. Statistical experiments show that the RL theory based <i>Q</i>(λ) controllers can effectively enhance the robustness and dynamic performance of AGC systems, and reduce the number of pulses and pulse reversals while the CPS compliances are ensured. The novel AGC scheme also provides a convenient way of controlling the degree of CPS compliance and relaxation by online tuning relaxation factors to implement the desirable relaxed control.",,,,"College of Electric Power, South China University of Technology, Guangzhou, China",T. Yu; B. Zhou; K. W. Chan; L. Chen; B. Yang,,28,,10.1109/TPWRS.2010.2102372,IEEE Xplore,1,,20110721,1282,,Aerospace electronics;Frequency control;Markov processes;Power grids;Power system dynamics;Standards,delays;learning (artificial intelligence);optimal control;power generation control;power system stability;robust control;state feedback;statistical analysis;stochastic processes;thermal power stations,AGC thermal plant;CPS compliance;CPS control;NERC control performance standard;RL theory based controller;interactive self learning rule;linear weighted aggregate method;long run discounted reward;multicriteria reward function;multistep Q(λ) learning algorithm;nonMarkov environment;online tuning relaxation factors;optimal AGC strategy;pulse reversal;reinforcement learning;state feedback input;statistical experiment;stochastic optimal relaxed automatic generation control;time delay control loop,,0885-8950;08858950,3,Aug. 2011,,IEEE Transactions on Power Systems,"AGC;CPS;multi-step <formula formulatype=""inline""><tex Notation=""TeX"">$Q(lambda)$</tex> </formula> learning;non-Markov environment;relaxed control;stochastic optimization",,,,,20110204,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5706397,,,,,,IEEE,31,,,,IEEE Transactions on Power Systems,1272,IEEE Journals & Magazines,,,26,,2011,,,,,,,,,,,,,,,,,,,
Towards end-to-end reinforcement learning of dialogue agents for information access,"This paper proposes KB-InfoBot1 - a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. Such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge. Previous systems achieved this by issuing a symbolic query to the KB to retrieve entries based on their attributes. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced ""soft"" posterior distribution over the KB that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users. We also present a fully neural end-to-end agent, trained entirely from user feedback, and discuss its application towards personalized dialogue agents. © 2017 Association for Computational Linguistics.",,"Carnegie Mellon University, Pittsburgh, PA, United States; Microsoft Research, Redmond, WA, United States; National, Taiwan University, Taipei, Taiwan",,,"Dhingra B., Li L., Li X., Gao J., Chen Y.-N., Ahmed F., Deng L.","Dhingra, B., Carnegie Mellon University, Pittsburgh, PA, United States; Li, L., Microsoft Research, Redmond, WA, United States; Li, X., Microsoft Research, Redmond, WA, United States; Gao, J., Microsoft Research, Redmond, WA, United States; Chen, Y.-N., National, Taiwan University, Taipei, Taiwan; Ahmed, F., Microsoft Research, Redmond, WA, United States; Deng, L., Microsoft Research, Redmond, WA, United States",3,,10.18653/v1/P17-1045,SCOPUS,1,,,,,,,,,,,,,"ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)",,,2-s2.0-85030087610,,,,,,495,484,,,,,Scopus,,,"ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030087610&doi=10.18653%2fv1%2fP17-1045&partnerID=40&md5=c571cfd88d6caaedbd803bdfd2feb1df,,1,,2017,,,,,,,,,,,,,,,,,,,
Expert Level Control of Ramp Metering Based on Multi-Task Deep Reinforcement Learning,"This paper shows how the recent breakthroughs in reinforcement learning (RL) that have enabled robots to learn to play arcade video games, walk, or assemble colored bricks, can be used to perform other tasks that are currently at the core of engineering cyberphysical systems. We present the first use of RL for the control of systems modeled by discretized non-linear partial differential equations (PDEs) and devise a novel algorithm to use non-parametric control techniques for large multi-agent systems. Cyberphysical systems (e.g., hydraulic channels, transportation systems, the energy grid, and electromagnetic systems) are commonly modeled by PDEs, which historically have been a reliable way to enable engineering applications in these domains. However, it is known that the control of these PDE models is notoriously difficult. We show how neural network-based RL enables the control of discretized PDEs whose parameters are unknown, random, and time-varying. We introduce an algorithm of mutual weight regularization (MWR), which alleviates the curse of dimensionality of multi-agent control schemes by sharing experience between agents while giving each agent the opportunity to specialize its action policy so as to tailor it to the local parameters of the part of the system it is located in. A discretized PDE, such as the scalar Lighthill-Whitham-Richards PDE can indeed be considered as a macroscopic freeway traffic simulator and which presents the most salient challenges for learning to control large cyberphysical system with multiple agents. We consider two different discretization procedures and show the opportunities offered by applying deep reinforcement for continuous control on both. Using a neural RL PDE controller on a traffic flow simulation based on a Godunov discretization of the San Francisco Bay Bridge, we are able to achieve precise adaptive metering without model calibration thereby beating the state of the art in traffic metering. Furthermore, with the m- re accurate BeATS simulator, we manage to achieve a control performance on par with ALINEA, a state-of-the-art parametric control scheme, and show how using MWR improves the learning procedure.",,,,"Computer Science Deptartment, University of California at Berkeley, Berkeley, CA, USA",F. Belletti; D. Haziza; G. Gomes; A. M. Bayen,,,,10.1109/TITS.2017.2725912,IEEE Xplore,1,,20180328,1207,,Biological system modeling;Control systems;Cyber-physical systems;Learning (artificial intelligence);Mathematical model;Neural networks;Transportation,computer games;digital simulation;learning (artificial intelligence);multi-agent systems;partial differential equations;road traffic;road traffic control;traffic control;traffic engineering computing,Godunov discretization;MWR;PDE models;arcade video games;assemble colored bricks;continuous control;control performance;cyberphysical system;different discretization procedures;discretized PDE;electromagnetic systems;engineering applications;engineering cyberphysical systems;expert level control;hydraulic channels;learning procedure;macroscopic freeway traffic simulator;model calibration;multiagent control schemes;multiagent systems;multiple agents;multitask deep reinforcement learning;mutual weight regularization;neural RL PDE controller;nonlinear partial differential equations;nonparametric control techniques;parametric control scheme;precise adaptive metering;ramp metering;scalar Lighthill-Whitham-Richards PDE;traffic metering;transportation systems,,1524-9050;15249050,4,April 2018,,IEEE Transactions on Intelligent Transportation Systems,Deep learning;continuous control;deep reinforcement learning;macroscopic traffic models;partial differential equations;reinforcement learning;transportation systems,,,,,20170816,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8011495,,,,,,IEEE,,,,,IEEE Transactions on Intelligent Transportation Systems,1198,IEEE Journals & Magazines,,,19,,2018,,,,,,,,,,,,,,,,,,,
Adaptive management of air traffic flow: A multiagent coordination approach,"This paper summarizes recent advances in the application of multiagent coordination algorithms to air traffic flow management. Indeed, air traffic flow management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today. This problem is particularly complex as it requires the integration and/or coordination of many factors including: new data (e.g., changing weather info), potentially conflicting priorities (e.g., different airlines), limited resources (e.g., air traffic controllers) and very heavy traffic volume (e.g., over 40,000 flights over the US airspace). The multiagent approach assigns an agent to a navigational fix (a specific location in 2D space) and uses three separate actions to control the airspace: setting the separation between airplanes, setting ground holds that delay aircraft departures and rerouting aircraft. Agents then use reinforcement learning to learn the best set of actions. Results based on FACET (a commercial simulator) show that agents receiving personalized rewards reduce congestion by up to 80% over agents receiving a global reward and by up to 85% over a current industry approach (Monte Carlo estimation). These results show that with proper selection of agents, their actions and their reward structures, multiagent coordination algorithms can be successfully applied to complex real world domains. Copyright © 2008.",,"Oregon State University, 204 Rogers Hall, Corvallis, OR 97331, United States; UCSC, NASA Ames Research Center, Mailstop 269-3, Moffett Field, CA 94035, United States",,,"Tumer K., Agogino A.","Tumer, K., Oregon State University, 204 Rogers Hall, Corvallis, OR 97331, United States; Agogino, A., UCSC, NASA Ames Research Center, Mailstop 269-3, Moffett Field, CA 94035, United States",4,,,SCOPUS,0,,,,,,,,,,,,,Proceedings of the National Conference on Artificial Intelligence,,,2-s2.0-57749098530,,,,,,1584,1581,,,,,Scopus,,,Proceedings of the National Conference on Artificial Intelligence,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57749098530&partnerID=40&md5=dbd59ebb6c6553c0f9800d71773ffaf3,,3,,2008,,,,,,,,,,,,,,,,,,,
Teaching mentoring program for the application of active methodologies and ICT tools,"This research is characterized by the effectiveness of the Teaching Mentoring Program for the application of active methodologies and ICT tools in Tecsup Norte teachers, Trujillo, Peru. A goal was set for the three locations of Tecsup Norte (Trujillo, City), Tecsup Centro (Lima, City), Tecsup Sur (Arequipa, City), to achieve that in 2016, 80%, apply at least one active methodology per semester and make use of ICT tools. The result for Tecsup Norte after completing the first semester was that 28% of teachers used an active methodology and 16% used ICT tools. At the beginning of the 2016 II semester, the Teaching Mentoring Program was designed, consisting of five stages: 1. first contact and interview with the teacher, 2. personalized training, 3. programming and revision of class material, 4. accompaniment in classroom and evaluation by rubrics, and 5. feedforward and recognition. For this, psychological strategies such as rapport, positive reinforcement and behavioral modeling were adapted, as well as the tool of coaching feedforward and evaluation rubrics were designed for the application of active methodologies. To guarantee the effectiveness of the program was a team of two specialists in active methodologies and ICT tools. Work schedule and control of man hours was approached using Gantt diagram. The results of the implementation of the program are relevant, since 95% of the teachers of the semester 2016 II applied active methodologies and ICT tools; in comparison to the result of the 2016 I semester. Tecsup Norte becomes the first site that exceeds the established goal of 80% in application of active methodologies and ICT tools, through the Teaching Mentoring program, the other venues that did not apply the Teaching Mentoring Program were: Tecsup Centro 62% and Tecsup Sur 50%. This educational innovation contributes a program for the training, accompaniment and evaluation of the teaching performance through active methodologies, such as: Flipped le- rning, case-based learning, problem-based learning, guided-learning project and the use of ICT tools.",,,,"Educational Innovation Deparment, Tecsup, Trujillo, Per&#x00FA;",N. E. Salazar,,,,10.1109/FIE.2017.8190607,IEEE Xplore,1,,20171214,6,,Google;Interviews;Mentoring;Technological innovation;Tools;Urban areas,computer aided instruction;teaching,Arequipa City;Gantt diagram;ICT tools;Lima City;Peru;Teaching Mentoring Program;Teaching Mentoring program;Teaching mentoring program;Tecsup Centro;Tecsup Norte teachers;Tecsup Sur;Trujillo,,,,18-21 Oct. 2017,,2017 IEEE Frontiers in Education Conference (FIE),ICT tools;Mentoring Teaching;active methodologies;evaluation rubrics;feedforward;personalized training,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8190607,,,,,,IEEE,,,Electronic:978-1-5090-5920-1; POD:978-1-5090-4920-2; USB:978-1-5090-5919-5,,2017 IEEE Frontiers in Education Conference (FIE),1,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
Harmful lie aversion and lie discovery in noisy expert advice games,"This study tests whether individuals are reluctant to tell lies, or perhaps only ""harmful lies"", in a previously untested environment: an expert sending a message to a decision maker whose interpretation of that message is subject to error, i.e. a noisy sender-receiver game. In the Aligned treatment, the expert can send a ""white lie"" to the receiver, eliminating the negative effects of noise and improving both parties' payoffs. In the Conflict treatment, lies are harmful and the inability to commit to truthtelling destroys all meaningful communication in equilibrium unless there is a cost of lying. In the experiment, receivers are overly trusting and experts learn to take advantage of this. As experts gain experience they tell stronger and more frequent lies in both treatments, consistent with models of reinforcement learning. The findings suggest that neither harmful nor universal lie aversion is a factor when communication is noisy, provided individuals have time to discover their personal benefits of lies. © 2013 Elsevier B.V.",,"Florida State University, Department of Economics, l 113 Collegiate Loop, Tallahassee, FL 32306, United States",,,Lightle J.P.,"Lightle, J.P., Florida State University, Department of Economics, l 113 Collegiate Loop, Tallahassee, FL 32306, United States",6,,10.1016/j.jebo.2013.04.006,SCOPUS,1,,,,,,,,,,,,,Journal of Economic Behavior and Organization,Cheap talk; Communication; Experts; Lie aversion; Overcommunication,,2-s2.0-84884411295,,,,,,362,347,,,,,Scopus,,,Journal of Economic Behavior and Organization,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884411295&doi=10.1016%2fj.jebo.2013.04.006&partnerID=40&md5=a97f7b89c26a02b4a84445edbf419758,,93,,2013,,,,,,,,,,,,,,,,,,,
A Neural Network Model of Multisensory Representation of Peripersonal Space: Effect of tool use,"This work describes an original neural network to simulate representation of the peripersonal space around one hand, in basal conditions and after training with a tool used to reach the far space. The model is composed of two unimodal areas (visual and tactile) connected to a third bimodal area (visual-tactile). Neurons in the bimodal area integrate visual and tactile information and are activated only when a stimulus falls inside the peripersonal space. Moreover, the model assumes that synapses linking unimodal to bimodal neurons can be reinforced by an Hebbian rule during training, but this reinforcement is also under the influence of attention mechanisms. Results show that the peripersonal space, which includes just a small visual space around the hand in normal conditions, becomes elongated in the direction of the tool after training. This expansion of the peripersonal space depends on an expansion of the visual receptive field of bimodal neurons, due to a reinforcement of visual synapses, which were just latent before training. The model may be of value to analyze the neural mechanisms responsible for representing and plastically shaping peripersonal space, and in perspective, for interpretation of psychophysical data on patients with brain damage.",,,,"Department of Electronics, Computer Science and Systems, University of Bologna, Bologna, Italy",M. Ursino; M. Zavaglia; E. Magosso; A. Serino; G. di Pellegrino,,2,,10.1109/IEMBS.2007.4352894,IEEE Xplore,1,,20071022,2739,,Animals;Biological neural networks;Computer science;Computer simulation;Humans;Neural networks;Neurons;Plastics;Psychology;Radio frequency,Hebbian learning;biology computing;neural nets;neurophysiology;touch (physiological);vision,Hebbian rule;attention mechanism;bimodal neurons;bimodal visual-tactile perception;hand peripersonal space;neural network model;peripersonal space multisensory representation;peripersonal space plastic shaping;synapses;unimodal neurons;unimodal tactile perception;unimodal visual perception;visual receptive field expansion;visual synapse reinforcement,,1094-687X;1094687X,,22-26 Aug. 2007,,2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,,,,,"Animals;Hand;Haplorhini;Humans;Neural Networks (Computer);Neurons, Afferent;Personal Space;Physical Stimulation;Synapses;Tool Use Behavior;Touch;Visual Perception",,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4352894,,,,,,IEEE,19,,CD-ROM:978-1-4244-0788-0; POD:978-1-4244-0787-3,,2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,2735,IEEE Conferences,,,,,2007,,,,,,,,,,,,,,,,,,,
Affective personalization of a social robot tutor for children's second language skills,"Though substantial research has been dedicated towards using technology to improve education, no current methods are as effective as one-on-one tutoring. A critical, though relatively understudied, aspect of effective tutoring is modulating the student's affective state throughout the tutoring session in order to maximize long-term learning gains. We developed an integrated experimental paradigm in which children play a second-language learning game on a tablet, in collaboration with a fully autonomous social robotic learning companion. As part of the system, we measured children's valence and engagement via an automatic facial expression analysis system. These signals were combined into a reward signal that fed into the robot's affective reinforcement learning algorithm. Over several sessions, the robot played the game and personalized its motivational strategies (using verbal and non-verbal actions) to each student. We evaluated this system with 34 children in preschool classrooms for a duration of two months. We saw that (1) children learned new words from the repeated tutoring sessions, (2) the affective policy personalized to students over the duration of the study, and (3) students who interacted with a robot that personalized its affective feedback strategy showed a significant increase in valence, as compared to students who interacted with a nonpersonalizing robot. This integrated system of tablet-based educational content, affective sensing, affective policy learning, and an autonomous social robot holds great promise for a more comprehensive approach to personalized tutoring. © Copyright 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Industrial Engineering Department, Curiosity Lab, Tel-Aviv Univerisity, Israel",,,"Gordon G., Spaulding S., Korywestlund J., Lee J.J., Plummer L., Martinez M., Das M., Breazeal C.","Gordon, G., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States, Industrial Engineering Department, Curiosity Lab, Tel-Aviv Univerisity, Israel; Spaulding, S., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Korywestlund, J., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Lee, J.J., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Plummer, L., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Martinez, M., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Das, M., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Breazeal, C., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States",16,,,SCOPUS,0,,,,,,,,,,,,,"30th AAAI Conference on Artificial Intelligence, AAAI 2016",,,2-s2.0-85007268746,,,,,,3957,3951,,,,,Scopus,,,"30th AAAI Conference on Artificial Intelligence, AAAI 2016",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007268746&partnerID=40&md5=1d1e6a4e8fcc81aca41b3aa559b142ac,,,,2016,,,,,,,,,,,,,,,,,,,
XCSF with tile coding in discontinuous action-value landscapes,"Tile coding is an effective reinforcement learning method that uses a rather ingenious generalization mechanism based on (1) a carefully designed parameter setting and (2) the assumption that nearby states in the problem space will correspond to similar payoff predictions in the action-value function. Previously, we extended XCSF with tile coding prediction and compared it to tabular tile coding, showing that (1) XCSF performs as well as parameter-optimized tile coding, while also (2) evolving individualized parameter settings in each problem subspace. Our comparison was based on a set of well-known reinforcement learning environments (2D Gridworld and the Mountain Car) that involved no action-value discontinuities and so posed no challenge to tabular tile coding. In this paper, we go a step further and test XCSF with tile coding on a set of problems designed to challenge tile coding by introducing discontinuities in the action value landscape. The new testbed (called MazeWorld) extends 2D Gridworld with impenetrable obstacles, a conceptually simple modification that can dramatically increase the problem complexity for tabular tile coding. We compare four versions of XCSF with tile coding (each adapting a different set of parameters) to tabular tile coding on four problems of increasing complexity. We show that our system (1) needs fewer training problems than standard tile coding to reach an optimal policy; (2) it can evolve adequate coding parameters in each subspace without any previous knowledge; and that (3) even when XCSF is not allowed to evolve these parameters, the genetic algorithm will still adapt classifier conditions to properly decompose the problem into subspaces thus being much less sensitive to the parameter settings than tabular tile coding. © 2015, Springer-Verlag Berlin Heidelberg.",,"Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milan, Italy",,,"Lanzi P.L., Loiacono D.","Lanzi, P.L., Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milan, Italy; Loiacono, D., Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milan, Italy",1,,10.1007/s12065-015-0129-7,SCOPUS,1,,,,,,,,,,2-3,,,Evolutionary Intelligence,Learning classifier systems; Reinforcement learning; Tile coding; XCSF,,2-s2.0-84929840754,,,,,,132,117,,,,,Scopus,,,Evolutionary Intelligence,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929840754&doi=10.1007%2fs12065-015-0129-7&partnerID=40&md5=8308915ac08dcb9ce69ff618441ae49b,,8,,2015,,,,,,,,,,,,,,,,,,,
QoS-Oriented Wireless Routing for Smart Meter Data Collection: Stochastic Learning on Graph,"To ensure resilient and reliable meter data collection that is essential for the smart grid operation, we propose a QoS-oriented wireless routing scheme. Specifically tailored for the heterogeneity of the meter data traffic in the smart grid, we first design a novel utility function that not only jointly accounts for system throughput and transmission latency, but also allows for flexible tradeoff between the two with a strict transmission latency constraint, as desired by various smart meter applications. Then, we model the interactions among smart meter data concentrators as a mixed-strategy network formation game. To avoid potential information exchange which is not always practical in meter data collection scenario, a stochastic reinforcement learning algorithm with only private and incomplete information is proposed to solve the network formation problem. Such a problem formulation, together with our proposed stochastic learning algorithm on graph, results in a steady probabilistic route. Both contributions are novel and unique in comparison with existing work on this topic. Another distinct feature of our approach is its capability of effectively maintaining the QoS of smart meter data collection, even when the network is under fault or attack, as verified by simulations.",,,,"Dept. of Electron. & Inf. Eng., Huazhong Univ. of Sci. & Technol., Wuhan, China",Y. Cao; D. Duan; X. Cheng; L. Yang; J. Wei,,5,,10.1109/TWC.2014.2314121,IEEE Xplore,1,,20140808,4482,Science Foundation for the Youth Scholar of Ministry of Education of China; 10.13039/100000001 - National Science Foundation; 10.13039/501100001809 - National Natural Science Foundation of China; ,Data collection;Games;Quality of service;Routing;Smart grids;Throughput;Wireless communication,graph theory;learning (artificial intelligence);power engineering computing;quality of service;smart meters;stochastic processes;telecommunication network routing;wireless channels,QoS-oriented wireless routing;mixed-strategy network formation game;network formation problem;smart grid operation;smart meter data collection;stochastic learning;stochastic reinforcement learning algorithm;transmission latency constraint;wireless channels,,1536-1276;15361276,8,Aug. 2014,,IEEE Transactions on Wireless Communications,Network formation;smart grid;stochastic learning;wireless routing,,,,,20140327,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6779690,,,,,,IEEE,40,,,,IEEE Transactions on Wireless Communications,4470,IEEE Journals & Magazines,,,13,,2014,,,,,,,,,,,,,,,,,,,
Research of personal decision process using event-related potentials,"To gain insights into the neural basis of such adaptive decision-making processes, we investigated the nature of learning process in humans playing a competitive game with binary choices, using a matching pennies game. As in reinforcement learning, the subject's choice during a competitive game was biased by its choice and reward history, as well as by the strategies of its opponent. Analyses of ERP data focused on the feedback-related negativity (FRN), we found that the magnitude of ERPs after losing to the computer opponent predicted whether subjects would change decision behavior on the subsequent trial. These findings provide novel evidence that humans engage a reinforcement learning process to adjust representations of competing decision options. © 2011 Copyright Society of Photo-Optical Instrumentation Engineers (SPIE).",,"Institution of Information and Technology, Jiangxi Blue Sky University, 330098, Nanchang, China",820526,,Li X.,"Li, X., Institution of Information and Technology, Jiangxi Blue Sky University, 330098, Nanchang, China",,,10.1117/12.905918,SCOPUS,1,,,,,,,,,,,,,Proceedings of SPIE - The International Society for Optical Engineering,Decision-making; ERP; event-related potential; feedback,,2-s2.0-81855189580,,,,,,,,,,,,Scopus,,,Proceedings of SPIE - The International Society for Optical Engineering,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855189580&doi=10.1117%2f12.905918&partnerID=40&md5=5961f2673bb1234d199456b6bee3601d,,8205,,2011,,,,,,,,,,,,,,,,,,,
Work in progress — Tools and technology to implement a students personal laboratory,"Today's students want to solve problems and experience engineering regardless of where they are - in lecture, in the laboratory, or the dorm room. Professors want to provide a hands-on learning experience to empower students who want to tinker, experiment, and explore concepts while improving the comprehension through reinforcement. Student access to affordable, low-cost technology enables educators to address limitations in the laboratory, including access to equipment, time on task, and cost. With a portable laboratory, a student can learn concepts in their preferred environments and provides a supplement to the traditional lecture and laboratory based courses.",,,,"Academic Product Manager, National Instruments",M. Walters,,1,,10.1109/FIE.2011.6143112,IEEE Xplore,1,,20120202,F1G-2,,Data acquisition;Educational institutions;Hardware;Instruments;Laboratories;Nickel;Signal generators,data acquisition;educational technology;engineering education;laboratory techniques;portable instruments;virtual instrumentation,equipment access;hands-on learning experience;low-cost educational technology;myDAQ;portable laboratory;students personal laboratory;work in progress,,0190-5848;01905848,,12-15 Oct. 2011,,2011 Frontiers in Education Conference (FIE),hands-on;myDAQ;student-laboratory;student-ownership,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143112,,,,,,IEEE,3,,Electronic:978-1-61284-469-5; POD:978-1-61284-468-8; USB:978-1-61284-467-1,,2011 Frontiers in Education Conference (FIE),F1G-1,IEEE Conferences,,,,,2011,,,,,,,,,,,,,,,,,,,
Source task creation for curriculum learning,"Transfer learning in reinforcement learning has been an active area of research over the past decade. In transfer learning, training on a source task is leveraged to speed up or otherwise improve learning on a target task. This paper presents the more ambitious problem of curriculum learning in reinforcement learning, in which the goal is to design a sequence of source tasks for an agent to train on, such that final performance or learning speed is improved. We take the position that each stage of such a curriculum should be tailored to the current ability of the agent in order to promote learning new behaviors. Thus, as a first step towards creating a curriculum, the trainer must be able to create novel, agent-specific source tasks. We explore how such a space of useful tasks can be created using a parameterized model of the domain and observed trajectories on the target task. We experimentally show that these methods can be used to form components of a curriculum and that such a curriculum can be used successfully for transfer learning in 2 challenging multiagent reinforcement learning domains. Copyright © 2016, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",,"Department of Computer Science, University of Texas at Austin, Austin, TX, United States",,,"Narvekar S., Sinapov J., Leonetti M., Stone P.","Narvekar, S., Department of Computer Science, University of Texas at Austin, Austin, TX, United States; Sinapov, J., Department of Computer Science, University of Texas at Austin, Austin, TX, United States; Leonetti, M., Department of Computer Science, University of Texas at Austin, Austin, TX, United States; Stone, P., Department of Computer Science, University of Texas at Austin, Austin, TX, United States",5,,,SCOPUS,0,,,,,,,,,,,,,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",Curriculum learning; Reinforcement learning; Transfer learning,,2-s2.0-85014299386,,,,,,574,566,,,,,Scopus,,,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014299386&partnerID=40&md5=1f7f26c873913eaa841534ac92cb8a78,,,,2016,,,,,,,,,,,,,,,,,,,
Curriculum learning in reinforcement learning,"Transfer learning in reinforcement learning is an area of research that seeks to speed up or improve learning of a complex target task, by leveraging knowledge from one or more source tasks. This thesis will extend the concept of transfer learning to curriculum learning, where the goal is to design a sequence of source tasks for an agent to train on, such that final performance or learning speed is improved. We discuss completed work on this topic, including methods for semi-automatically generating source tasks tailored to an agent and the characteristics of a target domain, and automatically sequencing such tasks into a curriculum. Finally, we also present ideas for future work.",,"Department of Computer Science, University of Texas at Austin, United States",,,Narvekar S.,"Narvekar, S., Department of Computer Science, University of Texas at Austin, United States",,,,SCOPUS,0,,,,,,,,,,,,,IJCAI International Joint Conference on Artificial Intelligence,,,2-s2.0-85031919399,,,,,,5196,5195,,,,,Scopus,,,IJCAI International Joint Conference on Artificial Intelligence,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031919399&partnerID=40&md5=bc1c2a4f7029255194de43ecc74eeb0d,,,,2017,,,,,,,,,,,,,,,,,,,
Autonomous task sequencing for customized curriculum design in reinforcement learning,"Transfer learning is a method where an agent reuses knowledge learned in a source task to improve learning on a target task. Recent work has shown that transfer learning can be extended to the idea of curriculum learning, where the agent incrementally accumulates knowledge over a sequence of tasks (i.e. a curriculum). In most existing work, such curricula have been constructed manually. Furthermore, they are fixed ahead of time, and do not adapt to the progress or abilities of the agent. In this paper, we formulate the design of a curriculum as a Markov Decision Process, which directly models the accumulation of knowledge as an agent interacts with tasks, and propose a method that approximates an execution of an optimal policy in this MDP to produce an agent-specific curriculum. We use our approach to automatically sequence tasks for 3 agents with varying sensing and action capabilities in an experimental domain, and show that our method produces curricula customized for each agent that improve performance relative to learning from scratch or using a different agent's curriculum.",,"Department of Computer Science, University of Texas, Austin, United States",,,"Narvekar S., Sinapov J., Stone P.","Narvekar, S., Department of Computer Science, University of Texas, Austin, United States; Sinapov, J., Department of Computer Science, University of Texas, Austin, United States; Stone, P., Department of Computer Science, University of Texas, Austin, United States",1,,,SCOPUS,0,,,,,,,,,,,,,IJCAI International Joint Conference on Artificial Intelligence,,,2-s2.0-85031926206,,,,,,2542,2536,,,,,Scopus,,,IJCAI International Joint Conference on Artificial Intelligence,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031926206&partnerID=40&md5=999d71abd37b44b4bd9f668a6b94f8b8,,,,2017,,,,,,,,,,,,,,,,,,,
A new multiagent reinforcement learning algorithm to solve the symmetric traveling salesman problem,"Travelling salesman problem (TSP) looks simple, however it is an important combinatorial problem. Its computational intractability has attracted a number of heuristic approaches to generate satisfactory, if not optimal solutions. In this paper, we present a new algorithm for the Symmetric TSP using Multiagent Reinforcement Learning (MARL) approach. Each agent in the multiagent system is an autonomous entity with personal declarative memory and behavioral components which are used to tour construction and then constructed tour of each agent is improved by 2-opt local search heuristic as tour improvement heuristic in order to reach optimal or near-optimal solutions in a reasonable time. The experiments in this paper are performed using the 29 datasets obtained from the TSPLIB. Also, the experimental results of the proposed method are compared with some well-known methods in the field. Our experimental results indicate that the proposed approach has a good performance with respect to the quality of the solution and the speed of computation.",,"Department of Computer Engineering, Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran",,,"Alipour M.M., Razavi S.N.","Alipour, M.M., Department of Computer Engineering, Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran; Razavi, S.N., Department of Computer Engineering, Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran",1,,10.3233/MGS-150232,SCOPUS,1,,,,,,,,,,2,,,Multiagent and Grid Systems,2-opt local search heuristic; Multiagent reinforcement learning (MARL); Traveling salesman problem (TSP),,2-s2.0-84940043679,,,,,,119,107,,,,,Scopus,,,Multiagent and Grid Systems,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940043679&doi=10.3233%2fMGS-150232&partnerID=40&md5=8efa63586c346e4fd35fbbb59b2e42dd,,11,,2015,,,,,,,,,,,,,,,,,,,
Cognitive control and counterproductive oculomotor capture by reward-related stimuli,"Two experiments investigated the extent to which value-modulated oculomotor capture is subject to top-down control. In these experiments, participants were never required to look at the reward-related stimuli; indeed, doing so was directly counterproductive because it caused omission of the reward that would otherwise have been obtained. In Experiment 1, participants were explicitly informed of this omission contingency. Nevertheless, they still showed counterproductive oculomotor capture by reward-related stimuli, suggesting that this effect is relatively immune to cognitive control. Experiment 2 more directly tested whether this capture is controllable by comparing the performance of participants who either had or had not been explicitly informed of the omission contingency. There was no evidence that value-modulated oculomotor capture differed between the two conditions, providing further evidence that this effect proceeds independently of cognitive control. Taken together, the results of the present research provide strong evidence for the automaticity and cognitive impenetrability of value-modulated attentional capture. © 2015 Taylor & Francis.",,"School of Psychology, University of New South Wales, Sydney, NSW, Australia",,,"Pearson D., Donkin C., Tran S.C., Most S.B., Le Pelley M.E.","Pearson, D., School of Psychology, University of New South Wales, Sydney, NSW, Australia; Donkin, C., School of Psychology, University of New South Wales, Sydney, NSW, Australia; Tran, S.C., School of Psychology, University of New South Wales, Sydney, NSW, Australia; Most, S.B., School of Psychology, University of New South Wales, Sydney, NSW, Australia; Le Pelley, M.E., School of Psychology, University of New South Wales, Sydney, NSW, Australia",19,,10.1080/13506285.2014.994252,SCOPUS,1,,,,,,,,,,1-2,,,Visual Cognition,Attentional capture; Eye movements; Reinforcement learning; Reward learning; Visual attention,,2-s2.0-84928938476,,,,,,66,41,,,,,Scopus,,,Visual Cognition,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928938476&doi=10.1080%2f13506285.2014.994252&partnerID=40&md5=1fa3d3bce5bf4240038cf63769c4d855,,23,,2015,,,,,,,,,,,,,,,,,,,
Reinforcement learning strategies for clinical trials in nonsmall cell lung cancer,"Typical regimens for advanced metastatic stage IIIB/IV nonsmall cell lung cancer (NSCLC) consist of multiple lines of treatment. We present an adaptive reinforcement learning approach to discover optimal individualized treatment regimens from a specially designed clinical trial (a ""clinical reinforcement trial"") of an experimental treatment for patients with advanced NSCLC who have not been treated previously with systemic therapy. In addition to the complexity of the problem of selecting optimal compounds for first- and second-line treatments based on prognostic factors, another primary goal is to determine the optimal time to initiate second-line therapy, either immediately or delayed after induction therapy, yielding the longest overall survival time. A reinforcement learning method calledQ-learning is utilized, which involves learning an optimal regimen from patient data generated from the clinical reinforcement trial. Approximating theQ-function with time-indexed parameters can be achieved by using a modification of support vector regression that can utilize censored data. Within this framework, a simulation study shows that the procedure can extract optimal regimens for two lines of treatment directly from clinical data without prior knowledge of the treatment effect mechanism. In addition, we demonstrate that the design reliably selects the best initial time for second-line therapy while taking into account the heterogeneity of NSCLC across patients. © 2011, The International Biometric Society.",,"Global Biostatistics and Epidemiology, Amgen Inc., One Amgen Center Drive, Thousand Oaks, CA 91320, United States; Department of Biostatistics, University of North Carolina at Chapel Hill, 3101 McGavran-Greenberg, CB 7420, Chapel Hill, NC 27599, United States; Department of Medicine, University of North Carolina at Chapel Hill, Physicians Office Building, 170 Manning Drive, Chapel Hill, NC 27599, United States",,,"Zhao Y., Zeng D., Socinski M.A., Kosorok M.R.","Zhao, Y., Global Biostatistics and Epidemiology, Amgen Inc., One Amgen Center Drive, Thousand Oaks, CA 91320, United States; Zeng, D., Department of Biostatistics, University of North Carolina at Chapel Hill, 3101 McGavran-Greenberg, CB 7420, Chapel Hill, NC 27599, United States; Socinski, M.A., Department of Medicine, University of North Carolina at Chapel Hill, Physicians Office Building, 170 Manning Drive, Chapel Hill, NC 27599, United States; Kosorok, M.R., Department of Biostatistics, University of North Carolina at Chapel Hill, 3101 McGavran-Greenberg, CB 7420, Chapel Hill, NC 27599, United States",50,,10.1111/j.1541-0420.2011.01572.x,SCOPUS,1,,,,,,,,,,4,,,Biometrics,Adaptive design; Dynamic treatment regime; Individualized therapy; Multistage decision problems; Nonsmall cell lung cancer; Personalized medicine; Q-learning; Reinforcement learning; Support vector regression,,2-s2.0-83655181241,,,,,,1433,1422,,,,,Scopus,,,Biometrics,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-83655181241&doi=10.1111%2fj.1541-0420.2011.01572.x&partnerID=40&md5=64ad4676c2bde9eeed5337ac997d331a,,67,,2011,,,,,,,,,,,,,,,,,,,
Reinforcement learning of context models for a ubiquitous personal assistant,"Ubiquitous environments may become a reality in a foreseeable future and research is aimed on making them more and more adapted and comfortable for users. Our work consists on applying reinforcement learning techniques in order to adapt services provided by a ubiquitous assistant to the user. The learning produces a context model, associating actions to perceived situations of the user. Associations are based on feedback given by the user as a reaction to the behavior of the assistant. Our method brings a solution to some of the problems encountered when applying reinforcement learning to systems where the user is in the loop. For instance, the behavior of the system is completely incoherent at the be-ginning and needs time to converge. The user does not accept to wait that long to train the system. The user's habits may change over time and the assistant needs to integrate these changes quickly. We study methods to accelerate the reinforced learning process. © 2009 Springer-Verlag Berlin Heidelberg.",,"Laboratoire LIG, 681 rue de la Passerelle, St-Martin d'Hères 38402, France",,,"Zaidenberg S., Reignier P., Crowley J.L.","Zaidenberg, S., Laboratoire LIG, 681 rue de la Passerelle, St-Martin d'Hères 38402, France; Reignier, P., Laboratoire LIG, 681 rue de la Passerelle, St-Martin d'Hères 38402, France; Crowley, J.L., Laboratoire LIG, 681 rue de la Passerelle, St-Martin d'Hères 38402, France",6,,10.1007/978-3-540-85867-6_30,SCOPUS,1,,,,,,,,,,,,,Advances in Soft Computing,,,2-s2.0-58149109424,,,,,,264,254,,,,,Scopus,,,Advances in Soft Computing,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149109424&doi=10.1007%2f978-3-540-85867-6_30&partnerID=40&md5=7b74657e0245e5a528b0a4029e063cd7,,51,,2009,,,,,,,,,,,,,,,,,,,
A Service Recommendation Using Reinforcement Learning for Network-based Robots in Ubiquitous Computing Environments,"Ubiquitous robotic companion (URC ) is a new concept for the network-based robot platform which can enable to be following its master wherever or whenever he/she be in order to provide necessary services. The robot platforms in present normally interest in providing services through the direct interaction in responding to the user's demands. On the other hand, URC services are required to be provided by the means of recognizing the circumstances and taking a user's preference into account. In this paper, we propose a service recommendation scheme for URC robots. The proposed service recommendation, developed based on the reinforcement learning, can be used to provide personalized services by learning users' preferences or tasks through the interaction with users. Using simulation for rapid testing, we evaluate of the proposed scheme under a variety of user modeling types and discount factors.",,,,"Electronice and Telecommunication Research Institute, 161 Gajeong-dong, Yuseong-gu, Daejeon, 305-350, Korea. Email: akmoon@etri.re.kr",A. Moon; T. Kang; H. Kim; H. Kim,,2,,10.1109/ROMAN.2007.4415198,IEEE Xplore,1,,20080116,826,,Control systems;Human robot interaction;Intelligent robots;Inventory management;Machine learning;Moon;Robot control;Robot sensing systems;Service robots;Ubiquitous computing,computer networks;control engineering computing;learning (artificial intelligence);man-machine systems;robots;ubiquitous computing;user modelling,human-robot interaction;network-based robot;network-based robot platform;rapid testing;reinforcement learning;service recommendation scheme;ubiquitous computing environment;ubiquitous robotic companion;user modeling,,1944-9445;19449445,,26-29 Aug. 2007,,RO-MAN 2007 - The 16th IEEE International Symposium on Robot and Human Interactive Communication,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4415198,,,,,,IEEE,15,,CD-ROM:978-1-4244-1635-6; POD:978-1-4244-1634-9,,RO-MAN 2007 - The 16th IEEE International Symposium on Robot and Human Interactive Communication,821,IEEE Conferences,,,,,2007,,,,,,,,,,,,,,,,,,,
Reinforcement learning approach towards effective content recommendation in MOOC environments,"Understanding the Learner requirements is an important aspect of any learning environment as it helps to recommend the LOs in a more personalized manner. With the growing demand for MOOCs offered by coursera, edx, etc. the learner information plays a vital role in understanding the extent to which the learners can gain out of such courses. The Learning Management Systems (LMS) across the web uses the explicit (rating, performance, etc.) and implicit feedback (LOs used) obtained through interaction with the learners to derive such information. As the requirements of the learners varies with the individual's interest and learning background, a common approach for recommending LOs may not cater the needs of all the learners. To overcome this issue, this paper proposes reinforcement learning based algorithm to analyze the learner information (derived from both implicit and explicit feedback) and generate the knowledge on the learner's requirements and capabilities inside a specific learning context. The reinforcement learning system (RILS) implemented as a part of this work utilizes the knowledge thus generated in order to recommend the appropriate LOs for the learners. The results have highlighted that the knowledge derived from the learning information analysis proved to effective in generating personalized recommendation policies that can cater the context specific requirements of the learners.",,,,"SCSE, VIT University, Vellore, Tamilnadu, India",V. R. Raghuveer; B. K. Tripathy; T. Singh; S. Khanna,,1,,10.1109/MITE.2014.7020289,IEEE Xplore,1,,20150126,289,,Collaboration;Conferences;Context;Educational institutions;Electronic learning;Technological innovation,Internet;computer aided instruction;educational courses;human computer interaction;information analysis;learning (artificial intelligence);recommender systems,LMS;MOOC environments;RILS;Web;content recommendation;explicit feedback;generating personalized recommendation policies;implicit feedback;knowledge utilization;learning information analysis;learning management systems;massive open online course;reinforcement learning system,,,,19-20 Dec. 2014,,"2014 IEEE International Conference on MOOC, Innovation and Technology in Education (MITE)",LO recommendation;MOOC;Reinforcement Learning;learning context;learning experience,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7020289,,,,,,IEEE,14,,Electronic:978-1-4799-6876-3; POD:978-1-4799-6877-0,,"2014 IEEE International Conference on MOOC, Innovation and Technology in Education (MITE)",285,IEEE Conferences,,,,,2014,,,,,,,,,,,,,,,,,,,
Video Annotation Through Search and Graph Reinforcement Mining,"Unlimited vocabulary annotation of multimedia documents remains elusive despite progress solving the problem in the case of a small, fixed lexicon. Taking advantage of the repetitive nature of modern information and online media databases with independent annotation instances, we present an approach to automatically annotate multimedia documents that uses mining techniques to discover new annotations from similar documents and to filter existing incorrect annotations. The annotation set is not limited to words that have training data or for which models have been created. It is limited only by the words in the collective annotation vocabulary of all the database documents. A graph reinforcement method driven by a particular modality (e.g., visual) is used to determine the contribution of a similar document to the annotation target. The graph supplies possible annotations of a different modality (e.g., text) that can be mined for annotations of the target. Experiments are performed using videos crawled from YouTube. A customized precision-recall metric shows that the annotations obtained using the proposed method are superior to those originally existing for the document. These extended, filtered tags are also superior to a state-of-the-art semi-supervised technique for graph reinforcement learning on the initial user-supplied annotations.",,,,"University of California at Santa Barbara, California, U.S.A.",E. Moxley; T. Mei; B. S. Manjunath,,28,,10.1109/TMM.2010.2041101,IEEE Xplore,1,,20100315,193,,,data mining;document handling;learning (artificial intelligence);video retrieval,annotations discovery;collective annotation vocabulary;customized precision recall metric;database documents;graph reinforcement mining;multimedia documents annotation;online media database;video annotation,,1520-9210;15209210,3,April 2010,,IEEE Transactions on Multimedia,Data mining;graph theory;video annotation;video content analysis,,,,,20100126,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5398917,,,,,,IEEE,36,,,,IEEE Transactions on Multimedia,184,IEEE Journals & Magazines,,,12,,2010,,,,,,,,,,,,,,,,,,,
Data-driven inverse learning of passenger preferences in urban public transits,"Urban public transit planning is crucial in reducing traffic congestion and enabling green transportation. However, there is no systematic way to integrate passengers' personal preferences in planning public transit routes and schedules so as to achieve high occupancy rates and efficiency gain of ride-sharing. In this paper, we take the first step tp exact passengers' preferences in planning from history public transit data. We propose a data-driven method to construct a Markov decision process model that characterizes the process of passengers making sequential public transit choices, in bus routes, subway lines, and transfer stops/stations. Using the model, we integrate softmax policy iteration into maximum entropy inverse reinforcement learning to infer the passenger's reward function from observed trajectory data. The inferred reward function will enable an urban planner to predict passengers' route planning decisions given some proposed transit plans, for example, opening a new bus route or subway line. Finally, we demonstrate the correctness and accuracy of our modeling and inference methods in a large-scale (three months) passenger-level public transit trajectory data from Shenzhen, China. Our method contributes to smart transportation design and human-centric urban planning.",,,,"Worcester Polytechnic Institute (WPI), 100 Institute Road, Worcester, MA 01609, USA",G. Wu; Y. Ding; Y. Li; J. Luo; F. Zhang; J. Fu,,,,10.1109/CDC.2017.8264410,IEEE Xplore,1,,20180122,5073,,Data models;Entropy;Markov processes;Planning;Public transportation;Roads;Trajectory,Markov processes;data analysis;entropy;learning (artificial intelligence);town and country planning;traffic engineering computing;transportation,China;Markov decision process model;Shenzhen;bus route;bus routes;data-driven inverse learning;data-driven method;enabling green transportation;green transportation;high occupancy rates;history public transit data;human-centric urban planning;inference methods;inferred reward function;large-scale passenger-level public transit trajectory data;maximum entropy inverse reinforcement;observed trajectory data;public transit routes;sequential public transit choices;smart transportation design;traffic congestion;transit plans;urban planner;urban public transit planning,,,,12-15 Dec. 2017,,2017 IEEE 56th Annual Conference on Decision and Control (CDC),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8264410,,,,,,IEEE,,,Electronic:978-1-5090-2873-3; POD:978-1-5090-2874-0; USB:978-1-5090-2872-6,,2017 IEEE 56th Annual Conference on Decision and Control (CDC),5068,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
The best privacy defense is a good privacy offense: obfuscating a search engine user’s profile,"User privacy on the internet is an important and unsolved problem. So far, no sufficient and comprehensive solution has been proposed that helps a user to protect his or her privacy while using the internet. Data are collected and assembled by numerous service providers. Solutions so far focused on the side of the service providers to store encrypted or transformed data that can be still used for analysis. This has a major flaw, as it relies on the service providers to do this. The user has no chance of actively protecting his or her privacy. In this work, we suggest a new approach, empowering the user to take advantage of the same tool the other side has, namely data mining to produce data which obfuscates the user’s profile. We apply this approach to search engine queries and use feedback of the search engines in terms of personalized advertisements in an algorithm similar to reinforcement learning to generate new queries potentially confusing the search engine. We evaluated the approach using a real-world data set. While evaluation is hard, we achieve results that indicate that it is possible to influence the user’s profile that the search engine generates. This shows that it is feasible to defend a user’s privacy from a new and more practical perspective. © 2017, The Author(s).",,"Institute of Computer Science, Johannes Gutenberg University Mainz, Staudingerweg 9, Mainz, Germany",,,"Wicker J., Kramer S.","Wicker, J., Institute of Computer Science, Johannes Gutenberg University Mainz, Staudingerweg 9, Mainz, Germany; Kramer, S., Institute of Computer Science, Johannes Gutenberg University Mainz, Staudingerweg 9, Mainz, Germany",,,10.1007/s10618-017-0524-z,SCOPUS,1,,,,,,,,,,5,,,Data Mining and Knowledge Discovery,Personalized ads; Privacy; Reinforcement learning; Search engines; Web mining,,2-s2.0-85025085865,,,,,,1443,1419,,,,,Scopus,,,Data Mining and Knowledge Discovery,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025085865&doi=10.1007%2fs10618-017-0524-z&partnerID=40&md5=247afda6050dd89a6a7d18d84d0dc5ee,,31,,2017,,,,,,,,,,,,,,,,,,,
A task-oriented service personalization scheme for smart environments using reinforcement learning,"Users want IoT environments to provide them with personalized support. These environments therefore need to be able to learn user preferences, such as what temperature the room should be or which lights should be turned on. We propose a novel system that separates the tasks of learning a user's preferences and realizing them within the environment. The system is able to capture user preferences by reinterpreting the problem as an optimization problem and applying inverse reinforcement learning to it. The system is shown to be able to accurately extract simple preferences given a small number of user demonstrations. These preferences are then realized by actuates devices running reinforcement learning-based agents to provide an environment consistent with the learnt preferences, also in situations not included in any user demonstration.",,,,"School of Computer Science, KAIST, Daejeon, S. Korea",B. Tegelund; H. Son; D. Lee,,2,,10.1109/PERCOMW.2016.7457110,IEEE Xplore,1,,20160421,6,,Brightness;Context;Context modeling;Learning (artificial intelligence);Motion pictures;Performance evaluation;Sensors,Internet of Things;ambient intelligence;learning (artificial intelligence);multi-agent systems,IoT environments;personalized support;reinforcement learning-based agents;smart environments;task-oriented service personalization scheme;user preference learning,,,,14-18 March 2016,,2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457110,,,,,,IEEE,10,,Electronic:978-1-5090-1941-0; POD:978-1-5090-1942-7,,2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops),1,IEEE Conferences,,,,,2016,,,,,,,,,,,,,,,,,,,
Risk-sensitivity through multi-objective reinforcement learning,"Usually in reinforcement learning, the goal of the agent is to maximize the expected return. However, in practical applications, algorithms that solely focus on maximizing the mean return could be inappropriate as they do not account for the variability of their solutions. Thereby, a variability measure could be included to accommodate for a risk-sensitive setting, i.e. where the system engineer can explicitly define the tolerated level of variance. Our approach is based on multi-objectivization where a standard single-objective environment is extended with one (or more) additional objectives. More precisely, we augment the standard feedback signal of an environment with an additional objective that defines the variance of the solution. We highlight that our algorithm, named risk-sensitive Pareto Q-learning, is (1) specifically tailored to learn a set of Pareto non-dominated policies that trade-off these two objectives. Additionally (2), the algorithm can also retrieve every policy that has been learned throughout the state-action space. This in contrast to standard risk-sensitive approaches where only a single trade-off between mean and variance is learned at a time.",,,,"Department of Computer Science, Vrije Universiteit Brussel, Brussels, Belgium",K. Van Moffaert; T. Brys; A. Nowé,,1,,10.1109/CEC.2015.7257098,IEEE Xplore,1,,20150914,1753,,,feedback;learning (artificial intelligence);risk analysis;sensitivity analysis,multiobjective reinforcement learning;nondominated policies;risk-sensitive Pareto Q-learning;standard feedback signal;standard single-objective environment;state-action space;variability measure,,1089-778X;1089778X,,25-28 May 2015,,2015 IEEE Congress on Evolutionary Computation (CEC),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7257098,,,,,,IEEE,27,,Electronic:978-1-4799-7492-4; POD:978-1-4799-7493-1,,2015 IEEE Congress on Evolutionary Computation (CEC),1746,IEEE Conferences,,,,,2015,,,,,,,,,,,,,,,,,,,
Satisfaction based Q-learning for integrated lighting and blind control,"Various lighting and blind control methods have been presented to improve user comfort and reduce energy consumption simultaneously. However, there are opportunities to improve control performances by introducing more recent information and machine learning technologies which allow more comprehensive consideration of the balance between user comfort and system energy consumption. To be more specific, in terms of user comfort, unified set-point may not be desirable since different people may have different comfort preferences. In terms of energy consumption, the excessive cooling load of HVAC system should be considered in summer when utilizing solar incidence to reduce the lighting electricity consumption. The setting of the blind slat angle still has great room to improve instead of the cut-off angle. Moreover, users' demands are not fully met, so sometimes they still want to override the automated control. Thus, a closed-loop satisfaction based system is developed in this paper, specifically we introduce an improved reinforcement learning controller to obtain an optimal control strategy of blinds and lights. It could provide a personalized service via introducing subjects perceptions of surroundings gathered by a novel interface as the feedback signal. The proposed system was implemented on a practical test-bed in an energy-efficient building. Compared with the traditional control, it can provide a more acceptable and energy-efficient luminous environment. © 2016 Elsevier B.V.",,"Center for Intelligent and Networked System, Department of Automation and TNList, Tsinghua University, Beijing, China; Department of Building Science, Tsinghua University, Beijing, China; United Technologies Research Center (China) Ltd., Shanghai, China",,,"Cheng Z., Zhao Q., Wang F., Jiang Y., Xia L., Ding J.","Cheng, Z., Center for Intelligent and Networked System, Department of Automation and TNList, Tsinghua University, Beijing, China; Zhao, Q., Center for Intelligent and Networked System, Department of Automation and TNList, Tsinghua University, Beijing, China; Wang, F., Department of Building Science, Tsinghua University, Beijing, China; Jiang, Y., Department of Building Science, Tsinghua University, Beijing, China; Xia, L., Center for Intelligent and Networked System, Department of Automation and TNList, Tsinghua University, Beijing, China; Ding, J., United Technologies Research Center (China) Ltd., Shanghai, China",7,,10.1016/j.enbuild.2016.05.067,SCOPUS,1,,,,,,,,,,,,,Energy and Buildings,Blinds; Day-light; Energy saving; Integrated control; Q-learning,,2-s2.0-84971643867,,,,,,55,43,,,,,Scopus,,,Energy and Buildings,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971643867&doi=10.1016%2fj.enbuild.2016.05.067&partnerID=40&md5=57f6a8db85f9f0b6c01f3c0021a579e0,,127,,2016,,,,,,,,,,,,,,,,,,,
Patient tailored virtual rehabilitation,"Virtual rehabilitation should be adaptable to the patient need and progress. To do so, patient in-game performace and ability are monitored to maintain an adequate level of challenge. A novel adaptation strategy is proposed by which patient control and speed are dynamically interrogated to adjust the game difficulty. The strategy is based on a Markov decision process seeding a therapist-guided reinforcement learning algorithm. The optimal learning scheme for the algorithm is established (α = 0.5). Convergence to an optimal therapeutic plan is demonstrated for patients with non-deterministic behaviour. The proposed adaptation algorithm can enhance existing virtual reality-based motor rehabilitation platforms by tailoring the games response to the patient changing needs. © 2013, Springer-Verlag Berlin Heidelberg.",,"Optics and Electronics, National Institute for Astrophysics, Puebla, Mexico",,,"Ávila-Sansores S., Orihuela-Espina F., Enrique-Sucar L.","Ávila-Sansores, S., Optics and Electronics, National Institute for Astrophysics, Puebla, Mexico; Orihuela-Espina, F., Optics and Electronics, National Institute for Astrophysics, Puebla, Mexico; Enrique-Sucar, L., Optics and Electronics, National Institute for Astrophysics, Puebla, Mexico",4,,10.1007/978-3-642-34546-3_143,SCOPUS,1,,,,,,,,,,,,,Biosystems and Biorobotics,,,2-s2.0-85014689131,,,,,,883,879,,,,,Scopus,,,Biosystems and Biorobotics,,Book Chapter,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014689131&doi=10.1007%2f978-3-642-34546-3_143&partnerID=40&md5=4843998cb02fe4a40767841dcace8bed,,1,,2013,,,,,,,,,,,,,,,,,,,
Automatic ad format selection via contextual bandits,"Visual design plays an important role in online display advertising: changing the layout of an online ad can increase or decrease its effectiveness, measured in terms of click-through rate (CTR) or total revenue. The decision of which layout to use for an ad involves a trade-off: using a layout provides feedback about its effectiveness (exploration), but collecting that feedback requires sacrificing the immediate reward of using a layout we already know is effective (exploitation). To balance exploration with exploitation, we pose automatic layout selection as a contextual bandit problem. There are many bandit algorithms, each generating a policy which must be evaluated. It is impractical to test each policy on live traffic. However, we have found that offline replay (a.k.a. exploration scavenging) can be adapted to provide an accurate estimator for the performance of ad layout policies at Linkedin, using only historical data about the effectiveness of layouts. We describe the development of our offline replayer, and benchmark a number of common bandit algorithms. Copyright 2013 ACM.",,"School of Computer Science, Florida International Univ., 11200 S.W. 8th St., Miami, FL 33199, United States; Applied Relevance Science LinkedIn, 2029 Stierlin Ct., Mountain View, CA 94043, United States",,,"Tang L., Rosales R., Singh A.P., Agarwal D.","Tang, L., School of Computer Science, Florida International Univ., 11200 S.W. 8th St., Miami, FL 33199, United States; Rosales, R., Applied Relevance Science LinkedIn, 2029 Stierlin Ct., Mountain View, CA 94043, United States; Singh, A.P., Applied Relevance Science LinkedIn, 2029 Stierlin Ct., Mountain View, CA 94043, United States; Agarwal, D., School of Computer Science, Florida International Univ., 11200 S.W. 8th St., Miami, FL 33199, United States",20,,10.1145/2505515.2514700,SCOPUS,1,,,,,,,,,,,,,"International Conference on Information and Knowledge Management, Proceedings",Bandit algorithms; Exploration/exploitation; Layout; Offline evaluation; Online advertising; Personalization; Recommender systems,,2-s2.0-84889567137,,,,,,1594,1587,,,,,Scopus,,,"International Conference on Information and Knowledge Management, Proceedings",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889567137&doi=10.1145%2f2505515.2514700&partnerID=40&md5=652c81fbee7adda7e08bcedb57239955,,,,2013,,,,,,,,,,,,,,,,,,,
Learning capabilities for improving automatic transmission control,"We analyzed the gear-box position selection (GPS) problem on automatic transmission (AT) and proposed an algorithm, based on learning control, to improve vehicle behavior and driver satisfaction. Our approach guarantees optimization of vehicle performance and adaptation to the driver's style with road condition sensitivity. This improvement has been achieved by combining three knowledge acquisition sources: embedded dynamic models of powertrain, inductive inspection of driver actions and AT designer expertise; and by adding learning capabilities in order to significantly increase the system autonomy. Technically, GPS raises the following four problems which this paper addresses: (1) To achieve vehicle performance optimization of multiple antagonistic criteria, locally and globally over time, we considered a parametric disciminant function depending on an evaluation of the driver satisfaction and so called driver-style-state functions, as a reward for the system, and applied a reinforcement learning algorithm, derived from Q-learning method and combined with a mechanism to escape local optima. (2) Learning directly from the driver is performed when he selects AT ratio in manual mode. (3) Each driver's personal style is represented by a Glass creation/selection mechanism. (4) GPS raises a few singularities which are addressed by a set of restriction rules derived from AT control expertise.",,,,"Dept. of Comput. Sci., Stanford Univ., CA, USA",L. Fournier,,1,,10.1109/IVS.1994.639561,IEEE Xplore,1,,20020806,460,,Algorithm design and analysis;Automatic control;Global Positioning System;Inspection;Knowledge acquisition;Mechanical power transmission;Power system modeling;Road vehicles;Vehicle driving;Vehicle dynamics,intelligent control;learning (artificial intelligence);learning systems;road vehicles,Glass creation/selection mechanism;Q-learning method;automatic transmission control;driver satisfaction;driver-style-state functions;embedded dynamic models;gear-box position selection;inductive inspection;knowledge acquisition;learning control;multiple antagonistic criteria;parametric disciminant function;performance optimization;powertrain;reinforcement learning algorithm;road condition sensitivity;singularities;vehicle behavior,,,,24-26 Oct. 1994,,"Intelligent Vehicles '94 Symposium, Proceedings of the",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=639561,,,,,1,IEEE,5,,POD:0-7803-2135-9,,"Intelligent Vehicles '94 Symposium, Proceedings of the",455,IEEE Conferences,,,,,1994,,,,,,,,,,,,,,,,,,,
"Towards efficient, personalized anesthesia using continuous reinforcement learning for propofol infusion control","We demonstrate the use of reinforcement learning algorithms for efficient and personalized control of patients' depth of general anesthesia during surgical procedures - an important aspect for Neurotechnology. We used the continuous actor-critic learning automaton technique, which was trained and tested in silico using published patient data, physiological simulation and the bispectral index (BIS) of patient EEG. Our two-stage technique learns first a generic effective control strategy based on average patient data (factory stage) and can then fine-tune itself to individual patients (personalization stage). The results showed that the reinforcement learner as compared to a bang-bang controller reduced the dose of the anesthetic agent administered by 9.4% and kept the patient closer to the target state, as measured by RMSE (4.90 compared to 8.47). It also kept the BIS error within a narrow, clinically acceptable range 93.9% of the time. Moreover, the policy was trained using only 50 simulated operations. Being able to learn a control strategy this quickly indicates that the reinforcement learner could also adapt regularly to a patient's changing responses throughout a live operation and facilitate the task of anesthesiologists by prompting them with recommended actions.",,,,"Dept. of Comput., Imperial Coll. London, London, UK",C. Lowery; A. A. Faisal,,1,,10.1109/NER.2013.6696208,IEEE Xplore,1,,20140102,1417,,Algorithm design and analysis;Anesthesia;Brain modeling;Indexes;Learning (artificial intelligence);Monitoring;Surgery,bang-bang control;drug delivery systems;drugs;electroencephalography;learning (artificial intelligence);medical control systems;neurophysiology;surgery,BIS error;EEG;RMSE;anesthesiology;anesthetic agent dose reduction;bang-bang controller;bispectral index;continuous actor-critic learning automaton technique;continuous reinforcement learning algorithm;control fine tuning;depth of general anesthesia control;efficient personalized anesthesia control;generic effective control strategy learning;neurotechnology;physiological simulation;propofol infusion control;surgical procedure,,1948-3546;19483546,,6-8 Nov. 2013,,2013 6th International IEEE/EMBS Conference on Neural Engineering (NER),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6696208,,,,,1,IEEE,20,,CD-ROM:978-1-4673-1968-3; Electronic:978-1-4673-1969-0; POD:978-1-4673-1967-6,,2013 6th International IEEE/EMBS Conference on Neural Engineering (NER),1414,IEEE Conferences,,,,,2013,,,,,,,,,,,,,,,,,,,
A scalable approach for periodical personalized recommendations,"We develop a highly scalable and effective contextual bandit approach towards periodical personalized recommendations. The online bootstrapping-based technique provides a princi- pled way for UCB-type exploitation-exploration algorithms, while being able to handle arbitrary sized datasets, well suited to learn the ever evolving user preference drift from streaming data, and essentially parameter-free. We further introduce techniques to handle arbitrary sized feature spaces using feature hashing, leverage existing state-of-art machine learning via learning reduction, and increase cache hits by managing bootstrapped models in memory effectively. The resulted model trains on millions of examples and billions of features within minutes on a single personal computer. It shows persistent performance in both offline and online eval- uation. We observe around 10% click through rate (CTR) and conversion lift over a collaborative filtering approach in real-world A/B testing across more than 40 million users on the major Ticketmaster email recommendation product. © 2016 ACM.",,"Ticketmaster, Hollywood, CA, United States",,,"Qin Z., Rishabh I., Carnahan J.","Qin, Z., Ticketmaster, Hollywood, CA, United States; Rishabh, I., Ticketmaster, Hollywood, CA, United States; Carnahan, J., Ticketmaster, Hollywood, CA, United States",,,10.1145/2959100.2959139,SCOPUS,1,,,,,,,,,,,,,RecSys 2016 - Proceedings of the 10th ACM Conference on Recommender Systems,Contextual Bandits; Learning Reductions; Online Learning; Personalization; Recommender Systems; Scalability,,2-s2.0-84991205405,,,,,,26,23,,,,,Scopus,,,RecSys 2016 - Proceedings of the 10th ACM Conference on Recommender Systems,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991205405&doi=10.1145%2f2959100.2959139&partnerID=40&md5=c2af9c4b44d2b019cd0dabd2f2537ed5,,,,2016,,,,,,,,,,,,,,,,,,,
Q-learning with censored data,"We develop methodology for a multistage decision problem with flexible number of stages in which the rewards are survival times that are subject to censoring. We present a novel Q-learning algorithm that is adjusted for censored data and allows a flexible number of stages. We provide finite sample bounds on the generalization error of the policy learned by the algorithm, and show that when the optimal Q-function belongs to the approximation space, the expected survival time for policies obtained by the algorithm converges to that of the optimal policy. We simulate a multistage clinical trial with flexible number of stages and apply the proposed censored-Q-learning algorithm to find individualized treatment regimens. The methodology presented in this paper has implications in the design of personalized medicine trials in cancer and in other life-threatening diseases. © Institute of Mathematical Statistics, 2012.",,"Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States",,,"Goldberg Y., Kosorok M.R.","Goldberg, Y., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States; Kosorok, M.R., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States",24,,10.1214/12-AOS968,SCOPUS,1,,,,,,,,,,1,,,Annals of Statistics,Generalization error; Q-learning; Reinforcement learning; Survival analysis,,2-s2.0-84861349230,,,,,,560,529,,,,,Scopus,,,Annals of Statistics,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861349230&doi=10.1214%2f12-AOS968&partnerID=40&md5=a0bad44aa6961dc76c7819dc739d8b8f,,40,,2012,,,,,,,,,,,,,,,,,,,
Reinforcement learning design for cancer clinical trials,"We develop reinforcement learning trials for discovering individualized treatment regimens for lifethreatening diseases such as cancer. A temporal-difference learning method called Q-learning is utilized that involves learning an optimal policy from a single training set of finite longitudinal patient trajectories. Approximating the Q-function with time-indexed parameters can be achieved by using support vector regression or extremely randomized trees. Within this framework, we demonstrate that the procedure can extract optimal strategies directly from clinical data without relying on the identification of any accurate mathematical models, unlike approaches based on adaptive design. We show that reinforcement learning has tremendous potential in clinical research because it can select actions that improve outcomes by taking into account delayed effects even when the relationship between actions and outcomes is not fully known. To support our claims, the methodology's practical utility is illustrated in a simulation analysis. In the immediate future, we will apply this general strategy to studying and identifying new treatments for advanced metastatic stage IIIB/IV non-small cell lung cancer, which usually includes multiple lines of chemotherapy treatment. Moreover, there is significant potential of the proposed methodology for developing personalized treatment strategies in other cancers, in cystic fibrosis, and in other life-threatening diseases. Copyright © 2009 John Wiley & Sons, Ltd.",,"Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States",,,"Zhao Y., Kosorok M.R., Zeng D.","Zhao, Y., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States; Kosorok, M.R., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States; Zeng, D., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States",68,,10.1002/sim.3720,SCOPUS,1,,,,,,,,,,26,,,Statistics in Medicine,Adaptive design; Clinical trials; Dynamic treatment regime; Extremely randomized trees; Multistage decision problems; Non-small cell lung cancer; Optimal policy; Reinforcement learning; Support vector regression,,2-s2.0-70449449564,,,,,,3315,3294,,,,,Scopus,,,Statistics in Medicine,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449449564&doi=10.1002%2fsim.3720&partnerID=40&md5=6d68df31db7c4ca684184e8a88243c93,,28,,2009,,,,,,,,,,,,,,,,,,,
Do investors overweight personal experience? evidence from IPO subscriptions,"We find a strong positive link between past IPO returns and future subscriptions at the investor level in Finland. Our setting allows us to trace this effect to the returns personally experienced by investors; the effect is not explained by patterns related to the IPO cycle, or wealth effects. This behavior is consistent with reinforcement learning, where personally experienced outcomes are overweighted compared to rational Bayesian learning. The results provide a microfoundation for the argument that investor sentiment drives IPO demand. The paper also contributes to understanding how popular investment styles develop, and has implications for the marketing of financial products. © 2008 The American Finance Association.",,"Helsinki School of Economics, Department of Accounting and Finance",,,"Kaustia M., Knüpfer S.","Kaustia, M., Helsinki School of Economics, Department of Accounting and Finance; Knüpfer, S., Helsinki School of Economics, Department of Accounting and Finance",64,,10.1111/j.1540-6261.2008.01411.x,SCOPUS,1,,,,,,,,,,6,,,Journal of Finance,,,2-s2.0-55949118727,,,,,,2702,2679,,,,,Scopus,,,Journal of Finance,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55949118727&doi=10.1111%2fj.1540-6261.2008.01411.x&partnerID=40&md5=38d5e3afd5363a1adc24a762e6bfcb59,,63,,2008,,,,,,,,,,,,,,,,,,,
Planning and learning algorithms for routing in Disruption-Tolerant Networks,"We give an overview of algorithms that we have been developing in the DARPA disruption-tolerant networking program, which aims at improving communication in networks with intermittent and episodic connectivity. Thanks to the use of network caching, this can be accomplished without the need for a simultaneous end-to-end path that is required by traditional Internet and mobile ad-hoc network (MANET) protocols. We employ a disciplined two-level approach that clearly distinguishes the dissemination of application content from the dissemination of network-related knowledge, each of which can be supported by different algorithms. Specifically, we present probabilisitc reflection, a single-message protocol enabling the dissemination of knowledge in strongly disrupted networks. For content dissemination, we present two approaches, namely a symbolic planning algorithm that exploits partially predictable temporal behavior, and a distributed and disruption-tolerant reinforcement learning algorithm that takes into account feedback about past performance.",,,,"SRI International, Computer Science Laboratory, Menlo Park, California 94025, USA",M. O. Stehr; C. Talcott,,10,,10.1109/MILCOM.2008.4753336,IEEE Xplore,1,,20090119,8,,Ad hoc networks;Costs;Disruption tolerant networking;IP networks;Mobile ad hoc networks;Personal digital assistants;Protocols;Routing;Satellites;Unmanned aerial vehicles,learning (artificial intelligence);protocols;telecommunication computing;telecommunication network planning;telecommunication network routing,DARPA;disruption-tolerant networks;probabilisitc reflection;reinforcement learning algorithm;routing;single-message protocol;symbolic planning algorithm,,2155-7578;21557578,,16-19 Nov. 2008,,MILCOM 2008 - 2008 IEEE Military Communications Conference,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4753336,,,,,,IEEE,21,,POD:978-1-4244-2676-8,,MILCOM 2008 - 2008 IEEE Military Communications Conference,1,IEEE Conferences,,,,,2008,,,,,,,,,,,,,,,,,,,
User modeling with limited data: Application to stakeholder-driven watershed design,"We have developed a web-based, interactive, watershed planning system called WRESTORE (Watershed Restoration Using Spatio-Temporal Optimization of Resources) (http://wrestore.iupui.edu) that allows stake-holder communities to participate in a democratic, collaborative form of optimization process for designing best management practices (BMPs) on their landscape, while also optimizing based on subjective, qualitative landowners' criteria beyond the usual socio-economic, physical, and ecological criteria. This system utilizes multiple advanced computational approaches including the SWAT (Soil and Water Assessment Tool) hydrologic model for watershed simulations, interactive genetic algorithms and reinforcement-based machine learning algorithms for search and optimization, and deep learning artificial neural networks for user modeling, within an encompassing human-computer interaction framework. A substantial user study of the WRESTORE system was conducted recently involving multiple real stakeholders varying from consultants, government officials, watershed alliance members, etc., with the objective of gaining insight about WRESTORE'S usability and utility. In particular focus was the user modeling component that develops a computational model of a user's preferences and criteria, based on real-time user-provided ratings for a subset of possible designs (similar to the idea of user profiling commonly done for human-computer interaction systems). The user model constructed based on the real user's personalized feedbacks can then be used to influence the automated search and optimization for BMP alternatives in WRESTORE. In this paper, we describe the methods developed for user modeling for interactive optimization, and the experimental set-up as well as results with real user studies. These results clearly demonstrate that development of user models for such personalized, interactive optimization is both feasible and valuable for developing community-based computa- ional water sustainability solutions.",,,,"Computer & Information Science, Indiana University Purdue University Indianapolis, USA",S. Mukhopadhyay; V. B. Singh; M. Babbar-Sebens,,0,,10.1109/SMC.2014.6974532,IEEE Xplore,1,,20141204,3860,,Adaptation models;Artificial neural networks;Computational modeling;Data models;Mathematical model;Optimization,Internet;environmental science computing;human computer interaction;hydrology;interactive systems;learning (artificial intelligence);neural nets;socio-economic effects;water resources,BMP;SWAT hydrologic model;WRESTORE;Web-based interactive watershed planning system;best management practices;community-based computational water sustainability solutions;deep learning artificial neural networks;ecological criteria;human-computer interaction framework;interactive genetic algorithms;limited data;physical criteria;reinforcement-based machine learning algorithms;socio-economic criteria;soil and water assessment tool;stakeholder-driven watershed design;user modeling;watershed restoration using spatio-temporal optimization of resources,,1062-922X;1062922X,,5-8 Oct. 2014,,"2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",decision support system;interactive optimization;machine learning;sustainability design;user modeling,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6974532,,,,,,IEEE,21,,Electronic:978-1-4799-3840-7; POD:978-1-4799-3841-4; USB:978-1-4799-3839-1,,"2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",3855,IEEE Conferences,,,,,2014,,,,,,,,,,,,,,,,,,,
Active learning of parameterized skills,"We introduce a method for actively learning parameterized skills. Parameterized skills are flexible behaviors that can solve any task drawn from a distribution of parameterized reinforcement learning problems. Approaches to learning such skills have been proposed, but limited attention has been given to identifying which training tasks allow for rapid skill acquisition. We construct a non-parametric Bayesian model of skill performance and derive analytical expressions for a novel acquisition criterion capable of identifying tasks that maximize expected improvement in skill performance. We also introduce a spatiotemporal kernel tailored for non-stationary skill performance models. The proposed method is agnostic to policy and skill representation and scales independently of task dimensionality. We evaluate it on a non-linear simulated catapult control problem over arbitrarily mountainous terrains. Copyright 2014 by the author(s).",,"School of Computer Science, University of Massachusetts, Amherst, MA, United States; Computer Science and Artificial Intelligence Lab., MIT, Cambridge, MA, United States",,,"Da Silva B.C., Konidaris G., Barto A.","Da Silva, B.C., School of Computer Science, University of Massachusetts, Amherst, MA, United States; Konidaris, G., Computer Science and Artificial Intelligence Lab., MIT, Cambridge, MA, United States; Barto, A., School of Computer Science, University of Massachusetts, Amherst, MA, United States",4,,,SCOPUS,0,,,,,,,,,,,,,"31st International Conference on Machine Learning, ICML 2014",,,2-s2.0-84919785739,,,,,,3745,3736,,,,,Scopus,,,"31st International Conference on Machine Learning, ICML 2014",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919785739&partnerID=40&md5=db1f1a55c3825562fd59e4582f00757c,,5,,2014,,,,,,,,,,,,,,,,,,,
Gesture controllers,"We introduce gesture controllers, a method for animating the body language of avatars engaged in live spoken conversation. A gesture controller is an optimal-policy controller that schedules gesture animations in real time based on acoustic features in the user's speech. The controller consists of an inference layer, which infers a distribution over a set of hidden states from the speech signal, and a control layer, which selects the optimal motion based on the inferred state distribution. The inference layer, consisting of a specialized conditional random field, learns the hidden structure in body language style and associates it with acoustic features in speech. The control layer uses reinforcement learning to construct an optimal policy for selecting motion clips from a distribution over the learned hidden states. The modularity of the proposed method allows customization of a character's gesture repertoire, animation of non-human characters, and the use of additional inputs such as speech recognition or direct user control. © 2010 ACM.",,"Stanford University, United States",124,,"Levine S., Krähenbühl P., Thrun S., Koltun V.","Levine, S., Stanford University, United States; Krähenbühl, P., Stanford University, United States; Thrun, S., Stanford University, United States; Koltun, V., Stanford University, United States",37,,10.1145/1778765.1778861,SCOPUS,1,,,,,,,,,,4,,,ACM Transactions on Graphics,Data-driven animation; Gesture synthesis; Human animation; Nonverbal behavior generation; Optimal control,,2-s2.0-77956388261,,,,,,,,,,,,Scopus,,,ACM Transactions on Graphics,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956388261&doi=10.1145%2f1778765.1778861&partnerID=40&md5=c5626a3962e7e2f070413dd84ee11ad0,,29,,2010,,,,,,,,,,,,,,,,,,,
Improving management of Anemia in End Stage Renal Disease using Reinforcement Learning,We present a reinforcement learning approach to elicit individualized dose adjustment policies for patients suffering anemia due to end stage renal disease. Our goal is to achieve stable steady-state anemia management in patients with exhibiting different levels of treatment response. The approach uses Q-learning with parsimonious parametric representation of the state-action value function. We show that this approach achieves stability even in highly responsive patients.,,,,"Department of Medicine, Division of Nephrology, University of Louisville, USA",A. E. Gaweda,,1,,10.1109/IJCNN.2009.5179004,IEEE Xplore,1,,20090731,958,,Automatic control;Cardiac disease;Conference management;Function approximation;Humans;Learning;Medical treatment;Neural networks;Protocols;Steady-state,diseases;kidney;learning (artificial intelligence);medical computing;patient treatment,Q-learning;end stage renal disease;individualized dose adjustment policy;parsimonious parametric representation;patient treatment;reinforcement learning;stable steady-state anemia management;state-action value function,,2161-4393;21614393,,14-19 June 2009,,2009 International Joint Conference on Neural Networks,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5179004,,,,,,IEEE,14,,POD:978-1-4244-3548-7,,2009 International Joint Conference on Neural Networks,953,IEEE Conferences,,,,,2009,,,,,,,,,,,,,,,,,,,
Recurrent neural-network training by a learning automaton approach for trajectory learning and control system design,"We present a training approach using concepts from the theory of stochastic learning automata that eliminates the need for computation of gradients. This approach also offers the flexibility of tailoring a number of specific training algorithms based on the selection of linear and nonlinear reinforcement rules for updating automaton action probabilities. The training efficiency is demonstrated by application to two complex temporal learning scenarios, viz, learning of time-dependent continuous trajectories and feedback controller designs for continuous dynamical plants. For the first problem, it is shown that training algorithms can be tailored following the present approach for a recurrent neural net to learn to generate a benchmark circular trajectory more accurately than possible with existing gradient-based training procedures. For the second problem, it is shown that recurrent neural-network-based feedback controllers can be trained for different control objectives",,,,"Dept. of Electr. & Comput. Eng., Arizona Univ., Tucson, AZ, USA",M. K. Sudareshan; T. A. Condarcure,,22,,10.1109/72.668879,IEEE Xplore,1,,20020806,368,,Adaptive control;Application software;Automatic control;Backpropagation;Computational complexity;Control systems;Learning automata;Microcomputers;Recurrent neural networks;Stochastic processes,control system synthesis;learning automata;learning systems;neurocontrollers;recurrent neural nets,control system design;feedback control;learning systems;recurrent neural-network;reinforcement rules;stochastic learning automaton;temporal learning;trajectory learning,,1045-9227;10459227,3,May 1998,,IEEE Transactions on Neural Networks,,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=668879,,,,,,IEEE,45,,,,IEEE Transactions on Neural Networks,354,IEEE Journals & Magazines,,,9,,1998,,,,,,,,,,,,,,,,,,,
A Unified Contextual Bandit Framework for Long- and Short-Term Recommendations,"We present a unified contextual bandit framework for recommendation problems that is able to capture long- and short-term interests of users. The model is devised in dual space and the derivation is consequentially carried out using Fenchel-Legrende conjugates and thus leverages to a wide range of tasks and settings. We detail two instantiations for regression and classification scenarios and obtain well-known algorithms for these special cases. The resulting general and unified framework allows for quickly adapting contextual bandits to different applications at-hand. The empirical study demonstrates that the proposed long- and short-term framework outperforms both, short-term and long-term models on data. Moreover, a tweak of the combined model proves beneficial in cold start problems. © 2017, Springer International Publishing AG.",,"Leuphana Universität Lüneburg, Lüneburg, Germany; Technische Universität Darmstadt, Darmstadt, Germany",,,"Tavakol M., Brefeld U.","Tavakol, M., Leuphana Universität Lüneburg, Lüneburg, Germany, Technische Universität Darmstadt, Darmstadt, Germany; Brefeld, U., Leuphana Universität Lüneburg, Lüneburg, Germany",,,10.1007/978-3-319-71246-8_17,SCOPUS,1,,,,,,,,,,,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Contextual bandits; Dual optimization; Personalization; Recommendation,,2-s2.0-85040222425,,,,,,284,269,,,,,Scopus,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040222425&doi=10.1007%2f978-3-319-71246-8_17&partnerID=40&md5=e4d3e682219cd10a53886c2234a9f5d5,,10535 LNAI,,2017,,,,,,,,,,,,,,,,,,,
Goal-directed online learning of predictive models,"We present an algorithmic approach for integrated learning and planning in predictive representations. The approach extends earlier work on predictive state representations to the case of online exploration, by allowing exploration of the domain to proceed in a goal-directed fashion and thus be more efficient. Our algorithm interleaves online learning of the models, with estimation of the value function. The framework is applicable to a variety of important learning problems, including scenarios such as apprenticeship learning, model customization, and decision-making in non-stationary domains. © 2012 Springer-Verlag.",,"School of Computer Science, McGill University, Montreal, QC, Canada",,,"Ong S.C.W., Grinberg Y., Pineau J.","Ong, S.C.W., School of Computer Science, McGill University, Montreal, QC, Canada; Grinberg, Y., School of Computer Science, McGill University, Montreal, QC, Canada; Pineau, J., School of Computer Science, McGill University, Montreal, QC, Canada",3,,10.1007/978-3-642-29946-9_6,SCOPUS,1,,,,,,,,,,,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),model-based reinforcement learning; online learning; predictive state representation,,2-s2.0-84861691760,,,,,,29,18,,,,,Scopus,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861691760&doi=10.1007%2f978-3-642-29946-9_6&partnerID=40&md5=cb911c2456dde19107bbd0f1fe48349f,,7188 LNAI,,2012,,,,,,,,,,,,,,,,,,,
An auction-based approach to spectrum allocation using multi-agent reinforcement learning,"We present an auction-based approach to spectrum management in a multi-operator context. Service providers compete for customers in real-time through live auctions. To automate the bidding process we implement a multi-agent reinforcement learning solution. We study the effect of real-time competition between service providers by considering the cases where there is a single provider and multiple providers. Furthermore, we demonstrate how users of varying types, based on application-type and willingness to pay, can be accommodated. We utilize a low-complexity bid-proportional allocation mechanism which ensures fairness. Our simulation results show that when there is a single provider, revenue can be maximized by artificially limiting supply and creating contention. However, when there are multiple providers from which the customers can dynamically choose, there is no longer an incentive to restrict supply due to the direct competition between service providers.",,,,"Dept. of Electrical and Computer Engineering, University of Toronto, Italy",N. Abji; A. Leon-Garcia,,4,,10.1109/PIMRC.2010.5671682,IEEE Xplore,1,,20101217,2238,,Land mobile radio,learning (artificial intelligence);multi-agent systems;radio spectrum management;telecommunication computing,auction-based approach;bidding process;low-complexity bid-proportional allocation mechanism;multiagent reinforcement learning;service providers;spectrum allocation;spectrum management,,2166-9570;21669570,,26-30 Sept. 2010,,"21st Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5671682,,,,,,IEEE,10,,Electronic:978-1-4244-8016-6; POD:978-1-4244-8017-3,,"21st Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications",2233,IEEE Conferences,,,,,2010,,,,,,,,,,,,,,,,,,,
Spectrum markets for service provider spectrum trading with reinforcement learning,We present an auction-based spectrum market approach to service provider spectrum trading. Service providers buy and sell spectrum amongst one another in a spectrum market and simultaneously compete for customers from a common pool. Multi-agent reinforcement learning solutions are applied in both customer nodes and service providers to dynamically manage participation in the market. We outline four possible regulatory scenarios with varying degrees of flexibility and competition. Simulations demonstrate that the allocation of spectrum is efficient and fair. Customers and service providers of varying size are shown to benefit from this approach while the system spectrum efficiency is also significantly improved.,,,,"University of Toronto, Canada",N. Abji; A. Leon-Garcia,,2,,10.1109/PIMRC.2011.6140043,IEEE Xplore,1,,20120126,655,,Base stations;FCC;Learning;Radio spectrum management;Real time systems;Resource management;Wireless communication,cognitive radio;multi-agent systems;telecommunication services,customer providers;multiagent reinforcement learning solutions;reinforcement learning;service provider spectrum trading,,2166-9570;21669570,,11-14 Sept. 2011,,"2011 IEEE 22nd International Symposium on Personal, Indoor and Mobile Radio Communications",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140043,,,,,,IEEE,11,,Electronic:978-1-4577-1348-4; POD:978-1-4577-1346-0; USB:978-1-4577-1347-7,,"2011 IEEE 22nd International Symposium on Personal, Indoor and Mobile Radio Communications",650,IEEE Conferences,,,,,2011,,,,,,,,,,,,,,,,,,,
Learning tactile skills through curious exploration,"We present curiosity-driven, autonomous acquisition of tactile exploratory skills on a biomimetic robot finger equipped with an array of microelectromechanical touch sensors. Instead of building tailored algorithms for solving a specific tactile task, we employ a more general curiosity-driven reinforcement learning approach that autonomously learns a set of motor skills in absence of an explicit teacher signal. In this approach, the acquisition of skills is driven by the information content of the sensory input signals relative to a learner that aims at representing sensory inputs using fewer and fewer computational resources. We show that, from initially random exploration of its environment, the robotic system autonomously develops a small set of basic motor skills that lead to different kinds of tactile input. Next, the system learns how to exploit the learned motor skills to solve supervised texture classification tasks. Our approach demonstrates the feasibility of autonomous acquisition of tactile skills on physical robotic platforms through curiosity-driven reinforcement learning, overcomes typical difficulties of engineered solutions for active tactile exploration and underactuated control, and provides a basis for studying developmental learning through intrinsic motivation in robots. © 2012 Pape, Oddo, Controzzi, Cipriani, Förster, Carrozza and Schmidhuber.",,"Istituto Dalle Molle di Studi sull'Intelligenza Artificiale, Università della Svizzera Italiana, Lugano, Switzerland; The BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy", Article 6,,"Pape L., Oddo C.M., Controzzi M., Cipriani C., Förster A., Carrozza M.C., Schmidhuber J.","Pape, L., Istituto Dalle Molle di Studi sull'Intelligenza Artificiale, Università della Svizzera Italiana, Lugano, Switzerland; Oddo, C.M., The BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy; Controzzi, M., The BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy; Cipriani, C., The BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy; Förster, A., Istituto Dalle Molle di Studi sull'Intelligenza Artificiale, Università della Svizzera Italiana, Lugano, Switzerland; Carrozza, M.C., The BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy; Schmidhuber, J., Istituto Dalle Molle di Studi sull'Intelligenza Artificiale, Università della Svizzera Italiana, Lugano, Switzerland",20,,10.3389/fnbot.2012.00006,SCOPUS,1,,,,,,,,,,JULY,,,Frontiers in Neurorobotics,Active learning; Biomimetic robotics; Curiosity; Intrinsic motivation; Reinforcement learning; Skill learning; Tactile sensing,,2-s2.0-84866027577,,,,,,,,,,,,Scopus,,,Frontiers in Neurorobotics,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866027577&doi=10.3389%2ffnbot.2012.00006&partnerID=40&md5=10f5035af6eb9cf4b2a2d65efcc15ce3,,,,2012,,,,,,,,,,,,,,,,,,,
Smartphone Interruptibility Using Density-Weighted Uncertainty Sampling with Reinforcement Learning,"We present the In-Context application for smart-phones, which combines signal processing, active learning, and reinforcement learning to autonomously create a personalized model of interruptibility for incoming phone calls. We empirically evaluate the system, and show that we can obtain an average of 96.12% classification accuracy when predicting interruptibility after a week of training. In contrast to previous work, we leverage density-weighted uncertainty sampling combined with a reinforcement learning framework applied to passively collected data to achieve comparable or superior classification accuracy using many fewer queries issued to the user.",,,,"Machine Learning Dept., Carnegie Mellon Univ., Pittsburgh, PA, USA",R. Fisher; R. Simmons,,4,,10.1109/ICMLA.2011.128,IEEE Xplore,1,,20120209,441,,Accuracy;Context;Data mining;Feature extraction;Support vector machines;Switches;Uncertainty,learning (artificial intelligence);smart phones,active learning;density weighted uncertainty sampling;reinforcement learning;signal processing;smartphone interruptibility,,,,18-21 Dec. 2011,,2011 10th International Conference on Machine Learning and Applications and Workshops,Active learning;interruptibility;mobile devices;reinforcement learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147012,,,,,,IEEE,20,,Electronic:978-0-7695-4607-0; POD:978-1-4577-2134-2,,2011 10th International Conference on Machine Learning and Applications and Workshops,436,IEEE Conferences,,,1,,2011,,,,,,,,,,,,,,,,,,,
Context-aware unified routing for VANETs based on virtual clustering,"We propose a context-aware routing protocol for vehicular ad hoc networks (VANETs). Two types of context information is considered in this paper specifically communication type (broadcast or unicast) and packet size. The proposed protocol constructs route based on virtual clustering which only exchanges beacon messages in one-hop neighborhood area. The packets are forwarded by the cluster heads, and the last 2-hop route is optimized by using a reinforcement learning algorithm which can attain good performance with low overhead. The advantage of the proposed protocol is shown by using computer simulations.",,,,"National Institute of Informatics, Tokyo, Japan",Y. Ji; C. Wu; T. Yoshinaga,,,,10.1109/PIMRC.2016.7794599,IEEE Xplore,1,,20161222,6,,Conferences;Context;Payloads;Routing protocols;Unicast;Vehicles,learning (artificial intelligence);mobile computing;pattern clustering;routing protocols;telecommunication computing;vehicular ad hoc networks,VANET;computer simulation;context-aware unified routing protocol;one-hop neighborhood area;packet size;reinforcement learning algorithm;vehicular ad hoc network;virtual clustering,,,,4-8 Sept. 2016,,"2016 IEEE 27th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7794599,,,,,,IEEE,,,Electronic:978-1-5090-3254-9; POD:978-1-5090-3255-6,,"2016 IEEE 27th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",1,IEEE Conferences,,,,,2016,,,,,,,,,,,,,,,,,,,
A dialogue game framework with personalized training using reinforcement learning for computer-assisted language learning,"We propose a framework for computer-assisted language learning as a pedagogical dialogue game. The goal is to offer personalized learning sentences on-line for each individual learner considering the learner's learning status, in order to strike a balance between more practice on poorly-pronounced units and complete practice on the whole set of pronunciation units. This objective is achieved using a Markov decision process (MDP) trained with reinforcement learning using simulated learners generated from real learner data. Preliminary experimental results on a subset of the example dialogue script show the effectiveness of the framework.",,,,"Graduate Institute of Communication Engineering, National Taiwan University, Taiwan",P. h. Su; Y. B. Wang; T. h. Yu; L. s. Lee,,6,,10.1109/ICASSP.2013.6639266,IEEE Xplore,1,,20131021,8217,,Educational institutions;Games;Hidden Markov models;Learning (artificial intelligence);Markov processes;Speech;Training,computer aided instruction;computer games;learning (artificial intelligence);natural languages;speech recognition,Markov decision process;computer assisted language learning;dialogue game framework;pedagogical dialogue game;personalized sentence learning;personalized training;reinforcement learning;simulated learner,,1520-6149;15206149,,26-31 May 2013,,"2013 IEEE International Conference on Acoustics, Speech and Signal Processing",Computer-Assisted Language Learning;Dialogue Game;Markov Decision Process;Reinforcement Learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6639266,,,,,,IEEE,35,,Electronic:978-1-4799-0356-6; POD:978-1-4799-0357-3; USB:978-1-4799-0355-9,,"2013 IEEE International Conference on Acoustics, Speech and Signal Processing",8213,IEEE Conferences,,,,,2013,,,,,,,,,,,,,,,,,,,
Greedy outcome weighted tree learning of optimal personalized treatment rules,"We propose a subgroup identification approach for inferring optimal and interpretable personalized treatment rules with high-dimensional covariates. Our approach is based on a two-step greedy tree algorithm to pursue signals in a high-dimensional space. In the first step, we transform the treatment selection problem into a weighted classification problem that can utilize tree-based methods. In the second step, we adopt a newly proposed tree-based method, known as reinforcement learning trees, to detect features involved in the optimal treatment rules and to construct binary splitting rules. The method is further extended to right censored survival data by using the accelerated failure time model and introducing double weighting to the classification trees. The performance of the proposed method is demonstrated via simulation studies, as well as analyses of the Cancer Cell Line Encyclopedia (CCLE) data and the Tamoxifen breast cancer data. © 2016, The International Biometric Society",,"University of Illinois at Urbana-Champaign, Champaign, IL, United States; Fred Hutchinson Cancer Research Center, Seattle, WA, United States; Vanderbilt University, Nashville, TN, United States; Yale University, New Haven, CT, United States",,,"Zhu R., Zhao Y.-Q., Chen G., Ma S., Zhao H.","Zhu, R., University of Illinois at Urbana-Champaign, Champaign, IL, United States; Zhao, Y.-Q., Fred Hutchinson Cancer Research Center, Seattle, WA, United States; Chen, G., Vanderbilt University, Nashville, TN, United States; Ma, S., Yale University, New Haven, CT, United States; Zhao, H., Yale University, New Haven, CT, United States",1,,10.1111/biom.12593,SCOPUS,1,,,,,,,,,,2,,,Biometrics,High-dimensional data; Optimal treatment rules; Personalized medicine; Reinforcement learning trees; Survival analysis; Tree-based method,,2-s2.0-84994875692,,,,,,400,391,,,,,Scopus,,,Biometrics,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994875692&doi=10.1111%2fbiom.12593&partnerID=40&md5=f7fb0d98e7a8421f1d12ffbc09a05fee,,73,,2017,,,,,,,,,,,,,,,,,,,
Video summarization using reinforcement learning in eigenspace,"We propose video summarization using reinforcement learning. The importance score of each frame in a video is calculated from the user's actions in handling similar previous frames; if such frames were watched rather than skipped, a high score is assigned. To calculate the score, instead of using raw feature vectors extracted from images, we use feature vectors projected on eigenspace: as a result, we can deal with the features comprehensively. We also give an algorithm that uses the reinforcement learning method to create a personalized video summary. The summarization algorithm is applied to a soccer video to confirm its effectiveness.",,,,"IBM Tokyo Res. Lab., Kanagawa, Japan",K. Masumitsu; T. Echigo,,4,,10.1109/ICIP.2000.899351,IEEE Xplore,1,,20020806,270 vol.2,,Data mining;Feature extraction;Information retrieval;Joining processes;Laboratories;Layout;Learning;Multimedia communication;TV broadcasting;Watches,feature extraction;image sequences;learning (artificial intelligence);video signal processing,algorithm;eigenspace;feature vectors extraction;personalized video summary;reinforcement learning;soccer video;summarization algorithm;video frame;video summarization,,1522-4880;15224880,,10-13 Sept. 2000,,Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101),,,,10 Sep 2000-13 Sep 2000,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=899351,,,,,3,IEEE,7,,POD:0-7803-6297-7,,Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101),267,IEEE Conferences,,,2,,2000,,,,,,,,,,,,,,,,,,,
"Probabilistic topic modeling, reinforcement learning, and crowdsourcing for personalized recommendations","We put forward an innovative use of probabilistic topic modeling (PTM) intertwined with reinforcement learning (RL), to provide personalized recommendations. Specifically, we model items under recommendation as mixtures of latent topics following a distribution with Dirichlet priors; this can be achieved via the exploitation of crowd-sourced information for each item. Similarly, we model the user herself as an “evolving” document represented by its respective mixture of latent topics. The user’s topic distribution is appropriately updated each time she consumes an item. Recommendations are subsequently based on the divergence between the topic distributions of the user and available items. However, to tackle the exploration versus exploitation dilemma, we apply RL to vary the user’s topic distribution update rate. Our method is immune to the notorious “cold start” problem, and it can effectively cope with changing user preferences. Moreover, it is shown to be competitive against state-of-the-art algorithms, outperforming them in terms of sequential performance. © Springer International Publishing AG 2017.",,"School of Electrical and Computer Engineering, Technical University of Crete, Chania, Greece",,,"Tripolitakis E., Chalkiadakis G.","Tripolitakis, E., School of Electrical and Computer Engineering, Technical University of Crete, Chania, Greece; Chalkiadakis, G., School of Electrical and Computer Engineering, Technical University of Crete, Chania, Greece",,,10.1007/978-3-319-59294-7_14,SCOPUS,1,,,,,,,,,,,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Applications of reinforcement learning; Crowdsourcing; Graphical models; Recommender systems,,2-s2.0-85022214474,,,,,,171,157,,,,,Scopus,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022214474&doi=10.1007%2f978-3-319-59294-7_14&partnerID=40&md5=e9c8b09a9368f99a2d652b2b8d3a9e8e,,10207 LNAI,,2017,,,,,,,,,,,,,,,,,,,
Swarm reinforcement learning algorithm based on particle swarm optimization whose personal bests have lifespans,"We recently proposed a swarm reinforcement learning algorithm based on particle swarm optimization (PSO) in order to find optimal policies rapidly. In this algorithm, multiple agents are prepared, and they learn not only by individual learning but also by an update procedure of PSO. In this procedure, state-action values are updated based on the personal best and the global best which are found by the agents so far. In this paper, we direct our attention to a problem that overvaluing personal bests brings inferior learning performance. In order not to update the state-action values based on the overvalued personal best, we propose a swarm reinforcement learning algorithm based on PSO in which the personal best of each agent has a lifespan. © 2009 Springer-Verlag Berlin Heidelberg.",,"Kyoto Institute of Technology, Matsugasaki, Sakyo-ku, Kyoto, Japan",,,"Iima H., Kuroe Y.","Iima, H., Kyoto Institute of Technology, Matsugasaki, Sakyo-ku, Kyoto, Japan; Kuroe, Y., Kyoto Institute of Technology, Matsugasaki, Sakyo-ku, Kyoto, Japan",1,,10.1007/978-3-642-10684-2_19,SCOPUS,1,,,,,,,,,,PART 2,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Particle swarm optimization; Reinforcement learning; Swarm intelligence,,2-s2.0-76249096804,,,,,,178,169,,,,,Scopus,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249096804&doi=10.1007%2f978-3-642-10684-2_19&partnerID=40&md5=ddde3d9d2f0fa887419a160c5928a5d5,,5864 LNCS,,2009,,,,,,,,,,,,,,,,,,,
Learning dialogue strategies for interactive database search,"We show how to learn optimal dialogue policies for a wide range of database search applications, concerning how many database search results to present to the user, and when to present them. We use Reinforcement Learning methods for a wide spectrum of different database simulations, turn penalty conditions, and noise conditions. Our objective is to show that our policy learning framework covers this spectrum. We can show that even for challenging cases learning significantly outperforms hand-coded policies tailored to the different operating situations. The polices are adaptive/context-sensitive in respect of both the overall operating situation (e.g. noise) and the local context of the interaction (e.g. user's last move). The learned policies produce an average relative increase in reward of 25.7% over the corresponding threshold-based hand-coded baseline policies.",,"Department of Computational Linguistics, Saarland University, Saarbrücken, Germany; School of Informatics, University of Edinburgh, Edinburgh, United Kingdom",,,"Rieser V., Lemon O.","Rieser, V., Department of Computational Linguistics, Saarland University, Saarbrücken, Germany; Lemon, O., School of Informatics, University of Edinburgh, Edinburgh, United Kingdom",,,,SCOPUS,0,,,,,,,,,,,,,"International Speech Communication Association - 8th Annual Conference of the International Speech Communication Association, Interspeech 2007",Adaptive strategies; Database search; Dialogue systems; Multimodality; Reinforcement learning,,2-s2.0-56149117477,,,,,,2044,2041,,,,,Scopus,,,"International Speech Communication Association - 8th Annual Conference of the International Speech Communication Association, Interspeech 2007",,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56149117477&partnerID=40&md5=0223cd1a2d0ddeecbd0b38a429aa35eb,,3,,2007,,,,,,,,,,,,,,,,,,,
Reinforcement learning and savings behavior,"We show that individual investors over-extrapolate from their personal experience when making savings decisions. Investors who experience particularly rewarding outcomes from 401(k) saving - a high average and/or low variance return - increase their 401(k) savings rate more than investors who have less rewarding experiences. This finding is not driven by aggregate time-series shocks, income effects, rational learning about investing skill, investor fixed effects, or time-varying investor-level heterogeneity that is correlated with portfolio allocations to stock, bond, and cash asset classes. We discuss implications for the equity premium puzzle and interventions aimed at improving household financial outcomes. © 2009 the American Finance Association.",,"Yale School of Management and, NBER, United States; Harvard University, Department of Economics, NBER, United States; Harvard Kennedy School, NBER, United States; Yale School of Management, NBER, United States",,,"Choi J.J., Laibson D., Madrian B.C., Metrick A.","Choi, J.J., Yale School of Management and, NBER, United States; Laibson, D., Harvard University, Department of Economics, NBER, United States; Madrian, B.C., Harvard Kennedy School, NBER, United States; Metrick, A., Yale School of Management, NBER, United States",61,,10.1111/j.1540-6261.2009.01509.x,SCOPUS,1,,,,,,,,,,6,,,Journal of Finance,,,2-s2.0-72249109179,,,,,,2534,2515,,,,,Scopus,,,Journal of Finance,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72249109179&doi=10.1111%2fj.1540-6261.2009.01509.x&partnerID=40&md5=4f2de206966b9a672be7e0bfe478ef04,,64,,2009,,,,,,,,,,,,,,,,,,,
Crises and confidence: Systemic banking crises and depositor behavior,"We show that individuals who have experienced a systemic banking crisis are 11 percentage points less likely to use banks in the U.S. than otherwise similar individuals who emigrated from the same country but did not live through a crisis. This finding is robust to controlling for exposure to other macroeconomic events and to various methods for addressing potential bias due to migrant self-selection. Consistent with the view that personal experience plays an important role in decision-making, the effects are larger for individuals who were older and more likely to have had wealth entrusted to the banking system at the time of the crisis and for people who experienced crises in countries without deposit insurance. © 2013 Elsevier B.V.",,"Indiana University-Purdue University at Indianapolis, United States; Federal Reserve Bank of Chicago, 230 S. LaSalle Street, Chicago, IL 60604, United States",,,"Osili U.O., Paulson A.","Osili, U.O., Indiana University-Purdue University at Indianapolis, United States; Paulson, A., Federal Reserve Bank of Chicago, 230 S. LaSalle Street, Chicago, IL 60604, United States",8,,10.1016/j.jfineco.2013.11.002,SCOPUS,1,,,,,,,,,,3,,,Journal of Financial Economics,Confidence; Deposit insurance; Immigrants; Reinforcement learning; Systemic banking crises,,2-s2.0-84893678217,,,,,,660,646,,,,,Scopus,,,Journal of Financial Economics,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893678217&doi=10.1016%2fj.jfineco.2013.11.002&partnerID=40&md5=95bbc174fc50998df2c38a801cd6c9a0,,111,,2014,,,,,,,,,,,,,,,,,,,
Markov decision process based adaptive web advertisements scheduling,"We study web advertisements scheduling problem by fully considering the interaction of web users and web advertisements publishing system. We construct a Markov Decision Process (MDP) based web advertisements scheduling model and schedule advertisements publishing during the whole process of web surfing by the users, thus we make maximal use of personal behavior characteristics of every web user in the scheduling model. We also track the user habit with reinforcement learning, solve the MDP model by TD(λ) algorithm combing the function approximator, and obtain adaptive online scheduling policies for web advertisements publishing. © (2013) Trans Tech Publications, Switzerland.",,"Department of Industrial Engineering, Dongguan University of Technology, Dongguan 523808, China",,,"Zhang Z.C., Li S., Hu K.S., Huang H.Y., Zhao S.Y.","Zhang, Z.C., Department of Industrial Engineering, Dongguan University of Technology, Dongguan 523808, China; Li, S., Department of Industrial Engineering, Dongguan University of Technology, Dongguan 523808, China; Hu, K.S., Department of Industrial Engineering, Dongguan University of Technology, Dongguan 523808, China; Huang, H.Y., Department of Industrial Engineering, Dongguan University of Technology, Dongguan 523808, China; Zhao, S.Y., Department of Industrial Engineering, Dongguan University of Technology, Dongguan 523808, China",,,10.4028/www.scientific.net/AMR.765-767.1436,SCOPUS,1,,,,,,,,,,,,,Advanced Materials Research,Reinforcement learning; Scheduling; Web advertisements,,2-s2.0-84885049231,,,,,,1440,1436,,,,,Scopus,,,Advanced Materials Research,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885049231&doi=10.4028%2fwww.scientific.net%2fAMR.765-767.1436&partnerID=40&md5=8ff8e90a548dd3e734a60cb6c1df0296,,765-767,,2013,,,,,,,,,,,,,,,,,,,
Intelligent agent for E-tourism: Personalization travel support agent using reinforcement learning,"Web personalization and one to one marketing have been introduced as strategy and marketing tools. By using historical and present information of customers, organizations can learn, predict customer's behaviors and develop products to fit potential customers. In this study, a Personalization Travel Support System is introduced to manage traveling information for user. It provides the information that matches the users' interests. This system applies the Reinforcement Learning to analyze, learn customer behaviors and recommend products to meet customer interests. There are two learning approaches using in this study. First, Personalization Learner by Group Properties is learning from all users in one group to find the group interests of travel information by using given data on user ages and genders. Second, Personalization Learner by User Behavior: user profile, user behaviors and trip features will be analyzed to find the unique interest of each web user. The results from this study reveal that it is possible to develop Personalization Travel Support System. Using weighted trip features improve effectiveness and increase the accuracy of the personalized engine. Precision, Recall and Harmonic Mean of the learned system are higher than the original one. This study offers useful information regarding the areas of personalization of web support system.",,"Department of Computer Science, Faculty of Science, Kasetsart University, Bangkok 10900, Thailand",,,"Srivihok A., Sukonmanee P.","Srivihok, A., Department of Computer Science, Faculty of Science, Kasetsart University, Bangkok 10900, Thailand; Sukonmanee, P., Department of Computer Science, Faculty of Science, Kasetsart University, Bangkok 10900, Thailand",,,,SCOPUS,0,,,,,,,,,,,,,CEUR Workshop Proceedings,Intelligent agent; Personalization; Recommendation algorithm; Reinforcement Learning,,2-s2.0-84883477666,,,,,4,,,,,,,Scopus,,,CEUR Workshop Proceedings,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883477666&partnerID=40&md5=b8091062d9e61ff7714e961f19c584d9,,143,,2005,,,,,,,,,,,,,,,,,,,
Teleporting universal intelligent agents,"When advanced AIs begin to choose their own destiny, one decision they will need to make is whether or not to transfer or copy themselves (software and memory) to new hardware devices. For humans this possibility is not (yet) available and so it is not obvious how such a question should be approached. Furthermore, the traditional single-agent reinforcement-learning framework is not adequate for exploring such questions, and so we base our analysis on the ""multi-slot"" framework introduced in a companion paper. In the present paper we attempt to understand what an AI with unlimited computational capacity might choose if presented with the option to transfer or copy itself to another machine. We consider two rigorously executed formal thought experiments deeply related to issues of personal identity: one where the agent must choose whether to be copied into a second location (called a""slot""), and another where the agent must make this choice when, after both copies exist, one of them will be deleted. These decisions depend on what the agents believe their futures will be, which in turn depends on the definition of their value function, and we provide formal results. © 2014 Springer International Publishing.",,"AgroParisTech, UMR 518 MIA, F-75005 Paris, France; INRA, UMR 518 MIA, F-75005 Paris, France",,,Orseau L.,"Orseau, L., AgroParisTech, UMR 518 MIA, F-75005 Paris, France, INRA, UMR 518 MIA, F-75005 Paris, France",2,,10.1007/978-3-319-09274-4_11,SCOPUS,1,,,,,,,,,,,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),AIXI; identity; teleportation; Universal AI,,2-s2.0-84905814917,,,,,,120,109,,,,,Scopus,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905814917&doi=10.1007%2f978-3-319-09274-4_11&partnerID=40&md5=790e058f08f7a6b8603c7a63a7b9255e,,8598 LNAI,,2014,,,,,,,,,,,,,,,,,,,
Towards a General Supporting Framework for Self-Adaptive Software Systems,"When confronted with their internal and environmental dynamics, various software systems increasingly require self-adaptation capabilities. The vision above requires the self-adaptation approach to tackle these challenges and some software systems have their own specific implementations. However, in this paper a general supporting framework is proposed for software systems to self-adapt with running environmental dynamics and meanwhile fulfill various user requirements. Three key issues are covered in the framework: 1) the overall control architecture, which adopts the double closed-loop style and respectively includes the self-adaptation loop and the self-learning loop; 2) a general descriptive language, which is a application-independent and unified language to represent self-adaptation knowledge about target systems; 3) three implementation mechanisms, including forward reasoning, planning and reinforcement learning using feedback, which are supported by the above descriptive language and executed at runtime in different modules. Finally, one scenario of on-demand services of massive data mining tasks is selected and the case study demonstrates how the framework is customized as required and how the approach works.",,,,"State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China",L. Wang; Y. Gao; C. Cao; L. Wang,,2,,10.1109/COMPSACW.2012.38,IEEE Xplore,1,,20121110,163,,Adaptation models;Cognition;Computer architecture;Learning;Monitoring;Planning;Software systems,data mining;inference mechanisms;software engineering;unsupervised learning,data mining;environmental dynamics;forward reasoning;general descriptive language;general supporting framework;on-demand service;overall control architecture;reinforcement learning;self-adaptation knowledge;self-adaptation loop;self-adaptive software system;self-learning loop,,,,16-20 July 2012,,2012 IEEE 36th Annual Computer Software and Applications Conference Workshops,Double Closed-loop Control Arthitecture;General Descriptive Language;Hierarchical Task Network;Rete Algorithm;Self-Adaptive Supporting Framework;Self-Adaptive System,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341568,,,,,,IEEE,18,,Electronic:978-0-7695-4758-9; POD:978-1-4673-2714-5,,2012 IEEE 36th Annual Computer Software and Applications Conference Workshops,158,IEEE Conferences,,,,,2012,,,,,,,,,,,,,,,,,,,
Distributed energy cooperation for energy harvesting nodes using reinforcement learning,"Wireless communication with nodes capable of harvesting energy emerges as a new technology challenge. In this paper, we investigate the problem of utilizing energy cooperation among energy-harvesting transmitters to maximize the data rate performance. We consider a general framework which can be applied to either cellular networks with base station energy cooperation through wired power grid or sensor networks with transmitting node energy cooperation through wireless power transfer. We model this energy cooperation problem as an infinite horizon Markov decision process (MDP), which can be optimally solved by the value iteration algorithm. Since the optimal value iteration algorithm has high complexity and requires non-causal information, we propose a distributed algorithm by using reinforcement learning and splitting the MDP into several small MDPs, each associated with a transmitter. Simulation results demonstrate the effectiveness of the proposed distributed energy cooperation algorithm.",,,,"Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan",W. T. Lin; I. W. Lai; C. H. Lee,,,,10.1109/PIMRC.2015.7343551,IEEE Xplore,1,,20151203,1588,,Batteries;Energy exchange;Power grids;Radio transmitters;Receivers;Wireless communication,Markov processes;cellular radio;computational complexity;energy harvesting;iterative methods;learning (artificial intelligence);power grids;radio transmitters;telecommunication computing;telecommunication power management,MDP;base station energy cooperation;cellular network;distributed energy cooperation;energy harvesting transmitter;infinite horizon Markov decision process;iteration algorithm;reinforcement learning;sensor network;wired power grid;wireless communication;wireless power transfer,,,,Aug. 30 2015-Sept. 2 2015,,"2015 IEEE 26th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7343551,,,,,,IEEE,16,,Electronic:978-1-4673-6782-0; POD:978-1-4673-6783-7,,"2015 IEEE 26th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",1584,IEEE Conferences,,,,,2015,,,,,,,,,,,,,,,,,,,
A tailored Q- Learning for routing in wireless sensor networks,Wireless sensor networks (WSNs) have major importance in distributed sensing applications. The important concern in the intend of wireless sensor networks is battery consumption which usually rely on non-renewable sources of energy. In this paper we have proposed a tailored Q-Learning algorithm for routing scheme in wireless sensor network. Our primary goal is to make an efficient routing algorithm with help of modified Q-Learning approach to minimize the energy consumption utilized by sensor nodes. This approach is a modified version of existing Q-Learning method for WSN that leads to the convergence problem.,,,,"Dept. of Comput. Sci. & Eng., Jaypee Polytech. & Training Centre, Rewa, India",V. K. Sharma; S. S. P. Shukla; V. Singh,,1,,10.1109/PDGC.2012.6449899,IEEE Xplore,1,,20130207,668,,Artificial neural networks;Lead;Wireless sensor networks,learning (artificial intelligence);telecommunication computing;telecommunication network routing;wireless sensor networks,Q- learning;Q-learning algorithm;WSN;battery consumption;distributed sensing applications;energy consumption;modified Q-learning approach;nonrenewable energy sources;routing algorithm;routing scheme;sensor nodes;wireless sensor network routing,,,,6-8 Dec. 2012,,"2012 2nd IEEE International Conference on Parallel, Distributed and Grid Computing",Convergence Problem;Q-Learning;Reinforcement learning;WSN Flooding Routing Protocol,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6449899,,,,,,IEEE,13,,Electronic:978-1-4673-2925-5; POD:978-1-4673-2922-4,,"2012 2nd IEEE International Conference on Parallel, Distributed and Grid Computing",663,IEEE Conferences,,,,,2012,,,,,,,,,,,,,,,,,,,
How to explore to maximize future return (invited talk),"With access to huge-scale distributed systems and more data than ever before, learning systems that learn to make good predictions break yesterday’s records on a daily basis. Although prediction problems are important, predicting what to do has its own challenges, which calls for specialized solution methods. In this talk, by means of some examples based on recent work on reinforcement learning, I will illustrate the unique opportunities and challenges that arise when a system must learn to make good decisions to maximize long-term return. In particular, I will start by demonstrating that passive data collection inevitably leads to catastrophic data sparsity in sequential decision making problems (no amount of data is big enough!), while clever algorithms, tailored to this setting, can escape data sparsity, learning essentially arbitrarily faster than what is possible under passive data collection. I will also describe current attempts to scale up such clever algorithms to work on large-scale problems. Amongst the possible approaches, I will discuss the role of sparsity to address this challenge in the practical, yet mathematically elegant setting of “linear bandits”. Interestingly, while in the related linear prediction problem, sparsity allows one to deal with huge dimensionality in a seamless fashion, the status of this question in the bandit setting is much less understood. © Springer International Publishing Switzerland 2015.",,"Department of Computing Science, University of Alberta, Edmonton, Canada",,,Wiebe J.,"Wiebe, J., Department of Computing Science, University of Alberta, Edmonton, Canada",,,,SCOPUS,0,,,,,,,,,,,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,,2-s2.0-84945580124,,,,,,,,,,,,Scopus,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945580124&partnerID=40&md5=00f775fd04c10724bcac7afdde539271,,9091,,2015,,,,,,,,,,,,,,,,,,,
A New Water and Carbon Conscious Electricity Market Model for the Electricity-Water-Climate Change Nexus,"With electricity, water, and climate change inextricably linked to each other, developing individualized policies or studying them in isolation is ineffectual and misguided. To understand the serious joint implications of the electricity-water-climate change nexus, the authors propose a conceptual framework of a joint carbon and water cap-and-trade model and present a multi-agent reinforcement learning-based predictive model to gain a deeper understanding of this nexus. © 2011 Elsevier Inc.",,"Department of Industrial Engineering at the University of Wisconsin-Milwaukee, University of South Florida, United States",,,"Nanduri V., Otieno W.","Nanduri, V., Department of Industrial Engineering at the University of Wisconsin-Milwaukee, University of South Florida, United States; Otieno, W., Department of Industrial Engineering at the University of Wisconsin-Milwaukee, University of South Florida, United States",7,,10.1016/j.tej.2011.09.021,SCOPUS,1,,,,,,,,,,9,,,Electricity Journal,,,2-s2.0-81155137757,,,,,,74,64,,,,,Scopus,,,Electricity Journal,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81155137757&doi=10.1016%2fj.tej.2011.09.021&partnerID=40&md5=10887ec2ca7863c2380da6bfe6a1dc5e,,24,,2011,,,,,,,,,,,,,,,,,,,
Learning-based ship design optimization approach,"With the development of computer applications in ship design, optimization, as a powerful approach, has been widely used in the design and analysis process. However, the running time, which often varies from several weeks to months in the current computing environment, has been a bottleneck problem for optimization applications, particularly in the structural design of ships. To speed up the optimization process and adjust the complex design environment, ship designers usually rely on their personal experience to assist the design work. However, traditional experience, which largely depends on the designer's personal skills, often makes the design quality very sensitive to the experience and decreases the robustness of the final design. This paper proposes a new machine-learning-based ship design optimization approach, which uses machine learning as an effective tool to give direction to optimization and improves the adaptability of optimization to the dynamic design environment. The natural human learning process is introduced into the optimization procedure to improve the efficiency of the algorithm. Q-learning, as an approach of reinforcement learning, is utilized to realize the learning function in the optimization process. The multi-objective particle swarm optimization method, multi-agent system, and CAE software are used to build an integrated optimization system. A bulk carrier structural design optimization was performed as a case study to evaluate the suitability of this method for real-world application. © 2011 Elsevier Ltd. All rights reserved.",,"Department of Naval Architecture and Marine Engineering, University of Strathclyde, Glasgow G4 0LZ, United Kingdom",,,"Cui H., Turan O., Sayer P.","Cui, H., Department of Naval Architecture and Marine Engineering, University of Strathclyde, Glasgow G4 0LZ, United Kingdom; Turan, O., Department of Naval Architecture and Marine Engineering, University of Strathclyde, Glasgow G4 0LZ, United Kingdom; Sayer, P., Department of Naval Architecture and Marine Engineering, University of Strathclyde, Glasgow G4 0LZ, United Kingdom",13,,10.1016/j.cad.2011.06.011,SCOPUS,1,,,,,,,,,,3,,,CAD Computer Aided Design,Machine learning; Multi-objective optimization; Ship design; Structure analysis; Structure optimization,,2-s2.0-84855691014,,,,,,195,186,,,,,Scopus,,,CAD Computer Aided Design,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855691014&doi=10.1016%2fj.cad.2011.06.011&partnerID=40&md5=77cd692740e8a4769780a062c5bfb89a,,44,,2012,,,,,,,,,,,,,,,,,,,
Analysis of re-sequencing buffer overflow probability based on stochastic delay characteristics,"With the development of multi-interface terminals, a host can connect to the Internet simultaneously by multiple access technologies. Under multi-access technology, a multi-path transmission can obtain high throughput, increased available bandwidth and enhanced reliability. However, the multi-path transmission with multi-access technology also has the problems that the packet re-ordering is unavoidable, and the fast retransmission is unnecessarily requested. Considering the stochastically varying transmission delay, the problems above may eventually result in a degradation of throughput. As a result, in this paper, we focus on the analysis of buffer overflow probability problem which is influenced by the transmission interval. First, we utilize Reinforcement Learning method to estimate the stochastic delay of end-to-end paths. Then, we discuss problems of re-sequencing buffer occupancy distribution and the overflow probability. In this paper, we model the stochastic delay as a continuous random variable, and then, discuss its mean value and variance. Simulation result shows that the re-sequencing buffer overflow probability is influenced by the transmission intervals and the variance of stochastic delay.",,,,"State Key Lab. of Integrated Service Networks, Xidian Univ., Xi'an, China",D. Zhou; H. Li; J. Li,,1,,10.1109/PIMRC.2013.6666565,IEEE Xplore,1,,20131125,2495,,Buffer storage;Delays;Estimation;Learning (artificial intelligence);Mathematical model;Sequential analysis;Stochastic processes,Internet;delays;learning (artificial intelligence);probability;random processes;stochastic processes,Internet;continuous random variable;end-to-end paths;mean value;mean variance;multiinterface terminals;multipath transmission;multiple access technology;packet re-ordering;re-sequencing buffer occupancy distribution problem;reinforcement learning method;resequencing buffer overflow probability analysis;stochastic delay characteristics;stochastic varying transmission delay;transmission interval,,2166-9570;21669570,,8-11 Sept. 2013,,"2013 IEEE 24th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",Concurrent Multi-path Transmission;Re-sequencing Buffer Overflow Probability;Reinforcement Learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6666565,,,,,,IEEE,20,,Electronic:978-1-4673-6235-1; POD:978-1-4673-6233-7; USB:978-1-4673-6234-4,,"2013 IEEE 24th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",2490,IEEE Conferences,,,,,2013,,,,,,,,,,,,,,,,,,,
Coordinating SON instances: Reinforcement learning with distributed value function,"With the emergence of Self-Organizing Network (SON) functions network operators are faced with a practical problem: coordination of SON instances. The SON functions are usually designed in a standalone manner, i.e. they do not take into account the possibility that other instances of the same or different SON functions may be running in the network. This creates the risk of conflicts and network instability. Therefore a SON COordinator (SONCO) is needed. In this paper we design an operator centric SONCO that sees the SON instances as black-boxes, i.e. it does not know the algorithm inside the SON functions. Our aim is to improve the network stability (i.e. number of parameter changes) for SON instances of the same SON function. We employ Reinforcement Learning (RL) in order to profit from the information on the past SONCO decisions. We simplify the expression of the action-value function and we use state aggregation to further reduce the required state space, making it scale linearly with the number of coordinated cells. We provide a study case with the Mobility Load Balancing (MLB) function independently instantiated on every cell. The results show that the proposed SONCO improves the network stability.",,,,"Orange Labs, 38-40 rue du General Leclerc 92130, Issy les Moulineaux, France",O. Iacoboaiea; B. Sayrac; S. Ben Jemaa; P. Bianchi,,2,,10.1109/PIMRC.2014.7136431,IEEE Xplore,1,,20150629,1646,,Algorithm design and analysis;Conferences;Heuristic algorithms;Kernel;Learning (artificial intelligence);Markov processes;Optimization,Long Term Evolution;cellular radio;learning (artificial intelligence);resource allocation;telecommunication computing,LTE;MLB;RL;SON instance coordination;SONCO operator centric design;action-value function;black-boxes;mobility load balancing coordinated cells;network stability;reinforcement learning;self-organizing network functions;state aggregation;state space,,2166-9570;21669570,,2-5 Sept. 2014,,"2014 IEEE 25th Annual International Symposium on Personal, Indoor, and Mobile Radio Communication (PIMRC)",Coordination;LTE;MLB;SON;SON instances;reinforcement learning;state aggregation,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7136431,,,,,,IEEE,16,,Electronic:978-1-4799-4912-0; POD:978-1-4799-4911-3,,"2014 IEEE 25th Annual International Symposium on Personal, Indoor, and Mobile Radio Communication (PIMRC)",1642,IEEE Conferences,,,,,2014,,,,,,,,,,,,,,,,,,,
A Reinforcement Learning-Based Adaptive Learning System,"With the plethora of educational and e-learning systems and the great variation in students’ personal and social factors that affect their learning behaviors and outcomes, it has become mandatory for all educational systems to adapt to the variability of these factors for each student. Since there is a large number of factors that need to be taken into consideration, the task is very challenging. In this paper, we present an approach that adapts to the most influential factors in a way that varies from one learner to another, and in different learning settings, including individual and collaborative learning. The approach utilizes reinforcement learning for building an intelligent environment that, not only provides a method for suggesting suitable learning materials, but also provides a methodology for accounting for the continuously-changing students’ states and acceptance of technology. We evaluate our system through simulations. The obtained results are promising and show the feasibility of the proposed approach. © 2018, Springer International Publishing AG.",,"Center for Learning Technologies, University of Science and Technology, Zewail City, Giza, Egypt",,,"Shawky D., Badawi A.","Shawky, D., Center for Learning Technologies, University of Science and Technology, Zewail City, Giza, Egypt; Badawi, A., Center for Learning Technologies, University of Science and Technology, Zewail City, Giza, Egypt",,,10.1007/978-3-319-74690-6_22,SCOPUS,1,,,,,,,,,,,,,Advances in Intelligent Systems and Computing,Adaptive learning; Computer-supported collaborative learning; Reinforcement Learning,,2-s2.0-85041837617,,,,,,231,221,,,,,Scopus,,,Advances in Intelligent Systems and Computing,,Conference Paper,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041837617&doi=10.1007%2f978-3-319-74690-6_22&partnerID=40&md5=7086141d27c694a3b289e9ba955acc61,,723,,2018,,,,,,,,,,,,,,,,,,,
Machine learning methods for big spectrum data processing,"With the rapid development of the mobile Internet and the Internet of Things, the number of personal wireless devices has grown exponentially, resulting in the increase of massive spectrum data. Therefore, the big spectrum data are literally formed. Meanwhile, the spectrum deficit is also increasingly precarious. Effective big spectrum data processing is significant in improving the spectrum utilization. Firstly, from a perspective of wireless communication, a definition of big spectrum data is presented and its characteristics are also analyzed. Then, promising machine learning methods to analyze and utilize the big spectrum data are summarized, such as, the distributed and parallel learning, extreme learning machine, kernel-based learning, deep learning, reinforcement learning, game learning, and transfer learning. Finally, several open issues and research trends are addressed. ©, 2015, Journal of Data Acquisition and Processing. All right reserved.",,"College of Communications Engineering, PLA University of Science and Technology, Nanjing, China",,,"Wu Q., Qiu J., Ding G.","Wu, Q., College of Communications Engineering, PLA University of Science and Technology, Nanjing, China; Qiu, J., College of Communications Engineering, PLA University of Science and Technology, Nanjing, China; Ding, G., College of Communications Engineering, PLA University of Science and Technology, Nanjing, China",3,,10.16337/j.1004-9037.2015.04.001,SCOPUS,1,,,,,,,,,,4,,,Shuju Caiji Yu Chuli/Journal of Data Acquisition and Processing,Big data; Big spectrum data; Data mining; Internet of Things; Machine learning; Wireless communication,,2-s2.0-84941894431,,,,,,713,703,,,,,Scopus,,,Shuju Caiji Yu Chuli/Journal of Data Acquisition and Processing,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941894431&doi=10.16337%2fj.1004-9037.2015.04.001&partnerID=40&md5=e9115001bb97be1c11c6026d71376b47,,30,,2015,,,,,,,,,,,,,,,,,,,
Automatically Learning User Preferences for Personalized Service Composition,"With the rapid growth of the web services technologies, users often leverage various web services to perform their daily activities, such as on-line shopping. Due to the massive amount of web services available, a user faces numerous choices to meet their personal preferences when selecting the desired services from the web services with the similar functionality. Therefore, it becomes tedious and cumbersome tasks for users to discover and compose services. To reduce user's cognitive burden, it is critical to support automated service composition and make efficient recommendation of personalized services to achieve user's overall goals. However, existing approaches only offer users with limited options designed for the interest of a group of users without considering individual users' interests. To allow users to compose personalized services without much manual specification, we propose a machine learning approach that applies a learning-to-rank algorithm, RankBoost, to automatically learn user preferences and the prioritization of the preferences from users' historical data. Moreover, our approach uses the multi-objective reinforcement learning (MORL) algorithm to make trade-offs among user preferences and recommends a collection of services to achieve the highest objective. We conduct an empirical study to evaluate our approach by collecting the historical data from 12 subjects. The results demonstrate that our approach outperforms the two well-established baseline approaches by 100%-200% in terms of precision on recommending services.",,,,"Queen's Univ., Kingston, ON, Canada",Y. Zhao; S. Wang; Y. Zou; J. Ng; T. Ng,,,,10.1109/ICWS.2017.93,IEEE Xplore,1,,20170911,783,,Data mining;Feature extraction;History;Learning (artificial intelligence);Machine learning algorithms;Time-frequency analysis;Web services,Internet;Web services;learning (artificial intelligence);recommender systems;retail data processing,MORL algorithm;RankBoost;Web service technologies;automated service composition;automatic user preference learning;learning-to-rank algorithm;machine learning approach;multiobjective reinforcement learning algorithm;online shopping;personalized service composition;service recommendation,,,,25-30 June 2017,,2017 IEEE International Conference on Web Services (ICWS),,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029835,,,,,,IEEE,,,Electronic:978-1-5386-0752-7; POD:978-1-5386-0753-4,,2017 IEEE International Conference on Web Services (ICWS),776,IEEE Conferences,,,,,2017,,,,,,,,,,,,,,,,,,,
Personalized automatic image annotation based on reinforcement learning,"With the rapidly increasing number of personal image collections on the web, it is of great importance to annotate these user-uploaded images in personalized manner. But personalized image annotation is largely ignored by the mainstream of image annotation research. In this paper, we focus on personalizing the automatic image annotation by proposing a general framework which jointly exploits the generic content-based image annotation, personal image tagging history and the content of personal history images. In our framework, two sets of candidate annotations are extracted for each image based on content-based annotation and personal image tagging history. Considering that the user's interest may not stay the same, when exploiting the personal image tagging history, we also take the content of personal history images into account to avoid the noise. To get the final annotations, we propose an unsupervised algorithm based on reinforcement learning to combine the above two candidate annotation sets. Encouraging results show that the proposed framework is effective and promising for personalizing automatic image annotation.",,,,"Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science, Zhejiang University, Hangzhou 310027, China",Yabo Ni; Miao Zheng; Jiajun Bu; Chun Chen; Dazhou Wang,,0,,10.1109/ICME.2013.6607456,IEEE Xplore,1,,20130926,6,,History;Learning (artificial intelligence);Noise;Semantics;Tagging;Unsupervised learning;Vocabulary,Internet;image classification;image retrieval;learning (artificial intelligence),Web;generic content-based image annotation;personal image collections;personal image tagging history;personalized automatic image annotation;reinforcement learning;user-uploaded images,,1945-7871;19457871,,15-19 July 2013,,2013 IEEE International Conference on Multimedia and Expo (ICME),Automatic image annotation;Personal image tagging history;Personalization;Reinforcement learning,,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607456,,,,,,IEEE,14,,Electronic:978-1-4799-0015-2; POD:978-1-4799-0014-5; USB:978-1-4799-0013-8,,2013 IEEE International Conference on Multimedia and Expo (ICME),1,IEEE Conferences,,,,,2013,,,,,,,,,,,,,,,,,,,
Word sense disambiguation as a traveling salesman problem,"Word sense disambiguation (WSD) is a difficult problem in Computational Linguistics, mostly because of the use of a fixed sense inventory and the deep level of granularity. This paper formulates WSD as a variant of the traveling salesman problem (TSP) to maximize the overall semantic relatedness of the context to be disambiguated. Ant colony optimization, a robust nature-inspired algorithm, was used in a reinforcement learning manner to solve the formulated TSP. We propose a novel measure based on the Lesk algorithm and Vector Space Model to calculate semantic relatedness. Our approach to WSD is comparable to state-of-the-art knowledge-based and unsupervised methods for benchmark datasets. In addition, we show that the combination of knowledge-based methods is superior to the most frequent sense heuristic and significantly reduces the difference between knowledge-based and supervised methods. The proposed approach could be customized for other lexical disambiguation tasks, such as Lexical Substitution or Word Domain Disambiguation. © 2011 Springer Science+Business Media B.V.",,"School of Electrical Engineering, University of Ulsan, 93 Daehakro, Nam-gu, Ulsan 680-749, South Korea",,,"Nguyen K.-H., Ock C.-Y.","Nguyen, K.-H., School of Electrical Engineering, University of Ulsan, 93 Daehakro, Nam-gu, Ulsan 680-749, South Korea; Ock, C.-Y., School of Electrical Engineering, University of Ulsan, 93 Daehakro, Nam-gu, Ulsan 680-749, South Korea",12,,10.1007/s10462-011-9288-9,SCOPUS,1,,,,,,,,,,4,,,Artificial Intelligence Review,Ant colony optimization; Lesk algorithm; Semantic relatedness; Traveling salesman problem; Word sense disambiguation; WordNet,,2-s2.0-84890425797,,,,,,427,405,,,,,Scopus,,,Artificial Intelligence Review,,Article,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890425797&doi=10.1007%2fs10462-011-9288-9&partnerID=40&md5=fae48f20c14c6054532d0e5c6fa0d6a0,,40,,2013,,,,,,,,,,,,,,,,,,,
Worst-case Delay Analysis of Variable Bit-rate Flows in Network-on-chip with Aggregate Scheduling,,,,,,Fahimeh  Jafari and Axel  Jantsch and Zhonghai  Lu,,,,,ACM-DL,0,,,541,,,,,978-3-9810801-8-6,,,,,"Proceedings of the Conference on Design, Automation and Test in Europe",,,2492842,,,,,,,,538--541,,EDA Consortium,,,,,"Proceedings of the Conference on Design, Automation and Test in Europe",538,article,,,,,2012,DATE '12,"Proceedings of the Conference on Design, Automation and Test in Europe",,"Dresden, Germany",,,,,,,4,,,,,"San Jose, CA, USA",,,
Wireless Multicast Scheduling with Switched Beamforming Antennas,,,,,,Honghai  Zhang and Yuanxi  Jiang and Karthik  Sundaresan and Sampath  Rangarajan and Baohua  Zhao,,,,10.1109/TNET.2012.2191977,ACM-DL,1,,,1607,,,,,,,,,IEEE/ACM Trans. Netw.,IEEE/ACM Trans. Netw.,"beamforming, bin-packing, multiple-input-multiple-output (MIMO), power allocation, scheduling, wireless multicast",,2428717,,,,,,,,1595--1607,,IEEE Press,,,,1063-6692,IEEE/ACM Trans. Netw.,1595,article,,,,20,2012,,,,,,,October 2012,5,,October,13,,,,,"Piscataway, NJ, USA",,,
Wireless Access to Internet via Bluetooth: Performance Evaluation of the EDC Scheduling Algorithm,,,,,,Raffaele  Bruno and Marco  Conti and Enrico  Gregori,,,,10.1145/381472.381568,ACM-DL,1,,,49,,,,,1-58113-423-1,,,,,Proceedings of the First Workshop on Wireless Mobile Internet,"Medium Access Control (MAC), TCP, automatic repeat request (ARQ), bluetooth, polling, scheduling",,381568,,,,,,,,43--49,,ACM,,,,,Proceedings of the First Workshop on Wireless Mobile Internet,43,article,,,,,2001,WMI '01,Proceedings of the First Workshop on Wireless Mobile Internet,,"Rome, Italy",,,,,,,7,,,,,"New York, NY, USA",,,
Volume Ranking and Sequential Selection in Programmatic Display Advertising,,,,,,Yuxuan  Song and Kan  Ren and Han  Cai and Weinan  Zhang and Yong  Yu,,,,10.1145/3132847.3132853,ACM-DL,1,,,1107,,,,,978-1-4503-4918-5,,,,,Proceedings of the 2017 ACM on Conference on Information and Knowledge Management,"article recommendation, noise contrastive estimation, text representation, transfer learning, word2vec",,3132853,,,,,,,,1099--1107,,ACM,,,,,Proceedings of the 2017 ACM on Conference on Information and Knowledge Management,1099,article,,,,,2017,CIKM '17,Proceedings of the 2017 ACM on Conference on Information and Knowledge Management,,"Singapore, Singapore",,,,,,,9,,,,,"New York, NY, USA",,,
Visualizing and Manipulating Brain Dynamics,,,,,,Mitsuo  Kawato,,,,10.1145/2750858.2815826,ACM-DL,1,,,1,,,,,978-1-4503-3574-4,,,,,Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing,,,2815826,,,,,,,,1--1,,ACM,,,,,Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing,1,article,,,,,2015,UbiComp '15,Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing,,"Osaka, Japan",,,,,,,1,,,,,"New York, NY, USA",,,
Variation-Aware Application Scheduling and Power Management for Chip Multiprocessors,,,,,,Radu  Teodorescu and Josep  Torrellas,,,,10.1109/ISCA.2008.40,ACM-DL,1,,,374,,,,,978-0-7695-3174-8,,,,,Proceedings of the 35th Annual International Symposium on Computer Architecture,"Process variation, power management, application scheduling",,1382152,,,,,,,,363--374,,IEEE Computer Society,,,,,Proceedings of the 35th Annual International Symposium on Computer Architecture,363,article,,,,,2008,ISCA '08,Proceedings of the 35th Annual International Symposium on Computer Architecture,,,,,,,,,12,,,,,"Washington, DC, USA",,,
Validation and Implementation of Whole-exome Sequencing Bioinformatics Processes for Clinical Applications,,,,,,Rimma  Shakhbatyan and Himanshu  Sharma and Ellen  Tsai and Mark J. Bowser and Birgit  Funke and Matthew S. Lebo,,,,10.1145/2649387.2660794,ACM-DL,1,,,597,,,,,978-1-4503-2894-4,,,,,"Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics","SNV, clinical bioinformatics, clinical validation, disease diagnostics, genetic testing, indel, variant annotation, variant filtration, whole-exome sequencing",,2660794,,,,,,,,596--597,,ACM,,,,,"Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics",596,article,,,,,2014,BCB '14,"Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics",,"Newport Beach, California",,,,,,,2,,,,,"New York, NY, USA",,,
Using Simulation to Evaluate System Performance,,,,,,"Edward K. Bowdon,Sr.",,,,,ACM-DL,0,,,365,,,,,,,,,,Proceedings of the 11th Design Automation Workshop,,,811414,,,,,,,,359--365,,IEEE Press,,,,,Proceedings of the 11th Design Automation Workshop,359,article,,,,,1974,DAC '74,Proceedings of the 11th Design Automation Workshop,,,,,,,,,7,,,,,"Piscataway, NJ, USA",,,
Using Real-time Queueing Theory to Control Lateness in Real-time Systems,,,,,,John P. Lehoczky,,,,10.1145/258612.258685,ACM-DL,1,,,168,,,,,0-89791-909-2,,,,,Proceedings of the 1997 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems,,,258685,,,,,,,,158--168,,ACM,,,,,Proceedings of the 1997 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems,158,article,,,,,1997,SIGMETRICS '97,Proceedings of the 1997 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems,,"Seattle, Washington, USA",,,,,,,11,,,,,"New York, NY, USA",,,
Using Graph Theory to Reduce Communication Overhead in Parallel Systems,,,,,,David R. Surma,,,,,ACM-DL,0,,,214,,,,,,,,,J. Comput. Sci. Coll.,J. Comput. Sci. Coll.,,,948766,,,,,,,,205--214,,Consortium for Computing Sciences in Colleges,,,,1937-4771,J. Comput. Sci. Coll.,205,article,,,,19,2003,,,,,,,October 2003,1,,October,10,,,,,USA,,,
Using Distributed W-learning for Multi-policy Optimization in Decentralized Autonomic Systems,,,,,,Ivana  Dusparic and Vinny  Cahill,,,,10.1145/1555228.1555247,ACM-DL,1,,,64,,,,,978-1-60558-564-2,,,,,Proceedings of the 6th International Conference on Autonomic Computing,"autonomic computing, decentralized systems, reinforcement learning",,1555247,,,,,,,,63--64,,ACM,,,,,Proceedings of the 6th International Conference on Autonomic Computing,63,article,,,,,2009,ICAC '09,Proceedings of the 6th International Conference on Autonomic Computing,,"Barcelona, Spain",,,,,,,2,,,,,"New York, NY, USA",,,
Use of Infeasible Individuals in Probabilistic Model Building Genetic Network Programming,,,,,,Xianneng  Li and Shingo  Mabu and Kotaro  Hirasawa,,,,10.1145/2001576.2001659,ACM-DL,1,,,608,,,,,978-1-4503-0557-0,,,,,Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation,"EDA, infeasible individuals, probabilistic model building genetic network programming, reinforcement learning",,2001659,,,,,,,,601--608,,ACM,,,,,Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation,601,article,,,,,2011,GECCO '11,Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation,,"Dublin, Ireland",,,,,,,8,,,,,"New York, NY, USA",,,
Underload Instabilities in Packet Networks with Flow Schedulers,,,,,,Marco Ajmone Marsan and Mirko  Franceschinis and Emilio  Leonardi and Fabio  Neri and Alessandro  Tarello,,,,10.1109/TNET.2004.838593,ACM-DL,1,,,1143,,,,,,,,,IEEE/ACM Trans. Netw.,IEEE/ACM Trans. Netw.,"QoS schedulers, fluid models, queueing analysis, stability, switching",,1046027,,,,,,,,1131--1143,,IEEE Press,,,,1063-6692,IEEE/ACM Trans. Netw.,1131,article,,,,12,2004,,,,,,,December 2004,6,,December,13,,,,,"Piscataway, NJ, USA",,,
Two-level Congestion Control Schemes for ATM Networks,,,,,,Jin-Long  Wang and Liang-Teh  Lee,,,,10.1145/190690.190693,ACM-DL,1,,,32,,,,,,,,,SIGICE Bull.,SIGICE Bull.,,,190693,,,,,,,,13--32,,ACM,,,,0893-2875,SIGICE Bull.,13,article,,,,20,1994,,,,,,,Oct. 1994,2,,October,20,,,,,"New York, NY, USA",,,
Traffic Regulation Under the Percentile-based Pricing Policy,,,,,,Jianping  Wang,,,,10.1145/1146847.1146851,ACM-DL,1,,,,,,,,1-59593-428-6,,,,,Proceedings of the 1st International Conference on Scalable Information Systems,,,1146851,,,,,,,,,,ACM,,,,,Proceedings of the 1st International Conference on Scalable Information Systems,4,article,,,4,,2006,InfoScale '06,Proceedings of the 1st International Conference on Scalable Information Systems,,Hong Kong,,,,,,,,,,,,"New York, NY, USA",,,
Towards Modeling the Traffic Data on Road Networks,,,,,,Ugur  Demiryurek and Bei  Pan and Farnoush  Banaei-Kashani and Cyrus  Shahabi,,,,10.1145/1645373.1645376,ACM-DL,1,,,18,,,,,978-1-60558-861-2,,,,,Proceedings of the Second International Workshop on Computational Transportation Science,,,1645376,,,,,,,,13--18,,ACM,,,,,Proceedings of the Second International Workshop on Computational Transportation Science,13,article,,,,,2009,IWCTS '09,Proceedings of the Second International Workshop on Computational Transportation Science,,"Seattle, Washington",,,,,,,6,,,,,"New York, NY, USA",,,
Towards Integrated Imitation of Strategic Planning and Motion Modeling in Interactive Computer Games,,,,,,Bernard  Gorman and Mark  Humphrys,,,,10.1145/1178418.1178432,ACM-DL,1,,,,,,,,,,,,Comput. Entertain.,Comput. Entertain.,"Quake, clustering, imitation learning, pattern recognition, reinforcement learning, statistical analysis",,1178432,,,,,,,,,,ACM,,,,1544-3574,Comput. Entertain.,10,article,,,10,4,2006,,,,,,,October-December 2006,4,,October,,,,,,"New York, NY, USA",,,
Three Automated Stock-trading Agents: A Comparative Study,,,,,,Alexander A. Sherstov and Peter  Stone,,,,10.1007/11575726_13,ACM-DL,1,,,187,,,,,"3-540-29737-5, 978-3-540-29737-6",,,,,Proceedings of the 6th AAMAS International Conference on Agent-Mediated Electronic Commerce: Theories for and Engineering of Distributed Mechanisms and Systems,,,2178465,,,,,,,,173--187,,Springer-Verlag,,,,,Proceedings of the 6th AAMAS International Conference on Agent-Mediated Electronic Commerce: Theories for and Engineering of Distributed Mechanisms and Systems,173,article,,,,,2005,AAMAS'04,Proceedings of the 6th AAMAS International Conference on Agent-Mediated Electronic Commerce: Theories for and Engineering of Distributed Mechanisms and Systems,,"New York, NY",,,,,,,15,,,,,"Berlin, Heidelberg",,,
The User's Role in a Simulation Based Scheduling System,,,,,,F. H. Grant and R. G. Lagoni,,,,10.1145/76738.76858,ACM-DL,1,,,941,,,,,0-911801-58-8,,,,,Proceedings of the 21st Conference on Winter Simulation,,,76858,,,,,,,,936--941,,ACM,,,,,Proceedings of the 21st Conference on Winter Simulation,936,article,,,,,1989,WSC '89,Proceedings of the 21st Conference on Winter Simulation,,"Washington, D.C., USA",,,,,,,6,,,,,"New York, NY, USA",,,
The Intrinsic Geometric Structure of Protein-protein Interaction Networks for Protein Interaction Prediction,,,,,,Yi  Fang and Mengtian  Sun and Guoxian  Dai and Karthik  Ramain,,,,10.1109/TCBB.2015.2456876,ACM-DL,1,,,85,,,,,,,,,IEEE/ACM Trans. Comput. Biol. Bioinformatics,IEEE/ACM Trans. Comput. Biol. Bioinformatics,"complex network, computational biology, protein protein interaction network",,2942469,,,,,,,,76--85,,IEEE Computer Society Press,,,,1545-5963,IEEE/ACM Trans. Comput. Biol. Bioinformatics,76,article,,,,13,2016,,,,,,,January/February 2016,1,,January,10,,,,,"Los Alamitos, CA, USA",,,
The Flying : Kinect Art Using OpenNI and Learning System,,,,,,Ok-Hue  Cho and Won-Hyung  Lee,,,,10.1145/2342896.2342931,ACM-DL,1,,,27,,,,,978-1-4503-1682-8,,,,,ACM SIGGRAPH 2012 Posters,,,2342931,,,,,,,,27:1--27:1,,ACM,,,,,ACM SIGGRAPH 2012 Posters,27,article,,,27,,2012,SIGGRAPH '12,ACM SIGGRAPH 2012 Posters,,"Los Angeles, California",,,,,,,1,,,,,"New York, NY, USA",,,
The Exploration-Exploitation Trade-off in Interactive Recommender Systems,,,,,,Andrea  Barraza-Urbina,,,,10.1145/3109859.3109866,ACM-DL,1,,,435,,,,,978-1-4503-4652-8,,,,,Proceedings of the Eleventh ACM Conference on Recommender Systems,"exploration exploitation trade-off, information discovery, multi-armed bandits, recommendation system, reinforcement learning",,3109866,,,,,,,,431--435,,ACM,,,,,Proceedings of the Eleventh ACM Conference on Recommender Systems,431,article,,,,,2017,RecSys '17,Proceedings of the Eleventh ACM Conference on Recommender Systems,,"Como, Italy",,,,,,,5,,,,,"New York, NY, USA",,,
The Design of Collectives of Agents to Control non-Markovian Systems,,,,,,John W. Lawson and David H. Wolpert,,,,,ACM-DL,0,,,337,,,,,0-262-51129-0,,,,,Eighteenth National Conference on Artificial Intelligence,,,777146,,,,,,,,332--337,,American Association for Artificial Intelligence,,,,,Eighteenth National Conference on Artificial Intelligence,332,article,,,,,2002,,Eighteenth National Conference on Artificial Intelligence,,"Edmonton, Alberta, Canada",,,,,,,6,,,,,"Menlo Park, CA, USA",,,
The Competitiveness of On-line Assignments,,,,,,Yossi  Azar and Joseph (Seffi) Naor and Raphael  Rom,,,,,ACM-DL,0,,,210,,,,,0-89791-466-X,,,,,Proceedings of the Third Annual ACM-SIAM Symposium on Discrete Algorithms,,,139450,,,,,,,,203--210,,Society for Industrial and Applied Mathematics,,,,,Proceedings of the Third Annual ACM-SIAM Symposium on Discrete Algorithms,203,article,,,,,1992,SODA '92,Proceedings of the Third Annual ACM-SIAM Symposium on Discrete Algorithms,,"Orlando, Florida, USA",,,,,,,8,,,,,"Philadelphia, PA, USA",,,
"The Combination of Scheduling, Allocation, and Mapping in a Single Algorithm",,,,,,Richard J. Cloutier and Donald E. Thomas,,,,10.1145/123186.123230,ACM-DL,1,,,76,,,,,0-89791-363-9,,,,,Proceedings of the 27th ACM/IEEE Design Automation Conference,,,123230,,,,,,,,71--76,,ACM,,,,,Proceedings of the 27th ACM/IEEE Design Automation Conference,71,article,,,,,1990,DAC '90,Proceedings of the 27th ACM/IEEE Design Automation Conference,,"Orlando, Florida, USA",,,,,,,6,,,,,"New York, NY, USA",,,
Temporal Difference Learning and TD-Gammon,,,,,,Gerald  Tesauro,,,,10.1145/203330.203343,ACM-DL,1,,,68,,,,,,,,,Commun. ACM,Commun. ACM,,,203343,,,,,,,,58--68,,ACM,,,,0001-0782,Commun. ACM,58,article,,,,38,1995,,,,,,,March 1995,3,,March,11,,,,,"New York, NY, USA",,,
Team Behavior in Interactive Dynamic Influence Diagrams with Applications to Ad Hoc Teams,,,,,,Muthukumaran  Chandrasekaran and Prashant  Doshi and Yifeng  Zeng and Yingke  Chen,,,,,ACM-DL,0,,,1560,,,,,978-1-4503-2738-1,,,,,Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems,"ad hoc teamwork, multiagent planning, reinforcement learning",,2616061,,,,,,,,1559--1560,,International Foundation for Autonomous Agents and Multiagent Systems,,,,,Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems,1559,article,,,,,2014,AAMAS '14,Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems,,"Paris, France",,,,,,,2,,,,,"Richland, SC",,,
TDMA Time Slot and Turn Optimization with Evolutionary Search Techniques,,,,,,Arne  Hamann and Rolf  Ernst,,,,10.1109/DATE.2005.299,ACM-DL,1,,,317,,,,,0-7695-2288-2,,,,,"Proceedings of the Conference on Design, Automation and Test in Europe - Volume 1",,,1049122,,,,,,,,312--317,,IEEE Computer Society,,,,,"Proceedings of the Conference on Design, Automation and Test in Europe - Volume 1",312,article,,,,,2005,DATE '05,"Proceedings of the Conference on Design, Automation and Test in Europe - Volume 1",,,,,,,,,6,,,,,"Washington, DC, USA",,,
Taking Turns in General Sum Markov Games,,,,,,Peter  Vrancx and Katja  Verbeeck and Ann  Now&#233;,,,,,ACM-DL,0,,,1440,,,,,978-0-9826571-1-9,,,,,Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: Volume 1 - Volume 1,"Markov games, agent coordination, learning agents, reinforcement learning",,1838421,,,,,,,,1439--1440,,International Foundation for Autonomous Agents and Multiagent Systems,,,,,Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: Volume 1 - Volume 1,1439,article,,,,,2010,AAMAS '10,Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: Volume 1 - Volume 1,,"Toronto, Canada",,,,,,,2,,,,,"Richland, SC",,,
TacTex'13: A Champion Adaptive Power Trading Agent,,,,,,Daniel  Urieli and Peter  Stone,,,,,ACM-DL,0,,,1448,,,,,978-1-4503-2738-1,,,,,Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems,"energy trading, machine learning, smart-grid",,2617516,,,,,,,,1447--1448,,International Foundation for Autonomous Agents and Multiagent Systems,,,,,Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems,1447,article,,,,,2014,AAMAS '14,Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems,,"Paris, France",,,,,,,2,,,,,"Richland, SC",,,
Synergies Between Evolutionary Computation and Multiagent Reinforcement Learning: The Benefits of Exchanging Solutions,,,,,,Ana L. C. Bazzan,,,,10.1145/3067695.3075970,ACM-DL,1,,,202,,,,,978-1-4503-4939-0,,,,,Proceedings of the Genetic and Evolutionary Computation Conference Companion,"evolutionary computation, multiagent systems, reinforcement learning",,3075970,,,,,,,,201--202,,ACM,,,,,Proceedings of the Genetic and Evolutionary Computation Conference Companion,201,article,,,,,2017,GECCO '17,Proceedings of the Genetic and Evolutionary Computation Conference Companion,,"Berlin, Germany",,,,,,,2,,,,,"New York, NY, USA",,,
Supply Chain Applications I: Designing the Support Logistics for the FAA ACE-IDS System,,,,,,Ricki G. Ingalls and John W. Nazemetz,,,,,ACM-DL,0,,,1122,,,,,0-7803-7309-X,,,,,Proceedings of the 33Nd Conference on Winter Simulation,,,564285,,,,,,,,1117--1122,,IEEE Computer Society,,,,,Proceedings of the 33Nd Conference on Winter Simulation,1117,article,,,,,2001,WSC '01,Proceedings of the 33Nd Conference on Winter Simulation,,"Arlington, Virginia",,,,,,,6,,,,,"Washington, DC, USA",,,
Stochastic Scheduling,,,,,,Jennifer M. Schopf and Francine  Berman,,,,10.1145/331532.331580,ACM-DL,1,,,,,,,,1-58113-091-0,,,,,Proceedings of the 1999 ACM/IEEE Conference on Supercomputing,,,331580,,,,,,,,,,ACM,,,,,Proceedings of the 1999 ACM/IEEE Conference on Supercomputing,48,article,,,48,,1999,SC '99,Proceedings of the 1999 ACM/IEEE Conference on Supercomputing,,"Portland, Oregon, USA",,,,,,,,,,,,"New York, NY, USA",,,
"Statistical Methods for Dynamic Treatment Regimes: Reinforcement Learning, Causal Inference, and Personalized Medicine. Vol. 76",,,,,,,,2,,,Google Scholar,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2013,,,4.03569837407792E+018,,,,,,,,,0,,,,,,"http://scholar.google.com/scholar?cites=4035698374077919339&as_sdt=2005&sciodt=0,5&hl=en",
Staff Scheduling for Inbound Call Centers and Customer Contact Centers,,,,,,Alex  Fukunaga and Ed  Hamilton and Jason  Fama and David  Andre and Ofer  Matan and Illah  Nourbakhsh,,,,,ACM-DL,0,,,829,,,,,0-262-51129-0,,,,,Eighteenth National Conference on Artificial Intelligence,,,777218,,,,,,,,822--829,,American Association for Artificial Intelligence,,,,,Eighteenth National Conference on Artificial Intelligence,822,article,,,,,2002,,Eighteenth National Conference on Artificial Intelligence,,"Edmonton, Alberta, Canada",,,,,,,8,,,,,"Menlo Park, CA, USA",,,
Stackelberg Scheduling Strategies,,,,,,Tim  Roughgarden,,,,10.1145/380752.380783,ACM-DL,1,,,113,,,,,1-58113-349-9,,,,,Proceedings of the Thirty-third Annual ACM Symposium on Theory of Computing,,,380783,,,,,,,,104--113,,ACM,,,,,Proceedings of the Thirty-third Annual ACM Symposium on Theory of Computing,104,article,,,,,2001,STOC '01,Proceedings of the Thirty-third Annual ACM Symposium on Theory of Computing,,"Hersonissos, Greece",,,,,,,10,,,,,"New York, NY, USA",,,
Stability Comparison in Single-server-multiple-queue Systems,,,,,,S.  Lam and Rocky K. C. Chang,,,,10.1145/507553.507567,ACM-DL,1,,,34,,,,,,,,,SIGMETRICS Perform. Eval. Rev.,SIGMETRICS Perform. Eval. Rev.,,,507567,,,,,,,,32--34,,ACM,,,,0163-5999,SIGMETRICS Perform. Eval. Rev.,32,article,,,,29,2001,,,,,,,December 2001,3,,December,3,,,,,"New York, NY, USA",,,
Sparsity Regret Bounds for Individual Sequences in Online Linear Regression,,,,,,S&#233;bastien  Gerchinovitz,,,,,ACM-DL,0,,,769,,,,,,,,,J. Mach. Learn. Res.,J. Mach. Learn. Res.,"adaptive regret bounds, individual sequences, online linear regression, sparsity",,2502604,,,,,,,,729--769,,JMLR.org,,,,1532-4435,J. Mach. Learn. Res.,729,article,,,,14,2013,,,,,,,January 2013,1,,March,41,,,,,,,,
SON Conflict Resolution Using Reinforcement Learning with State Aggregation,,,,,,Ovidiu Constantin Iacoboaiea and Berna  Sayrac and Sana  Ben Jemaa and Pascal  Bianchi,,,,10.1145/2627585.2627591,ACM-DL,1,,,20,,,,,978-1-4503-2990-3,,,,,"Proceedings of the 4th Workshop on All Things Cellular: Operations, Applications, &#38; Challenges","LTE, MLB, MRO, SON coordination, SON instances, reinforcement learning, state aggregation",,2627591,,,,,,,,15--20,,ACM,,,,,"Proceedings of the 4th Workshop on All Things Cellular: Operations, Applications, &#38; Challenges",15,article,,,,,2014,AllThingsCellular '14,"Proceedings of the 4th Workshop on All Things Cellular: Operations, Applications, &#38; Challenges",,"Chicago, Illinois, USA",,,,,,,6,,,,,"New York, NY, USA",,,
Solving Large Vehicle Routing and Scheduling Problems in Small Core,,,,,,Lawrence  Bodin,,,,10.1145/800173.809693,ACM-DL,1,,,37,,,,,0-89791-120-2,,,,,Proceedings of the 1983 Annual Conference on Computers : Extending the Human Resource,,,809693,,,,,,,,27--37,,ACM,,,,,Proceedings of the 1983 Annual Conference on Computers : Extending the Human Resource,27,article,,,,,1983,ACM '83,Proceedings of the 1983 Annual Conference on Computers : Extending the Human Resource,,,,,,,,,11,,,,,"New York, NY, USA",,,
Sojourn Times in (Discrete) Time Shared Systems and Their Continuous Time Limits,,,,,,Arzad A. Kherani,,,,10.1145/1190095.1190099,ACM-DL,1,,,,,,,,1-59593-504-5,,,,,Proceedings of the 1st International Conference on Performance Evaluation Methodolgies and Tools,,,1190099,,,,,,,,,,ACM,,,,,Proceedings of the 1st International Conference on Performance Evaluation Methodolgies and Tools,4,article,,,4,,2006,valuetools '06,Proceedings of the 1st International Conference on Performance Evaluation Methodolgies and Tools,,"Pisa, Italy",,,,,,,,,,,,"New York, NY, USA",,,
Software Synthesis for Real-time Information Processing Systems,,,,,,Filip  Thoen and Marco  Cornero and Gert  Goossens and Hugo  De Man,,,,10.1145/216636.216658,ACM-DL,1,,,69,,,,,,,,,,"Proceedings of the ACM SIGPLAN 1995 Workshop on Languages, Compilers, &Amp; Tools for Real-time Systems",,,216658,,,,,,,,60--69,,ACM,,,,,"Proceedings of the ACM SIGPLAN 1995 Workshop on Languages, Compilers, &Amp; Tools for Real-time Systems",60,article,,,,,1995,LCTES '95,"Proceedings of the ACM SIGPLAN 1995 Workshop on Languages, Compilers, &Amp; Tools for Real-time Systems",,"La Jolla, California, USA",,,,,,,10,,,,,"New York, NY, USA",,,
Social Reinforcement Learning for Changing Environments,,,,,,Juan A. Garcia-Pardo and J.  Soler and C.  Carrascosa,,,,10.1109/WI-IAT.2010.160,ACM-DL,1,,,272,,,,,978-0-7695-4191-4,,,,,Proceedings of the 2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 02,"Multi-Agent Systems, Reinforcement Learning, Multi-Agent Learning, Emergent Behavior, Social Agents",,1913855,,,,,,,,269--272,,IEEE Computer Society,,,,,Proceedings of the 2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 02,269,article,,,,,2010,WI-IAT '10,Proceedings of the 2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 02,,,,,,,,,4,,,,,"Washington, DC, USA",,,
Simplification by Cooperating Decision Procedures,,,,,,Greg  Nelson and Derek C. Oppen,,,,10.1145/357073.357079,ACM-DL,1,,,257,,,,,,,,,ACM Trans. Program. Lang. Syst.,ACM Trans. Program. Lang. Syst.,,,357079,,,,,,,,245--257,,ACM,,,,0164-0925,ACM Trans. Program. Lang. Syst.,245,article,,,,1,1979,,,,,,,Oct. 1979,2,,October,13,,,,,"New York, NY, USA",,,
Server Scheduling in the Lp Norm: A Rising Tide Lifts All Boat,,,,,,Nikhil  Bansal and Kirk  Pruhs,,,,10.1145/780542.780580,ACM-DL,1,,,250,,,,,1-58113-674-9,,,,,Proceedings of the Thirty-fifth Annual ACM Symposium on Theory of Computing,"flow time, multilevel feedback, resource augmentation, scheduling, shortest elapsed time first, shortest job first, shortest remaining processing time",,780580,,,,,,,,242--250,,ACM,,,,,Proceedings of the Thirty-fifth Annual ACM Symposium on Theory of Computing,242,article,,,,,2003,STOC '03,Proceedings of the Thirty-fifth Annual ACM Symposium on Theory of Computing,,"San Diego, CA, USA",,,,,,,9,,,,,"New York, NY, USA",,,
Self-managed Decentralised Systems Using K-components and Collaborative Reinforcement Learning,,,,,,Jim  Dowling and Vinny  Cahill,,,,10.1145/1075405.1075413,ACM-DL,1,,,43,,,,,1-58113-989-6,,,,,Proceedings of the 1st ACM SIGSOFT Workshop on Self-managed Systems,"architectural reflection, collaborative reinforcement learning, decentralised self-adaptive systems",,1075413,,,,,,,,39--43,,ACM,,,,,Proceedings of the 1st ACM SIGSOFT Workshop on Self-managed Systems,39,article,,,,,2004,WOSS '04,Proceedings of the 1st ACM SIGSOFT Workshop on Self-managed Systems,,"Newport Beach, California",,,,,,,5,,,,,"New York, NY, USA",,,
Self-fertilization Based Genetic Algorithm for University Timetabling Problem,,,,,,Zan  Wang and Jin-lan  Liu and Xue  Yu,,,,10.1145/1543834.1543993,ACM-DL,1,,,1004,,,,,978-1-60558-326-6,,,,,Proceedings of the First ACM/SIGEVO Summit on Genetic and Evolutionary Computation,"advisor, genetic algorithm, self-fertilization, simulated annealing, timetabling",,1543993,,,,,,,,1001--1004,,ACM,,,,,Proceedings of the First ACM/SIGEVO Summit on Genetic and Evolutionary Computation,1001,article,,,,,2009,GEC '09,Proceedings of the First ACM/SIGEVO Summit on Genetic and Evolutionary Computation,,"Shanghai, China",,,,,,,4,,,,,"New York, NY, USA",,,
SciNet: A System for Browsing Scientific Literature Through Keyword Manipulation,,,,,,Dorota  G&#322;owacka and Tuukka  Ruotsalo and Ksenia  Konyushkova and Kumaripaba  Athukorala and Samuel  Kaski and Giulio  Jacucci,,,,10.1145/2451176.2451199,ACM-DL,1,,,62,,,,,978-1-4503-1966-9,,,,,Proceedings of the Companion Publication of the 2013 International Conference on Intelligent User Interfaces Companion,"adaptive interfaces, datamining and machine learning, information retrieval, recommender/filtering systems",,2451199,,,,,,,,61--62,,ACM,,,,,Proceedings of the Companion Publication of the 2013 International Conference on Intelligent User Interfaces Companion,61,article,,,,,2013,IUI '13 Companion,Proceedings of the Companion Publication of the 2013 International Conference on Intelligent User Interfaces Companion,,"Santa Monica, California, USA",,,,,,,2,,,,,"New York, NY, USA",,,
Scheduling with Preemption for Incident Management: When Interrupting Tasks is Not Such a Bad Idea,,,,,,Marcos D. Assun&#231;&#227;o and Victor F. Cavalcante and Maira A. de C. Gatti and Marco A. S. Netto and Claudio S. Pinhanez and Cleidson R. B. de Souza,,,,,ACM-DL,0,,,415,,,,,,,,,,Proceedings of the Winter Simulation Conference,,,2430280,,,,,,,,403:1--403:12,,Winter Simulation Conference,,,,,Proceedings of the Winter Simulation Conference,403,article,,,403,,2012,WSC '12,Proceedings of the Winter Simulation Conference,,"Berlin, Germany",,,,,,,12,,,,,,,,
Scheduling Video Programs in Near Video-on-demand Systems,,,,,,Emmanuel L. Abram-Profeta and Kang G. Shin,,,,10.1145/266180.266387,ACM-DL,1,,,369,,,,,0-89791-991-2,,,,,Proceedings of the Fifth ACM International Conference on Multimedia,"batching, near video-on-demand, partially patient customers, quasi video-on-demand, video server throughput",,266387,,,,,,,,359--369,,ACM,,,,,Proceedings of the Fifth ACM International Conference on Multimedia,359,article,,,,,1997,MULTIMEDIA '97,Proceedings of the Fifth ACM International Conference on Multimedia,,"Seattle, Washington, USA",,,,,,,11,,,,,"New York, NY, USA",,,
Scheduling Time-constrained Instructions on Pipelined Processors,,,,,,Allen  Leung and Krishna V. Palem and Amir  Pnueli,,,,10.1145/383721.383733,ACM-DL,1,,,103,,,,,,,,,ACM Trans. Program. Lang. Syst.,ACM Trans. Program. Lang. Syst.,,,383733,,,,,,,,73--103,,ACM,,,,0164-0925,ACM Trans. Program. Lang. Syst.,73,article,,,,23,2001,,,,,,,Jan. 2001,1,,January,31,,,,,"New York, NY, USA",,,
Scheduling Shared Scans of Large Data Files,,,,,,Parag  Agrawal and Daniel  Kifer and Christopher  Olston,,,,10.14778/1453856.1453960,ACM-DL,1,,,969,,,,,,,,,Proc. VLDB Endow.,Proc. VLDB Endow.,,,1453960,,,,,,,,958--969,,VLDB Endowment,,,,2150-8097,Proc. VLDB Endow.,958,article,,,,1,2008,,,,,,,August 2008,1,,August,12,,,,,,,,
Scheduling Reductions on Realistic Machines,,,,,,Gautam  Gupta and Sanjay  Rajopadhye and Patrice  Quinton,,,,10.1145/564870.564888,ACM-DL,1,,,126,,,,,1-58113-529-7,,,,,Proceedings of the Fourteenth Annual ACM Symposium on Parallel Algorithms and Architectures,"affine recurrence equations, automatic parallelization, dependence analysis, parametric integer programming, polyhedral model, scheduling theory",,564888,,,,,,,,117--126,,ACM,,,,,Proceedings of the Fourteenth Annual ACM Symposium on Parallel Algorithms and Architectures,117,article,,,,,2002,SPAA '02,Proceedings of the Fourteenth Annual ACM Symposium on Parallel Algorithms and Architectures,,"Winnipeg, Manitoba, Canada",,,,,,,10,,,,,"New York, NY, USA",,,
Scheduling Multiple Queries on a Parallel Machine,,,,,,Joel L. Wolf and John  Turek and Ming-Syan  Chen and Philip S. Yu,,,,10.1145/183019.183024,ACM-DL,1,,,55,,,,,,,,,SIGMETRICS Perform. Eval. Rev.,SIGMETRICS Perform. Eval. Rev.,,,183024,,,,,,,,45--55,,ACM,,,,0163-5999,SIGMETRICS Perform. Eval. Rev.,45,article,,,,22,1994,,,,,,,May 1994,1,,May,11,,,,,"New York, NY, USA",,,
Scheduling MPEG-compressed Video Streams with Firm Deadline Constraints,,,,,,Ching-Chih  Han and Kang G. Shin,,,,10.1145/217279.215305,ACM-DL,1,,,422,,,,,0-89791-751-0,,,,,Proceedings of the Third ACM International Conference on Multimedia,"MPEG-compressed video, backwards-EDF, firm deadline, pre-scheduling, urgent/normal frames",,215305,,,,,,,,411--422,,ACM,,,,,Proceedings of the Third ACM International Conference on Multimedia,411,article,,,,,1995,MULTIMEDIA '95,Proceedings of the Third ACM International Conference on Multimedia,,"San Francisco, California, USA",,,,,,,12,,,,,"New York, NY, USA",,,
Scaling Ant Colony Optimization with Hierarchical Reinforcement Learning Partitioning,,,,,,Erik J. Dries and Gilbert L. Peterson,,,,10.1145/1389095.1389100,ACM-DL,1,,,32,,,,,978-1-60558-130-9,,,,,Proceedings of the 10th Annual Conference on Genetic and Evolutionary Computation,"ant colony optimization, hierarchical reinforcement learning, swarm intelligence",,1389100,,,,,,,,25--32,,ACM,,,,,Proceedings of the 10th Annual Conference on Genetic and Evolutionary Computation,25,article,,,,,2008,GECCO '08,Proceedings of the 10th Annual Conference on Genetic and Evolutionary Computation,,"Atlanta, GA, USA",,,,,,,8,,,,,"New York, NY, USA",,,
Scalable Scheduling on a Network of Workstations,,,,,,Sanglu  Lu and Li  Xie,,,,10.1145/346152.346330,ACM-DL,1,,,83,,,,,,,,,SIGOPS Oper. Syst. Rev.,SIGOPS Oper. Syst. Rev.,"coscheduling, load balancing, migration, scalable design, scheduling",,346330,,,,,,,,74--83,,ACM,,,,0163-5980,SIGOPS Oper. Syst. Rev.,74,article,,,,34,2000,,,,,,,"April, 2000",2,,April,10,,,,,"New York, NY, USA",,,
Sample-based Learning and Search with Permanent and Transient Memories,,,,,,David  Silver and Richard S. Sutton and Martin  M&#252;ller,,,,10.1145/1390156.1390278,ACM-DL,1,,,975,,,,,978-1-60558-205-4,,,,,Proceedings of the 25th International Conference on Machine Learning,,,1390278,,,,,,,,968--975,,ACM,,,,,Proceedings of the 25th International Conference on Machine Learning,968,article,,,,,2008,ICML '08,Proceedings of the 25th International Conference on Machine Learning,,"Helsinki, Finland",,,,,,,8,,,,,"New York, NY, USA",,,
RUBIC: Online Parallelism Tuning for Co-located Transactional Memory Applications,,,,,,Amin  Mohtasham and Jo&#227;o  Barreto,,,,10.1145/2935764.2935770,ACM-DL,1,,,108,,,,,978-1-4503-4210-0,,,,,Proceedings of the 28th ACM Symposium on Parallelism in Algorithms and Architectures,"concurrency control, feedback-driven systems, resource allocation, software transactional memory",,2935770,,,,,,,,99--108,,ACM,,,,,Proceedings of the 28th ACM Symposium on Parallelism in Algorithms and Architectures,99,article,,,,,2016,SPAA '16,Proceedings of the 28th ACM Symposium on Parallelism in Algorithms and Architectures,,"Pacific Grove, California, USA",,,,,,,10,,,,,"New York, NY, USA",,,
RIO: A Real-time Multimedia Object Server,,,,,,Richard  Muntz and Jose Renato Santos and Steve  Berson,,,,10.1145/262391.262398,ACM-DL,1,,,35,,,,,,,,,SIGMETRICS Perform. Eval. Rev.,SIGMETRICS Perform. Eval. Rev.,,,262398,,,,,,,,29--35,,ACM,,,,0163-5999,SIGMETRICS Perform. Eval. Rev.,29,article,,,,25,1997,,,,,,,Sept. 1997,2,,September,7,,,,,"New York, NY, USA",,,
Reinforcement-Learning-Based Double Auction Design for Dynamic Spectrum Access in Cognitive Radio Networks.,,,,,,"Yinglei Teng, F. Richard Yu, Ke Han, Yifei Wei, Yong Zhang",,,,10.1007/S11277-012-0611-9,DBLP,1,,,791,,,,,,,,,,,,,1324406,,,,https://doi.org/10.1007/s11277-012-0611-9,,,,,,,,,,,,771,Journal Articles,https://dblp.org/rec/journals/wpc/TengYHWZ13,,69,,2013,,,,,,,,,journals/wpc/TengYHWZ13,,,,2,771-791,,,,,Wireless Personal Communications
"Reinforcement Learning: The Sooner the Better, or the Later the Better?",,,,,,Shitian  Shen and Min  Chi,,,,10.1145/2930238.2930247,ACM-DL,1,,,44,,,,,978-1-4503-4368-8,,,,,Proceedings of the 2016 Conference on User Modeling Adaptation and Personalization,"delayed reward, immediate reward, pedagogical strategy, problem solving, reinforcement learning, worked example",,2930247,,,,,,,,37--44,,ACM,,,,,Proceedings of the 2016 Conference on User Modeling Adaptation and Personalization,37,article,,,,,2016,UMAP '16,Proceedings of the 2016 Conference on User Modeling Adaptation and Personalization,,"Halifax, Nova Scotia, Canada",,,,,,,8,,,,,"New York, NY, USA",,,
"Reinforcement Learning for Multiple Access Control in Wireless Sensor Networks - Review, Model, and Open Issues.",,,,,,"Mohammad Fathi, Vafa Maihami, Parham Moradi",,,,10.1007/S11277-013-1028-9,DBLP,1,,,547,,,,,,,,,,,,,1323952,,,,https://doi.org/10.1007/s11277-013-1028-9,,,,,,,,,,,,535,Journal Articles,https://dblp.org/rec/journals/wpc/FathiMM13,,72,,2013,,,,,,,,,journals/wpc/FathiMM13,,,,1,535-547,,,,,Wireless Personal Communications
Reinforcement Learning for Games: Failures and Successes,,,,,,Wolfgang  Konen and Thomas  Bartz-Beielstein,,,,10.1145/1570256.1570375,ACM-DL,1,,,2648,,,,,978-1-60558-505-5,,,,,Proceedings of the 11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers,"evolution strategies, failures, games, learning",,1570375,,,,,,,,2641--2648,,ACM,,,,,Proceedings of the 11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers,2641,article,,,,,2009,GECCO '09,Proceedings of the 11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers,,"Montreal, Qu&#233;bec, Canada",,,,,,,8,,,,,"New York, NY, USA",,,
Reinforcement Learning Enhanced Iterative Power Allocation in Stochastic Cognitive Wireless Mesh Networks.,,,,,,"Xianfu Chen, Zhifeng Zhao, Honggang Zhang, Tao Chen",,,,10.1007/S11277-010-0008-6,DBLP,1,,,104,,,,,,,,,,,,,1827810,,,,https://doi.org/10.1007/s11277-010-0008-6,,,,,,,,,,,,89,Journal Articles,https://dblp.org/rec/journals/wpc/ChenZZC11,,57,,2011,,,,,,,,,journals/wpc/ChenZZC11,,,,1,89-104,,,,,Wireless Personal Communications
Reducing the Complexity of Multiagent Reinforcement Learning,,,,,,Andriy  Burkov and Brahim  Chaib-draa,,,,10.1145/1329125.1329178,ACM-DL,1,,,47,,,,,978-81-904262-7-5,,,,,Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems,"Q-learning, initialization, multiagent learning, stochastic games",,1329178,,,,,,,,44:1--44:3,,ACM,,,,,Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems,44,article,,,44,,2007,AAMAS '07,Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems,,"Honolulu, Hawaii",,,,,,,3,,,,,"New York, NY, USA",,,
Real-Time Robot Personality Adaptation Based on Reinforcement Learning and Social Signals,,,,,,Hannes  Ritschel and Elisabeth  Andr&#233;,,,,10.1145/3029798.3038381,ACM-DL,1,,,266,,,,,978-1-4503-4885-0,,,,,Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction,"adaptation, dialog, introversion/extraversion, personality, reinforcement learning, social robotics, social signals",,3038381,,,,,,,,265--266,,ACM,,,,,Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction,265,article,,,,,2017,HRI '17,Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction,,"Vienna, Austria",,,,,,,2,,,,,"New York, NY, USA",,,
Real Time Analysis and Priority Scheduler Generation for Hardware-software Systems with a Synthesized Run-time System,,,,,,"Vincent J. Mooney,III and Giovanni  De Micheli",,,,,ACM-DL,0,,,612,,,,,0-8186-8200-0,,,,,Proceedings of the 1997 IEEE/ACM International Conference on Computer-aided Design,"hardware-software codesign, real-time analysis, run-time scheduler, worst-case execution time, rtos",,266563,,,,,,,,605--612,,IEEE Computer Society,,,,,Proceedings of the 1997 IEEE/ACM International Conference on Computer-aided Design,605,article,,,,,1997,ICCAD '97,Proceedings of the 1997 IEEE/ACM International Conference on Computer-aided Design,,"San Jose, California, USA",,,,,,,8,,,,,"Washington, DC, USA",,,
Protein Function Prediction Using Multi-label Ensemble Classification,,,,,,Guoxian  Yu and Huzefa  Rangwala and Carlotta  Domeniconi and Guoji  Zhang and Zhiwen  Yu,,,,10.1109/TCBB.2013.111,ACM-DL,1,,,1,,,,,,,,,IEEE/ACM Trans. Comput. Biol. Bioinformatics,IEEE/ACM Trans. Comput. Biol. Bioinformatics,"Proteins,Kernel,Correlation,Bioinformatics,Vectors,IEEE transactions,Computational biology,Mining methods and algorithms,Data mining,Clustering,classification,and association rules,Bioinformatics (genome or protein) databases,Knowledge management applications",,2564692,,,,,,,,1--1,,IEEE Computer Society Press,,,,1545-5963,IEEE/ACM Trans. Comput. Biol. Bioinformatics,1,article,,,,10,2013,,,,,,,July 2013,4,,July,1,,,,,"Los Alamitos, CA, USA",,,
Proportional-share Scheduling in Single-server and Multiple-server Computing Systems,,,,,,D. H. J. Epema and J. F. C. M. de Jongh,,,,10.1145/340242.340295,ACM-DL,1,,,10,,,,,,,,,SIGMETRICS Perform. Eval. Rev.,SIGMETRICS Perform. Eval. Rev.,,,340295,,,,,,,,7--10,,ACM,,,,0163-5999,SIGMETRICS Perform. Eval. Rev.,7,article,,,,27,1999,,,,,,,Dec. 1999,3,,December,4,,,,,"New York, NY, USA",,,
Proportional Differentiated Services: Delay Differentiation and Packet Scheduling,,,,,,Constantinos  Dovrolis and Dimitrios  Stiliadis and Parameswaran  Ramanathan,,,,10.1145/316194.316211,ACM-DL,1,,,120,,,,,,,,,SIGCOMM Comput. Commun. Rev.,SIGCOMM Comput. Commun. Rev.,,,316211,,,,,,,,109--120,,ACM,,,,0146-4833,SIGCOMM Comput. Commun. Rev.,109,article,,,,29,1999,,,,,,,Oct. 1999,4,,August,12,,,,,"New York, NY, USA",,,
Procedures for Optimization Problems with a Mixture of Bounds and General Linear Constraints,,,,,,Philip E. Gill and Walter  Murray and Michael A. Saunders and Margaret H. Wright,,,,10.1145/1271.1276,ACM-DL,1,,,298,,,,,,,,,ACM Trans. Math. Softw.,ACM Trans. Math. Softw.,,,1276,,,,,,,,282--298,,ACM,,,,0098-3500,ACM Trans. Math. Softw.,282,article,,,,10,1984,,,,,,,Sept. 1984,3,,August,17,,,,,"New York, NY, USA",,,
Power in Unity: Forming Teams in Large-scale Community Systems,,,,,,Aris  Anagnostopoulos and Luca  Becchetti and Carlos  Castillo and Aristides  Gionis and Stefano  Leonardi,,,,10.1145/1871437.1871515,ACM-DL,1,,,608,,,,,978-1-4503-0099-5,,,,,Proceedings of the 19th ACM International Conference on Information and Knowledge Management,"scheduling, task assignment, team formation",,1871515,,,,,,,,599--608,,ACM,,,,,Proceedings of the 19th ACM International Conference on Information and Knowledge Management,599,article,,,,,2010,CIKM '10,Proceedings of the 19th ACM International Conference on Information and Knowledge Management,,"Toronto, ON, Canada",,,,,,,10,,,,,"New York, NY, USA",,,
Policies for Dynamic Clock Scheduling,,,,,,"Dirk  Grunwald and Charles B. Morrey,III and Philip  Levis and Michael  Neufeld and Keith I. Farkas",,,,,ACM-DL,0,,,,,,,,,,,,,Proceedings of the 4th Conference on Symposium on Operating System Design & Implementation - Volume 4,,,1251235,,,,,,,,,,USENIX Association,,,,,Proceedings of the 4th Conference on Symposium on Operating System Design & Implementation - Volume 4,6,article,,,6,,2000,OSDI'00,Proceedings of the 4th Conference on Symposium on Operating System Design & Implementation - Volume 4,,"San Diego, California",,,,,,,,,,,,"Berkeley, CA, USA",,,
Personalizing a Dialogue System With Transfer Reinforcement Learning.,,,,,,"Kaixiang Mo, Yu Zhang, Shuangyin Li, Jiajun Li, Qiang Yang ",,,,,DBLP,0,,,,,,,,,,,,,,,,53863,,,,https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16104,,,,,,,,,,,,,Conference and Workshop Papers,https://dblp.org/rec/conf/aaai/MoZLLY18,,,,2018,,,,,,,,,conf/aaai/MoZLLY18,,,,,,,,,,AAAI
Personal Computing vs. Personal Computers,,,,,,John  Lehman,,,,10.1145/16687.16699,ACM-DL,1,,,102,,,,,0-89791-156-3,,,,,Proceedings of the Twenty-first Annual Conference on Computer Personnel Research,,,16699,,,,,,,,97--102,,ACM,,,,,Proceedings of the Twenty-first Annual Conference on Computer Personnel Research,97,article,,,,,1985,SIGCPR '85,Proceedings of the Twenty-first Annual Conference on Computer Personnel Research,,"Minneapolis, Minnesota, USA",,,,,,,6,,,,,"New York, NY, USA",,,
PARLGRAN: Parallelism Granularity Selection for Scheduling Task Chains on Dynamically Reconfigurable Architectures,,,,,,Sudarshan  Banerjee and Elaheh  Bozorgzadeh and Nikil  Dutt,,,,10.1145/1118299.1118419,ACM-DL,1,,,496,,,,,0-7803-9451-8,,,,,Proceedings of the 2006 Asia and South Pacific Design Automation Conference,"data-parallelism, granularity selection, linear placement, partial dynamic reconfiguration, scheduling",,1118419,,,,,,,,491--496,,IEEE Press,,,,,Proceedings of the 2006 Asia and South Pacific Design Automation Conference,491,article,,,,,2006,ASP-DAC '06,Proceedings of the 2006 Asia and South Pacific Design Automation Conference,,"Yokohama, Japan",,,,,,,6,,,,,"Piscataway, NJ, USA",,,
Packing to Angles and Sectors,,,,,,Piotr  Berman and Jieun  Jeong and Shiva Prasad Kasiviswanathan and Bhuvan  Urgaonkar,,,,10.1145/1248377.1248405,ACM-DL,1,,,180,,,,,978-1-59593-667-7,,,,,Proceedings of the Nineteenth Annual ACM Symposium on Parallel Algorithms and Architectures,"approximation algorithms, bin packing, directional antenna, geometric covering",,1248405,,,,,,,,171--180,,ACM,,,,,Proceedings of the Nineteenth Annual ACM Symposium on Parallel Algorithms and Architectures,171,article,,,,,2007,SPAA '07,Proceedings of the Nineteenth Annual ACM Symposium on Parallel Algorithms and Architectures,,"San Diego, California, USA",,,,,,,10,,,,,"New York, NY, USA",,,
Outpatients Appointment Scheduling with Multi-doctor Sharing Resources,,,,,,Nara  Yeon and Taesik  Lee and Hoon  Jang,,,,,ACM-DL,0,,,3329,,,,,978-1-4244-9864-2,,,,,Proceedings of the Winter Simulation Conference,,,2433920,,,,,,,,3318--3329,,Winter Simulation Conference,,,,,Proceedings of the Winter Simulation Conference,3318,article,,,,,2010,WSC '10,Proceedings of the Winter Simulation Conference,,"Baltimore, Maryland",,,,,,,12,,,,,,,,
Orion: Scaling Genomic Sequence Matching with Fine-grained Parallelization,,,,,,Kanak  Mahadik and Somali  Chaterji and Bowen  Zhou and Milind  Kulkarni and Saurabh  Bagchi,,,,10.1109/SC.2014.42,ACM-DL,1,,,460,,,,,978-1-4799-5500-8,,,,,"Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",,,2683643,,,,,,,,449--460,,IEEE Press,,,,,"Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",449,article,,,,,2014,SC '14,"Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",,"New Orleans, Louisana",,,,,,,12,,,,,"Piscataway, NJ, USA",,,
Optimization and System Selection: Simulation/Optimization Using Real-world Applications,,,,,,Jay  April and Fred  Glover and James  Kelly and Manuel  Laguna,,,,,ACM-DL,0,,,138,,,,,0-7803-7309-X,,,,,Proceedings of the 33Nd Conference on Winter Simulation,,,564143,,,,,,,,134--138,,IEEE Computer Society,,,,,Proceedings of the 33Nd Conference on Winter Simulation,134,article,,,,,2001,WSC '01,Proceedings of the 33Nd Conference on Winter Simulation,,"Arlington, Virginia",,,,,,,5,,,,,"Washington, DC, USA",,,
Optimal Testing for Crowd Workers,,,,,,Jonathan  Bragg and   Mausam and Daniel S. Weld,,,,,ACM-DL,0,,,974,,,,,978-1-4503-4239-1,,,,,Proceedings of the 2016 International Conference on Autonomous Agents &#38; Multiagent Systems,"crowdsourcing, reinforcement learning",,2937066,,,,,,,,966--974,,International Foundation for Autonomous Agents and Multiagent Systems,,,,,Proceedings of the 2016 International Conference on Autonomous Agents &#38; Multiagent Systems,966,article,,,,,2016,AAMAS '16,Proceedings of the 2016 International Conference on Autonomous Agents &#38; Multiagent Systems,,"Singapore, Singapore",,,,,,,9,,,,,"Richland, SC",,,
Optimal Sleep/Wake Scheduling for Time-synchronized Sensor Networks with QoS Guarantees,,,,,,Yan  Wu and Sonia  Fahmy and Ness B. Shroff,,,,10.1109/TNET.2008.2010450,ACM-DL,1,,,1521,,,,,,,,,IEEE/ACM Trans. Netw.,IEEE/ACM Trans. Netw.,"energy efficiency, sensor networks, sleep/wake scheduling, time synchronization",,1665850,,,,,,,,1508--1521,,IEEE Press,,,,1063-6692,IEEE/ACM Trans. Netw.,1508,article,,,,17,2009,,,,,,,October 2009,5,,October,14,,,,,"Piscataway, NJ, USA",,,
Optimal Scheduling Policies for a Class of Queues with Customer Deadlines to the Beginning of Service,,,,,,Shivendra S. Panwar and Don  Towsley and Jack K. Wolf,,,,10.1145/48014.48019,ACM-DL,1,,,844,,,,,,,,,J. ACM,J. ACM,,,48019,,,,,,,,832--844,,ACM,,,,0004-5411,J. ACM,832,article,,,,35,1988,,,,,,,Oct. 1988,4,,October,13,,,,,"New York, NY, USA",,,
Optimal Scheduling in Queueing Network Models of High-volume Commercial Web Sites,,,,,,Mark S. Squillante and Cathy H. Xia and Li  Zhang,,,,10.1145/507553.507573,ACM-DL,1,,,48,,,,,,,,,SIGMETRICS Perform. Eval. Rev.,SIGMETRICS Perform. Eval. Rev.,,,507573,,,,,,,,47--48,,ACM,,,,0163-5999,SIGMETRICS Perform. Eval. Rev.,47,article,,,,29,2001,,,,,,,December 2001,3,,December,2,,,,,"New York, NY, USA",,,
Optimal Channel Probing and Transmission Scheduling for Opportunistic Spectrum Access,,,,,,Nicholas B. Chang and Mingyan  Liu,,,,10.1109/TNET.2009.2014460,ACM-DL,1,,,1818,,,,,,,,,IEEE/ACM Trans. Netw.,IEEE/ACM Trans. Netw.,"channel probing, cognitive radio, dynamic programming, opportunistic spectrum access (OSA), optimal stopping, scheduling, stochastic optimization",,1721720,,,,,,,,1805--1818,,IEEE Press,,,,1063-6692,IEEE/ACM Trans. Netw.,1805,article,,,,17,2009,,,,,,,December 2009,6,,December,14,,,,,"Piscataway, NJ, USA",,,
Opportunities for Research on Numerical Control Machining,,,,,,David D. Grossman,,,,10.1145/5948.5951,ACM-DL,1,,,522,,,,,,,,,Commun. ACM,Commun. ACM,,,5951,,,,,,,,515--522,,ACM,,,,0001-0782,Commun. ACM,515,article,,,,29,1986,,,,,,,June 1986,6,,June,8,,,,,"New York, NY, USA",,,
Online Scheduling of Targeted Advertisements for IPTV,,,,,,Murali  Kodialam and T. V. Lakshman and Sarit  Mukherjee and Limin  Wang,,,,10.1109/TNET.2011.2143725,ACM-DL,1,,,1834,,,,,,,,,IEEE/ACM Trans. Netw.,IEEE/ACM Trans. Netw.,"IPTV, ad bid, ad budget, ad insertion, b-myopic algorithm, online scheduler, personalization, primal-dual scheme, targeted advertisement, triple-play service provider",,2365076,,,,,,,,1825--1834,,IEEE Press,,,,1063-6692,IEEE/ACM Trans. Netw.,1825,article,,,,19,2011,,,,,,,December 2011,6,,December,10,,,,,"Piscataway, NJ, USA",,,
Online Scheduling for Multi-core Shared Reconfigurable Fabric,,,,,,Liang  Chen and Thomas  Marconi and Tulika  Mitra,,,,,ACM-DL,0,,,585,,,,,978-3-9810801-8-6,,,,,"Proceedings of the Conference on Design, Automation and Test in Europe",,,2492853,,,,,,,,582--585,,EDA Consortium,,,,,"Proceedings of the Conference on Design, Automation and Test in Europe",582,article,,,,,2012,DATE '12,"Proceedings of the Conference on Design, Automation and Test in Europe",,"Dresden, Germany",,,,,,,4,,,,,"San Jose, CA, USA",,,
Online Learning of Timeout Policies for Dynamic Power Management,,,,,,Umair Ali Khan and Bernhard  Rinner,,,,10.1145/2529992,ACM-DL,1,,,121,,,,,,,,,ACM Trans. Embed. Comput. Syst.,ACM Trans. Embed. Comput. Syst.,"Dynamic power management, online learning, reinforcement learning, timeout policies, traffic monitoring",,2529992,,,,,,,,96:1--96:25,,ACM,,,,1539-9087,ACM Trans. Embed. Comput. Syst.,96,article,,,96,13,2014,,,,,,,November 2014,4,,March,25,,,,,"New York, NY, USA",,,
On-line Evolutionary Computation for Reinforcement Learning in Stochastic Domains,,,,,,Shimon  Whiteson and Peter  Stone,,,,10.1145/1143997.1144252,ACM-DL,1,,,1584,,,,,1-59593-186-4,,,,,Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation,"evolutionary computation, neural networks, on-line learning, reinforcement learning",,1144252,,,,,,,,1577--1584,,ACM,,,,,Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation,1577,article,,,,,2006,GECCO '06,Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation,,"Seattle, Washington, USA",,,,,,,8,,,,,"New York, NY, USA",,,
On Verifying Game Designs and Playing Strategies Using Reinforcement Learning,,,,,,Dimitrios  Kalles and Panagiotis  Kanellopoulos,,,,10.1145/372202.372204,ACM-DL,1,,,11,,,,,1-58113-287-5,,,,,Proceedings of the 2001 ACM Symposium on Applied Computing,"design verification, games, machine learning, playability, reinforcement learning",,372204,,,,,,,,6--11,,ACM,,,,,Proceedings of the 2001 ACM Symposium on Applied Computing,6,article,,,,,2001,SAC '01,Proceedings of the 2001 ACM Symposium on Applied Computing,,"Las Vegas, Nevada, USA",,,,,,,6,,,,,"New York, NY, USA",,,
On the Optimization of Schedules for MapReduce Workloads in the Presence of Shared Scans,,,,,,Joel  Wolf and Andrey  Balmin and Deepak  Rajan and Kirsten  Hildrum and Rohit  Khandekar and Sujay  Parekh and Kun-Lung  Wu and Rares  Vernica,,,,10.1007/s00778-012-0279-5,ACM-DL,1,,,609,,,,,,,,,The VLDB Journal,The VLDB Journal,"Allocation, Amortization, MapReduce, Optimization, Scheduling, Shared scans",,2387350,,,,,,,,589--609,,"Springer-Verlag New York, Inc.",,,,1066-8888,The VLDB Journal,589,article,,,,21,2012,,,,,,,October   2012,5,,October,21,,,,,"Secaucus, NJ, USA",,,
On Scheduling All-to-all Personalized Connections and Cost-effective Designs in WDM Rings,,,,,,Xijun  Zhang and Chunming  Qiao,,,,10.1109/90.779215,ACM-DL,1,,,445,,,,,,,,,IEEE/ACM Trans. Netw.,IEEE/ACM Trans. Netw.,"all-optical paths, all-to-all communications, lower bound, wavelength requirement",,312262,,,,,,,,435--445,,IEEE Press,,,,1063-6692,IEEE/ACM Trans. Netw.,435,article,,,,7,1999,,,,,,,June 1999,3,,June,11,,,,,"Piscataway, NJ, USA",,,
On Optimization of Polling Policy Represented by Neural Network,,,,,,Yutaka  Matsumoto,,,,10.1145/190809.190331,ACM-DL,1,,,190,,,,,,,,,SIGCOMM Comput. Commun. Rev.,SIGCOMM Comput. Commun. Rev.,,,190331,,,,,,,,181--190,,ACM,,,,0146-4833,SIGCOMM Comput. Commun. Rev.,181,article,,,,24,1994,,,,,,,Oct. 1994,4,,October,10,,,,,"New York, NY, USA",,,
Object-Focused Advice in Reinforcement Learning,,,,,,Samantha  Krening and Brent  Harrison and Karen M. Feigh and Charles  Isbell and Andrea  Thomaz,,,,,ACM-DL,0,,,1448,,,,,978-1-4503-4239-1,,,,,Proceedings of the 2016 International Conference on Autonomous Agents &#38; Multiagent Systems,"advice, human teachers, reinforcement learning",,2937203,,,,,,,,1447--1448,,International Foundation for Autonomous Agents and Multiagent Systems,,,,,Proceedings of the 2016 International Conference on Autonomous Agents &#38; Multiagent Systems,1447,article,,,,,2016,AAMAS '16,Proceedings of the 2016 International Conference on Autonomous Agents &#38; Multiagent Systems,,"Singapore, Singapore",,,,,,,2,,,,,"Richland, SC",,,
Nondecreasing Paths in a Weighted Graph or: How to Optimally Read a Train Schedule,,,,,,Virginia  Vassilevska,,,,,ACM-DL,0,,,472,,,,,,,,,,Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms,,,1347133,,,,,,,,465--472,,Society for Industrial and Applied Mathematics,,,,,Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms,465,article,,,,,2008,SODA '08,Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms,,"San Francisco, California",,,,,,,8,,,,,"Philadelphia, PA, USA",,,
Non-symmetric Preferences in the IPA Market with Reinforcement Learning,,,,,,Eduardo Rodrigues Gomes and Ryszard  Kowalczyk,,,,10.1109/WIIAT.2008.77,ACM-DL,1,,,430,,,,,978-0-7695-3496-1,,,,,Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 02,"Market-based Resource Allocation, Iterative Price Adjustment, Reinforcement Learning, Individual, Social Rewards",,1487187,,,,,,,,424--430,,IEEE Computer Society,,,,,Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 02,424,article,,,,,2008,WI-IAT '08,Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 02,,,,,,,,,7,,,,,"Washington, DC, USA",,,
New Approach for Advanced Cooperative Learning Algorithms Using RL Methods (ACLA),,,,,,Deepak A. Vidhate and Parag  Kulkarni,,,,10.1145/2983402.2983411,ACM-DL,1,,,20,,,,,978-1-4503-4301-5,,,,,Proceedings of the Third International Symposium on Computer Vision and the Internet,"Cooperative Learning, Dynamic Buyer Behavior, Reinforcement learning, Sarsa Learning",,2983411,,,,,,,,12--20,,ACM,,,,,Proceedings of the Third International Symposium on Computer Vision and the Internet,12,article,,,,,2016,VisionNet'16,Proceedings of the Third International Symposium on Computer Vision and the Internet,,"Jaipur, India",,,,,,,9,,,,,"New York, NY, USA",,,
Multi-Agent Systems Performance by Adaptive/Non-Adaptive Agent Selection,,,,,,Toshiharu  Sugawara and Kensuke  Fukuda and Toshio  Hirotsu and Shin-ya  Sato and Satoshi  Kurihara,,,,10.1109/IAT.2006.93,ACM-DL,1,,,559,,,,,0-7695-2748-5,,,,,Proceedings of the IEEE/WIC/ACM International Conference on Intelligent Agent Technology,,,1194603,,,,,,,,555--559,,IEEE Computer Society,,,,,Proceedings of the IEEE/WIC/ACM International Conference on Intelligent Agent Technology,555,article,,,,,2006,IAT '06,Proceedings of the IEEE/WIC/ACM International Conference on Intelligent Agent Technology,,,,,,,,,5,,,,,"Washington, DC, USA",,,
Multi-agent Reward Analysis for Learning in Noisy Domains,,,,,,Adrian  Agogino and Kagan  Turner,,,,10.1145/1082473.1082486,ACM-DL,1,,,88,,,,,1-59593-093-0,,,,,Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multiagent Systems,"multiagent systems, reinforcement learning, visualization",,1082486,,,,,,,,81--88,,ACM,,,,,Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multiagent Systems,81,article,,,,,2005,AAMAS '05,Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multiagent Systems,,The Netherlands,,,,,,,8,,,,,"New York, NY, USA",,,
Multi-Agent Patrolling with Reinforcement Learning,,,,,,Hugo  Santana and Geber  Ramalho and Vincent  Corruble and Bohdana  Ratitch,,,,10.1109/AAMAS.2004.180,ACM-DL,1,,,1129,,,,,1-58113-864-4,,,,,Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 3,,,1018881,,,,,,,,1122--1129,,IEEE Computer Society,,,,,Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 3,1122,article,,,,,2004,AAMAS '04,Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 3,,"New York, New York",,,,,,,8,,,,,"Washington, DC, USA",,,
MRL-SCSO - Multi-agent Reinforcement Learning-Based Self-Configuration and Self-Optimization Protocol for Unattended Wireless Sensor Networks.,,,,,,"A. Pravin Renold, S. Chandrakala",,,,10.1007/S11277-016-3729-3,DBLP,1,,,5079,,,,,,,,,,,,,206442,,,,https://doi.org/10.1007/s11277-016-3729-3,,,,,,,,,,,,5061,Journal Articles,https://dblp.org/rec/journals/wpc/RenoldC17,,96,,2017,,,,,,,,,journals/wpc/RenoldC17,,,,4,5061-5079,,,,,Wireless Personal Communications
Motivated Reinforcement Learning for Non-player Characters in Persistent Computer Game Worlds,,,,,,Kathryn  Merrick and Mary Lou Maher,,,,10.1145/1178823.1178828,ACM-DL,1,,,,,,,,1-59593-380-8,,,,,Proceedings of the 2006 ACM SIGCHI International Conference on Advances in Computer Entertainment Technology,"computer games, motivation, persistent virtual worlds, reinforcement learning",,1178828,,,,,,,,,,ACM,,,,,Proceedings of the 2006 ACM SIGCHI International Conference on Advances in Computer Entertainment Technology,3,article,,,3,,2006,ACE '06,Proceedings of the 2006 ACM SIGCHI International Conference on Advances in Computer Entertainment Technology,,"Hollywood, California, USA",,,,,,,,,,,,"New York, NY, USA",,,
Modelling User Behaviors with Evolving Users and Catalogs of Evolving Items,,,,,,Leonardo  Cella,,,,10.1145/3099023.3102251,ACM-DL,1,,,116,,,,,978-1-4503-5067-9,,,,,"Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization","evolving items, evolving users, online learning, recommender systems, reinforcement learning, sequental decision making",,3102251,,,,,,,,115--116,,ACM,,,,,"Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization",115,article,,,,,2017,UMAP '17,"Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization",,"Bratislava, Slovakia",,,,,,,2,,,,,"New York, NY, USA",,,
Modeling Motivation for Adaptive Nonplayer Characters in Dynamic Computer Game Worlds,,,,,,Kathryn  Merrick,,,,10.1145/1324198.1324203,ACM-DL,1,,,37,,,,,,,,,Comput. Entertain.,Comput. Entertain.,"computer games, motivation, neural nets, nonplayer characters, persistent virtual worlds, reinforcement learning",,1324203,,,,,,,,5:1--5:32,,ACM,,,,1544-3574,Comput. Entertain.,5,article,,,5,5,2008,,,,,,,10/01/2007,4,,March,32,,,,,"New York, NY, USA",,,
Minimum-latency Aggregation Scheduling in Wireless Sensor Networks Under Physical Interference Model,,,,,,Hongxing  Li and Qiang Sheng  Hua and Chuan  Wu and Francis Chi Moon Lau,,,,10.1145/1868521.1868581,ACM-DL,1,,,367,,,,,978-1-4503-0274-6,,,,,"Proceedings of the 13th ACM International Conference on Modeling, Analysis, and Simulation of Wireless and Mobile Systems","data aggregation, minimum latency, physical interference model, wireless sensor networks",,1868581,,,,,,,,360--367,,ACM,,,,,"Proceedings of the 13th ACM International Conference on Modeling, Analysis, and Simulation of Wireless and Mobile Systems",360,article,,,,,2010,MSWIM '10,"Proceedings of the 13th ACM International Conference on Modeling, Analysis, and Simulation of Wireless and Mobile Systems",,"Bodrum, Turkey",,,,,,,8,,,,,"New York, NY, USA",,,
Minimizing Latency and Memory in DSMS: A Unified Approach to Quasi-optimal Scheduling,,,,,,Yijian  Bai and Carlo  Zaniolo,,,,10.1145/1379272.1379284,ACM-DL,1,,,67,,,,,978-159593-963-0,,,,,Proceedings of the 2Nd International Workshop on Scalable Stream Processing System,"data stream management systems, operator scheduling",,1379284,,,,,,,,58--67,,ACM,,,,,Proceedings of the 2Nd International Workshop on Scalable Stream Processing System,58,article,,,,,2008,SSPS '08,Proceedings of the 2Nd International Workshop on Scalable Stream Processing System,,"Nantes, France",,,,,,,10,,,,,"New York, NY, USA",,,
Memory Efficient Minimum Substring Partitioning,,,,,,Yang  Li and Pegah  Kamousi and Fangqiu  Han and Shengqi  Yang and Xifeng  Yan and Subhash  Suri,,,,10.14778/2535569.2448951,ACM-DL,1,,,180,,,,,,,,,Proc. VLDB Endow.,Proc. VLDB Endow.,,,2448951,,,,,,,,169--180,,VLDB Endowment,,,,2150-8097,Proc. VLDB Endow.,169,article,,,,6,2013,,,,,,,January 2013,3,,January,12,,,,,,,,
Measurement-based Admission Control with Aggregate Traffic Envelopes,,,,,,Jingyu  Qiu and Edward W. Knightly,,,,10.1109/90.917076,ACM-DL,1,,,210,,,,,,,,,IEEE/ACM Trans. Netw.,IEEE/ACM Trans. Netw.,"admission control, quality of service, real-time flows, traffic envelopes",,379389,,,,,,,,199--210,,IEEE Press,,,,1063-6692,IEEE/ACM Trans. Netw.,199,article,,,,9,2001,,,,,,,April 2001,2,,April,12,,,,,"Piscataway, NJ, USA",,,
Management of Crowdsourced First-person Video: Street View Live,,,,,,Steven  Bohez and Jens  Mostaert and Tim  Verbelen and Pieter  Simoens and Bart  Dhoedt,,,,10.1145/2677972.2677984,ACM-DL,1,,,19,,,,,978-1-4503-3304-7,,,,,Proceedings of the 13th International Conference on Mobile and Ubiquitous Multimedia,"cloud computing, cloudlets, crowdsourcing, mobile computing, street view",,2677984,,,,,,,,11--19,,ACM,,,,,Proceedings of the 13th International Conference on Mobile and Ubiquitous Multimedia,11,article,,,,,2014,MUM '14,Proceedings of the 13th International Conference on Mobile and Ubiquitous Multimedia,,"Melbourne, Victoria, Australia",,,,,,,9,,,,,"New York, NY, USA",,,
Low-energy Intra-task Voltage Scheduling Using Static Timing Analysis,,,,,,Dongkun  Shin and Jihong  Kim and Seongsoo  Lee,,,,10.1145/378239.378551,ACM-DL,1,,,443,,,,,1-58113-297-2,,,,,Proceedings of the 38th Annual Design Automation Conference,,,378551,,,,,,,,438--443,,ACM,,,,,Proceedings of the 38th Annual Design Automation Conference,438,article,,,,,2001,DAC '01,Proceedings of the 38th Annual Design Automation Conference,,"Las Vegas, Nevada, USA",,,,,,,6,,,,,"New York, NY, USA",,,
Load Balancing for Parallel Forwarding,,,,,,Weiguang  Shi and M. H. MacGregor and Pawel  Gburzynski,,,,10.1109/TNET.2005.852881,ACM-DL,1,,,801,,,,,,,,,IEEE/ACM Trans. Netw.,IEEE/ACM Trans. Netw.,"Zipf-like distribution, load balancing, parallel IP forwarding",,1088749,,,,,,,,790--801,,IEEE Press,,,,1063-6692,IEEE/ACM Trans. Netw.,790,article,,,,13,2005,,,,,,,August 2005,4,,August,12,,,,,"Piscataway, NJ, USA",,,
Leveraging Human Routine Models to Detect and Generate Human Behaviors,,,,,,Nikola  Banovic and Anqi  Wang and Yanfeng  Jin and Christie  Chang and Julian  Ramos and Anind  Dey and Jennifer  Mankoff,,,,10.1145/3025453.3025571,ACM-DL,1,,,6694,,,,,978-1-4503-4655-9,,,,,Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems,"inverse reinforcement learning, maximum entropy",,3025571,,,,,,,,6683--6694,,ACM,,,,,Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems,6683,article,,,,,2017,CHI '17,Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems,,"Denver, Colorado, USA",,,,,,,12,,,,,"New York, NY, USA",,,
Learning User Preferences for Wireless Services Provisioning,,,,,,G.  Lee and S.  Bauer and P.  Faratin and J.  Wroclawski,,,,10.1109/AAMAS.2004.161,ACM-DL,1,,,487,,,,,1-58113-864-4,,,,,Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 1,,,1018782,,,,,,,,480--487,,IEEE Computer Society,,,,,Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 1,480,article,,,,,2004,AAMAS '04,Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 1,,"New York, New York",,,,,,,8,,,,,"Washington, DC, USA",,,
Learning User Preferences by Observing User-Items Interactions in an IoT Augmented Space,,,,,,David  Massimo and Mehdi  Elahi and Francesco  Ricci,,,,10.1145/3099023.3099070,ACM-DL,1,,,40,,,,,978-1-4503-5067-9,,,,,"Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization","cultural heritage, implicit feedback, internet of things, inverse reinforcement learning, museum, recommder systems, reinforcement learning, user modelling",,3099070,,,,,,,,35--40,,ACM,,,,,"Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization",35,article,,,,,2017,UMAP '17,"Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization",,"Bratislava, Slovakia",,,,,,,6,,,,,"New York, NY, USA",,,
Learning to Collaborate: Multi-Scenario Ranking via Multi-Agent Reinforcement Learning,,,,,,Jun  Feng and Heng  Li and Minlie  Huang and Shichen  Liu and Wenwu  Ou and Zhirong  Wang and Xiaoyan  Zhu,,,,10.1145/3178876.3186165,ACM-DL,1,,,1948,,,,,978-1-4503-5639-8,,,,,Proceedings of the 2018 World Wide Web Conference,"joint optimization, learning to rank, multi-agent learning, reinforcement learning",,3186165,,,,,,,,1939--1948,,International World Wide Web Conferences Steering Committee,,,,,Proceedings of the 2018 World Wide Web Conference,1939,article,,,,,2018,WWW '18,Proceedings of the 2018 World Wide Web Conference,,"Lyon, France",,,,,,,10,,,,,"Republic and Canton of Geneva, Switzerland",,,
Learning to Be Fair in Multiplayer Ultimatum Games: (Extended Abstract),,,,,,Fernando P. Santos and Francisco C. Santos and Francisco  Melo and Ana  Paiva and Jorge M. Pacheco,,,,,ACM-DL,0,,,1382,,,,,978-1-4503-4239-1,,,,,Proceedings of the 2016 International Conference on Autonomous Agents &#38; Multiagent Systems,"fairness, groups, learning, multiagent systems, ultimatum game",,2937170,,,,,,,,1381--1382,,International Foundation for Autonomous Agents and Multiagent Systems,,,,,Proceedings of the 2016 International Conference on Autonomous Agents &#38; Multiagent Systems,1381,article,,,,,2016,AAMAS '16,Proceedings of the 2016 International Conference on Autonomous Agents &#38; Multiagent Systems,,"Singapore, Singapore",,,,,,,2,,,,,"Richland, SC",,,
Learning the IPA Market with Individual and Social Rewards,,,,,,Eduardo Rodrigues Gomes and Ryszard  Kowalczyk,,,,10.1109/IAT.2007.69,ACM-DL,1,,,334,,,,,0-7695-3027-3,,,,,Proceedings of the 2007 IEEE/WIC/ACM International Conference on Intelligent Agent Technology,,,1331681,,,,,,,,328--334,,IEEE Computer Society,,,,,Proceedings of the 2007 IEEE/WIC/ACM International Conference on Intelligent Agent Technology,328,article,,,,,2007,IAT '07,Proceedings of the 2007 IEEE/WIC/ACM International Conference on Intelligent Agent Technology,,,,,,,,,7,,,,,"Washington, DC, USA",,,
Learning Sequences of Actions in Collectives of Autonomous Agents,,,,,,Kagan  Tumer and Adrian K. Agogino and David H. Wolpert,,,,10.1145/544741.544832,ACM-DL,1,,,385,,,,,1-58113-480-0,,,,,Proceedings of the First International Joint Conference on Autonomous Agents and Multiagent Systems: Part 1,"MAS, Q-learning, reinforcement learning",,544832,,,,,,,,378--385,,ACM,,,,,Proceedings of the First International Joint Conference on Autonomous Agents and Multiagent Systems: Part 1,378,article,,,,,2002,AAMAS '02,Proceedings of the First International Joint Conference on Autonomous Agents and Multiagent Systems: Part 1,,"Bologna, Italy",,,,,,,8,,,,,"New York, NY, USA",,,
Learning Individual Mating Preferences,,,,,,Lisa M. Guntly and Daniel R. Tauritz,,,,10.1145/2001576.2001721,ACM-DL,1,,,1076,,,,,978-1-4503-0557-0,,,,,Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation,"evolutionary algorithm, mate selection, reinforcement learning",,2001721,,,,,,,,1069--1076,,ACM,,,,,Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation,1069,article,,,,,2011,GECCO '11,Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation,,"Dublin, Ireland",,,,,,,8,,,,,"New York, NY, USA",,,
Learning Cooperation Through Bidding,,,,,,Ron  Sun and Dehu  Qi,,,,10.1109/AAMAS.2004.158,ACM-DL,1,,,1291,,,,,1-58113-864-4,,,,,Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 3,,,1018920,,,,,,,,1290--1291,,IEEE Computer Society,,,,,Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 3,1290,article,,,,,2004,AAMAS '04,Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 3,,"New York, New York",,,,,,,2,,,,,"Washington, DC, USA",,,
Learning Classifier System Ensemble for Data Mining,,,,,,Yang  Gao and Joshua Zhexue Huang and Hongqiang  Rong and Daqian  Gu,,,,10.1145/1102256.1102268,ACM-DL,1,,,66,,,,,,,,,,Proceedings of the 7th Annual Workshop on Genetic and Evolutionary Computation,"ensemble, learning classifier system",,1102268,,,,,,,,63--66,,ACM,,,,,Proceedings of the 7th Annual Workshop on Genetic and Evolutionary Computation,63,article,,,,,2005,GECCO '05,Proceedings of the 7th Annual Workshop on Genetic and Evolutionary Computation,,"Washington, D.C.",,,,,,,4,,,,,"New York, NY, USA",,,
LEAP: Learning to Prescribe Effective and Safe Treatment Combinations for Multimorbidity,,,,,,Yutao  Zhang and Robert  Chen and Jie  Tang and Walter F. Stewart and Jimeng  Sun,,,,10.1145/3097983.3098109,ACM-DL,1,,,1324,,,,,978-1-4503-4887-4,,,,,Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,"multi-instance multilabel learning, multimorbidity, treatment recommendation",,3098109,,,,,,,,1315--1324,,ACM,,,,,Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,1315,article,,,,,2017,KDD '17,Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,,"Halifax, NS, Canada",,,,,,,10,,,,,"New York, NY, USA",,,
Knowledge Revision for Reinforcement Learning with Abstract MDPs,,,,,,Kyriakos  Efthymiadis and Sam  Devlin and Daniel  Kudenko,,,,,ACM-DL,0,,,1536,,,,,978-1-4503-2738-1,,,,,Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems,"knowledge revision, reinforcement learning, reward shaping",,2616049,,,,,,,,1535--1536,,International Foundation for Autonomous Agents and Multiagent Systems,,,,,Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems,1535,article,,,,,2014,AAMAS '14,Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems,,"Paris, France",,,,,,,2,,,,,"Richland, SC",,,
Joint Scheduling and Admission Control for ATS-based Switching Nodes,,,,,,Jay  Hyman and Aurel A. Lazar and Giovanni  Pacifici,,,,10.1145/144179.144286,ACM-DL,1,,,234,,,,,0-89791-525-9,,,,,Conference Proceedings on Communications Architectures &Amp; Protocols,,,144286,,,,,,,,223--234,,ACM,,,,,Conference Proceedings on Communications Architectures &Amp; Protocols,223,article,,,,,1992,SIGCOMM '92,Conference Proceedings on Communications Architectures &Amp; Protocols,,"Baltimore, Maryland, USA",,,,,,,12,,,,,"New York, NY, USA",,,
Interaction-aware Scheduling of Report-generation Workloads,,,,,,Mumtaz  Ahmad and Ashraf  Aboulnaga and Shivnath  Babu and Kamesh  Munagala,,,,10.1007/s00778-011-0217-y,ACM-DL,1,,,615,,,,,,,,,The VLDB Journal,The VLDB Journal,"Business intelligence, Experiment-driven performance modeling, Query interactions, Report generation, Scheduling, Workload management",,2035115,,,,,,,,589--615,,"Springer-Verlag New York, Inc.",,,,1066-8888,The VLDB Journal,589,article,,,,20,2011,,,,,,,August    2011,4,,August,27,,,,,"Secaucus, NJ, USA",,,
Intelligent Agent Strategies for Residential Customers in Local Electricity Markets,,,,,,Esther  Mengelkamp and Johannes  G&#228;rttner and Christof  Weinhardt,,,,10.1145/3208903.3208907,ACM-DL,1,,,107,,,,,978-1-4503-5767-8,,,,,Proceedings of the Ninth International Conference on Future Energy Systems,"Local electricity market, intelligent agents, reinforcement learning",,3208907,,,,,,,,97--107,,ACM,,,,,Proceedings of the Ninth International Conference on Future Energy Systems,97,article,,,,,2018,e-Energy '18,Proceedings of the Ninth International Conference on Future Energy Systems,,"Karlsruhe, Germany",,,,,,,11,,,,,"New York, NY, USA",,,
Intelligent Adapted e-Learning System Based on Deep Reinforcement Learning,,,,,,Mohammed  El Fouki and Noura  Aknin and K. Ed El. Kadiri,,,,10.1145/3167486.3167574,ACM-DL,1,,,91,,,,,978-1-4503-5306-9,,,,,Proceedings of the 2Nd International Conference on Computing and Wireless Communication Systems,"Decision Support System, Deep Neural network, E-learning, Learning Management System (LMS), Personalized learning, Reinforcement Learning",,3167574,,,,,,,,85:1--85:6,,ACM,,,,,Proceedings of the 2Nd International Conference on Computing and Wireless Communication Systems,85,article,,,85,,2017,ICCWCS'17,Proceedings of the 2Nd International Conference on Computing and Wireless Communication Systems,,"Larache, Morocco",,,,,,,6,,,,,"New York, NY, USA",,,
Instruction Set Extension Exploration in Multiple-issue Architecture,,,,,,I-Wei  Wu and Zhi-Yuan  Chen and Jyh-Jiun  Shann and Chung-Ping  Chung,,,,10.1145/1403375.1403560,ACM-DL,1,,,769,,,,,978-3-9810801-3-1,,,,,"Proceedings of the Conference on Design, Automation and Test in Europe",,,1403560,,,,,,,,764--769,,ACM,,,,,"Proceedings of the Conference on Design, Automation and Test in Europe",764,article,,,,,2008,DATE '08,"Proceedings of the Conference on Design, Automation and Test in Europe",,"Munich, Germany",,,,,,,6,,,,,"New York, NY, USA",,,
Instance-sensitive Robustness Guarantees for Sequencing with Unknown Packing and Covering Constraints,,,,,,Nicole  Megow and Julian  Mestre,,,,10.1145/2422436.2422490,ACM-DL,1,,,504,,,,,978-1-4503-1859-4,,,,,Proceedings of the 4th Conference on Innovations in Theoretical Computer Science,"instance-sensitive worst-case guarantee, knapsack, robustness factor, sequencing, universal solution",,2422490,,,,,,,,495--504,,ACM,,,,,Proceedings of the 4th Conference on Innovations in Theoretical Computer Science,495,article,,,,,2013,ITCS '13,Proceedings of the 4th Conference on Innovations in Theoretical Computer Science,,"Berkeley, California, USA",,,,,,,10,,,,,"New York, NY, USA",,,
Incentives for Sharing in Peer-to-peer Networks,,,,,,Philippe  Golle and Kevin  Leyton-Brown and Ilya  Mironov,,,,10.1145/501158.501193,ACM-DL,1,,,267,,,,,1-58113-387-1,,,,,Proceedings of the 3rd ACM Conference on Electronic Commerce,,,501193,,,,,,,,264--267,,ACM,,,,,Proceedings of the 3rd ACM Conference on Electronic Commerce,264,article,,,,,2001,EC '01,Proceedings of the 3rd ACM Conference on Electronic Commerce,,"Tampa, Florida, USA",,,,,,,4,,,,,"New York, NY, USA",,,
Improved AP Association Management Using Machine Learning,,,,,,Tingting  Sun and Wade  Trappe and Yanyong  Zhang,,,,10.1145/1942268.1942271,ACM-DL,1,,,6,,,,,,,,,SIGMOBILE Mob. Comput. Commun. Rev.,SIGMOBILE Mob. Comput. Commun. Rev.,,,1942271,,,,,,,,4--6,,ACM,,,,1559-1662,SIGMOBILE Mob. Comput. Commun. Rev.,4,article,,,,14,2010,,,,,,,October 2010,4,,November,3,,,,,"New York, NY, USA",,,
iCBS: Incremental Cost-based Scheduling Under Piecewise Linear SLAs,,,,,,Yun  Chi and Hyun Jin Moon and Hakan  Hacig&#252;m&#252;&#351;,,,,10.14778/2002938.2002942,ACM-DL,1,,,574,,,,,,,,,Proc. VLDB Endow.,Proc. VLDB Endow.,,,2002942,,,,,,,,563--574,,VLDB Endowment,,,,2150-8097,Proc. VLDB Endow.,563,article,,,,4,2011,,,,,,,June 2011,9,,June,12,,,,,,,,
Hoeffding and Bernstein Races for Selecting Policies in Evolutionary Direct Policy Search,,,,,,Verena  Heidrich-Meisner and Christian  Igel,,,,10.1145/1553374.1553426,ACM-DL,1,,,408,,,,,978-1-60558-516-1,,,,,Proceedings of the 26th Annual International Conference on Machine Learning,,,1553426,,,,,,,,401--408,,ACM,,,,,Proceedings of the 26th Annual International Conference on Machine Learning,401,article,,,,,2009,ICML '09,Proceedings of the 26th Annual International Conference on Machine Learning,,"Montreal, Quebec, Canada",,,,,,,8,,,,,"New York, NY, USA",,,
High-Dimensional Function Approximation for Knowledge-Free Reinforcement Learning: A Case Study in SZ-Tetris,,,,,,Wojciech  Ja&#347;kowski and Marcin  Szubert and Pawe&#322;  Liskowski and Krzysztof  Krawiec,,,,10.1145/2739480.2754783,ACM-DL,1,,,573,,,,,978-1-4503-3472-3,,,,,Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation,"cma-es, covariance matrix adaptation, function approximation, knowledge-free representations, video games, n-tuple system, reinforcement learning, vd-cma",,2754783,,,,,,,,567--573,,ACM,,,,,Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation,567,article,,,,,2015,GECCO '15,Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation,,"Madrid, Spain",,,,,,,7,,,,,"New York, NY, USA",,,
Group-guaranteed Channel Capacity in Multimedia Storage Servers,,,,,,Athanassios K. Tsiolis and Mary K. Vernon,,,,10.1145/258612.258697,ACM-DL,1,,,297,,,,,0-89791-909-2,,,,,Proceedings of the 1997 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems,,,258697,,,,,,,,285--297,,ACM,,,,,Proceedings of the 1997 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems,285,article,,,,,1997,SIGMETRICS '97,Proceedings of the 1997 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems,,"Seattle, Washington, USA",,,,,,,13,,,,,"New York, NY, USA",,,
Group Priority Scheduling,,,,,,Simon S. Lam and Geoffrey G. Xie,,,,10.1109/90.588083,ACM-DL,1,,,218,,,,,,,,,IEEE/ACM Trans. Netw.,IEEE/ACM Trans. Netw.,"ATM block transfer, burst scheduling network, delay guarantee, group priority, integrated services, packet scheduling, real-time flow",,255722,,,,,,,,205--218,,IEEE Press,,,,1063-6692,IEEE/ACM Trans. Netw.,205,article,,,,5,1997,,,,,,,April 1997,2,,April,14,,,,,"Piscataway, NJ, USA",,,
Globally Optimal Multi-agent Reinforcement Learning Parameters in Distributed Task Assignment,,,,,,Dominik  Dahlem and William  Harrison,,,,10.1109/WI-IAT.2009.122,ACM-DL,1,,,35,,,,,978-0-7695-3801-3,,,,,Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology - Volume 02,"Queueing Networks, Markov Decision Problem, Multi-agent Reinforcement Learning, Kriging",,1632482,,,,,,,,28--35,,IEEE Computer Society,,,,,Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology - Volume 02,28,article,,,,,2009,WI-IAT '09,Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology - Volume 02,,,,,,,,,8,,,,,"Washington, DC, USA",,,
Genomic Region Operation Kit for Flexible Processing of Deep Sequencing Data,,,,,,Kristian  Ovaska and Lauri  Lyly and Biswajyoti  Sahu and Olli A. Janne and Sampsa  Hautaniemi,,,,10.1109/TCBB.2012.170,ACM-DL,1,,,206,,,,,,,,,IEEE/ACM Trans. Comput. Biol. Bioinformatics,IEEE/ACM Trans. Comput. Biol. Bioinformatics,"Bioinformatics,Genomics,Databases,Benchmark testing,Algebra,Software,Complexity theory,software,Bioinformatics,deep sequencing,genomic data analysis,region set algebra",,2491947,,,,,,,,200--206,,IEEE Computer Society Press,,,,1545-5963,IEEE/ACM Trans. Comput. Biol. Bioinformatics,200,article,,,,10,2013,,,,,,,January 2013,1,,January,7,,,,,"Los Alamitos, CA, USA",,,
Genetic Algorithms to Solve a Single Machine Multiple Orders Per Job Scheduling Problem,,,,,,Oleh  Sobeyko and Lars  M&#246;nch,,,,,ACM-DL,0,,,2503,,,,,978-1-4244-9864-2,,,,,Proceedings of the Winter Simulation Conference,,,2433815,,,,,,,,2493--2503,,Winter Simulation Conference,,,,,Proceedings of the Winter Simulation Conference,2493,article,,,,,2010,WSC '10,Proceedings of the Winter Simulation Conference,,"Baltimore, Maryland",,,,,,,11,,,,,,,,
Generating a Deterministic Task Migration Path for Multiprocessor Scheduling,,,,,,Ming-fang  Wang,,,,10.1145/326619.326816,ACM-DL,1,,,482,,,,,0-89791-647-6,,,,,Proceedings of the 1994 ACM Symposium on Applied Computing,,,326816,,,,,,,,478--482,,ACM,,,,,Proceedings of the 1994 ACM Symposium on Applied Computing,478,article,,,,,1994,SAC '94,Proceedings of the 1994 ACM Symposium on Applied Computing,,"Phoenix, Arizona, USA",,,,,,,5,,,,,"New York, NY, USA",,,
Gene Name Disambiguation Using Multi-scope Species Detection,,,,,,Jui-Chen  Hsiao and Chih-Hsuan  Wei and Hung-Yu  Kao,,,,10.1109/TCBB.2013.139,ACM-DL,1,,,62,,,,,,,,,IEEE/ACM Trans. Comput. Biol. Bioinformatics,IEEE/ACM Trans. Comput. Biol. Bioinformatics,"biomedical text mining, focus species detection, gene name disambiguation",,2674802,,,,,,,,55--62,,IEEE Computer Society Press,,,,1545-5963,IEEE/ACM Trans. Comput. Biol. Bioinformatics,55,article,,,,11,2014,,,,,,,January/February 2014,1,,January,8,,,,,"Los Alamitos, CA, USA",,,
Flexible Bandwidth Allocation in High-capacity Packet Switches,,,,,,Aleksandra  Smiljani&#263;,,,,10.1109/90.993308,ACM-DL,1,,,293,,,,,,,,,IEEE/ACM Trans. Netw.,IEEE/ACM Trans. Netw.,"packet switching, scalability, scheduling, switch with input buffers",,508336,,,,,,,,287--293,,IEEE Press,,,,1063-6692,IEEE/ACM Trans. Netw.,287,article,,,,10,2002,,,,,,,April 2002,2,,April,7,,,,,"Piscataway, NJ, USA",,,
First-Person Shooter Game for Virtual Reality Headset with Advanced Multi-Agent Intelligent System,,,,,,Ilya  Makarov and Mikhail  Tokmakov and Pavel  Polyakov and Peter  Zyuzin and Maxim  Martynov and Oleg  Konoplya and George  Kuznetsov and Ivan  Guschenko-Cheverda and Maxim  Uriev and Ivan  Mokeev and Olga  Gerasimova and Lada  Tokmakova and Alexey  Kosmachev,,,,10.1145/2964284.2973826,ACM-DL,1,,,736,,,,,978-1-4503-3603-1,,,,,Proceedings of the 2016 ACM on Multimedia Conference,"first-person shooter, game artificial intelligence, reinforcement learning, unreal engine, virtual reality",,2973826,,,,,,,,735--736,,ACM,,,,,Proceedings of the 2016 ACM on Multimedia Conference,735,article,,,,,2016,MM '16,Proceedings of the 2016 ACM on Multimedia Conference,,"Amsterdam, The Netherlands",,,,,,,2,,,,,"New York, NY, USA",,,
Finite Automata with Time-delay Blocks,,,,,,Krishnendu  Chatterjee and Thomas A. Henzinger and Vinayak S. Prabhu,,,,10.1145/2380356.2380370,ACM-DL,1,,,52,,,,,978-1-4503-1425-1,,,,,Proceedings of the Tenth ACM International Conference on Embedded Software,"buffers, model checking, queues, time-delay systems",,2380370,,,,,,,,43--52,,ACM,,,,,Proceedings of the Tenth ACM International Conference on Embedded Software,43,article,,,,,2012,EMSOFT '12,Proceedings of the Tenth ACM International Conference on Embedded Software,,"Tampere, Finland",,,,,,,10,,,,,"New York, NY, USA",,,
Finding Patterns in Protein Sequences by Using a Hybrid Multiobjective Teaching Learning Based Optimization Algorithm,,,,,,David L. Gonz&#225;lez-&#193;lvarez and Miguel A. Vega-Rodr&#237;guez and &#193;lvaro  Rubio-Largo,,,,10.1109/TCBB.2014.2369043,ACM-DL,1,,,666,,,,,,,,,IEEE/ACM Trans. Comput. Biol. Bioinformatics,IEEE/ACM Trans. Comput. Biol. Bioinformatics,"PROSITE, hybrid algorithm, multiobjective optimization, proteins, teaching learning based optimization",,2817065,,,,,,,,656--666,,IEEE Computer Society Press,,,,1545-5963,IEEE/ACM Trans. Comput. Biol. Bioinformatics,656,article,,,,12,2015,,,,,,,May/June 2015,3,,May,11,,,,,"Los Alamitos, CA, USA",,,
Finding Better Teammates in a Semi-cooperative Multi-agent System,,,,,,Sara  Amini and Mohsen  Afsharchi,,,,10.1109/WI-IAT.2014.161,ACM-DL,1,,,150,,,,,978-1-4799-4143-8,,,,,Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 03,"Team Formation, Reinforcement Learning, Distributed Resource Allocation",,2682672,,,,,,,,143--150,,IEEE Computer Society,,,,,Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 03,143,article,,,,,2014,WI-IAT '14,Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 03,,,,,,,,,8,,,,,"Washington, DC, USA",,,
Feature-based Dynamic Pricing,,,,,,Maxime C. Cohen and Ilan  Lobel and Renato  Paes Leme,,,,10.1145/2940716.2940728,ACM-DL,1,,,817,,,,,978-1-4503-3936-0,,,,,Proceedings of the 2016 ACM Conference on Economics and Computation,"contextual bandits, ellipsoid method, online ads, online learning, revenue management",,2940728,,,,,,,,817--817,,ACM,,,,,Proceedings of the 2016 ACM Conference on Economics and Computation,817,article,,,,,2016,EC '16,Proceedings of the 2016 ACM Conference on Economics and Computation,,"Maastricht, The Netherlands",,,,,,,1,,,,,"New York, NY, USA",,,
Fairness Incentives for Myopic Agents,,,,,,Sampath  Kannan and Michael  Kearns and Jamie  Morgenstern and Mallesh  Pai and Aaron  Roth and Rakesh  Vohra and Zhiwei Steven Wu,,,,10.1145/3033274.3085154,ACM-DL,1,,,386,,,,,978-1-4503-4527-9,,,,,Proceedings of the 2017 ACM Conference on Economics and Computation,,,3085154,,,,,,,,369--386,,ACM,,,,,Proceedings of the 2017 ACM Conference on Economics and Computation,369,article,,,,,2017,EC '17,Proceedings of the 2017 ACM Conference on Economics and Computation,,"Cambridge, Massachusetts, USA",,,,,,,18,,,,,"New York, NY, USA",,,
Fair Algorithms for Machine Learning,,,,,,Michael  Kearns,,,,10.1145/3033274.3084096,ACM-DL,1,,,1,,,,,978-1-4503-4527-9,,,,,Proceedings of the 2017 ACM Conference on Economics and Computation,keynote,,3084096,,,,,,,,1--1,,ACM,,,,,Proceedings of the 2017 ACM Conference on Economics and Computation,1,article,,,,,2017,EC '17,Proceedings of the 2017 ACM Conference on Economics and Computation,,"Cambridge, Massachusetts, USA",,,,,,,1,,,,,"New York, NY, USA",,,
Face Image Generation System Using Attribute Information with DCGANs,,,,,,Yurika  Sagawa and Masafumi  Hagiwara,,,,10.1145/3184066.3184071,ACM-DL,1,,,113,,,,,978-1-4503-6336-5,,,,,Proceedings of the 2Nd International Conference on Machine Learning and Soft Computing,"convolutional neural networks, deep convolutional generative adversarial networks, image generation",,3184071,,,,,,,,109--113,,ACM,,,,,Proceedings of the 2Nd International Conference on Machine Learning and Soft Computing,109,article,,,,,2018,ICMLSC '18,Proceedings of the 2Nd International Conference on Machine Learning and Soft Computing,,"Phu Quoc Island, Viet Nam",,,,,,,5,,,,,"New York, NY, USA",,,
Exploiting Structure and Utilizing Agent-centric Rewards to Promote Coordination in Large Multiagent Systems,,,,,,Chris  HolmesParker and Adrian  Agogino and Kagan  Tumer,,,,,ACM-DL,0,,,1182,,,,,978-1-4503-1993-5,,,,,Proceedings of the 2013 International Conference on Autonomous Agents and Multi-agent Systems,"factored MDPs, reward shaping, scalability",,2485132,,,,,,,,1181--1182,,International Foundation for Autonomous Agents and Multiagent Systems,,,,,Proceedings of the 2013 International Conference on Autonomous Agents and Multi-agent Systems,1181,article,,,,,2013,AAMAS '13,Proceedings of the 2013 International Conference on Autonomous Agents and Multi-agent Systems,,"St. Paul, MN, USA",,,,,,,2,,,,,"Richland, SC",,,
Exploiting Mobility in Proportional Fair Cellular Scheduling: Measurements and Algorithms,,,,,,Robert  Margolies and Ashwin  Sridharan and Vaneet  Aggarwal and Rittwik  Jana and N. K. Shankaranarayanan and Vinay A. Vaishampayan and Gil  Zussman,,,,10.1109/TNET.2014.2362928,ACM-DL,1,,,367,,,,,,,,,IEEE/ACM Trans. Netw.,IEEE/ACM Trans. Netw.,"cellular networks, channel state prediction, measurements, mobility, proportional fairness, slow-fading",,2942504,,,,,,,,355--367,,IEEE Press,,,,1063-6692,IEEE/ACM Trans. Netw.,355,article,,,,24,2016,,,,,,,February 2016,1,,February,13,,,,,"Piscataway, NJ, USA",,,
Evolving Neural Network Ensembles for Control Problems,,,,,,David  Pardoe and Michael  Ryoo and Risto  Miikkulainen,,,,10.1145/1068009.1068230,ACM-DL,1,,,1384,,,,,1-59593-010-8,,,,,Proceedings of the 7th Annual Conference on Genetic and Evolutionary Computation,"ensembles, genetic algorithms, neural networks, reinforcement learning",,1068230,,,,,,,,1379--1384,,ACM,,,,,Proceedings of the 7th Annual Conference on Genetic and Evolutionary Computation,1379,article,,,,,2005,GECCO '05,Proceedings of the 7th Annual Conference on Genetic and Evolutionary Computation,,"Washington DC, USA",,,,,,,6,,,,,"New York, NY, USA",,,
Evolutionary Function Approximation for Reinforcement Learning,,,,,,Shimon  Whiteson and Peter  Stone,,,,,ACM-DL,0,,,917,,,,,,,,,J. Mach. Learn. Res.,J. Mach. Learn. Res.,,,1248578,,,,,,,,877--917,,JMLR.org,,,,1532-4435,J. Mach. Learn. Res.,877,article,,,,7,2006,,,,,,,12/1/2006,,,December,41,,,,,,,,
Evaluation of a Workflow Scheduler Using Integrated Performance Modelling and Batch Queue Wait Time Prediction,,,,,,Daniel  Nurmi and Anirban  Mandal and John  Brevik and Chuck  Koelbel and Rich  Wolski and Ken  Kennedy,,,,10.1145/1188455.1188579,ACM-DL,1,,,,,,,,0-7695-2700-0,,,,,Proceedings of the 2006 ACM/IEEE Conference on Supercomputing,,,1188579,,,,,,,,,,ACM,,,,,Proceedings of the 2006 ACM/IEEE Conference on Supercomputing,119,article,,,119,,2006,SC '06,Proceedings of the 2006 ACM/IEEE Conference on Supercomputing,,"Tampa, Florida",,,,,,,,,,,,"New York, NY, USA",,,
Evaluating Job Shop Simulation Results,,,,,,"Carter L. Franklin,II",,,,10.1145/800293.811583,ACM-DL,1,,,297,,,,,,,,,,Proceedings of the 6th Conference on Winter Simulation,,,811583,,,,,,,,289--297,,ACM,,,,,Proceedings of the 6th Conference on Winter Simulation,289,article,,,,,1973,WSC '73,Proceedings of the 6th Conference on Winter Simulation,,"San Francisco, CA",,,,,,,9,,,,,"New York, NY, USA",,,
Enhancement of a Body Area Network to Support Smart Health Monitoring at the Digital Home,,,,,,Laura  Vadillo and Miguel A. Valero and Gema  Gil,,,,10.4108/icst.bodynets.2013.253578,ACM-DL,1,,,216,,,,,978-1-936968-89-3,,,,,Proceedings of the 8th International Conference on Body Area Networks,"context aware system, multiagent home platform, smart health",,2555364,,,,,,,,213--216,,"ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)",,,,,Proceedings of the 8th International Conference on Body Area Networks,213,article,,,,,2013,BodyNets '13,Proceedings of the 8th International Conference on Body Area Networks,,"Boston, Massachusetts",,,,,,,4,,,,,"ICST, Brussels, Belgium, Belgium",,,
Efficient Data Aggregation Scheduling in Wireless Sensor Networks with Multi-channel Links,,,,,,Miloud  Bagaa and Mohamed  Younis and Nadjib  Badache,,,,10.1145/2507924.2507995,ACM-DL,1,,,124,,,,,978-1-4503-2353-6,,,,,"Proceedings of the 16th ACM International Conference on Modeling, Analysis &#38; Simulation of Wireless and Mobile Systems","data aggregation, media access scheduling, multi-channels, wireless sensor network",,2507995,,,,,,,,119--124,,ACM,,,,,"Proceedings of the 16th ACM International Conference on Modeling, Analysis &#38; Simulation of Wireless and Mobile Systems",119,article,,,,,2013,MSWiM '13,"Proceedings of the 16th ACM International Conference on Modeling, Analysis &#38; Simulation of Wireless and Mobile Systems",,"Barcelona, Spain",,,,,,,6,,,,,"New York, NY, USA",,,
Dynamic Task Allocation Within an Open Service-oriented MAS Architecture,,,,,,Ivan J. Jureta and Stephane  Faulkner and Youssef  Achbany and Marco  Saerens,,,,10.1145/1329125.1329375,ACM-DL,1,,,209,,,,,978-81-904262-7-5,,,,,Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems,"agent architectures, architecture, mediation, multiagent systems, reinforcement learning, task allocation",,1329375,,,,,,,,206:1--206:3,,ACM,,,,,Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems,206,article,,,206,,2007,AAMAS '07,Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems,,"Honolulu, Hawaii",,,,,,,3,,,,,"New York, NY, USA",,,
Dynamic Control Flow in Large-scale Machine Learning,,,,,,Yuan  Yu and Mart&#237;n  Abadi and Paul  Barham and Eugene  Brevdo and Mike  Burrows and Andy  Davis and Jeff  Dean and Sanjay  Ghemawat and Tim  Harley and Peter  Hawkins and Michael  Isard and Manjunath  Kudlur and Rajat  Monga and Derek  Murray and Xiaoqiang  Zheng,,,,10.1145/3190508.3190551,ACM-DL,1,,,33,,,,,978-1-4503-5584-1,,,,,Proceedings of the Thirteenth EuroSys Conference,,,3190551,,,,,,,,18:1--18:15,,ACM,,,,,Proceedings of the Thirteenth EuroSys Conference,18,article,,,18,,2018,EuroSys '18,Proceedings of the Thirteenth EuroSys Conference,,"Porto, Portugal",,,,,,,15,,,,,"New York, NY, USA",,,
DRN: A Deep Reinforcement Learning Framework for News Recommendation,,,,,,Guanjie  Zheng and Fuzheng  Zhang and Zihan  Zheng and Yang  Xiang and Nicholas Jing  Yuan and Xing  Xie and Zhenhui  Li,,,,10.1145/3178876.3185994,ACM-DL,1,,,176,,,,,978-1-4503-5639-8,,,,,Proceedings of the 2018 World Wide Web Conference,"deep Q-Learning, news recommendation, reinforcement learning",,3185994,,,,,,,,167--176,,International World Wide Web Conferences Steering Committee,,,,,Proceedings of the 2018 World Wide Web Conference,167,article,,,,,2018,WWW '18,Proceedings of the 2018 World Wide Web Conference,,"Lyon, France",,,,,,,10,,,,,"Republic and Canton of Geneva, Switzerland",,,
DReL: A Middleware for Wireless Sensor Networks Management Using Reinforcement Learning Techniques,,,,,,Kunal  Shah and Mohan  Kumar,,,,10.1145/1890784.1890786,ACM-DL,1,,,7,,,,,978-1-4503-0454-2,,,,,"Proceedings of the 5th International Workshop on Middleware Tools, Services and Run-Time Support for Sensor Networks","adaptive framework, data-centric communication, distributed sensor management, reinforcement learning",,1890786,,,,,,,,1--7,,ACM,,,,,"Proceedings of the 5th International Workshop on Middleware Tools, Services and Run-Time Support for Sensor Networks",1,article,,,,,2010,MidSens '10,"Proceedings of the 5th International Workshop on Middleware Tools, Services and Run-Time Support for Sensor Networks",,"Bangalore, India",,,,,,,7,,,,,"New York, NY, USA",,,
Documentation: Effective AND Literate,,,,,,"Paul S Burdett,Jr.",,,,10.1145/10563.10582,ACM-DL,1,,,113,,,,,0-89791-186-5,,,,,Proceedings of the 4th Annual International Conference on Systems Documentation,,,10582,,,,,,,,110--113,,ACM,,,,,Proceedings of the 4th Annual International Conference on Systems Documentation,110,article,,,,,1985,SIGDOC '85,Proceedings of the 4th Annual International Conference on Systems Documentation,,"Ithaca, New York, USA",,,,,,,4,,,,,"New York, NY, USA",,,
DJ-MC: A Reinforcement-Learning Agent for Music Playlist Recommendation,,,,,,Elad  Liebman and Maytal  Saar-Tsechansky and Peter  Stone,,,,,ACM-DL,0,,,599,,,,,978-1-4503-3413-6,,,,,Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,,,2772954,,,,,,,,591--599,,International Foundation for Autonomous Agents and Multiagent Systems,,,,,Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,591,article,,,,,2015,AAMAS '15,Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,,"Istanbul, Turkey",,,,,,,9,,,,,"Richland, SC",,,
Distributed Schedule Management in the Tiger Video Fileserver,,,,,,William J. Bolosky and Robert P. Fitzgerald and John R. Douceur,,,,10.1145/268998.266692,ACM-DL,1,,,223,,,,,0-89791-916-5,,,,,Proceedings of the Sixteenth ACM Symposium on Operating Systems Principles,,,266692,,,,,,,,212--223,,ACM,,,,,Proceedings of the Sixteenth ACM Symposium on Operating Systems Principles,212,article,,,,,1997,SOSP '97,Proceedings of the Sixteenth ACM Symposium on Operating Systems Principles,,"Saint Malo, France",,,,,,,12,,,,,"New York, NY, USA",,,
Distributed Order Scheduling and Its Application to Multi-core Dram Controllers,,,,,,Thomas  Moscibroda and Onur  Mutlu,,,,10.1145/1400751.1400799,ACM-DL,1,,,374,,,,,978-1-59593-989-0,,,,,Proceedings of the Twenty-seventh ACM Symposium on Principles of Distributed Computing,"distributed approximation, distributed scheduling, dram memory controllers, multi-core, order scheduling",,1400799,,,,,,,,365--374,,ACM,,,,,Proceedings of the Twenty-seventh ACM Symposium on Principles of Distributed Computing,365,article,,,,,2008,PODC '08,Proceedings of the Twenty-seventh ACM Symposium on Principles of Distributed Computing,,"Toronto, Canada",,,,,,,10,,,,,"New York, NY, USA",,,
Distributed Message Routing and Run-time Support for Message-passing Parallel Programs Derived from Ordinary Programs,,,,,,Tom  Bennet,,,,10.1145/326619.326905,ACM-DL,1,,,514,,,,,0-89791-647-6,,,,,Proceedings of the 1994 ACM Symposium on Applied Computing,"multicomputers, parallelism, scheduling",,326905,,,,,,,,510--514,,ACM,,,,,Proceedings of the 1994 ACM Symposium on Applied Computing,510,article,,,,,1994,SAC '94,Proceedings of the 1994 ACM Symposium on Applied Computing,,"Phoenix, Arizona, USA",,,,,,,5,,,,,"New York, NY, USA",,,
Distributed Dynamic Data Driven Prediction Based on Reinforcement Learning Approach,,,,,,Szu-Yin  Lin and Kuo-Ming  Chao and Chi-Chun  Lo and Nick  Godwin,,,,10.1145/2480362.2480511,ACM-DL,1,,,784,,,,,978-1-4503-1656-9,,,,,Proceedings of the 28th Annual ACM Symposium on Applied Computing,"Q-learning, dynamic data driven application systems, reinforcement learning, typhoon rainfall prediction",,2480511,,,,,,,,779--784,,ACM,,,,,Proceedings of the 28th Annual ACM Symposium on Applied Computing,779,article,,,,,2013,SAC '13,Proceedings of the 28th Annual ACM Symposium on Applied Computing,,"Coimbra, Portugal",,,,,,,6,,,,,"New York, NY, USA",,,
Discovering Rubik's Cube Subgroups Using Coevolutionary GP: A Five Twist Experiment,,,,,,Robert J. Smith and Stephen  Kelly and Malcolm I. Heywood,,,,10.1145/2908812.2908887,ACM-DL,1,,,796,,,,,978-1-4503-4206-3,,,,,Proceedings of the Genetic and Evolutionary Computation Conference 2016,"coevolution, games, genetic programming, reinforcement learning, task transfer",,2908887,,,,,,,,789--796,,ACM,,,,,Proceedings of the Genetic and Evolutionary Computation Conference 2016,789,article,,,,,2016,GECCO '16,Proceedings of the Genetic and Evolutionary Computation Conference 2016,,"Denver, Colorado, USA",,,,,,,8,,,,,"New York, NY, USA",,,
Directing Exploratory Search: Reinforcement Learning from User Interactions with Keywords,,,,,,Dorota  Glowacka and Tuukka  Ruotsalo and Ksenia  Konuyshkova and kumaripaba  Athukorala and Samuel  Kaski and Giulio  Jacucci,,,,10.1145/2449396.2449413,ACM-DL,1,,,128,,,,,978-1-4503-1965-2,,,,,Proceedings of the 2013 International Conference on Intelligent User Interfaces,"adaptive interfaces, data mining, information filtering, machine learning, recommender systems",,2449413,,,,,,,,117--128,,ACM,,,,,Proceedings of the 2013 International Conference on Intelligent User Interfaces,117,article,,,,,2013,IUI '13,Proceedings of the 2013 International Conference on Intelligent User Interfaces,,"Santa Monica, California, USA",,,,,,,12,,,,,"New York, NY, USA",,,
Developing Learning from Demonstration Techniques for Individuals with Physical Disabilities,,,,,,William  Curran,,,,10.1145/2701973.2702710,ACM-DL,1,,,234,,,,,978-1-4503-3318-4,,,,,Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction Extended Abstracts,"human-robot interaction, learning from demonstration",,2702710,,,,,,,,233--234,,ACM,,,,,Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction Extended Abstracts,233,article,,,,,2015,HRI'15 Extended Abstracts,Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction Extended Abstracts,,"Portland, Oregon, USA",,,,,,,2,,,,,"New York, NY, USA",,,
Developing a Healthcare Robot with Personalized Behaviors and Social Skills for the Elderly,,,,,,Roxana Madalina Agrigoroaie and Adriana  Tapus,,,,,ACM-DL,0,,,590,,,,,978-1-4673-8370-7,,,,,The Eleventh ACM/IEEE International Conference on Human Robot Interaction,"adaptation, episodic memory, human-robot interaction, learning",,2906995,,,,,,,,589--590,,IEEE Press,,,,,The Eleventh ACM/IEEE International Conference on Human Robot Interaction,589,article,,,,,2016,HRI '16,The Eleventh ACM/IEEE International Conference on Human Robot Interaction,,"Christchurch, New Zealand",,,,,,,2,,,,,"Piscataway, NJ, USA",,,
Designing Intelligent Sales-agent for Online Selling,,,,,,Shiu-li  Huang and Fu-ren  Lin,,,,10.1145/1089551.1089605,ACM-DL,1,,,286,,,,,1-59593-112-0,,,,,Proceedings of the 7th International Conference on Electronic Commerce,"abstract argumentation framework, negotiation, persuasion, reinforcement learning, sales-agent",,1089605,,,,,,,,279--286,,ACM,,,,,Proceedings of the 7th International Conference on Electronic Commerce,279,article,,,,,2005,ICEC '05,Proceedings of the 7th International Conference on Electronic Commerce,,"Xi'an, China",,,,,,,8,,,,,"New York, NY, USA",,,
Demonstrating Cognitive Packet Network Resilience to Worm Attacks,,,,,,Georgia  Sakellari and Erol  Gelenbe,,,,10.1145/1866307.1866380,ACM-DL,1,,,638,,,,,978-1-4503-0245-6,,,,,Proceedings of the 17th ACM Conference on Computer and Communications Security,"cognitive packet network, network worms, reliability, routing protocols, self-aware networks",,1866380,,,,,,,,636--638,,ACM,,,,,Proceedings of the 17th ACM Conference on Computer and Communications Security,636,article,,,,,2010,CCS '10,Proceedings of the 17th ACM Conference on Computer and Communications Security,,"Chicago, Illinois, USA",,,,,,,3,,,,,"New York, NY, USA",,,
Delay Analysis for Maximal Scheduling with Flow Control in Wireless Networks with Bursty Traffic,,,,,,Michael J. Neely,,,,10.1109/TNET.2008.2008232,ACM-DL,1,,,1159,,,,,,,,,IEEE/ACM Trans. Netw.,IEEE/ACM Trans. Netw.,"Markov chains, flow control, queueing analysis",,1618573,,,,,,,,1146--1159,,IEEE Press,,,,1063-6692,IEEE/ACM Trans. Netw.,1146,article,,,,17,2009,,,,,,,August 2009,4,,August,14,,,,,"Piscataway, NJ, USA",,,
Deep Reinforcement Learning of Abstract Reasoning from Demonstrations,,,,,,Madison  Clark-Turner and Momotaz  Begum,,,,10.1145/3173386.3177537,ACM-DL,1,,,372,,,,,978-1-4503-5615-2,,,,,Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction,"abstract reasoning, deep learning, learning from demonstration",,3177537,,,,,,,,372--372,,ACM,,,,,Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction,372,article,,,,,2018,HRI '18,Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction,,"Chicago, IL, USA",,,,,,,1,,,,,"New York, NY, USA",,,
Declarative-procedural Memory Interaction in Learning Agents,,,,,,Wenwen  Wang and Ah-Hwee  Tan and Loo-Nin  Teow and Yuan-Sin  Tan,,,,,ACM-DL,0,,,1476,,,,,978-1-4503-2738-1,,,,,Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems,"declarative memory, memory interaction, semantic memory",,2617530,,,,,,,,1475--1476,,International Foundation for Autonomous Agents and Multiagent Systems,,,,,Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems,1475,article,,,,,2014,AAMAS '14,Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems,,"Paris, France",,,,,,,2,,,,,"Richland, SC",,,
"Decentralized, Adaptive Resource Allocation for Sensor Networks",,,,,,Geoffrey  Mainland and David C. Parkes and Matt  Welsh,,,,,ACM-DL,0,,,328,,,,,,,,,,Proceedings of the 2Nd Conference on Symposium on Networked Systems Design & Implementation - Volume 2,,,1251226,,,,,,,,315--328,,USENIX Association,,,,,Proceedings of the 2Nd Conference on Symposium on Networked Systems Design & Implementation - Volume 2,315,article,,,,,2005,NSDI'05,Proceedings of the 2Nd Conference on Symposium on Networked Systems Design & Implementation - Volume 2,,,,,,,,,14,,,,,"Berkeley, CA, USA",,,
Data Requirement for Phylogenetic Inference from Multiple Loci: A New Distance Method,,,,,,Gautam  Dasarathy and Robert  Nowak and Sebastien  Roch,,,,10.1109/TCBB.2014.2361685,ACM-DL,1,,,432,,,,,,,,,IEEE/ACM Trans. Comput. Biol. Bioinformatics,IEEE/ACM Trans. Comput. Biol. Bioinformatics,"distance methods, incomplete lineage sorting, molecular clock, multispecies coalescent, phylogenetic inference, sample complexity",,2817088,,,,,,,,422--432,,IEEE Computer Society Press,,,,1545-5963,IEEE/ACM Trans. Comput. Biol. Bioinformatics,422,article,,,,12,2015,,,,,,,March/April 2015,2,,March,11,,,,,"Los Alamitos, CA, USA",,,
Data Cache Energy Minimizations Through Programmable Tag Size Matching to the Applications,,,,,,Peter  Petrov and Alex  Orailoglu,,,,10.1145/500001.500028,ACM-DL,1,,,117,,,,,1-58113-418-5,,,,,Proceedings of the 14th International Symposium on Systems Synthesis,,,500028,,,,,,,,113--117,,ACM,,,,,Proceedings of the 14th International Symposium on Systems Synthesis,113,article,,,,,2001,ISSS '01,Proceedings of the 14th International Symposium on Systems Synthesis,,"Montr&#233;al, P.Q., Canada",,,,,,,5,,,,,"New York, NY, USA",,,
Cross Channel Optimized Marketing by Reinforcement Learning,,,,,,Naoki  Abe and Naval  Verma and Chid  Apte and Robert  Schroko,,,,10.1145/1014052.1016912,ACM-DL,1,,,772,,,,,1-58113-888-1,,,,,Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,"CRM, cost sensitive learning, customer life time value, reinforcement learning, targeted marketing",,1016912,,,,,,,,767--772,,ACM,,,,,Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,767,article,,,,,2004,KDD '04,Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,,"Seattle, WA, USA",,,,,,,6,,,,,"New York, NY, USA",,,
"CPU Reservations and Time Constraints: Efficient, Predictable Scheduling of Independent Activities",,,,,,Michael B. Jones and Daniela  Ro&#351;u and Marcel-C&#259;t&#259;lin  Ro&#351;u,,,,10.1145/268998.266689,ACM-DL,1,,,211,,,,,0-89791-916-5,,,,,Proceedings of the Sixteenth ACM Symposium on Operating Systems Principles,,,266689,,,,,,,,198--211,,ACM,,,,,Proceedings of the Sixteenth ACM Symposium on Operating Systems Principles,198,article,,,,,1997,SOSP '97,Proceedings of the Sixteenth ACM Symposium on Operating Systems Principles,,"Saint Malo, France",,,,,,,14,,,,,"New York, NY, USA",,,
Count Down Protocol: Asynchronous Consistent Protocol in P2P Virtual Ball Game,,,,,,Yoshihiro  Kawano and Tatsuhiro  Yonekura,,,,10.1145/1230040.1230042,ACM-DL,1,,,,,,,,1-59593-589-4,,,,,Proceedings of 5th ACM SIGCOMM Workshop on Network and System Support for Games,"AtoZ (allocated topographical zone), DVE (distributed virtual environment), P2P (peer-to-peer), count down protocol (CDP), critical case",,1230042,,,,,,,,,,ACM,,,,,Proceedings of 5th ACM SIGCOMM Workshop on Network and System Support for Games,32,article,,,32,,2006,NetGames '06,Proceedings of 5th ACM SIGCOMM Workshop on Network and System Support for Games,,Singapore,,,,,,,,,,,,"New York, NY, USA",,,
Coping with Network Dynamics Using Reinforcement Learning Based Network Optimization in Wireless Sensor Networks.,,,,,,"Milos Rovcanin, Eli De Poorter, Ingrid Moerman, Piet Demeester",,,,10.1007/S11277-014-1684-4,DBLP,1,,,191,,,,,,,,,,,,,1055734,,,,https://doi.org/10.1007/s11277-014-1684-4,,,,,,,,,,,,169,Journal Articles,https://dblp.org/rec/journals/wpc/RovcaninPMD14,,76,,2014,,,,,,,,,journals/wpc/RovcaninPMD14,,,,2,169-191,,,,,Wireless Personal Communications
"Cooperative Multi-Agent Reinforcement Learning-Based Co-optimization of Cores, Caches, and On-chip Network",,,,,,Rahul  Jain and Preeti Ranjan Panda and Sreenivas  Subramoney,,,,10.1145/3132170,ACM-DL,1,,,48,,,,,,,,,ACM Trans. Archit. Code Optim.,ACM Trans. Archit. Code Optim.,"Energy efficient computation, and frequency scaling, cache management, computer architectures, dynamic voltage, low power architectures",,3132170,,,,,,,,32:1--32:25,,ACM,,,,1544-3566,ACM Trans. Archit. Code Optim.,32,article,,,32,14,2017,,,,,,,December 2017,4,,November,25,,,,,"New York, NY, USA",,,
Cooperative Cross Layer MAC Protocols for Directional Antenna Ad Hoc Networks,,,,,,Andrea  Munari and Francesco  Rossetto and Michele  Zorzi,,,,10.1145/1394555.1394558,ACM-DL,1,,,30,,,,,,,,,SIGMOBILE Mob. Comput. Commun. Rev.,SIGMOBILE Mob. Comput. Commun. Rev.,,,1394558,,,,,,,,12--30,,ACM,,,,1559-1662,SIGMOBILE Mob. Comput. Commun. Rev.,12,article,,,,12,2008,,,,,,,April 2008,2,,April,19,,,,,"New York, NY, USA",,,
Conversion of Limited-Entry Decision Tables to Optimal Computer Programs II: Minimum Storage Requirement,,,,,,Lewis T. Reinwald and Richard M. Soland,,,,10.1145/321420.321433,ACM-DL,1,,,756,,,,,,,,,J. ACM,J. ACM,,,321433,,,,,,,,742--756,,ACM,,,,0004-5411,J. ACM,742,article,,,,14,1967,,,,,,,Oct. 1967,4,,October,15,,,,,"New York, NY, USA",,,
Computation Offloading to Save Energy on Handheld Devices: A Partition Scheme,,,,,,Zhiyuan  Li and Cheng  Wang and Rong  Xu,,,,10.1145/502217.502257,ACM-DL,1,,,246,,,,,1-58113-399-5,,,,,"Proceedings of the 2001 International Conference on Compilers, Architecture, and Synthesis for Embedded Systems",,,502257,,,,,,,,238--246,,ACM,,,,,"Proceedings of the 2001 International Conference on Compilers, Architecture, and Synthesis for Embedded Systems",238,article,,,,,2001,CASES '01,"Proceedings of the 2001 International Conference on Compilers, Architecture, and Synthesis for Embedded Systems",,"Atlanta, Georgia, USA",,,,,,,9,,,,,"New York, NY, USA",,,
Competitive Algorithms from Competitive Equilibria: Non-clairvoyant Scheduling Under Polyhedral Constraints,,,,,,Sungjin  Im and Janardhan  Kulkarni and Kamesh  Munagala,,,,10.1145/2591796.2591814,ACM-DL,1,,,322,,,,,978-1-4503-2710-7,,,,,Proceedings of the Forty-sixth Annual ACM Symposium on Theory of Computing,"equilibria, flow time, non-clairvoyance, online scheduling, polyhedral constraints, proportional fairness, unrelated machines",,2591814,,,,,,,,313--322,,ACM,,,,,Proceedings of the Forty-sixth Annual ACM Symposium on Theory of Computing,313,article,,,,,2014,STOC '14,Proceedings of the Forty-sixth Annual ACM Symposium on Theory of Computing,,"New York, New York",,,,,,,10,,,,,"New York, NY, USA",,,
Competency-Based Intelligent Curriculum Sequencing: Comparing Two Evolutionary Approaches,,,,,,Luis  de-Marcos and Roberto  Barchino and Jos&#233;-Javier  Mart&#237;nez and Jos&#233;-Antonio  Guti&#233;rrez and Jos&#233;-Ram&#243;n  Hilera,,,,10.1109/WIIAT.2008.279,ACM-DL,1,,,342,,,,,978-0-7695-3496-1,,,,,Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 03,"Learning Object, Competency, Sequencing, Swarm Intelligence, Genetic Algorithm",,1487356,,,,,,,,339--342,,IEEE Computer Society,,,,,Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 03,339,article,,,,,2008,WI-IAT '08,Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 03,,,,,,,,,4,,,,,"Washington, DC, USA",,,
Commitment Management Through Constraint Reification,,,,,,Stuart  Chalmers and Alun  Preece and Timothy J. Norman and Peter M. D. Gray,,,,10.1109/AAMAS.2004.86,ACM-DL,1,,,437,,,,,1-58113-864-4,,,,,Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 1,,,1018776,,,,,,,,430--437,,IEEE Computer Society,,,,,Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 1,430,article,,,,,2004,AAMAS '04,Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 1,,"New York, New York",,,,,,,8,,,,,"Washington, DC, USA",,,
Collaborative Multiagent Reinforcement Learning by Payoff Propagation,,,,,,Jelle R. Kok and Nikos  Vlassis,,,,,ACM-DL,0,,,1828,,,,,,,,,J. Mach. Learn. Res.,J. Mach. Learn. Res.,,,1248612,,,,,,,,1789--1828,,JMLR.org,,,,1532-4435,J. Mach. Learn. Res.,1789,article,,,,7,2006,,,,,,,12/1/2006,,,December,40,,,,,,,,
Collaborative Function Approximation in Social Multiagent Systems,,,,,,Dominik  Dahlem and William  Harrison,,,,10.1109/WI-IAT.2010.276,ACM-DL,1,,,55,,,,,978-0-7695-4191-4,,,,,Proceedings of the 2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 02,"Queueing Networks, Markov Decision Problem, Multi-agent Reinforcement Learning, Kriging",,1913821,,,,,,,,48--55,,IEEE Computer Society,,,,,Proceedings of the 2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 02,48,article,,,,,2010,WI-IAT '10,Proceedings of the 2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 02,,,,,,,,,8,,,,,"Washington, DC, USA",,,
Cognitive Radio with Reinforcement Learning Applied to Multicast Downlink Transmission with Power Adjustment.,,,,,,"Mengfei Yang, David Grace",,,,10.1007/S11277-010-0007-7,DBLP,1,,,87,,,,,,,,,,,,,1828048,,,,https://doi.org/10.1007/s11277-010-0007-7,,,,,,,,,,,,73,Journal Articles,https://dblp.org/rec/journals/wpc/YangG11,,57,,2011,,,,,,,,,journals/wpc/YangG11,,,,1,73-87,,,,,Wireless Personal Communications
Clustering Household Preferences in Local Electricity Markets,,,,,,Esther  Mengelkamp and Christof  Weinhardt,,,,10.1145/3208903.3214348,ACM-DL,1,,,543,,,,,978-1-4503-5767-8,,,,,Proceedings of the Ninth International Conference on Future Energy Systems,"Local electricity market, household agents, intelligent agents, reinforcement learning",,3214348,,,,,,,,538--543,,ACM,,,,,Proceedings of the Ninth International Conference on Future Energy Systems,538,article,,,,,2018,e-Energy '18,Proceedings of the Ninth International Conference on Future Energy Systems,,"Karlsruhe, Germany",,,,,,,6,,,,,"New York, NY, USA",,,
Building a Social Multi-agent System Simulation Management Toolbox,,,,,,Chairi  Kiourt and Dimitris  Kalles,,,,10.1145/2490257.2490293,ACM-DL,1,,,70,,,,,978-1-4503-1851-8,,,,,Proceedings of the 6th Balkan Conference in Informatics,"multi-agent system, reinforcement learning, simulation, social organization",,2490293,,,,,,,,66--70,,ACM,,,,,Proceedings of the 6th Balkan Conference in Informatics,66,article,,,,,2013,BCI '13,Proceedings of the 6th Balkan Conference in Informatics,,"Thessaloniki, Greece",,,,,,,5,,,,,"New York, NY, USA",,,
Brief Announcement: Towards Robust Medium Access in Multi-hop Networks,,,,,,Andrea  Richa and Jin  Zhang and Christian  Scheideler and Stefan  Schmid,,,,10.1145/1835698.1835726,ACM-DL,1,,,115,,,,,978-1-60558-888-9,,,,,Proceedings of the 29th ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,"jamming, mac protocols, wireless ad-hoc networks",,1835726,,,,,,,,114--115,,ACM,,,,,Proceedings of the 29th ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,114,article,,,,,2010,PODC '10,Proceedings of the 29th ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,,"Zurich, Switzerland",,,,,,,2,,,,,"New York, NY, USA",,,
Brief Announcement: Are Lock-free Concurrent Algorithms Practically Wait-free?,,,,,,Dan  Alistarh and Keren  Censor-Hillel and Nir  Shavit,,,,10.1145/2611462.2611502,ACM-DL,1,,,52,,,,,978-1-4503-2944-6,,,,,Proceedings of the 2014 ACM Symposium on Principles of Distributed Computing,"distributed computing, lock-free algorithms, schedulers, shared memory, wait-free algorithms",,2611502,,,,,,,,50--52,,ACM,,,,,Proceedings of the 2014 ACM Symposium on Principles of Distributed Computing,50,article,,,,,2014,PODC '14,Proceedings of the 2014 ACM Symposium on Principles of Distributed Computing,,"Paris, France",,,,,,,3,,,,,"New York, NY, USA",,,
Biological Sequence Classification with Multivariate String Kernels,,,,,,Pavel P. Kuksa,,,,10.1109/TCBB.2013.15,ACM-DL,1,,,1210,,,,,,,,,IEEE/ACM Trans. Comput. Biol. Bioinformatics,IEEE/ACM Trans. Comput. Biol. Bioinformatics,"Biological sequence classification, kernel methods",,2564670,,,,,,,,1201--1210,,IEEE Computer Society Press,,,,1545-5963,IEEE/ACM Trans. Comput. Biol. Bioinformatics,1201,article,,,,10,2013,,,,,,,September 2013,5,,September,10,,,,,"Los Alamitos, CA, USA",,,
Bio-Inspired Virtual Populations: Adaptive Behavior with Affective Feedback,,,,,,Rui Filipe Antunes and Nadia  Magnenat-Thalmann,,,,10.1145/2915926.2915929,ACM-DL,1,,,110,,,,,978-1-4503-4745-7,,,,,Proceedings of the 29th International Conference on Computer Animation and Social Agents,"Artificial Life, Artificial Societies, Crowd Simulation, Emotional Characters, Multi-Agent Frameworks",,2915929,,,,,,,,101--110,,ACM,,,,,Proceedings of the 29th International Conference on Computer Animation and Social Agents,101,article,,,,,2016,CASA '16,Proceedings of the 29th International Conference on Computer Animation and Social Agents,,"Geneva, Switzerland",,,,,,,10,,,,,"New York, NY, USA",,,
Automatic Generation of Scheduling and Communication Code in Real-time Parallel Programs,,,,,,Andr&#233;  Bakkers and Johan  Sunter and Evert  Ploeg,,,,10.1145/216636.216679,ACM-DL,1,,,145,,,,,,,,,,"Proceedings of the ACM SIGPLAN 1995 Workshop on Languages, Compilers, &Amp; Tools for Real-time Systems",,,216679,,,,,,,,134--145,,ACM,,,,,"Proceedings of the ACM SIGPLAN 1995 Workshop on Languages, Compilers, &Amp; Tools for Real-time Systems",134,article,,,,,1995,LCTES '95,"Proceedings of the ACM SIGPLAN 1995 Workshop on Languages, Compilers, &Amp; Tools for Real-time Systems",,"La Jolla, California, USA",,,,,,,12,,,,,"New York, NY, USA",,,
Automatic Computer Game Balancing: A Reinforcement Learning Approach,,,,,,Gustavo  Andrade and Geber  Ramalho and Hugo  Santana and Vincent  Corruble,,,,10.1145/1082473.1082648,ACM-DL,1,,,1112,,,,,1-59593-093-0,,,,,Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multiagent Systems,"adaptive agents, game balancing, reinforcement learning",,1082648,,,,,,,,1111--1112,,ACM,,,,,Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multiagent Systems,1111,article,,,,,2005,AAMAS '05,Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multiagent Systems,,The Netherlands,,,,,,,2,,,,,"New York, NY, USA",,,
Automated Index Management for Distributed Web Search,,,,,,Rinat  Khoussainov and Nicholas  Kushmerick,,,,10.1145/956863.956937,ACM-DL,1,,,393,,,,,1-58113-723-0,,,,,Proceedings of the Twelfth International Conference on Information and Knowledge Management,"distributed web search, reinforcement learning, stochastic game",,956937,,,,,,,,386--393,,ACM,,,,,Proceedings of the Twelfth International Conference on Information and Knowledge Management,386,article,,,,,2003,CIKM '03,Proceedings of the Twelfth International Conference on Information and Knowledge Management,,"New Orleans, LA, USA",,,,,,,8,,,,,"New York, NY, USA",,,
Artificial Intelligence in XPRIZE DeepQ Tricorder,,,,,,Edward Y. Chang and Meng-Hsi  Wu and Kai-Fu Tang  Tang and Hao-Cheng  Kao and Chun-Nan  Chou,,,,10.1145/3132635.3132637,ACM-DL,1,,,18,,,,,978-1-4503-5504-9,,,,,Proceedings of the 2Nd International Workshop on Multimedia for Personal Health and Health Care,"artificial intelligence, deepq, medical iot, xprize tricorder",,3132637,,,,,,,,11--18,,ACM,,,,,Proceedings of the 2Nd International Workshop on Multimedia for Personal Health and Health Care,11,article,,,,,2017,MMHealth '17,Proceedings of the 2Nd International Workshop on Multimedia for Personal Health and Health Care,,"Mountain View, California, USA",,,,,,,8,,,,,"New York, NY, USA",,,
Artificial Agents Learning Human Fairness,,,,,,Steven  de Jong and Karl  Tuyls and Katja  Verbeeck,,,,,ACM-DL,0,,,870,,,,,978-0-9817381-1-6,,,,,Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 2,"fairness, homo egualis, reinforcement learning",,1402344,,,,,,,,863--870,,International Foundation for Autonomous Agents and Multiagent Systems,,,,,Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 2,863,article,,,,,2008,AAMAS '08,Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 2,,"Estoril, Portugal",,,,,,,8,,,,,"Richland, SC",,,
Appointment Scheduling Using Optimisation via Simulation,,,,,,Paulien  Koeleman and Ger  Koole,,,,,ACM-DL,0,,,27,,,,,,,,,,Proceedings of the Winter Simulation Conference,,,2429787,,,,,,,,22:1--22:5,,Winter Simulation Conference,,,,,Proceedings of the Winter Simulation Conference,22,article,,,22,,2012,WSC '12,Proceedings of the Winter Simulation Conference,,"Berlin, Germany",,,,,,,5,,,,,,,,
Analyze Influenza Virus Sequences Using Binary Encoding Approach,,,,,,HamChing  Lam and Daniel  Boley,,,,10.1145/2003351.2003355,ACM-DL,1,,,11,,,,,978-1-4503-0839-7,,,,,Proceedings of the Tenth International Workshop on Data Mining in Bioinformatics,"binary encoding, evolution, influenza virus, principal component analysis",,2003355,,,,,,,,4:1--4:7,,ACM,,,,,Proceedings of the Tenth International Workshop on Data Mining in Bioinformatics,4,article,,,4,,2011,BIOKDD '11,Proceedings of the Tenth International Workshop on Data Mining in Bioinformatics,,"San Diego, California",,,,,,,7,,,,,"New York, NY, USA",,,
Analysis of Round-robin Variants: Favoring Newly Arrived Jobs,,,,,,Feng  Zhang and Sarah  Tasneem and Lester  Lipsky and Steve  Thompson,,,,,ACM-DL,0,,,46,,,,,,,,,,Proceedings of the 2009 Spring Simulation Multiconference,"foreground-background (<i>FB</i>), last-come-first-served with preemptive resume (<i>LCFSPR</i>), processor sharing (<i>PS</i>), round-robin (<i>RR</i>), shortest remaining processing time (<i>SRPT</i>)",,1639850,,,,,,,,39:1--39:7,,Society for Computer Simulation International,,,,,Proceedings of the 2009 Spring Simulation Multiconference,39,article,,,39,,2009,SpringSim '09,Proceedings of the 2009 Spring Simulation Multiconference,,"San Diego, California",,,,,,,7,,,,,"San Diego, CA, USA",,,
An RL Approach to Common-interest Continuous Action Games,,,,,,Abdel  Rodr&#237;guez and Peter  Vrancx and Ricardo  Grau and Ann  Now&#233;,,,,,ACM-DL,0,,,1402,,,,,"0-9817381-3-3, 978-0-9817381-3-0",,,,,Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 3,"coalition formation and coordination, implicit cooperation, multi-agent learning, teamwork",,2344027,,,,,,,,1401--1402,,International Foundation for Autonomous Agents and Multiagent Systems,,,,,Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 3,1401,article,,,,,2012,AAMAS '12,Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 3,,"Valencia, Spain",,,,,,,2,,,,,"Richland, SC",,,
An Operational Semantics for Parallel Lazy Evaluation,,,,,,Clem  Baker-Finch and David J. King and Phil  Trinder,,,,10.1145/357766.351256,ACM-DL,1,,,173,,,,,,,,,SIGPLAN Not.,SIGPLAN Not.,,,351256,,,,,,,,162--173,,ACM,,,,0362-1340,SIGPLAN Not.,162,article,,,,35,2000,,,,,,,Sept. 2000,9,,September,12,,,,,"New York, NY, USA",,,
An Operational Semantics for Parallel Lazy Evaluation,,,,,,Clem  Baker-Finch and David J. King and Phil  Trinder,,,,10.1145/351240.351256,ACM-DL,1,,,173,,,,,1-58113-202-6,,,,,Proceedings of the Fifth ACM SIGPLAN International Conference on Functional Programming,,,351256,,,,,,,,162--173,,ACM,,,,,Proceedings of the Fifth ACM SIGPLAN International Conference on Functional Programming,162,article,,,,,2000,ICFP '00,Proceedings of the Fifth ACM SIGPLAN International Conference on Functional Programming,,,,,,,,,12,,,,,"New York, NY, USA",,,
An Enhanced PSO-based Algorithm for Multiobjective Flexible Job Shop Scheduling Problems(FJSSPs),,,,,,Souad  Mekni and Besma Fay&#233;ch Cha&#226;r,,,,10.1145/2668260.2668289,ACM-DL,1,,,183,,,,,978-1-4503-2767-1,,,,,Proceedings of the 6th International Conference on Management of Emergent Digital EcoSystems,"Nature Inspired computing, Particle Swarm Optimization, Swarm intelligence, TRIBES, multiobjective flexible job shop scheduling problems",,2668289,,,,,,,,30:178--30:182,,ACM,,,,,Proceedings of the 6th International Conference on Management of Emergent Digital EcoSystems,178,article,,,30,,2014,MEDES '14,Proceedings of the 6th International Conference on Management of Emergent Digital EcoSystems,,"Buraidah, Al Qassim, Saudi Arabia",,,,,,,5,,,,,"New York, NY, USA",,,
An Efficient Bandit Algorithm for Realtime Multivariate Optimization,,,,,,Daniel N. Hill and Houssam  Nassif and Yi  Liu and Anand  Iyer and S.V.N.  Vishwanathan,,,,10.1145/3097983.3098184,ACM-DL,1,,,1821,,,,,978-1-4503-4887-4,,,,,Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,"a/b testing, hill-climbing, multi-armed bandit, multivariate optimization",,3098184,,,,,,,,1813--1821,,ACM,,,,,Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,1813,article,,,,,2017,KDD '17,Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,,"Halifax, NS, Canada",,,,,,,9,,,,,"New York, NY, USA",,,
An Automatic Construction and Organization Strategy for Ensemble Learning on Data Streams,,,,,,Yi  Zhang and Xiaoming  Jin,,,,10.1145/1168092.1168096,ACM-DL,1,,,33,,,,,,,,,SIGMOD Rec.,SIGMOD Rec.,,,1168096,,,,,,,,28--33,,ACM,,,,0163-5808,SIGMOD Rec.,28,article,,,,35,2006,,,,,,,September 2006,3,,September,6,,,,,"New York, NY, USA",,,
An Appliance Classification Method for Residential Appliance Scheduling,,,,,,Man  Su and Jianting  Ji and Yulin  Che and Ting  Liu and Siyun  Chen and Zhanbo  Xu,,,,10.1145/2800835.2801641,ACM-DL,1,,,1570,,,,,978-1-4503-3575-1,,,,,Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers,"appliance characteristic, intelligent energy consumption, smart home, user preference",,2801641,,,,,,,,1567--1570,,ACM,,,,,Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers,1567,article,,,,,2015,UbiComp/ISWC'15 Adjunct,Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers,,"Osaka, Japan",,,,,,,4,,,,,"New York, NY, USA",,,
An Agent-based Model of Regional Food Supply Chain Disintermediation,,,,,,Teri J. Craven and Caroline C. Krejci,,,,,ACM-DL,0,,,18,,,,,,,,,,Proceedings of the Agent-Directed Simulation Symposium,"agent-based modeling, regional food hubs, reinforcement learning, supply chain management",,3106086,,,,,,,,8:1--8:10,,Society for Computer Simulation International,,,,,Proceedings of the Agent-Directed Simulation Symposium,8,article,,,8,,2017,ADS '17,Proceedings of the Agent-Directed Simulation Symposium,,"Virginia Beach, Virginia",,,,,,,10,,,,,"San Diego, CA, USA",,,
An Adaptive Window Length Strategy for Eukaryotic CDS Prediction,,,,,,Devendra K. Shakya and Rajiv  Saxena and Sanjeev N. Sharma,,,,10.1109/TCBB.2013.76,ACM-DL,1,,,1252,,,,,,,,,IEEE/ACM Trans. Comput. Biol. Bioinformatics,IEEE/ACM Trans. Comput. Biol. Bioinformatics,"Bioinformatics, signal processing, window function, short time Fourier transform (STFT), deoxyribonucleic acid (DNA)",,2564674,,,,,,,,1241--1252,,IEEE Computer Society Press,,,,1545-5963,IEEE/ACM Trans. Comput. Biol. Bioinformatics,1241,article,,,,10,2013,,,,,,,September 2013,5,,September,12,,,,,"Los Alamitos, CA, USA",,,
An Adaptive Learning Framework for Efficient Emergence of Social Norms: (Extended Abstract),,,,,,Chao  Yu and Hongtao  Lv and Sandip  Sen and Jianye  Hao and Fenghui  Ren and Rui  Liu,,,,,ACM-DL,0,,,1308,,,,,978-1-4503-4239-1,,,,,Proceedings of the 2016 International Conference on Autonomous Agents &#38; Multiagent Systems,"norm emergence, reinforcement learning, social networks",,2937133,,,,,,,,1307--1308,,International Foundation for Autonomous Agents and Multiagent Systems,,,,,Proceedings of the 2016 International Conference on Autonomous Agents &#38; Multiagent Systems,1307,article,,,,,2016,AAMAS '16,Proceedings of the 2016 International Conference on Autonomous Agents &#38; Multiagent Systems,,"Singapore, Singapore",,,,,,,2,,,,,"Richland, SC",,,
Ambiguity in Context-free Grammars,,,,,,Bruce S. N. Cheung,,,,10.1145/315891.315991,ACM-DL,1,,,276,,,,,0-89791-658-1,,,,,Proceedings of the 1995 ACM Symposium on Applied Computing,,,315991,,,,,,,,272--276,,ACM,,,,,Proceedings of the 1995 ACM Symposium on Applied Computing,272,article,,,,,1995,SAC '95,Proceedings of the 1995 ACM Symposium on Applied Computing,,"Nashville, Tennessee, USA",,,,,,,5,,,,,"New York, NY, USA",,,
Amazon in the White Space: Social Recommendation Aided Distributed Spectrum Access,,,,,,Xu  Chen and Xiaowen  Gong and Lei  Yang and Junshan  Zhang,,,,10.1109/TNET.2016.2587736,ACM-DL,1,,,549,,,,,,,,,IEEE/ACM Trans. Netw.,IEEE/ACM Trans. Netw.,,,3068747,,,,,,,,536--549,,IEEE Press,,,,1063-6692,IEEE/ACM Trans. Netw.,536,article,,,,25,2017,,,,,,,February 2017,1,,February,14,,,,,"Piscataway, NJ, USA",,,
Alleviating Reader Collision Problem in Mobile RFID Networks,,,,,,Ching-Hsien  Hsu and Shih-Chang  Chen and Chia-Hao  Yu and Jong Hyuk Park,,,,10.1007/s00779-009-0224-9,ACM-DL,1,,,497,,,,,,,,,Personal Ubiquitous Comput.,Personal Ubiquitous Comput.,"Anti-collision, Hidden terminal, RFID scheduling, Reader collision problem, TPDM, Wireless network",,1613033,,,,,,,,489--497,,Springer-Verlag,,,,1617-4909,Personal Ubiquitous Comput.,489,article,,,,13,2009,,,,,,,October   2009,7,,October,9,,,,,"London, UK, UK",,,
Algorithmic Aspects of Bandwidth Trading,,,,,,Randeep  Bhatia and Julia  Chuzhoy and Ari  Freund and Joseph (Seffi)  Naor,,,,10.1145/1186810.1186820,ACM-DL,1,,,29,,,,,,,,,ACM Trans. Algorithms,ACM Trans. Algorithms,"Approximation algorithms for NP-hard problems, dynamic storage allocation, general caching, resource allocation, scheduling",,1186820,,,,,,,,10:1--10:19,,ACM,,,,1549-6325,ACM Trans. Algorithms,10,article,,,10,3,2007,,,,,,,February 2007,1,,February,19,,,,,"New York, NY, USA",,,
Aggregation in Multiagent Systems and the Problem of Truth-tracking,,,,,,Gabriella  Pigozzi and Stephan  Hartmann,,,,10.1145/1329125.1329245,ACM-DL,1,,,100,,,,,978-81-904262-7-5,,,,,Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems,"belief merging, doctrinal paradox, group decision-making, information fusion, judgment aggregation, truth-tracking",,1329245,,,,,,,,97:1--97:3,,ACM,,,,,Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems,97,article,,,97,,2007,AAMAS '07,Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems,,"Honolulu, Hawaii",,,,,,,3,,,,,"New York, NY, USA",,,
Adaptive Value Function Approximations in Classifier Systems,,,,,,Lashon B. Booker,,,,10.1145/1102256.1102276,ACM-DL,1,,,91,,,,,,,,,,Proceedings of the 7th Annual Workshop on Genetic and Evolutionary Computation,"function approximation, hyperplane coding, learning classifier systems, reinforcement learning, tile coding",,1102276,,,,,,,,90--91,,ACM,,,,,Proceedings of the 7th Annual Workshop on Genetic and Evolutionary Computation,90,article,,,,,2005,GECCO '05,Proceedings of the 7th Annual Workshop on Genetic and Evolutionary Computation,,"Washington, D.C.",,,,,,,2,,,,,"New York, NY, USA",,,
Adaptive Policies for Selecting Groupon Style Chunked Reward Ads in a Stochastic Knapsack Framework,,,,,,Michael  Grabchak and Narayan  Bhamidipati and Rushi  Bhatt and Dinesh  Garg,,,,10.1145/1963405.1963432,ACM-DL,1,,,176,,,,,978-1-4503-0632-4,,,,,Proceedings of the 20th International Conference on World Wide Web,"ad selection, chunked rewards, groupon, revenue maximization",,1963432,,,,,,,,167--176,,ACM,,,,,Proceedings of the 20th International Conference on World Wide Web,167,article,,,,,2011,WWW '11,Proceedings of the 20th International Conference on World Wide Web,,"Hyderabad, India",,,,,,,10,,,,,"New York, NY, USA",,,
Adaptive Path Scheduling for Mobile Element to Prolong the Lifetime of Wireless Sensor Networks,,,,,,Dakai  Zhu and Ali Saman Tosun,,,,10.1145/1399583.1399584,ACM-DL,1,,,4,,,,,,,,,SIGBED Rev.,SIGBED Rev.,,,1399584,,,,,,,,1:1--1:4,,ACM,,,,1551-3688,SIGBED Rev.,1,article,,,1,5,2008,,,,,,,July 2008,2,,July,4,,,,,"New York, NY, USA",,,
Adaptive Cognitive Orthotics: Combining Reinforcement Learning and Constraint-based Temporal Reasoning,,,,,,Matthew  Rudary and Satinder  Singh and Martha E. Pollack,,,,10.1145/1015330.1015411,ACM-DL,1,,,,,,,,1-58113-838-5,,,,,Proceedings of the Twenty-first International Conference on Machine Learning,,,1015411,,,,,,,,91--,,ACM,,,,,Proceedings of the Twenty-first International Conference on Machine Learning,91,article,,,,,2004,ICML '04,Proceedings of the Twenty-first International Conference on Machine Learning,,"Banff, Alberta, Canada",,,,,,,,,,,,"New York, NY, USA",,,
Adaptive and Hierarchical Runtime Manager for Energy-Aware Thermal Management of Embedded Systems,,,,,,Anup  Das and Bashir M. Al-Hashimi and Geoff V. Merrett,,,,10.1145/2834120,ACM-DL,1,,,49,,,,,,,,,ACM Trans. Embed. Comput. Syst.,ACM Trans. Embed. Comput. Syst.,"Embedded systems, Linux operating system",,2834120,,,,,,,,24:1--24:25,,ACM,,,,1539-9087,ACM Trans. Embed. Comput. Syst.,24,article,,,24,15,2016,,,,,,,May 2016,2,,January,25,,,,,"New York, NY, USA",,,
Ad Recommendation Systems for Life-Time Value Optimization,,,,,,Georgios  Theocharous and Philip S. Thomas and Mohammad  Ghavamzadeh,,,,10.1145/2740908.2741998,ACM-DL,1,,,1310,,,,,978-1-4503-3473-0,,,,,Proceedings of the 24th International Conference on World Wide Web,"ad recommendation, off-policy evaluation, reinforcement learning",,2741998,,,,,,,,1305--1310,,ACM,,,,,Proceedings of the 24th International Conference on World Wide Web,1305,article,,,,,2015,WWW '15 Companion,Proceedings of the 24th International Conference on World Wide Web,,"Florence, Italy",,,,,,,6,,,,,"New York, NY, USA",,,
Active Contextual Policy Search,,,,,,Alexander  Fabisch and Jan Hendrik Metzen,,,,,ACM-DL,0,,,3399,,,,,,,,,J. Mach. Learn. Res.,J. Mach. Learn. Res.,"active learning, movement primitives, multi-task learning, policy search, reinforcement learning",,2697072,,,,,,,,3371--3399,,JMLR.org,,,,1532-4435,J. Mach. Learn. Res.,3371,article,,,,15,2014,,,,,,,January 2014,1,,January,29,,,,,,,,
Accelerated Neural Evolution Through Cooperatively Coevolved Synapses,,,,,,Faustino  Gomez and J&#252;rgen  Schmidhuber and Risto  Miikkulainen,,,,,ACM-DL,0,,,965,,,,,,,,,J. Mach. Learn. Res.,J. Mach. Learn. Res.,,,1390712,,,,,,,,937--965,,JMLR.org,,,,1532-4435,J. Mach. Learn. Res.,937,article,,,,9,2008,,,,,,,6/1/2008,,,June,29,,,,,,,,
A Zeroth-Level Classifier System for Real Time Strategy Games,,,,,,Michalis T. Tsapanos and Kyriakos C. Chatzidimitriou and Pericles A. Mitkas,,,,10.1109/WI-IAT.2011.177,ACM-DL,1,,,247,,,,,978-0-7695-4513-4,,,,,Proceedings of the 2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Volume 02,"Learning Classifier Systems, Real Time Strategy Games",,2052191,,,,,,,,244--247,,IEEE Computer Society,,,,,Proceedings of the 2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Volume 02,244,article,,,,,2011,WI-IAT '11,Proceedings of the 2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Volume 02,,,,,,,,,4,,,,,"Washington, DC, USA",,,
A Unified Approach to Scheduling on Unrelated Parallel Machines,,,,,,V. S. Anil Kumar and Madhav V. Marathe and Srinivasan  Parthasarathy and Aravind  Srinivasan,,,,10.1145/1552285.1552289,ACM-DL,1,,,59,,,,,,,,,J. ACM,J. ACM,"Approximation algorithms, randomized rounding, scheduling under multiple criteria",,1552289,,,,,,,,28:1--28:31,,ACM,,,,0004-5411,J. ACM,28,article,,,28,56,2009,,,,,,,August 2009,5,,August,31,,,,,"New York, NY, USA",,,
A Tensor Factorization Approach to Generalization in Multi-agent Reinforcement Learning,,,,,,Stefano  Bromuri,,,,10.1109/WI-IAT.2012.21,ACM-DL,1,,,281,,,,,978-0-7695-4880-7,,,,,Proceedings of the The 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology - Volume 02,"Software Agents, Learning Systems, Algorithms, Dynamic Programming",,2457768,,,,,,,,274--281,,IEEE Computer Society,,,,,Proceedings of the The 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology - Volume 02,274,article,,,,,2012,WI-IAT '12,Proceedings of the The 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology - Volume 02,,,,,,,,,8,,,,,"Washington, DC, USA",,,
A System for Scheduling Student Computer Consultants,,,,,,Daniel E. Lane and Raymond  Cote and Jackson  Shaw,,,,10.1145/99186.99236,ACM-DL,1,,,224,,,,,0-89791-406-6,,,,,Proceedings of the 18th Annual ACM SIGUCCS Conference on User Services,,,99236,,,,,,,,205--224,,ACM,,,,,Proceedings of the 18th Annual ACM SIGUCCS Conference on User Services,205,article,,,,,1990,SIGUCCS '90,Proceedings of the 18th Annual ACM SIGUCCS Conference on User Services,,"Cincinnati, Ohio, USA",,,,,,,20,,,,,"New York, NY, USA",,,
A Super-scheduler for Embedded Reconfigurable Systems,,,,,,S. Ogrenci Memik and E.  Bozorgzadeh and R.  Kastner and M.  Sarrafzadeh,,,,,ACM-DL,0,,,394,,,,,0-7803-7249-2,,,,,Proceedings of the 2001 IEEE/ACM International Conference on Computer-aided Design,,,603176,,,,,,,,391--394,,IEEE Press,,,,,Proceedings of the 2001 IEEE/ACM International Conference on Computer-aided Design,391,article,,,,,2001,ICCAD '01,Proceedings of the 2001 IEEE/ACM International Conference on Computer-aided Design,,"San Jose, California",,,,,,,4,,,,,"Piscataway, NJ, USA",,,
A Study of Green Natural Learning Method at the Improvement of Cognitive Function Take the Elderly with Mild Dementia As Example,,,,,,Yu-Che  Huang and Tai-Shen  Huang,,,,10.1145/3094243.3094253,ACM-DL,1,,,35,,,,,978-1-4503-5232-1,,,,,Proceedings of the 2017 International Conference on Deep Learning Technologies,"Cognitive function, Green natural learning, Landscape therapy, The Elderly with mild Dementia",,3094253,,,,,,,,28--35,,ACM,,,,,Proceedings of the 2017 International Conference on Deep Learning Technologies,28,article,,,,,2017,ICDLT '17,Proceedings of the 2017 International Conference on Deep Learning Technologies,,"Chengdu, China",,,,,,,8,,,,,"New York, NY, USA",,,
A Sparse Matrix Representation for Production Scheduling Using Genetic Algorithms,,,,,,Simon J. T. Liang and John M. Lewis,,,,10.1145/315891.316004,ACM-DL,1,,,317,,,,,0-89791-658-1,,,,,Proceedings of the 1995 ACM Symposium on Applied Computing,"genetic algorithms, job shop scheduling, representing",,316004,,,,,,,,313--317,,ACM,,,,,Proceedings of the 1995 ACM Symposium on Applied Computing,313,article,,,,,1995,SAC '95,Proceedings of the 1995 ACM Symposium on Applied Computing,,"Nashville, Tennessee, USA",,,,,,,5,,,,,"New York, NY, USA",,,
A Simulation Model for Evaluating Bus Routes and Schedules in Small Cities,,,,,,John M. Gleason,,,,10.1145/800287.811193,ACM-DL,1,,,336,,,,,,,,,,Proceedings of the 7th Conference on Winter Simulation - Volume 1,,,811193,,,,,,,,329--336,,ACM,,,,,Proceedings of the 7th Conference on Winter Simulation - Volume 1,329,article,,,,,1974,WSC '74,Proceedings of the 7th Conference on Winter Simulation - Volume 1,,"Washington, DC",,,,,,,8,,,,,"New York, NY, USA",,,
A Self-tuning Self-optimizing Approach for Automated Network Anomaly Detection Systems,,,,,,Dennis  Ippoliti and Xiaobo  Zhou,,,,10.1145/2371536.2371551,ACM-DL,1,,,90,,,,,978-1-4503-1520-3,,,,,Proceedings of the 9th International Conference on Autonomic Computing,"network anomaly detection, self-optimizing, self-tuning",,2371551,,,,,,,,85--90,,ACM,,,,,Proceedings of the 9th International Conference on Autonomic Computing,85,article,,,,,2012,ICAC '12,Proceedings of the 9th International Conference on Autonomic Computing,,"San Jose, California, USA",,,,,,,6,,,,,"New York, NY, USA",,,
A Scheduling Method Considering Heterogeneous Clients for NVoD Systems,,,,,,Yusuke  Gotoh and Tomoki  Yoshihisa and Hideo  Taniguchi and Masanori  Kanazawa,,,,10.1145/2428955.2428998,ACM-DL,1,,,239,,,,,978-1-4503-1307-0,,,,,Proceedings of the 10th International Conference on Advances in Mobile Computing &#38; Multimedia,"broadcasting, continuous media data, scheduling, waiting time",,2428998,,,,,,,,232--239,,ACM,,,,,Proceedings of the 10th International Conference on Advances in Mobile Computing &#38; Multimedia,232,article,,,,,2012,MoMM '12,Proceedings of the 10th International Conference on Advances in Mobile Computing &#38; Multimedia,,"Bali, Indonesia",,,,,,,8,,,,,"New York, NY, USA",,,
A Scalable Parallel Q-Learning Algorithm for Resource Constrained Decentralized Computing Environments,,,,,,Miguel  Camelo and Jeroen  Famaey and Steven  Latr&#233;,,,,10.1109/MLHPC.2016.7,ACM-DL,1,,,35,,,,,978-1-5090-3882-4,,,,,Proceedings of the Workshop on Machine Learning in High Performance Computing Environments,,,3018878,,,,,,,,27--35,,IEEE Press,,,,,Proceedings of the Workshop on Machine Learning in High Performance Computing Environments,27,article,,,,,2016,MLHPC '16,Proceedings of the Workshop on Machine Learning in High Performance Computing Environments,,"Salt Lake City, Utah",,,,,,,9,,,,,"Piscataway, NJ, USA",,,
A Resource-allocation Queueing Fairness Measure,,,,,,David  Raz and Hanoch  Levy and Benjamin  Avi-Itzhak,,,,10.1145/1005686.1005704,ACM-DL,1,,,141,,,,,1-58113-873-3,,,,,Proceedings of the Joint International Conference on Measurement and Modeling of Computer Systems,"FCFS, M/M/1, PS, fairness, job scheduling, processor sharing, queue disciplines, resource allocation, unfairness",,1005704,,,,,,,,130--141,,ACM,,,,,Proceedings of the Joint International Conference on Measurement and Modeling of Computer Systems,130,article,,,,,2004,SIGMETRICS '04/Performance '04,Proceedings of the Joint International Conference on Measurement and Modeling of Computer Systems,,"New York, NY, USA",,,,,,,12,,,,,"New York, NY, USA",,,
A Reinforcement-Learning Based Cognitive Scheme for Opportunistic Spectrum Access.,,,,,,"Angeliki V. Kordali, Panayotis G. Cottis",,,,10.1007/S11277-015-2955-4,DBLP,1,,,769,,,,,,,,,,,,,495464,,,,https://doi.org/10.1007/s11277-015-2955-4,,,,,,,,,,,,751,Journal Articles,https://dblp.org/rec/journals/wpc/KordaliC16,,86,,2016,,,,,,,,,journals/wpc/KordaliC16,,,,2,751-769,,,,,Wireless Personal Communications
A Performance Evaluation Technique for the Measurement of a Facility's Ability to Process the Proper Jobs,,,,,,J. J. Babiel and B. Z. Duhl,,,,10.1145/800293.811664,ACM-DL,1,,,,,,,,,,,,,Proceedings of the 6th Conference on Winter Simulation,,,811664,,,,,,,,893--,,ACM,,,,,Proceedings of the 6th Conference on Winter Simulation,893,article,,,,,1973,WSC '73,Proceedings of the 6th Conference on Winter Simulation,,"San Francisco, CA",,,,,,,,,,,,"New York, NY, USA",,,
A Parallel Island Model Genetic Algorithm for the Multiprocessor Scheduling Problem,,,,,,Arthur L. Corcoran and Roger L. Wainwright,,,,10.1145/326619.326817,ACM-DL,1,,,487,,,,,0-89791-647-6,,,,,Proceedings of the 1994 ACM Symposium on Applied Computing,"genetic algorithms, multiprocessor scheduling, parallel island model, parallel processing",,326817,,,,,,,,483--487,,ACM,,,,,Proceedings of the 1994 ACM Symposium on Applied Computing,483,article,,,,,1994,SAC '94,Proceedings of the 1994 ACM Symposium on Applied Computing,,"Phoenix, Arizona, USA",,,,,,,5,,,,,"New York, NY, USA",,,
A Novel Sexual Adaptive Genetic Algorithm Based on Two-step Evolutionary Scenario of Baldwin Effect and Analysis of Global Convergence,,,,,,Mingming  Zhang and Shuguang  Zhao and Xu  Wang,,,,10.1145/1543834.1543935,ACM-DL,1,,,744,,,,,978-1-60558-326-6,,,,,Proceedings of the First ACM/SIGEVO Summit on Genetic and Evolutionary Computation,"adaptation, baldwin effect, genetic algorithm, global convergence, niche, sexual reproduction",,1543935,,,,,,,,737--744,,ACM,,,,,Proceedings of the First ACM/SIGEVO Summit on Genetic and Evolutionary Computation,737,article,,,,,2009,GEC '09,Proceedings of the First ACM/SIGEVO Summit on Genetic and Evolutionary Computation,,"Shanghai, China",,,,,,,8,,,,,"New York, NY, USA",,,
A Novel Scheduling Scheme to Share Dropping Ratio While Guaranteeing a Delay Bound in a multiCode-CDMA Network,,,,,,Peng-Yong  Kong and Kee-Chaing  Chua and Brahim  Bensaou,,,,10.1109/TNET.2003.820249,ACM-DL,1,,,1006,,,,,,,,,IEEE/ACM Trans. Netw.,IEEE/ACM Trans. Netw.,"multicode-CDMA, proportional dropping ratio guarantee, upper delay guarantee, variable capacity",,966008,,,,,,,,994--1006,,IEEE Press,,,,1063-6692,IEEE/ACM Trans. Netw.,994,article,,,,11,2003,,,,,,,01 December 2003,6,,December,13,,,,,"Piscataway, NJ, USA",,,
A New Technique of Switch & Feedback Job Scheduling Mechanism in a Distributed System,,,,,,Syed  Rizvi and Bevin  Thomas and Khaled  Elleithy and Aasia  Riasat,,,,,ACM-DL,0,,,125,,,,,,,,,,Proceedings of the 2009 Spring Simulation Multiconference,"job scheduling, parallel and distributed systems, performance optimization",,1639937,,,,,,,,121:1--121:4,,Society for Computer Simulation International,,,,,Proceedings of the 2009 Spring Simulation Multiconference,121,article,,,121,,2009,SpringSim '09,Proceedings of the 2009 Spring Simulation Multiconference,,"San Diego, California",,,,,,,4,,,,,"San Diego, CA, USA",,,
A New Hybrid Broadcast Scheduling Algorithm for Asymmetric Communication Systems,,,,,,Yufei  Guo and M. Cristina Pinotti and Sajal K. Das,,,,10.1145/584051.584055,ACM-DL,1,,,54,,,,,,,,,SIGMOBILE Mob. Comput. Commun. Rev.,SIGMOBILE Mob. Comput. Commun. Rev.,,,584055,,,,,,,,39--54,,ACM,,,,1559-1662,SIGMOBILE Mob. Comput. Commun. Rev.,39,article,,,,5,2001,,,,,,,July 2001,3,,July,16,,,,,"New York, NY, USA",,,
A New Approach to Schedule Operations Across Nested-ifs and Nested-loops,,,,,,Shih-Hsu  Huang and Cheng-Tsung  Hwang and Yu-Chin  Hsu and Yen-Jen  Oyang,,,,,ACM-DL,0,,,271,,,,,0-8186-3175-9,,,,,Proceedings of the 25th Annual International Symposium on Microarchitecture,,,145832,,,,,,,,268--271,,IEEE Computer Society Press,,,,,Proceedings of the 25th Annual International Symposium on Microarchitecture,268,article,,,,,1992,MICRO 25,Proceedings of the 25th Annual International Symposium on Microarchitecture,,"Portland, Oregon, USA",,,,,,,4,,,,,"Los Alamitos, CA, USA",,,
A New Approach for Multi-label Classification Based on Default Hierarchies and Organizational Learning,,,,,,Rosane M.M. Vallim and David E. Goldberg and Xavier  Llor&#224; and Thyago S.P.C. Duque and Andr&#233; C.P.L.F. Carvalho,,,,10.1145/1388969.1389015,ACM-DL,1,,,2022,,,,,978-1-60558-131-6,,,,,Proceedings of the 10th Annual Conference Companion on Genetic and Evolutionary Computation,"LCS, default hierarchies, organizational sizing",,1389015,,,,,,,,2017--2022,,ACM,,,,,Proceedings of the 10th Annual Conference Companion on Genetic and Evolutionary Computation,2017,article,,,,,2008,GECCO '08,Proceedings of the 10th Annual Conference Companion on Genetic and Evolutionary Computation,,"Atlanta, GA, USA",,,,,,,6,,,,,"New York, NY, USA",,,
A Near-optimal Real-time Hardware Scheduler for Large Cardinality Crossbar Switches,,,,,,Raymond R. Hoare and Zhu  Ding and Alex K. Jones,,,,10.1145/1188455.1188554,ACM-DL,1,,,,,,,,0-7695-2700-0,,,,,Proceedings of the 2006 ACM/IEEE Conference on Supercomputing,,,1188554,,,,,,,,,,ACM,,,,,Proceedings of the 2006 ACM/IEEE Conference on Supercomputing,94,article,,,94,,2006,SC '06,Proceedings of the 2006 ACM/IEEE Conference on Supercomputing,,"Tampa, Florida",,,,,,,,,,,,"New York, NY, USA",,,
A Multiagent Reinforcement Learning Algorithm by Dynamically Merging Markov Decision Processes,,,,,,Mohammad  Ghavamzadeh and Sridhar  Mahadevan,,,,10.1145/544862.544940,ACM-DL,1,,,846,,,,,1-58113-480-0,,,,,Proceedings of the First International Joint Conference on Autonomous Agents and Multiagent Systems: Part 2,,,544940,,,,,,,,845--846,,ACM,,,,,Proceedings of the First International Joint Conference on Autonomous Agents and Multiagent Systems: Part 2,845,article,,,,,2002,AAMAS '02,Proceedings of the First International Joint Conference on Autonomous Agents and Multiagent Systems: Part 2,,"Bologna, Italy",,,,,,,2,,,,,"New York, NY, USA",,,
A Multi-agent Reinforcement Learning Method for a Partially-observable Competitive Game,,,,,,Yoichiro  Matsuno and Tatsuya  Ymazaki and Shin  Ishii and Jun  Matsuno,,,,10.1145/375735.375856,ACM-DL,1,,,40,,,,,1-58113-326-X,,,,,Proceedings of the Fifth International Conference on Autonomous Agents,"actor-critic model, competitive game, multi-agent, reinforcement learning",,375856,,,,,,,,39--40,,ACM,,,,,Proceedings of the Fifth International Conference on Autonomous Agents,39,article,,,,,2001,AGENTS '01,Proceedings of the Fifth International Conference on Autonomous Agents,,"Montreal, Quebec, Canada",,,,,,,2,,,,,"New York, NY, USA",,,
A Model-Based Reinforcement Learning Algorithm for Routing in Energy Harvesting Mobile Ad-Hoc Networks.,,,,,,"Maryam M. Maleki, Vesal Hakami, Mehdi Dehghan",,,,10.1007/S11277-017-3987-8,DBLP,1,,,3139,,,,,,,,,,,,,206200,,,,https://doi.org/10.1007/s11277-017-3987-8,,,,,,,,,,,,3119,Journal Articles,https://dblp.org/rec/journals/wpc/MalekiHD17,,95,,2017,,,,,,,,,journals/wpc/MalekiHD17,,,,3,3119-3139,,,,,Wireless Personal Communications
A Learning-based Framework to Handle Multi-round Multi-party Influence Maximization on Social Networks,,,,,,Su-Chen  Lin and Shou-De  Lin and Ming-Syan  Chen,,,,10.1145/2783258.2783392,ACM-DL,1,,,704,,,,,978-1-4503-3664-2,,,,,Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,"influence maximization, reinforcement learning, social network",,2783392,,,,,,,,695--704,,ACM,,,,,Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,695,article,,,,,2015,KDD '15,Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,,"Sydney, NSW, Australia",,,,,,,10,,,,,"New York, NY, USA",,,
A Learning Interface Agent for Scheduling Meetings,,,,,,Robyn  Kozierok and Pattie  Maes,,,,10.1145/169891.169908,ACM-DL,1,,,88,,,,,0-89791-556-9,,,,,Proceedings of the 1st International Conference on Intelligent User Interfaces,"interface agents, learning interface agents, machine learning, personal assistants, software agents",,169908,,,,,,,,81--88,,ACM,,,,,Proceedings of the 1st International Conference on Intelligent User Interfaces,81,article,,,,,1993,IUI '93,Proceedings of the 1st International Conference on Intelligent User Interfaces,,"Orlando, Florida, USA",,,,,,,8,,,,,"New York, NY, USA",,,
A Learning Algorithm for the Longest Common Subsequence Problem,,,,,,Eric A. Breimer and Mark K. Goldberg and Darren T. Lim,,,,10.1145/996546.996552,ACM-DL,1,,,,,,,,,,,,J. Exp. Algorithmics,J. Exp. Algorithmics,,,996552,,,,,,,,,,ACM,,,,1084-6654,J. Exp. Algorithmics,2.1,article,,,2.1,8,2003,,,,,,,2003,,,December,,,,,,"New York, NY, USA",,,
A Hybrid Scheduling Technique for Hierarchical Logic Simulators or &Ldquo;Close Encounters of the Simulated Kind&Rdquo;,,,,,,Will  Sherwood,,,,,ACM-DL,0,,,254,,,,,,,,,,Proceedings of the 16th Design Automation Conference,"Event-driven and fixed event list scheduling techniques, Logic Simulation, RT/Gate level descriptions, Software breadboard",,811718,,,,,,,,249--254,,IEEE Press,,,,,Proceedings of the 16th Design Automation Conference,249,article,,,,,1979,DAC '79,Proceedings of the 16th Design Automation Conference,,"San Diego, CA, USA",,,,,,,6,,,,,"Piscataway, NJ, USA",,,
A High Level Multi-lingual Multiprocessor KMP/II,,,,,,Mario  Tokoro and Kiichiro  Tamaru and Masaaki  Mizuno and Masao  Hori,,,,10.1145/800053.801941,ACM-DL,1,,,333,,,,,,,,,,Proceedings of the 7th Annual Symposium on Computer Architecture,,,801941,,,,,,,,325--333,,ACM,,,,,Proceedings of the 7th Annual Symposium on Computer Architecture,325,article,,,,,1980,ISCA '80,Proceedings of the 7th Annual Symposium on Computer Architecture,,"La Baule, USA",,,,,,,9,,,,,"New York, NY, USA",,,
A Greedy Algorithm for the Unforecasted Energy Dispatch Problemwith Storage in Smart Grids,,,,,,Georgios  Georgiadis and Marina  Papatriantafilou,,,,10.1145/2487166.2487203,ACM-DL,1,,,274,,,,,978-1-4503-2052-8,,,,,Proceedings of the Fourth International Conference on Future Energy Systems,"online scheduling, resource allocation, smart grid",,2487203,,,,,,,,273--274,,ACM,,,,,Proceedings of the Fourth International Conference on Future Energy Systems,273,article,,,,,2013,e-Energy '13,Proceedings of the Fourth International Conference on Future Energy Systems,,"Berkeley, California, USA",,,,,,,2,,,,,"New York, NY, USA",,,
A Distributed Evolutionary Method to Design Scheduling Policies for Volunteer Computing,,,,,,Trilce  Estrada and Olac  Fuentes and Michela  Taufer,,,,10.1145/1481506.1481515,ACM-DL,1,,,49,,,,,,,,,SIGMETRICS Perform. Eval. Rev.,SIGMETRICS Perform. Eval. Rev.,"distributed systems, genetic algorithms, global computing, volatile systems",,1481515,,,,,,,,40--49,,ACM,,,,0163-5999,SIGMETRICS Perform. Eval. Rev.,40,article,,,,36,2008,,,,,,,December 2008,3,,November,10,,,,,"New York, NY, USA",,,
A Contextual Bandits Framework for Personalized Learning Action Selection.,,,,,,"Andrew S. Lan, Richard G. Baraniuk",,,,,DBLP,0,,,429,,,,,,,,,,,,,534724,,,,http://www.educationaldatamining.org/EDM2016/proceedings/paper_18.pdf,,,,,,,,,,,,424,Conference and Workshop Papers,https://dblp.org/rec/conf/edm/LanB16,,,,2016,,,,,,,,,conf/edm/LanB16,,,,,424-429,,,,,EDM
A Constraint Satisfaction Approach to Operative Management of Aircraft Routing,,,,,,Franco  Torquati and Massimo  Paltrinieri and Alberto  Momigliano,,,,10.1145/98894.99140,ACM-DL,1,,,1146,,,,,0-89791-372-8,,,,,Proceedings of the 3rd International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems - Volume 2,,,99140,,,,,,,,1140--1146,,ACM,,,,,Proceedings of the 3rd International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems - Volume 2,1140,article,,,,,1990,IEA/AIE '90,Proceedings of the 3rd International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems - Volume 2,,"Charleston, South Carolina, USA",,,,,,,7,,,,,"New York, NY, USA",,,
A Combined Tactical and Strategic Hierarchical Learning Framework in Multi-agent Games,,,,,,Chek Tien Tan and Ho-lun  Cheng,,,,10.1145/1401843.1401865,ACM-DL,1,,,122,,,,,978-1-60558-173-6,,,,,Proceedings of the 2008 ACM SIGGRAPH Symposium on Video Games,"game agent architecture, game artificial intelligence, learning, multi-agent cooperation, strategic planning, tactical behavior",,1401865,,,,,,,,115--122,,ACM,,,,,Proceedings of the 2008 ACM SIGGRAPH Symposium on Video Games,115,article,,,,,2008,Sandbox '08,Proceedings of the 2008 ACM SIGGRAPH Symposium on Video Games,,"Los Angeles, California",,,,,,,8,,,,,"New York, NY, USA",,,
A Cognitive Model of How People Make Decisions Through Interaction with Visual Displays,,,,,,Xiuli  Chen and Sandra Dorothee Starke and Chris  Baber and Andrew  Howes,,,,10.1145/3025453.3025596,ACM-DL,1,,,1216,,,,,978-1-4503-4655-9,,,,,Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems,"cognitive modeling, decision making., eye movements, markov decision process, reinforcement learning, visual search",,3025596,,,,,,,,1205--1216,,ACM,,,,,Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems,1205,article,,,,,2017,CHI '17,Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems,,"Denver, Colorado, USA",,,,,,,12,,,,,"New York, NY, USA",,,
