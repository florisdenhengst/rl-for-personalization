ID,Preference Author1,Preference Author2,Preference Author3,Preference Author4,Title,Abstract / URL / DOI,Abstract,Access Type,Affiliations,Art. No.,Author Affiliations,Keywords,Screening,Check,A RL or CB algorithm is used (reject/ accept/ uncertain),Check (agree/ disagree),Definition of personalization & direction (reject/ accept/ uncertain),Check (agree/ disagree).1,Peer-reviewed & English (reject/ accept/ uncertain),Check (agree/ disagree).2,Domain,Check (agree/ disagree).3,Remarks,Remarks.1,"Screening result (a/r/u): automatic
",Check result (a/d): automatic,Source Title,PDF Link,URL,Authors,Authors with affiliations,Citation Count,Copyright Year,DOI,Database,DOI counts,DOI count,Date Added To Xplore,End Page,Funding Information,IEEE Terms,INSPEC Controlled Terms,INSPEC Non-Controlled Terms,ISBN,ISSN,Issue,Issue Date,Journal,License,Local ID,Meeting Date,Mesh_Terms,Online Date,Page count,Page end,Page start,Pages,Patent Citation Count,Publisher,Reference Count,Source,Source ISBN,Source ISSN,Start Page,Type,Unnamed: 64,Unnamed: 12,Volume,Volume.1,Year if accepted,Year,0,1,2,Egibility check
0,1,1,1,1,Personalized automatic image annotation based on reinforcement learning,"With the rapidly increasing number of personal image collections on the web, it is of great importance to annotate these user-uploaded images in personalized manner. But personalized image annotation is largely ignored by the mainstream of image annotation research. In this paper, we focus on personalizing the automatic image annotation by proposing a general framework which jointly exploits the generic content-based image annotation, personal image tagging history and the content of personal history images. In our framework, two sets of candidate annotations are extracted for each image based on content-based annotation and personal image tagging history. Considering that the user's interest may not stay the same, when exploiting the personal image tagging history, we also take the content of personal history images into account to avoid the noise. To get the final annotations, we propose an unsupervised algorithm based on reinforcement learning to combine the above two candidate annotation sets. Encouraging results show that the proposed framework is effective and promising for personalizing automatic image annotation.","With the rapidly increasing number of personal image collections on the web, it is of great importance to annotate these user-uploaded images in personalized manner. But personalized image annotation is largely ignored by the mainstream of image annotation research. In this paper, we focus on personalizing the automatic image annotation by proposing a general framework which jointly exploits the generic content-based image annotation, personal image tagging history and the content of personal history images. In our framework, two sets of candidate annotations are extracted for each image based on content-based annotation and personal image tagging history. Considering that the user's interest may not stay the same, when exploiting the personal image tagging history, we also take the content of personal history images into account to avoid the noise. To get the final annotations, we propose an unsupervised algorithm based on reinforcement learning to combine the above two candidate annotation sets. Encouraging results show that the proposed framework is effective and promising for personalizing automatic image annotation.",,,,"Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science, Zhejiang University, Hangzhou 310027, China",Automatic image annotation;Personal image tagging history;Personalization;Reinforcement learning,Author4,Author1,a,a,a,a,a,a,Personal Assistants,a,Not 100% sure about the domain...,,a,a,2013 IEEE International Conference on Multimedia and Expo (ICME),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607456,,Yabo Ni; Miao Zheng; Jiajun Bu; Chun Chen; Dazhou Wang,,0,,10.1109/ICME.2013.6607456,IEEE Xplore,1,,20130926,6,,History;Learning (artificial intelligence);Noise;Semantics;Tagging;Unsupervised learning;Vocabulary,Internet;image classification;image retrieval;learning (artificial intelligence),Web;generic content-based image annotation;personal image collections;personal image tagging history;personalized automatic image annotation;reinforcement learning;user-uploaded images,,1945-7871;19457871,,15-19 July 2013,,,,,,,,,,,,IEEE,14,,Electronic:978-1-4799-0015-2; POD:978-1-4799-0014-5; USB:978-1-4799-0013-8,,1,IEEE Conferences,,,,,2013,2013,0,4,0,Author2
1,1,1,1,1,Automatically Learning User Preferences for Personalized Service Composition,"With the rapid growth of the web services technologies, users often leverage various web services to perform their daily activities, such as on-line shopping. Due to the massive amount of web services available, a user faces numerous choices to meet their personal preferences when selecting the desired services from the web services with the similar functionality. Therefore, it becomes tedious and cumbersome tasks for users to discover and compose services. To reduce user's cognitive burden, it is critical to support automated service composition and make efficient recommendation of personalized services to achieve user's overall goals. However, existing approaches only offer users with limited options designed for the interest of a group of users without considering individual users' interests. To allow users to compose personalized services without much manual specification, we propose a machine learning approach that applies a learning-to-rank algorithm, RankBoost, to automatically learn user preferences and the prioritization of the preferences from users' historical data. Moreover, our approach uses the multi-objective reinforcement learning (MORL) algorithm to make trade-offs among user preferences and recommends a collection of services to achieve the highest objective. We conduct an empirical study to evaluate our approach by collecting the historical data from 12 subjects. The results demonstrate that our approach outperforms the two well-established baseline approaches by 100%-200% in terms of precision on recommending services.","With the rapid growth of the web services technologies, users often leverage various web services to perform their daily activities, such as on-line shopping. Due to the massive amount of web services available, a user faces numerous choices to meet their personal preferences when selecting the desired services from the web services with the similar functionality. Therefore, it becomes tedious and cumbersome tasks for users to discover and compose services. To reduce user's cognitive burden, it is critical to support automated service composition and make efficient recommendation of personalized services to achieve user's overall goals. However, existing approaches only offer users with limited options designed for the interest of a group of users without considering individual users' interests. To allow users to compose personalized services without much manual specification, we propose a machine learning approach that applies a learning-to-rank algorithm, RankBoost, to automatically learn user preferences and the prioritization of the preferences from users' historical data. Moreover, our approach uses the multi-objective reinforcement learning (MORL) algorithm to make trade-offs among user preferences and recommends a collection of services to achieve the highest objective. We conduct an empirical study to evaluate our approach by collecting the historical data from 12 subjects. The results demonstrate that our approach outperforms the two well-established baseline approaches by 100%-200% in terms of precision on recommending services.",,,,"Queen's Univ., Kingston, ON, Canada",,Author1,Author2,a,a,a,a,a,a,Recommender Systems,,,,a,a,2017 IEEE International Conference on Web Services (ICWS),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029835,,Y. Zhao; S. Wang; Y. Zou; J. Ng; T. Ng,,,,10.1109/ICWS.2017.93,IEEE Xplore,1,,20170911,783,,Data mining;Feature extraction;History;Learning (artificial intelligence);Machine learning algorithms;Time-frequency analysis;Web services,Internet;Web services;learning (artificial intelligence);recommender systems;retail data processing,MORL algorithm;RankBoost;Web service technologies;automated service composition;automatic user preference learning;learning-to-rank algorithm;machine learning approach;multiobjective reinforcement learning algorithm;online shopping;personalized service composition;service recommendation,,,,25-30 June 2017,,,,,,,,,,,,IEEE,,,Electronic:978-1-5386-0752-7; POD:978-1-5386-0753-4,,776,IEEE Conferences,,,,,2017,2017,0,4,0,Author2
2,0,0,0,0,Machine learning methods for big spectrum data processing,"With the rapid development of the mobile Internet and the Internet of Things, the number of personal wireless devices has grown exponentially, resulting in the increase of massive spectrum data. Therefore, the big spectrum data are literally formed. Meanwhile, the spectrum deficit is also increasingly precarious. Effective big spectrum data processing is significant in improving the spectrum utilization. Firstly, from a perspective of wireless communication, a definition of big spectrum data is presented and its characteristics are also analyzed. Then, promising machine learning methods to analyze and utilize the big spectrum data are summarized, such as, the distributed and parallel learning, extreme learning machine, kernel-based learning, deep learning, reinforcement learning, game learning, and transfer learning. Finally, several open issues and research trends are addressed. ©, 2015, Journal of Data Acquisition and Processing. All right reserved.","With the rapid development of the mobile Internet and the Internet of Things, the number of personal wireless devices has grown exponentially, resulting in the increase of massive spectrum data. Therefore, the big spectrum data are literally formed. Meanwhile, the spectrum deficit is also increasingly precarious. Effective big spectrum data processing is significant in improving the spectrum utilization. Firstly, from a perspective of wireless communication, a definition of big spectrum data is presented and its characteristics are also analyzed. Then, promising machine learning methods to analyze and utilize the big spectrum data are summarized, such as, the distributed and parallel learning, extreme learning machine, kernel-based learning, deep learning, reinforcement learning, game learning, and transfer learning. Finally, several open issues and research trends are addressed. ©, 2015, Journal of Data Acquisition and Processing. All right reserved.",,"College of Communications Engineering, PLA University of Science and Technology, Nanjing, China",,,Big data; Big spectrum data; Data mining; Internet of Things; Machine learning; Wireless communication,Author2,Author3,a,a,u,a,a,a,"Other (please in ""remark"")",a,Spectrum data,"From this url [0] it seems that this is a publication in Chinese.
[0] https://www-scopus-com.vu-nl.idm.oclc.org/record/display.uri?eid=2-s2.0-84941894431&doi=10.16337%2fj.1004-9037.2015.04.001&origin=inward&txGid=ab1985e0ab1e9afc52eaaa133f0a9be5",u,a,Shuju Caiji Yu Chuli/Journal of Data Acquisition and Processing,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941894431&doi=10.16337%2fj.1004-9037.2015.04.001&partnerID=40&md5=e9115001bb97be1c11c6026d71376b47,"Wu Q., Qiu J., Ding G.","Wu, Q., College of Communications Engineering, PLA University of Science and Technology, Nanjing, China; Qiu, J., College of Communications Engineering, PLA University of Science and Technology, Nanjing, China; Ding, G., College of Communications Engineering, PLA University of Science and Technology, Nanjing, China",3,,10.16337/j.1004-9037.2015.04.001,SCOPUS,1,,,,,,,,,,4,,,,2-s2.0-84941894431,,,,,713,703,,,,,Scopus,,,,Article,,,30,,,2015,4,0,0,Author2
3,2,2,2,2,A Reinforcement Learning-Based Adaptive Learning System,"With the plethora of educational and e-learning systems and the great variation in students’ personal and social factors that affect their learning behaviors and outcomes, it has become mandatory for all educational systems to adapt to the variability of these factors for each student. Since there is a large number of factors that need to be taken into consideration, the task is very challenging. In this paper, we present an approach that adapts to the most influential factors in a way that varies from one learner to another, and in different learning settings, including individual and collaborative learning. The approach utilizes reinforcement learning for building an intelligent environment that, not only provides a method for suggesting suitable learning materials, but also provides a methodology for accounting for the continuously-changing students’ states and acceptance of technology. We evaluate our system through simulations. The obtained results are promising and show the feasibility of the proposed approach. © 2018, Springer International Publishing AG.","With the plethora of educational and e-learning systems and the great variation in students’ personal and social factors that affect their learning behaviors and outcomes, it has become mandatory for all educational systems to adapt to the variability of these factors for each student. Since there is a large number of factors that need to be taken into consideration, the task is very challenging. In this paper, we present an approach that adapts to the most influential factors in a way that varies from one learner to another, and in different learning settings, including individual and collaborative learning. The approach utilizes reinforcement learning for building an intelligent environment that, not only provides a method for suggesting suitable learning materials, but also provides a methodology for accounting for the continuously-changing students’ states and acceptance of technology. We evaluate our system through simulations. The obtained results are promising and show the feasibility of the proposed approach. © 2018, Springer International Publishing AG.",,"Center for Learning Technologies, University of Science and Technology, Zewail City, Giza, Egypt",,,Adaptive learning; Computer-supported collaborative learning; Reinforcement Learning,Author3,Author4,a,a,a,a,a,a,Intelligent Tutors,a,,,a,a,Advances in Intelligent Systems and Computing,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041837617&doi=10.1007%2f978-3-319-74690-6_22&partnerID=40&md5=7086141d27c694a3b289e9ba955acc61,"Shawky D., Badawi A.","Shawky, D., Center for Learning Technologies, University of Science and Technology, Zewail City, Giza, Egypt; Badawi, A., Center for Learning Technologies, University of Science and Technology, Zewail City, Giza, Egypt",,,10.1007/978-3-319-74690-6_22,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85041837617,,,,,231,221,,,,,Scopus,,,,Conference Paper,,,723,,2018,2018,0,0,4,Author4
4,0,0,0,0,Learning-based ship design optimization approach,"With the development of computer applications in ship design, optimization, as a powerful approach, has been widely used in the design and analysis process. However, the running time, which often varies from several weeks to months in the current computing environment, has been a bottleneck problem for optimization applications, particularly in the structural design of ships. To speed up the optimization process and adjust the complex design environment, ship designers usually rely on their personal experience to assist the design work. However, traditional experience, which largely depends on the designer's personal skills, often makes the design quality very sensitive to the experience and decreases the robustness of the final design. This paper proposes a new machine-learning-based ship design optimization approach, which uses machine learning as an effective tool to give direction to optimization and improves the adaptability of optimization to the dynamic design environment. The natural human learning process is introduced into the optimization procedure to improve the efficiency of the algorithm. Q-learning, as an approach of reinforcement learning, is utilized to realize the learning function in the optimization process. The multi-objective particle swarm optimization method, multi-agent system, and CAE software are used to build an integrated optimization system. A bulk carrier structural design optimization was performed as a case study to evaluate the suitability of this method for real-world application. © 2011 Elsevier Ltd. All rights reserved.","With the development of computer applications in ship design, optimization, as a powerful approach, has been widely used in the design and analysis process. However, the running time, which often varies from several weeks to months in the current computing environment, has been a bottleneck problem for optimization applications, particularly in the structural design of ships. To speed up the optimization process and adjust the complex design environment, ship designers usually rely on their personal experience to assist the design work. However, traditional experience, which largely depends on the designer's personal skills, often makes the design quality very sensitive to the experience and decreases the robustness of the final design. This paper proposes a new machine-learning-based ship design optimization approach, which uses machine learning as an effective tool to give direction to optimization and improves the adaptability of optimization to the dynamic design environment. The natural human learning process is introduced into the optimization procedure to improve the efficiency of the algorithm. Q-learning, as an approach of reinforcement learning, is utilized to realize the learning function in the optimization process. The multi-objective particle swarm optimization method, multi-agent system, and CAE software are used to build an integrated optimization system. A bulk carrier structural design optimization was performed as a case study to evaluate the suitability of this method for real-world application. © 2011 Elsevier Ltd. All rights reserved.",,"Department of Naval Architecture and Marine Engineering, University of Strathclyde, Glasgow G4 0LZ, United Kingdom",,,Machine learning; Multi-objective optimization; Ship design; Structure analysis; Structure optimization,Author2,Author3,a,a,u,a,a,a,"Other (please in ""remark"")",d,Help in ship design,"I think this fits our definition of personalization as the optimization process is sped up and thus increases its personal relevance to that designer. If we find that the speedup is shared for all users, then we can discard it on the basis of 'no personalization'. Regarding the domain: I think we can make it more generic and put something like 'structural engineering'.",u,a,CAD Computer Aided Design,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855691014&doi=10.1016%2fj.cad.2011.06.011&partnerID=40&md5=77cd692740e8a4769780a062c5bfb89a,"Cui H., Turan O., Sayer P.","Cui, H., Department of Naval Architecture and Marine Engineering, University of Strathclyde, Glasgow G4 0LZ, United Kingdom; Turan, O., Department of Naval Architecture and Marine Engineering, University of Strathclyde, Glasgow G4 0LZ, United Kingdom; Sayer, P., Department of Naval Architecture and Marine Engineering, University of Strathclyde, Glasgow G4 0LZ, United Kingdom",13,,10.1016/j.cad.2011.06.011,SCOPUS,1,,,,,,,,,,3,,,,2-s2.0-84855691014,,,,,195,186,,,,,Scopus,,,,Article,,,44,,,2012,4,0,0,Author4
5,0,1,0,0,Towards a General Supporting Framework for Self-Adaptive Software Systems,"When confronted with their internal and environmental dynamics, various software systems increasingly require self-adaptation capabilities. The vision above requires the self-adaptation approach to tackle these challenges and some software systems have their own specific implementations. However, in this paper a general supporting framework is proposed for software systems to self-adapt with running environmental dynamics and meanwhile fulfill various user requirements. Three key issues are covered in the framework: 1) the overall control architecture, which adopts the double closed-loop style and respectively includes the self-adaptation loop and the self-learning loop; 2) a general descriptive language, which is a application-independent and unified language to represent self-adaptation knowledge about target systems; 3) three implementation mechanisms, including forward reasoning, planning and reinforcement learning using feedback, which are supported by the above descriptive language and executed at runtime in different modules. Finally, one scenario of on-demand services of massive data mining tasks is selected and the case study demonstrates how the framework is customized as required and how the approach works.","When confronted with their internal and environmental dynamics, various software systems increasingly require self-adaptation capabilities. The vision above requires the self-adaptation approach to tackle these challenges and some software systems have their own specific implementations. However, in this paper a general supporting framework is proposed for software systems to self-adapt with running environmental dynamics and meanwhile fulfill various user requirements. Three key issues are covered in the framework: 1) the overall control architecture, which adopts the double closed-loop style and respectively includes the self-adaptation loop and the self-learning loop; 2) a general descriptive language, which is a application-independent and unified language to represent self-adaptation knowledge about target systems; 3) three implementation mechanisms, including forward reasoning, planning and reinforcement learning using feedback, which are supported by the above descriptive language and executed at runtime in different modules. Finally, one scenario of on-demand services of massive data mining tasks is selected and the case study demonstrates how the framework is customized as required and how the approach works.",,,,"State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China",Double Closed-loop Control Arthitecture;General Descriptive Language;Hierarchical Task Network;Rete Algorithm;Self-Adaptive Supporting Framework;Self-Adaptive System,Author3,Author4,a,a,u,a,a,a,other,,"It is uncertain what constitutes the ""environment"" in the abstract. If it incorporates user information, it adheres to the definition. 

domain: software engineering","I would say that this fits, but you are right it can be debated. This is what Author3 will be working on during the next couple of weeks.",u,a,2012 IEEE 36th Annual Computer Software and Applications Conference Workshops,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341568,,L. Wang; Y. Gao; C. Cao; L. Wang,,2,,10.1109/COMPSACW.2012.38,IEEE Xplore,1,,20121110,163,,Adaptation models;Cognition;Computer architecture;Learning;Monitoring;Planning;Software systems,data mining;inference mechanisms;software engineering;unsupervised learning,data mining;environmental dynamics;forward reasoning;general descriptive language;general supporting framework;on-demand service;overall control architecture;reinforcement learning;self-adaptation knowledge;self-adaptation loop;self-adaptive software system;self-learning loop,,,,16-20 July 2012,,,,,,,,,,,,IEEE,18,,Electronic:978-0-7695-4758-9; POD:978-1-4673-2714-5,,158,IEEE Conferences,,,,,,2012,3,1,0,Author2
6,1,0,2,1,Intelligent agent for E-tourism: Personalization travel support agent using reinforcement learning,"Web personalization and one to one marketing have been introduced as strategy and marketing tools. By using historical and present information of customers, organizations can learn, predict customer's behaviors and develop products to fit potential customers. In this study, a Personalization Travel Support System is introduced to manage traveling information for user. It provides the information that matches the users' interests. This system applies the Reinforcement Learning to analyze, learn customer behaviors and recommend products to meet customer interests. There are two learning approaches using in this study. First, Personalization Learner by Group Properties is learning from all users in one group to find the group interests of travel information by using given data on user ages and genders. Second, Personalization Learner by User Behavior: user profile, user behaviors and trip features will be analyzed to find the unique interest of each web user. The results from this study reveal that it is possible to develop Personalization Travel Support System. Using weighted trip features improve effectiveness and increase the accuracy of the personalized engine. Precision, Recall and Harmonic Mean of the learned system are higher than the original one. This study offers useful information regarding the areas of personalization of web support system.","Web personalization and one to one marketing have been introduced as strategy and marketing tools. By using historical and present information of customers, organizations can learn, predict customer's behaviors and develop products to fit potential customers. In this study, a Personalization Travel Support System is introduced to manage traveling information for user. It provides the information that matches the users' interests. This system applies the Reinforcement Learning to analyze, learn customer behaviors and recommend products to meet customer interests. There are two learning approaches using in this study. First, Personalization Learner by Group Properties is learning from all users in one group to find the group interests of travel information by using given data on user ages and genders. Second, Personalization Learner by User Behavior: user profile, user behaviors and trip features will be analyzed to find the unique interest of each web user. The results from this study reveal that it is possible to develop Personalization Travel Support System. Using weighted trip features improve effectiveness and increase the accuracy of the personalized engine. Precision, Recall and Harmonic Mean of the learned system are higher than the original one. This study offers useful information regarding the areas of personalization of web support system.",,"Department of Computer Science, Faculty of Science, Kasetsart University, Bangkok 10900, Thailand",,,Intelligent agent; Personalization; Recommendation algorithm; Reinforcement Learning,Author1,Author2,a,a,a,a,a,a,Recommender Systems,,,,a,a,CEUR Workshop Proceedings,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883477666&partnerID=40&md5=b8091062d9e61ff7714e961f19c584d9,"Srivihok A., Sukonmanee P.","Srivihok, A., Department of Computer Science, Faculty of Science, Kasetsart University, Bangkok 10900, Thailand; Sukonmanee, P., Department of Computer Science, Faculty of Science, Kasetsart University, Bangkok 10900, Thailand",,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-84883477666,,,,4,,,,,,,Scopus,,,,Conference Paper,,,143,,2005,2005,1,2,1,Author3
7,2,1,2,2,Markov decision process based adaptive web advertisements scheduling,"We study web advertisements scheduling problem by fully considering the interaction of web users and web advertisements publishing system. We construct a Markov Decision Process (MDP) based web advertisements scheduling model and schedule advertisements publishing during the whole process of web surfing by the users, thus we make maximal use of personal behavior characteristics of every web user in the scheduling model. We also track the user habit with reinforcement learning, solve the MDP model by TD(λ) algorithm combing the function approximator, and obtain adaptive online scheduling policies for web advertisements publishing. © (2013) Trans Tech Publications, Switzerland.","We study web advertisements scheduling problem by fully considering the interaction of web users and web advertisements publishing system. We construct a Markov Decision Process (MDP) based web advertisements scheduling model and schedule advertisements publishing during the whole process of web surfing by the users, thus we make maximal use of personal behavior characteristics of every web user in the scheduling model. We also track the user habit with reinforcement learning, solve the MDP model by TD(λ) algorithm combing the function approximator, and obtain adaptive online scheduling policies for web advertisements publishing. © (2013) Trans Tech Publications, Switzerland.",,"Department of Industrial Engineering, Dongguan University of Technology, Dongguan 523808, China",,,Reinforcement learning; Scheduling; Web advertisements,Author2,Author3,a,a,a,a,a,a,Recommender Systems,a,,,a,a,Advanced Materials Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885049231&doi=10.4028%2fwww.scientific.net%2fAMR.765-767.1436&partnerID=40&md5=8ff8e90a548dd3e734a60cb6c1df0296,"Zhang Z.C., Li S., Hu K.S., Huang H.Y., Zhao S.Y.","Zhang, Z.C., Department of Industrial Engineering, Dongguan University of Technology, Dongguan 523808, China; Li, S., Department of Industrial Engineering, Dongguan University of Technology, Dongguan 523808, China; Hu, K.S., Department of Industrial Engineering, Dongguan University of Technology, Dongguan 523808, China; Huang, H.Y., Department of Industrial Engineering, Dongguan University of Technology, Dongguan 523808, China; Zhao, S.Y., Department of Industrial Engineering, Dongguan University of Technology, Dongguan 523808, China",,,10.4028/www.scientific.net/AMR.765-767.1436,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84885049231,,,,,1440,1436,,,,,Scopus,,,,Conference Paper,,,765-767,,2013,2013,0,1,3,Author4
8,1,0,2,1,Learning dialogue strategies for interactive database search,"We show how to learn optimal dialogue policies for a wide range of database search applications, concerning how many database search results to present to the user, and when to present them. We use Reinforcement Learning methods for a wide spectrum of different database simulations, turn penalty conditions, and noise conditions. Our objective is to show that our policy learning framework covers this spectrum. We can show that even for challenging cases learning significantly outperforms hand-coded policies tailored to the different operating situations. The polices are adaptive/context-sensitive in respect of both the overall operating situation (e.g. noise) and the local context of the interaction (e.g. user's last move). The learned policies produce an average relative increase in reward of 25.7% over the corresponding threshold-based hand-coded baseline policies.","We show how to learn optimal dialogue policies for a wide range of database search applications, concerning how many database search results to present to the user, and when to present them. We use Reinforcement Learning methods for a wide spectrum of different database simulations, turn penalty conditions, and noise conditions. Our objective is to show that our policy learning framework covers this spectrum. We can show that even for challenging cases learning significantly outperforms hand-coded policies tailored to the different operating situations. The polices are adaptive/context-sensitive in respect of both the overall operating situation (e.g. noise) and the local context of the interaction (e.g. user's last move). The learned policies produce an average relative increase in reward of 25.7% over the corresponding threshold-based hand-coded baseline policies.",,"Department of Computational Linguistics, Saarland University, Saarbrücken, Germany; School of Informatics, University of Edinburgh, Edinburgh, United Kingdom",,,Adaptive strategies; Database search; Dialogue systems; Multimodality; Reinforcement learning,Author1,Author2,a,a,a,a,a,a,Interfaces / HCI,,,,a,a,"International Speech Communication Association - 8th Annual Conference of the International Speech Communication Association, Interspeech 2007",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56149117477&partnerID=40&md5=0223cd1a2d0ddeecbd0b38a429aa35eb,"Rieser V., Lemon O.","Rieser, V., Department of Computational Linguistics, Saarland University, Saarbrücken, Germany; Lemon, O., School of Informatics, University of Edinburgh, Edinburgh, United Kingdom",,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-56149117477,,,,,2044,2041,,,,,Scopus,,,,Conference Paper,,,3,,2007,2007,1,2,1,Author3
9,1,1,1,1,Swarm reinforcement learning algorithm based on particle swarm optimization whose personal bests have lifespans,"We recently proposed a swarm reinforcement learning algorithm based on particle swarm optimization (PSO) in order to find optimal policies rapidly. In this algorithm, multiple agents are prepared, and they learn not only by individual learning but also by an update procedure of PSO. In this procedure, state-action values are updated based on the personal best and the global best which are found by the agents so far. In this paper, we direct our attention to a problem that overvaluing personal bests brings inferior learning performance. In order not to update the state-action values based on the overvalued personal best, we propose a swarm reinforcement learning algorithm based on PSO in which the personal best of each agent has a lifespan. © 2009 Springer-Verlag Berlin Heidelberg.","We recently proposed a swarm reinforcement learning algorithm based on particle swarm optimization (PSO) in order to find optimal policies rapidly. In this algorithm, multiple agents are prepared, and they learn not only by individual learning but also by an update procedure of PSO. In this procedure, state-action values are updated based on the personal best and the global best which are found by the agents so far. In this paper, we direct our attention to a problem that overvaluing personal bests brings inferior learning performance. In order not to update the state-action values based on the overvalued personal best, we propose a swarm reinforcement learning algorithm based on PSO in which the personal best of each agent has a lifespan. © 2009 Springer-Verlag Berlin Heidelberg.",,"Kyoto Institute of Technology, Matsugasaki, Sakyo-ku, Kyoto, Japan",,,Particle swarm optimization; Reinforcement learning; Swarm intelligence,Author2,Author3,a,a,u,a,a,a,"Other (please in ""remark"")",a,Particle swarm optimization,"The application is not part of the abstract, so we cannot reject this paper based on 'no personalization'. I propose we accept it for now and check what the proposed algorithm is validated on.",u,a,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249096804&doi=10.1007%2f978-3-642-10684-2_19&partnerID=40&md5=ddde3d9d2f0fa887419a160c5928a5d5,"Iima H., Kuroe Y.","Iima, H., Kyoto Institute of Technology, Matsugasaki, Sakyo-ku, Kyoto, Japan; Kuroe, Y., Kyoto Institute of Technology, Matsugasaki, Sakyo-ku, Kyoto, Japan",1,,10.1007/978-3-642-10684-2_19,SCOPUS,1,,,,,,,,,,PART 2,,,,2-s2.0-76249096804,,,,,178,169,,,,,Scopus,,,,Conference Paper,,,5864 LNCS,,,2009,0,4,0,Author2
10,2,1,2,2,"Probabilistic topic modeling, reinforcement learning, and crowdsourcing for personalized recommendations","We put forward an innovative use of probabilistic topic modeling (PTM) intertwined with reinforcement learning (RL), to provide personalized recommendations. Specifically, we model items under recommendation as mixtures of latent topics following a distribution with Dirichlet priors; this can be achieved via the exploitation of crowd-sourced information for each item. Similarly, we model the user herself as an “evolving” document represented by its respective mixture of latent topics. The user’s topic distribution is appropriately updated each time she consumes an item. Recommendations are subsequently based on the divergence between the topic distributions of the user and available items. However, to tackle the exploration versus exploitation dilemma, we apply RL to vary the user’s topic distribution update rate. Our method is immune to the notorious “cold start” problem, and it can effectively cope with changing user preferences. Moreover, it is shown to be competitive against state-of-the-art algorithms, outperforming them in terms of sequential performance. © Springer International Publishing AG 2017.","We put forward an innovative use of probabilistic topic modeling (PTM) intertwined with reinforcement learning (RL), to provide personalized recommendations. Specifically, we model items under recommendation as mixtures of latent topics following a distribution with Dirichlet priors; this can be achieved via the exploitation of crowd-sourced information for each item. Similarly, we model the user herself as an “evolving” document represented by its respective mixture of latent topics. The user’s topic distribution is appropriately updated each time she consumes an item. Recommendations are subsequently based on the divergence between the topic distributions of the user and available items. However, to tackle the exploration versus exploitation dilemma, we apply RL to vary the user’s topic distribution update rate. Our method is immune to the notorious “cold start” problem, and it can effectively cope with changing user preferences. Moreover, it is shown to be competitive against state-of-the-art algorithms, outperforming them in terms of sequential performance. © Springer International Publishing AG 2017.",,"School of Electrical and Computer Engineering, Technical University of Crete, Chania, Greece",,,Applications of reinforcement learning; Crowdsourcing; Graphical models; Recommender systems,Author3,Author4,a,a,a,a,a,a,recommender Systems,,,,a,a,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022214474&doi=10.1007%2f978-3-319-59294-7_14&partnerID=40&md5=e9c8b09a9368f99a2d652b2b8d3a9e8e,"Tripolitakis E., Chalkiadakis G.","Tripolitakis, E., School of Electrical and Computer Engineering, Technical University of Crete, Chania, Greece; Chalkiadakis, G., School of Electrical and Computer Engineering, Technical University of Crete, Chania, Greece",,,10.1007/978-3-319-59294-7_14,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85022214474,,,,,171,157,,,,,Scopus,,,,Conference Paper,,,10207 LNAI,,2017,2017,0,1,3,Author4
11,1,0,1,1,Video summarization using reinforcement learning in eigenspace,"We propose video summarization using reinforcement learning. The importance score of each frame in a video is calculated from the user's actions in handling similar previous frames; if such frames were watched rather than skipped, a high score is assigned. To calculate the score, instead of using raw feature vectors extracted from images, we use feature vectors projected on eigenspace: as a result, we can deal with the features comprehensively. We also give an algorithm that uses the reinforcement learning method to create a personalized video summary. The summarization algorithm is applied to a soccer video to confirm its effectiveness.","We propose video summarization using reinforcement learning. The importance score of each frame in a video is calculated from the user's actions in handling similar previous frames; if such frames were watched rather than skipped, a high score is assigned. To calculate the score, instead of using raw feature vectors extracted from images, we use feature vectors projected on eigenspace: as a result, we can deal with the features comprehensively. We also give an algorithm that uses the reinforcement learning method to create a personalized video summary. The summarization algorithm is applied to a soccer video to confirm its effectiveness.",,,,"IBM Tokyo Res. Lab., Kanagawa, Japan",,Author4,Author1,a,a,a,a,a,a,Interfaces / HCI,,About analysis of videos. Based on the abstract hard to derive what the personalization is.,,a,a,Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=899351,,K. Masumitsu; T. Echigo,,4,,10.1109/ICIP.2000.899351,IEEE Xplore,1,,20020806,270 vol.2,,Data mining;Feature extraction;Information retrieval;Joining processes;Laboratories;Layout;Learning;Multimedia communication;TV broadcasting;Watches,feature extraction;image sequences;learning (artificial intelligence);video signal processing,algorithm;eigenspace;feature vectors extraction;personalized video summary;reinforcement learning;soccer video;summarization algorithm;video frame;video summarization,,1522-4880;15224880,,10-13 Sept. 2000,,,,10 Sep 2000-13 Sep 2000,,,,,,,3,IEEE,7,,POD:0-7803-6297-7,,267,IEEE Conferences,,,2,,2000,2000,1,3,0,Author4
12,2,2,0,2,Greedy outcome weighted tree learning of optimal personalized treatment rules,"We propose a subgroup identification approach for inferring optimal and interpretable personalized treatment rules with high-dimensional covariates. Our approach is based on a two-step greedy tree algorithm to pursue signals in a high-dimensional space. In the first step, we transform the treatment selection problem into a weighted classification problem that can utilize tree-based methods. In the second step, we adopt a newly proposed tree-based method, known as reinforcement learning trees, to detect features involved in the optimal treatment rules and to construct binary splitting rules. The method is further extended to right censored survival data by using the accelerated failure time model and introducing double weighting to the classification trees. The performance of the proposed method is demonstrated via simulation studies, as well as analyses of the Cancer Cell Line Encyclopedia (CCLE) data and the Tamoxifen breast cancer data. © 2016, The International Biometric Society","We propose a subgroup identification approach for inferring optimal and interpretable personalized treatment rules with high-dimensional covariates. Our approach is based on a two-step greedy tree algorithm to pursue signals in a high-dimensional space. In the first step, we transform the treatment selection problem into a weighted classification problem that can utilize tree-based methods. In the second step, we adopt a newly proposed tree-based method, known as reinforcement learning trees, to detect features involved in the optimal treatment rules and to construct binary splitting rules. The method is further extended to right censored survival data by using the accelerated failure time model and introducing double weighting to the classification trees. The performance of the proposed method is demonstrated via simulation studies, as well as analyses of the Cancer Cell Line Encyclopedia (CCLE) data and the Tamoxifen breast cancer data. © 2016, The International Biometric Society",,"University of Illinois at Urbana-Champaign, Champaign, IL, United States; Fred Hutchinson Cancer Research Center, Seattle, WA, United States; Vanderbilt University, Nashville, TN, United States; Yale University, New Haven, CT, United States",,,High-dimensional data; Optimal treatment rules; Personalized medicine; Reinforcement learning trees; Survival analysis; Tree-based method,Author1,Author2,a,a,a,a,a,a,Healthcare,,,,a,a,Biometrics,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994875692&doi=10.1111%2fbiom.12593&partnerID=40&md5=f7fb0d98e7a8421f1d12ffbc09a05fee,"Zhu R., Zhao Y.-Q., Chen G., Ma S., Zhao H.","Zhu, R., University of Illinois at Urbana-Champaign, Champaign, IL, United States; Zhao, Y.-Q., Fred Hutchinson Cancer Research Center, Seattle, WA, United States; Chen, G., Vanderbilt University, Nashville, TN, United States; Ma, S., Yale University, New Haven, CT, United States; Zhao, H., Yale University, New Haven, CT, United States",1,,10.1111/biom.12593,SCOPUS,1,,,,,,,,,,2,,,,2-s2.0-84994875692,,,,,400,391,,,,,Scopus,,,,Article,,,73,,2017,2017,1,0,3,Author4
13,1,2,1,1,A dialogue game framework with personalized training using reinforcement learning for computer-assisted language learning,"We propose a framework for computer-assisted language learning as a pedagogical dialogue game. The goal is to offer personalized learning sentences on-line for each individual learner considering the learner's learning status, in order to strike a balance between more practice on poorly-pronounced units and complete practice on the whole set of pronunciation units. This objective is achieved using a Markov decision process (MDP) trained with reinforcement learning using simulated learners generated from real learner data. Preliminary experimental results on a subset of the example dialogue script show the effectiveness of the framework.","We propose a framework for computer-assisted language learning as a pedagogical dialogue game. The goal is to offer personalized learning sentences on-line for each individual learner considering the learner's learning status, in order to strike a balance between more practice on poorly-pronounced units and complete practice on the whole set of pronunciation units. This objective is achieved using a Markov decision process (MDP) trained with reinforcement learning using simulated learners generated from real learner data. Preliminary experimental results on a subset of the example dialogue script show the effectiveness of the framework.",,,,"Graduate Institute of Communication Engineering, National Taiwan University, Taiwan",Computer-Assisted Language Learning;Dialogue Game;Markov Decision Process;Reinforcement Learning,Author2,Author3,a,a,a,a,a,a,Intelligent Tutors,a,,,a,a,"2013 IEEE International Conference on Acoustics, Speech and Signal Processing",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6639266,,P. h. Su; Y. B. Wang; T. h. Yu; L. s. Lee,,6,,10.1109/ICASSP.2013.6639266,IEEE Xplore,1,,20131021,8217,,Educational institutions;Games;Hidden Markov models;Learning (artificial intelligence);Markov processes;Speech;Training,computer aided instruction;computer games;learning (artificial intelligence);natural languages;speech recognition,Markov decision process;computer assisted language learning;dialogue game framework;pedagogical dialogue game;personalized sentence learning;personalized training;reinforcement learning;simulated learner,,1520-6149;15206149,,26-31 May 2013,,,,,,,,,,,,IEEE,35,,Electronic:978-1-4799-0356-6; POD:978-1-4799-0357-3; USB:978-1-4799-0355-9,,8213,IEEE Conferences,,,,,2013,2013,0,3,1,Author2
14,1,1,1,1,Smartphone Interruptibility Using Density-Weighted Uncertainty Sampling with Reinforcement Learning,"We present the In-Context application for smart-phones, which combines signal processing, active learning, and reinforcement learning to autonomously create a personalized model of interruptibility for incoming phone calls. We empirically evaluate the system, and show that we can obtain an average of 96.12% classification accuracy when predicting interruptibility after a week of training. In contrast to previous work, we leverage density-weighted uncertainty sampling combined with a reinforcement learning framework applied to passively collected data to achieve comparable or superior classification accuracy using many fewer queries issued to the user.","We present the In-Context application for smart-phones, which combines signal processing, active learning, and reinforcement learning to autonomously create a personalized model of interruptibility for incoming phone calls. We empirically evaluate the system, and show that we can obtain an average of 96.12% classification accuracy when predicting interruptibility after a week of training. In contrast to previous work, we leverage density-weighted uncertainty sampling combined with a reinforcement learning framework applied to passively collected data to achieve comparable or superior classification accuracy using many fewer queries issued to the user.",,,,"Machine Learning Dept., Carnegie Mellon Univ., Pittsburgh, PA, USA",Active learning;interruptibility;mobile devices;reinforcement learning,Author4,Author1,a,a,a,a,a,a,Personal Assistants,,,,a,a,2011 10th International Conference on Machine Learning and Applications and Workshops,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147012,,R. Fisher; R. Simmons,,4,,10.1109/ICMLA.2011.128,IEEE Xplore,1,,20120209,441,,Accuracy;Context;Data mining;Feature extraction;Support vector machines;Switches;Uncertainty,learning (artificial intelligence);smart phones,active learning;density weighted uncertainty sampling;reinforcement learning;signal processing;smartphone interruptibility,,,,18-21 Dec. 2011,,,,,,,,,,,,IEEE,20,,Electronic:978-0-7695-4607-0; POD:978-1-4577-2134-2,,436,IEEE Conferences,,,1,,2011,2011,0,4,0,Author2
15,2,1,1,1,A Unified Contextual Bandit Framework for Long- and Short-Term Recommendations,"We present a unified contextual bandit framework for recommendation problems that is able to capture long- and short-term interests of users. The model is devised in dual space and the derivation is consequentially carried out using Fenchel-Legrende conjugates and thus leverages to a wide range of tasks and settings. We detail two instantiations for regression and classification scenarios and obtain well-known algorithms for these special cases. The resulting general and unified framework allows for quickly adapting contextual bandits to different applications at-hand. The empirical study demonstrates that the proposed long- and short-term framework outperforms both, short-term and long-term models on data. Moreover, a tweak of the combined model proves beneficial in cold start problems. © 2017, Springer International Publishing AG.","We present a unified contextual bandit framework for recommendation problems that is able to capture long- and short-term interests of users. The model is devised in dual space and the derivation is consequentially carried out using Fenchel-Legrende conjugates and thus leverages to a wide range of tasks and settings. We detail two instantiations for regression and classification scenarios and obtain well-known algorithms for these special cases. The resulting general and unified framework allows for quickly adapting contextual bandits to different applications at-hand. The empirical study demonstrates that the proposed long- and short-term framework outperforms both, short-term and long-term models on data. Moreover, a tweak of the combined model proves beneficial in cold start problems. © 2017, Springer International Publishing AG.",,"Leuphana Universität Lüneburg, Lüneburg, Germany; Technische Universität Darmstadt, Darmstadt, Germany",,,Contextual bandits; Dual optimization; Personalization; Recommendation,Author1,Author2,a,a,a,a,a,a,Recommender Systems,,,CB used for learning LSTM interests of users. ,a,a,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040222425&doi=10.1007%2f978-3-319-71246-8_17&partnerID=40&md5=e4d3e682219cd10a53886c2234a9f5d5,"Tavakol M., Brefeld U.","Tavakol, M., Leuphana Universität Lüneburg, Lüneburg, Germany, Technische Universität Darmstadt, Darmstadt, Germany; Brefeld, U., Leuphana Universität Lüneburg, Lüneburg, Germany",,,10.1007/978-3-319-71246-8_17,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85040222425,,,,,284,269,,,,,Scopus,,,,Conference Paper,,,10535 LNAI,,2017,2017,0,3,1,Author1
16,2,2,0,2,Improving management of Anemia in End Stage Renal Disease using Reinforcement Learning,We present a reinforcement learning approach to elicit individualized dose adjustment policies for patients suffering anemia due to end stage renal disease. Our goal is to achieve stable steady-state anemia management in patients with exhibiting different levels of treatment response. The approach uses Q-learning with parsimonious parametric representation of the state-action value function. We show that this approach achieves stability even in highly responsive patients.,We present a reinforcement learning approach to elicit individualized dose adjustment policies for patients suffering anemia due to end stage renal disease. Our goal is to achieve stable steady-state anemia management in patients with exhibiting different levels of treatment response. The approach uses Q-learning with parsimonious parametric representation of the state-action value function. We show that this approach achieves stability even in highly responsive patients.,,,,"Department of Medicine, Division of Nephrology, University of Louisville, USA",,Author3,Author4,a,a,a,a,a,a,Healthcare,a,,,a,a,2009 International Joint Conference on Neural Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5179004,,A. E. Gaweda,,1,,10.1109/IJCNN.2009.5179004,IEEE Xplore,1,,20090731,958,,Automatic control;Cardiac disease;Conference management;Function approximation;Humans;Learning;Medical treatment;Neural networks;Protocols;Steady-state,diseases;kidney;learning (artificial intelligence);medical computing;patient treatment,Q-learning;end stage renal disease;individualized dose adjustment policy;parsimonious parametric representation;patient treatment;reinforcement learning;stable steady-state anemia management;state-action value function,,2161-4393;21614393,,14-19 June 2009,,,,,,,,,,,,IEEE,14,,POD:978-1-4244-3548-7,,953,IEEE Conferences,,,,,2009,2009,1,0,3,Author4
17,0,1,0,0,User modeling with limited data: Application to stakeholder-driven watershed design,"We have developed a web-based, interactive, watershed planning system called WRESTORE (Watershed Restoration Using Spatio-Temporal Optimization of Resources) (http://wrestore.iupui.edu) that allows stake-holder communities to participate in a democratic, collaborative form of optimization process for designing best management practices (BMPs) on their landscape, while also optimizing based on subjective, qualitative landowners' criteria beyond the usual socio-economic, physical, and ecological criteria. This system utilizes multiple advanced computational approaches including the SWAT (Soil and Water Assessment Tool) hydrologic model for watershed simulations, interactive genetic algorithms and reinforcement-based machine learning algorithms for search and optimization, and deep learning artificial neural networks for user modeling, within an encompassing human-computer interaction framework. A substantial user study of the WRESTORE system was conducted recently involving multiple real stakeholders varying from consultants, government officials, watershed alliance members, etc., with the objective of gaining insight about WRESTORE'S usability and utility. In particular focus was the user modeling component that develops a computational model of a user's preferences and criteria, based on real-time user-provided ratings for a subset of possible designs (similar to the idea of user profiling commonly done for human-computer interaction systems). The user model constructed based on the real user's personalized feedbacks can then be used to influence the automated search and optimization for BMP alternatives in WRESTORE. In this paper, we describe the methods developed for user modeling for interactive optimization, and the experimental set-up as well as results with real user studies. These results clearly demonstrate that development of user models for such personalized, interactive optimization is both feasible and valuable for developing community-based computa- ional water sustainability solutions.","We have developed a web-based, interactive, watershed planning system called WRESTORE (Watershed Restoration Using Spatio-Temporal Optimization of Resources) (http://wrestore.iupui.edu) that allows stake-holder communities to participate in a democratic, collaborative form of optimization process for designing best management practices (BMPs) on their landscape, while also optimizing based on subjective, qualitative landowners' criteria beyond the usual socio-economic, physical, and ecological criteria. This system utilizes multiple advanced computational approaches including the SWAT (Soil and Water Assessment Tool) hydrologic model for watershed simulations, interactive genetic algorithms and reinforcement-based machine learning algorithms for search and optimization, and deep learning artificial neural networks for user modeling, within an encompassing human-computer interaction framework. A substantial user study of the WRESTORE system was conducted recently involving multiple real stakeholders varying from consultants, government officials, watershed alliance members, etc., with the objective of gaining insight about WRESTORE'S usability and utility. In particular focus was the user modeling component that develops a computational model of a user's preferences and criteria, based on real-time user-provided ratings for a subset of possible designs (similar to the idea of user profiling commonly done for human-computer interaction systems). The user model constructed based on the real user's personalized feedbacks can then be used to influence the automated search and optimization for BMP alternatives in WRESTORE. In this paper, we describe the methods developed for user modeling for interactive optimization, and the experimental set-up as well as results with real user studies. These results clearly demonstrate that development of user models for such personalized, interactive optimization is both feasible and valuable for developing community-based computa- ional water sustainability solutions.",,,,"Computer & Information Science, Indiana University Purdue University Indianapolis, USA",decision support system;interactive optimization;machine learning;sustainability design;user modeling,Author2,Author3,a,a,a,a,a,a,Recommender Systems,a,,,a,a,"2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6974532,,S. Mukhopadhyay; V. B. Singh; M. Babbar-Sebens,,0,,10.1109/SMC.2014.6974532,IEEE Xplore,1,,20141204,3860,,Adaptation models;Artificial neural networks;Computational modeling;Data models;Mathematical model;Optimization,Internet;environmental science computing;human computer interaction;hydrology;interactive systems;learning (artificial intelligence);neural nets;socio-economic effects;water resources,BMP;SWAT hydrologic model;WRESTORE;Web-based interactive watershed planning system;best management practices;community-based computational water sustainability solutions;deep learning artificial neural networks;ecological criteria;human-computer interaction framework;interactive genetic algorithms;limited data;physical criteria;reinforcement-based machine learning algorithms;socio-economic criteria;soil and water assessment tool;stakeholder-driven watershed design;user modeling;watershed restoration using spatio-temporal optimization of resources,,1062-922X;1062922X,,5-8 Oct. 2014,,,,,,,,,,,,IEEE,21,,Electronic:978-1-4799-3840-7; POD:978-1-4799-3841-4; USB:978-1-4799-3839-1,,3855,IEEE Conferences,,,,,2014,2014,3,1,0,Author2
18,2,2,0,2,Reinforcement learning design for cancer clinical trials,"We develop reinforcement learning trials for discovering individualized treatment regimens for lifethreatening diseases such as cancer. A temporal-difference learning method called Q-learning is utilized that involves learning an optimal policy from a single training set of finite longitudinal patient trajectories. Approximating the Q-function with time-indexed parameters can be achieved by using support vector regression or extremely randomized trees. Within this framework, we demonstrate that the procedure can extract optimal strategies directly from clinical data without relying on the identification of any accurate mathematical models, unlike approaches based on adaptive design. We show that reinforcement learning has tremendous potential in clinical research because it can select actions that improve outcomes by taking into account delayed effects even when the relationship between actions and outcomes is not fully known. To support our claims, the methodology's practical utility is illustrated in a simulation analysis. In the immediate future, we will apply this general strategy to studying and identifying new treatments for advanced metastatic stage IIIB/IV non-small cell lung cancer, which usually includes multiple lines of chemotherapy treatment. Moreover, there is significant potential of the proposed methodology for developing personalized treatment strategies in other cancers, in cystic fibrosis, and in other life-threatening diseases. Copyright © 2009 John Wiley & Sons, Ltd.","We develop reinforcement learning trials for discovering individualized treatment regimens for lifethreatening diseases such as cancer. A temporal-difference learning method called Q-learning is utilized that involves learning an optimal policy from a single training set of finite longitudinal patient trajectories. Approximating the Q-function with time-indexed parameters can be achieved by using support vector regression or extremely randomized trees. Within this framework, we demonstrate that the procedure can extract optimal strategies directly from clinical data without relying on the identification of any accurate mathematical models, unlike approaches based on adaptive design. We show that reinforcement learning has tremendous potential in clinical research because it can select actions that improve outcomes by taking into account delayed effects even when the relationship between actions and outcomes is not fully known. To support our claims, the methodology's practical utility is illustrated in a simulation analysis. In the immediate future, we will apply this general strategy to studying and identifying new treatments for advanced metastatic stage IIIB/IV non-small cell lung cancer, which usually includes multiple lines of chemotherapy treatment. Moreover, there is significant potential of the proposed methodology for developing personalized treatment strategies in other cancers, in cystic fibrosis, and in other life-threatening diseases. Copyright © 2009 John Wiley & Sons, Ltd.",,"Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States",,,Adaptive design; Clinical trials; Dynamic treatment regime; Extremely randomized trees; Multistage decision problems; Non-small cell lung cancer; Optimal policy; Reinforcement learning; Support vector regression,Author1,Author2,a,a,a,a,a,a,Healthcare,,,,a,a,Statistics in Medicine,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449449564&doi=10.1002%2fsim.3720&partnerID=40&md5=6d68df31db7c4ca684184e8a88243c93,"Zhao Y., Kosorok M.R., Zeng D.","Zhao, Y., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States; Kosorok, M.R., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States; Zeng, D., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States",68,,10.1002/sim.3720,SCOPUS,1,,,,,,,,,,26,,,,2-s2.0-70449449564,,,,,3315,3294,,,,,Scopus,,,,Article,,,28,,2009,2009,1,0,3,Author2
19,2,0,0,0,Q-learning with censored data,"We develop methodology for a multistage decision problem with flexible number of stages in which the rewards are survival times that are subject to censoring. We present a novel Q-learning algorithm that is adjusted for censored data and allows a flexible number of stages. We provide finite sample bounds on the generalization error of the policy learned by the algorithm, and show that when the optimal Q-function belongs to the approximation space, the expected survival time for policies obtained by the algorithm converges to that of the optimal policy. We simulate a multistage clinical trial with flexible number of stages and apply the proposed censored-Q-learning algorithm to find individualized treatment regimens. The methodology presented in this paper has implications in the design of personalized medicine trials in cancer and in other life-threatening diseases. © Institute of Mathematical Statistics, 2012.","We develop methodology for a multistage decision problem with flexible number of stages in which the rewards are survival times that are subject to censoring. We present a novel Q-learning algorithm that is adjusted for censored data and allows a flexible number of stages. We provide finite sample bounds on the generalization error of the policy learned by the algorithm, and show that when the optimal Q-function belongs to the approximation space, the expected survival time for policies obtained by the algorithm converges to that of the optimal policy. We simulate a multistage clinical trial with flexible number of stages and apply the proposed censored-Q-learning algorithm to find individualized treatment regimens. The methodology presented in this paper has implications in the design of personalized medicine trials in cancer and in other life-threatening diseases. © Institute of Mathematical Statistics, 2012.",,"Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States",,,Generalization error; Q-learning; Reinforcement learning; Survival analysis,Author2,Author3,a,a,a,a,a,a,Healthcare,a,,,a,a,Annals of Statistics,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861349230&doi=10.1214%2f12-AOS968&partnerID=40&md5=a0bad44aa6961dc76c7819dc739d8b8f,"Goldberg Y., Kosorok M.R.","Goldberg, Y., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States; Kosorok, M.R., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States",24,,10.1214/12-AOS968,SCOPUS,1,,,,,,,,,,1,,,,2-s2.0-84861349230,,,,,560,529,,,,,Scopus,,,,Article,,,40,,2012,2012,3,0,1,Author1
20,2,1,2,2,A scalable approach for periodical personalized recommendations,"We develop a highly scalable and effective contextual bandit approach towards periodical personalized recommendations. The online bootstrapping-based technique provides a princi- pled way for UCB-type exploitation-exploration algorithms, while being able to handle arbitrary sized datasets, well suited to learn the ever evolving user preference drift from streaming data, and essentially parameter-free. We further introduce techniques to handle arbitrary sized feature spaces using feature hashing, leverage existing state-of-art machine learning via learning reduction, and increase cache hits by managing bootstrapped models in memory effectively. The resulted model trains on millions of examples and billions of features within minutes on a single personal computer. It shows persistent performance in both offline and online eval- uation. We observe around 10% click through rate (CTR) and conversion lift over a collaborative filtering approach in real-world A/B testing across more than 40 million users on the major Ticketmaster email recommendation product. © 2016 ACM.","We develop a highly scalable and effective contextual bandit approach towards periodical personalized recommendations. The online bootstrapping-based technique provides a princi- pled way for UCB-type exploitation-exploration algorithms, while being able to handle arbitrary sized datasets, well suited to learn the ever evolving user preference drift from streaming data, and essentially parameter-free. We further introduce techniques to handle arbitrary sized feature spaces using feature hashing, leverage existing state-of-art machine learning via learning reduction, and increase cache hits by managing bootstrapped models in memory effectively. The resulted model trains on millions of examples and billions of features within minutes on a single personal computer. It shows persistent performance in both offline and online eval- uation. We observe around 10% click through rate (CTR) and conversion lift over a collaborative filtering approach in real-world A/B testing across more than 40 million users on the major Ticketmaster email recommendation product. © 2016 ACM.",,"Ticketmaster, Hollywood, CA, United States",,,Contextual Bandits; Learning Reductions; Online Learning; Personalization; Recommender Systems; Scalability,Author3,Author4,a,a,a,a,a,a,Recommender Systems,a,,,a,a,RecSys 2016 - Proceedings of the 10th ACM Conference on Recommender Systems,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991205405&doi=10.1145%2f2959100.2959139&partnerID=40&md5=c2af9c4b44d2b019cd0dabd2f2537ed5,"Qin Z., Rishabh I., Carnahan J.","Qin, Z., Ticketmaster, Hollywood, CA, United States; Rishabh, I., Ticketmaster, Hollywood, CA, United States; Carnahan, J., Ticketmaster, Hollywood, CA, United States",,,10.1145/2959100.2959139,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84991205405,,,,,26,23,,,,,Scopus,,,,Conference Paper,,,,,2016,2016,0,1,3,Author4
21,2,2,0,2,"Towards efficient, personalized anesthesia using continuous reinforcement learning for propofol infusion control","We demonstrate the use of reinforcement learning algorithms for efficient and personalized control of patients' depth of general anesthesia during surgical procedures - an important aspect for Neurotechnology. We used the continuous actor-critic learning automaton technique, which was trained and tested in silico using published patient data, physiological simulation and the bispectral index (BIS) of patient EEG. Our two-stage technique learns first a generic effective control strategy based on average patient data (factory stage) and can then fine-tune itself to individual patients (personalization stage). The results showed that the reinforcement learner as compared to a bang-bang controller reduced the dose of the anesthetic agent administered by 9.4% and kept the patient closer to the target state, as measured by RMSE (4.90 compared to 8.47). It also kept the BIS error within a narrow, clinically acceptable range 93.9% of the time. Moreover, the policy was trained using only 50 simulated operations. Being able to learn a control strategy this quickly indicates that the reinforcement learner could also adapt regularly to a patient's changing responses throughout a live operation and facilitate the task of anesthesiologists by prompting them with recommended actions.","We demonstrate the use of reinforcement learning algorithms for efficient and personalized control of patients' depth of general anesthesia during surgical procedures - an important aspect for Neurotechnology. We used the continuous actor-critic learning automaton technique, which was trained and tested in silico using published patient data, physiological simulation and the bispectral index (BIS) of patient EEG. Our two-stage technique learns first a generic effective control strategy based on average patient data (factory stage) and can then fine-tune itself to individual patients (personalization stage). The results showed that the reinforcement learner as compared to a bang-bang controller reduced the dose of the anesthetic agent administered by 9.4% and kept the patient closer to the target state, as measured by RMSE (4.90 compared to 8.47). It also kept the BIS error within a narrow, clinically acceptable range 93.9% of the time. Moreover, the policy was trained using only 50 simulated operations. Being able to learn a control strategy this quickly indicates that the reinforcement learner could also adapt regularly to a patient's changing responses throughout a live operation and facilitate the task of anesthesiologists by prompting them with recommended actions.",,,,"Dept. of Comput., Imperial Coll. London, London, UK",,Author4,Author1,a,a,a,a,a,a,Healthcare,,,,a,a,2013 6th International IEEE/EMBS Conference on Neural Engineering (NER),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6696208,,C. Lowery; A. A. Faisal,,1,,10.1109/NER.2013.6696208,IEEE Xplore,1,,20140102,1417,,Algorithm design and analysis;Anesthesia;Brain modeling;Indexes;Learning (artificial intelligence);Monitoring;Surgery,bang-bang control;drug delivery systems;drugs;electroencephalography;learning (artificial intelligence);medical control systems;neurophysiology;surgery,BIS error;EEG;RMSE;anesthesiology;anesthetic agent dose reduction;bang-bang controller;bispectral index;continuous actor-critic learning automaton technique;continuous reinforcement learning algorithm;control fine tuning;depth of general anesthesia control;efficient personalized anesthesia control;generic effective control strategy learning;neurotechnology;physiological simulation;propofol infusion control;surgical procedure,,1948-3546;19483546,,6-8 Nov. 2013,,,,,,,,,,,1,IEEE,20,,CD-ROM:978-1-4673-1968-3; Electronic:978-1-4673-1969-0; POD:978-1-4673-1967-6,,1414,IEEE Conferences,,,,,2013,2013,1,0,3,Author2
22,0,0,0,0,Learning capabilities for improving automatic transmission control,"We analyzed the gear-box position selection (GPS) problem on automatic transmission (AT) and proposed an algorithm, based on learning control, to improve vehicle behavior and driver satisfaction. Our approach guarantees optimization of vehicle performance and adaptation to the driver's style with road condition sensitivity. This improvement has been achieved by combining three knowledge acquisition sources: embedded dynamic models of powertrain, inductive inspection of driver actions and AT designer expertise; and by adding learning capabilities in order to significantly increase the system autonomy. Technically, GPS raises the following four problems which this paper addresses: (1) To achieve vehicle performance optimization of multiple antagonistic criteria, locally and globally over time, we considered a parametric disciminant function depending on an evaluation of the driver satisfaction and so called driver-style-state functions, as a reward for the system, and applied a reinforcement learning algorithm, derived from Q-learning method and combined with a mechanism to escape local optima. (2) Learning directly from the driver is performed when he selects AT ratio in manual mode. (3) Each driver's personal style is represented by a Glass creation/selection mechanism. (4) GPS raises a few singularities which are addressed by a set of restriction rules derived from AT control expertise.","We analyzed the gear-box position selection (GPS) problem on automatic transmission (AT) and proposed an algorithm, based on learning control, to improve vehicle behavior and driver satisfaction. Our approach guarantees optimization of vehicle performance and adaptation to the driver's style with road condition sensitivity. This improvement has been achieved by combining three knowledge acquisition sources: embedded dynamic models of powertrain, inductive inspection of driver actions and AT designer expertise; and by adding learning capabilities in order to significantly increase the system autonomy. Technically, GPS raises the following four problems which this paper addresses: (1) To achieve vehicle performance optimization of multiple antagonistic criteria, locally and globally over time, we considered a parametric disciminant function depending on an evaluation of the driver satisfaction and so called driver-style-state functions, as a reward for the system, and applied a reinforcement learning algorithm, derived from Q-learning method and combined with a mechanism to escape local optima. (2) Learning directly from the driver is performed when he selects AT ratio in manual mode. (3) Each driver's personal style is represented by a Glass creation/selection mechanism. (4) GPS raises a few singularities which are addressed by a set of restriction rules derived from AT control expertise.",,,,"Dept. of Comput. Sci., Stanford Univ., CA, USA",,Author1,Author2,a,a,a,a,a,a,"Other (please in ""remark"")",,Robotics,,a,a,"Intelligent Vehicles '94 Symposium, Proceedings of the",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=639561,,L. Fournier,,1,,10.1109/IVS.1994.639561,IEEE Xplore,1,,20020806,460,,Algorithm design and analysis;Automatic control;Global Positioning System;Inspection;Knowledge acquisition;Mechanical power transmission;Power system modeling;Road vehicles;Vehicle driving;Vehicle dynamics,intelligent control;learning (artificial intelligence);learning systems;road vehicles,Glass creation/selection mechanism;Q-learning method;automatic transmission control;driver satisfaction;driver-style-state functions;embedded dynamic models;gear-box position selection;inductive inspection;knowledge acquisition;learning control;multiple antagonistic criteria;parametric disciminant function;performance optimization;powertrain;reinforcement learning algorithm;road condition sensitivity;singularities;vehicle behavior,,,,24-26 Oct. 1994,,,,,,,,,,,1,IEEE,5,,POD:0-7803-2135-9,,455,IEEE Conferences,,,,,1994,1994,4,0,0,Author1
23,1,0,1,1,Automatic ad format selection via contextual bandits,"Visual design plays an important role in online display advertising: changing the layout of an online ad can increase or decrease its effectiveness, measured in terms of click-through rate (CTR) or total revenue. The decision of which layout to use for an ad involves a trade-off: using a layout provides feedback about its effectiveness (exploration), but collecting that feedback requires sacrificing the immediate reward of using a layout we already know is effective (exploitation). To balance exploration with exploitation, we pose automatic layout selection as a contextual bandit problem. There are many bandit algorithms, each generating a policy which must be evaluated. It is impractical to test each policy on live traffic. However, we have found that offline replay (a.k.a. exploration scavenging) can be adapted to provide an accurate estimator for the performance of ad layout policies at Linkedin, using only historical data about the effectiveness of layouts. We describe the development of our offline replayer, and benchmark a number of common bandit algorithms. Copyright 2013 ACM.","Visual design plays an important role in online display advertising: changing the layout of an online ad can increase or decrease its effectiveness, measured in terms of click-through rate (CTR) or total revenue. The decision of which layout to use for an ad involves a trade-off: using a layout provides feedback about its effectiveness (exploration), but collecting that feedback requires sacrificing the immediate reward of using a layout we already know is effective (exploitation). To balance exploration with exploitation, we pose automatic layout selection as a contextual bandit problem. There are many bandit algorithms, each generating a policy which must be evaluated. It is impractical to test each policy on live traffic. However, we have found that offline replay (a.k.a. exploration scavenging) can be adapted to provide an accurate estimator for the performance of ad layout policies at Linkedin, using only historical data about the effectiveness of layouts. We describe the development of our offline replayer, and benchmark a number of common bandit algorithms. Copyright 2013 ACM.",,"School of Computer Science, Florida International Univ., 11200 S.W. 8th St., Miami, FL 33199, United States; Applied Relevance Science LinkedIn, 2029 Stierlin Ct., Mountain View, CA 94043, United States",,,Bandit algorithms; Exploration/exploitation; Layout; Offline evaluation; Online advertising; Personalization; Recommender systems,Author2,Author3,a,a,a,a,a,a,Recommender Systems,a,,,a,a,"International Conference on Information and Knowledge Management, Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889567137&doi=10.1145%2f2505515.2514700&partnerID=40&md5=652c81fbee7adda7e08bcedb57239955,"Tang L., Rosales R., Singh A.P., Agarwal D.","Tang, L., School of Computer Science, Florida International Univ., 11200 S.W. 8th St., Miami, FL 33199, United States; Rosales, R., Applied Relevance Science LinkedIn, 2029 Stierlin Ct., Mountain View, CA 94043, United States; Singh, A.P., Applied Relevance Science LinkedIn, 2029 Stierlin Ct., Mountain View, CA 94043, United States; Agarwal, D., School of Computer Science, Florida International Univ., 11200 S.W. 8th St., Miami, FL 33199, United States",20,,10.1145/2505515.2514700,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84889567137,,,,,1594,1587,,,,,Scopus,,,,Conference Paper,,,,,2013,2013,1,3,0,Author4
24,2,2,0,2,Patient tailored virtual rehabilitation,"Virtual rehabilitation should be adaptable to the patient need and progress. To do so, patient in-game performace and ability are monitored to maintain an adequate level of challenge. A novel adaptation strategy is proposed by which patient control and speed are dynamically interrogated to adjust the game difficulty. The strategy is based on a Markov decision process seeding a therapist-guided reinforcement learning algorithm. The optimal learning scheme for the algorithm is established (α = 0.5). Convergence to an optimal therapeutic plan is demonstrated for patients with non-deterministic behaviour. The proposed adaptation algorithm can enhance existing virtual reality-based motor rehabilitation platforms by tailoring the games response to the patient changing needs. © 2013, Springer-Verlag Berlin Heidelberg.","Virtual rehabilitation should be adaptable to the patient need and progress. To do so, patient in-game performace and ability are monitored to maintain an adequate level of challenge. A novel adaptation strategy is proposed by which patient control and speed are dynamically interrogated to adjust the game difficulty. The strategy is based on a Markov decision process seeding a therapist-guided reinforcement learning algorithm. The optimal learning scheme for the algorithm is established (α = 0.5). Convergence to an optimal therapeutic plan is demonstrated for patients with non-deterministic behaviour. The proposed adaptation algorithm can enhance existing virtual reality-based motor rehabilitation platforms by tailoring the games response to the patient changing needs. © 2013, Springer-Verlag Berlin Heidelberg.",,"Optics and Electronics, National Institute for Astrophysics, Puebla, Mexico",,,,Author3,Author4,a,a,a,a,a,a,Healthcare,a,"Although authors describe a 'game', the goal of the game seems for patients to rehabilitate. This would make Healthcare the main domain in my view.","Yep, I would also call this healthcare.",a,a,Biosystems and Biorobotics,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014689131&doi=10.1007%2f978-3-642-34546-3_143&partnerID=40&md5=4843998cb02fe4a40767841dcace8bed,"Ávila-Sansores S., Orihuela-Espina F., Enrique-Sucar L.","Ávila-Sansores, S., Optics and Electronics, National Institute for Astrophysics, Puebla, Mexico; Orihuela-Espina, F., Optics and Electronics, National Institute for Astrophysics, Puebla, Mexico; Enrique-Sucar, L., Optics and Electronics, National Institute for Astrophysics, Puebla, Mexico",4,,10.1007/978-3-642-34546-3_143,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85014689131,,,,,883,879,,,,,Scopus,,,,Book Chapter,,,1,,2013,2013,1,0,3,Author4
25,0,1,1,1,Satisfaction based Q-learning for integrated lighting and blind control,"Various lighting and blind control methods have been presented to improve user comfort and reduce energy consumption simultaneously. However, there are opportunities to improve control performances by introducing more recent information and machine learning technologies which allow more comprehensive consideration of the balance between user comfort and system energy consumption. To be more specific, in terms of user comfort, unified set-point may not be desirable since different people may have different comfort preferences. In terms of energy consumption, the excessive cooling load of HVAC system should be considered in summer when utilizing solar incidence to reduce the lighting electricity consumption. The setting of the blind slat angle still has great room to improve instead of the cut-off angle. Moreover, users' demands are not fully met, so sometimes they still want to override the automated control. Thus, a closed-loop satisfaction based system is developed in this paper, specifically we introduce an improved reinforcement learning controller to obtain an optimal control strategy of blinds and lights. It could provide a personalized service via introducing subjects perceptions of surroundings gathered by a novel interface as the feedback signal. The proposed system was implemented on a practical test-bed in an energy-efficient building. Compared with the traditional control, it can provide a more acceptable and energy-efficient luminous environment. © 2016 Elsevier B.V.","Various lighting and blind control methods have been presented to improve user comfort and reduce energy consumption simultaneously. However, there are opportunities to improve control performances by introducing more recent information and machine learning technologies which allow more comprehensive consideration of the balance between user comfort and system energy consumption. To be more specific, in terms of user comfort, unified set-point may not be desirable since different people may have different comfort preferences. In terms of energy consumption, the excessive cooling load of HVAC system should be considered in summer when utilizing solar incidence to reduce the lighting electricity consumption. The setting of the blind slat angle still has great room to improve instead of the cut-off angle. Moreover, users' demands are not fully met, so sometimes they still want to override the automated control. Thus, a closed-loop satisfaction based system is developed in this paper, specifically we introduce an improved reinforcement learning controller to obtain an optimal control strategy of blinds and lights. It could provide a personalized service via introducing subjects perceptions of surroundings gathered by a novel interface as the feedback signal. The proposed system was implemented on a practical test-bed in an energy-efficient building. Compared with the traditional control, it can provide a more acceptable and energy-efficient luminous environment. © 2016 Elsevier B.V.",,"Center for Intelligent and Networked System, Department of Automation and TNList, Tsinghua University, Beijing, China; Department of Building Science, Tsinghua University, Beijing, China; United Technologies Research Center (China) Ltd., Shanghai, China",,,Blinds; Day-light; Energy saving; Integrated control; Q-learning,Author4,Author1,a,a,a,a,a,a,Personal Assistants,,,,a,a,Energy and Buildings,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971643867&doi=10.1016%2fj.enbuild.2016.05.067&partnerID=40&md5=57f6a8db85f9f0b6c01f3c0021a579e0,"Cheng Z., Zhao Q., Wang F., Jiang Y., Xia L., Ding J.","Cheng, Z., Center for Intelligent and Networked System, Department of Automation and TNList, Tsinghua University, Beijing, China; Zhao, Q., Center for Intelligent and Networked System, Department of Automation and TNList, Tsinghua University, Beijing, China; Wang, F., Department of Building Science, Tsinghua University, Beijing, China; Jiang, Y., Department of Building Science, Tsinghua University, Beijing, China; Xia, L., Center for Intelligent and Networked System, Department of Automation and TNList, Tsinghua University, Beijing, China; Ding, J., United Technologies Research Center (China) Ltd., Shanghai, China",7,,10.1016/j.enbuild.2016.05.067,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84971643867,,,,,55,43,,,,,Scopus,,,,Article,,,127,,2016,2016,1,3,0,Author4
26,0,0,0,0,A task-oriented service personalization scheme for smart environments using reinforcement learning,"Users want IoT environments to provide them with personalized support. These environments therefore need to be able to learn user preferences, such as what temperature the room should be or which lights should be turned on. We propose a novel system that separates the tasks of learning a user's preferences and realizing them within the environment. The system is able to capture user preferences by reinterpreting the problem as an optimization problem and applying inverse reinforcement learning to it. The system is shown to be able to accurately extract simple preferences given a small number of user demonstrations. These preferences are then realized by actuates devices running reinforcement learning-based agents to provide an environment consistent with the learnt preferences, also in situations not included in any user demonstration.","Users want IoT environments to provide them with personalized support. These environments therefore need to be able to learn user preferences, such as what temperature the room should be or which lights should be turned on. We propose a novel system that separates the tasks of learning a user's preferences and realizing them within the environment. The system is able to capture user preferences by reinterpreting the problem as an optimization problem and applying inverse reinforcement learning to it. The system is shown to be able to accurately extract simple preferences given a small number of user demonstrations. These preferences are then realized by actuates devices running reinforcement learning-based agents to provide an environment consistent with the learnt preferences, also in situations not included in any user demonstration.",,,,"School of Computer Science, KAIST, Daejeon, S. Korea",,Author2,Author3,a,a,a,a,a,a,Personal Assistants,a,,,a,a,2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457110,,B. Tegelund; H. Son; D. Lee,,2,,10.1109/PERCOMW.2016.7457110,IEEE Xplore,1,,20160421,6,,Brightness;Context;Context modeling;Learning (artificial intelligence);Motion pictures;Performance evaluation;Sensors,Internet of Things;ambient intelligence;learning (artificial intelligence);multi-agent systems,IoT environments;personalized support;reinforcement learning-based agents;smart environments;task-oriented service personalization scheme;user preference learning,,,,14-18 March 2016,,,,,,,,,,,,IEEE,10,,Electronic:978-1-5090-1941-0; POD:978-1-5090-1942-7,,1,IEEE Conferences,,,,,2016,2016,4,0,0,Author2
27,1,0,2,1,The best privacy defense is a good privacy offense: obfuscating a search engine user’s profile,"User privacy on the internet is an important and unsolved problem. So far, no sufficient and comprehensive solution has been proposed that helps a user to protect his or her privacy while using the internet. Data are collected and assembled by numerous service providers. Solutions so far focused on the side of the service providers to store encrypted or transformed data that can be still used for analysis. This has a major flaw, as it relies on the service providers to do this. The user has no chance of actively protecting his or her privacy. In this work, we suggest a new approach, empowering the user to take advantage of the same tool the other side has, namely data mining to produce data which obfuscates the user’s profile. We apply this approach to search engine queries and use feedback of the search engines in terms of personalized advertisements in an algorithm similar to reinforcement learning to generate new queries potentially confusing the search engine. We evaluated the approach using a real-world data set. While evaluation is hard, we achieve results that indicate that it is possible to influence the user’s profile that the search engine generates. This shows that it is feasible to defend a user’s privacy from a new and more practical perspective. © 2017, The Author(s).","User privacy on the internet is an important and unsolved problem. So far, no sufficient and comprehensive solution has been proposed that helps a user to protect his or her privacy while using the internet. Data are collected and assembled by numerous service providers. Solutions so far focused on the side of the service providers to store encrypted or transformed data that can be still used for analysis. This has a major flaw, as it relies on the service providers to do this. The user has no chance of actively protecting his or her privacy. In this work, we suggest a new approach, empowering the user to take advantage of the same tool the other side has, namely data mining to produce data which obfuscates the user’s profile. We apply this approach to search engine queries and use feedback of the search engines in terms of personalized advertisements in an algorithm similar to reinforcement learning to generate new queries potentially confusing the search engine. We evaluated the approach using a real-world data set. While evaluation is hard, we achieve results that indicate that it is possible to influence the user’s profile that the search engine generates. This shows that it is feasible to defend a user’s privacy from a new and more practical perspective. © 2017, The Author(s).",,"Institute of Computer Science, Johannes Gutenberg University Mainz, Staudingerweg 9, Mainz, Germany",,,Personalized ads; Privacy; Reinforcement learning; Search engines; Web mining,Author3,Author4,u,a,a,a,a,a,other,a,domain: privacy,"They use an algorithm quite similar to reinforcement learning. In my opinion, that means that it is not RL right? :)",u,a,Data Mining and Knowledge Discovery,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025085865&doi=10.1007%2fs10618-017-0524-z&partnerID=40&md5=247afda6050dd89a6a7d18d84d0dc5ee,"Wicker J., Kramer S.","Wicker, J., Institute of Computer Science, Johannes Gutenberg University Mainz, Staudingerweg 9, Mainz, Germany; Kramer, S., Institute of Computer Science, Johannes Gutenberg University Mainz, Staudingerweg 9, Mainz, Germany",,,10.1007/s10618-017-0524-z,SCOPUS,1,,,,,,,,,,5,,,,2-s2.0-85025085865,,,,,1443,1419,,,,,Scopus,,,,Article,,,31,,,2017,1,2,1,Author3
28,0,0,0,0,Data-driven inverse learning of passenger preferences in urban public transits,"Urban public transit planning is crucial in reducing traffic congestion and enabling green transportation. However, there is no systematic way to integrate passengers' personal preferences in planning public transit routes and schedules so as to achieve high occupancy rates and efficiency gain of ride-sharing. In this paper, we take the first step tp exact passengers' preferences in planning from history public transit data. We propose a data-driven method to construct a Markov decision process model that characterizes the process of passengers making sequential public transit choices, in bus routes, subway lines, and transfer stops/stations. Using the model, we integrate softmax policy iteration into maximum entropy inverse reinforcement learning to infer the passenger's reward function from observed trajectory data. The inferred reward function will enable an urban planner to predict passengers' route planning decisions given some proposed transit plans, for example, opening a new bus route or subway line. Finally, we demonstrate the correctness and accuracy of our modeling and inference methods in a large-scale (three months) passenger-level public transit trajectory data from Shenzhen, China. Our method contributes to smart transportation design and human-centric urban planning.","Urban public transit planning is crucial in reducing traffic congestion and enabling green transportation. However, there is no systematic way to integrate passengers' personal preferences in planning public transit routes and schedules so as to achieve high occupancy rates and efficiency gain of ride-sharing. In this paper, we take the first step tp exact passengers' preferences in planning from history public transit data. We propose a data-driven method to construct a Markov decision process model that characterizes the process of passengers making sequential public transit choices, in bus routes, subway lines, and transfer stops/stations. Using the model, we integrate softmax policy iteration into maximum entropy inverse reinforcement learning to infer the passenger's reward function from observed trajectory data. The inferred reward function will enable an urban planner to predict passengers' route planning decisions given some proposed transit plans, for example, opening a new bus route or subway line. Finally, we demonstrate the correctness and accuracy of our modeling and inference methods in a large-scale (three months) passenger-level public transit trajectory data from Shenzhen, China. Our method contributes to smart transportation design and human-centric urban planning.",,,,"Worcester Polytechnic Institute (WPI), 100 Institute Road, Worcester, MA 01609, USA",,Author4,Author1,a,a,a,a,a,a,Personal Assistants,,,,a,a,2017 IEEE 56th Annual Conference on Decision and Control (CDC),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8264410,,G. Wu; Y. Ding; Y. Li; J. Luo; F. Zhang; J. Fu,,,,10.1109/CDC.2017.8264410,IEEE Xplore,1,,20180122,5073,,Data models;Entropy;Markov processes;Planning;Public transportation;Roads;Trajectory,Markov processes;data analysis;entropy;learning (artificial intelligence);town and country planning;traffic engineering computing;transportation,China;Markov decision process model;Shenzhen;bus route;bus routes;data-driven inverse learning;data-driven method;enabling green transportation;green transportation;high occupancy rates;history public transit data;human-centric urban planning;inference methods;inferred reward function;large-scale passenger-level public transit trajectory data;maximum entropy inverse reinforcement;observed trajectory data;public transit routes;sequential public transit choices;smart transportation design;traffic congestion;transit plans;urban planner;urban public transit planning,,,,12-15 Dec. 2017,,,,,,,,,,,,IEEE,,,Electronic:978-1-5090-2873-3; POD:978-1-5090-2874-0; USB:978-1-5090-2872-6,,5068,IEEE Conferences,,,,,2017,2017,4,0,0,Author4
29,2,1,2,2,Reinforcement learning approach towards effective content recommendation in MOOC environments,"Understanding the Learner requirements is an important aspect of any learning environment as it helps to recommend the LOs in a more personalized manner. With the growing demand for MOOCs offered by coursera, edx, etc. the learner information plays a vital role in understanding the extent to which the learners can gain out of such courses. The Learning Management Systems (LMS) across the web uses the explicit (rating, performance, etc.) and implicit feedback (LOs used) obtained through interaction with the learners to derive such information. As the requirements of the learners varies with the individual's interest and learning background, a common approach for recommending LOs may not cater the needs of all the learners. To overcome this issue, this paper proposes reinforcement learning based algorithm to analyze the learner information (derived from both implicit and explicit feedback) and generate the knowledge on the learner's requirements and capabilities inside a specific learning context. The reinforcement learning system (RILS) implemented as a part of this work utilizes the knowledge thus generated in order to recommend the appropriate LOs for the learners. The results have highlighted that the knowledge derived from the learning information analysis proved to effective in generating personalized recommendation policies that can cater the context specific requirements of the learners.","Understanding the Learner requirements is an important aspect of any learning environment as it helps to recommend the LOs in a more personalized manner. With the growing demand for MOOCs offered by coursera, edx, etc. the learner information plays a vital role in understanding the extent to which the learners can gain out of such courses. The Learning Management Systems (LMS) across the web uses the explicit (rating, performance, etc.) and implicit feedback (LOs used) obtained through interaction with the learners to derive such information. As the requirements of the learners varies with the individual's interest and learning background, a common approach for recommending LOs may not cater the needs of all the learners. To overcome this issue, this paper proposes reinforcement learning based algorithm to analyze the learner information (derived from both implicit and explicit feedback) and generate the knowledge on the learner's requirements and capabilities inside a specific learning context. The reinforcement learning system (RILS) implemented as a part of this work utilizes the knowledge thus generated in order to recommend the appropriate LOs for the learners. The results have highlighted that the knowledge derived from the learning information analysis proved to effective in generating personalized recommendation policies that can cater the context specific requirements of the learners.",,,,"SCSE, VIT University, Vellore, Tamilnadu, India",LO recommendation;MOOC;Reinforcement Learning;learning context;learning experience,Author2,Author3,a,a,a,a,a,a,Recommender Systems,a,,,a,a,"2014 IEEE International Conference on MOOC, Innovation and Technology in Education (MITE)",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7020289,,V. R. Raghuveer; B. K. Tripathy; T. Singh; S. Khanna,,1,,10.1109/MITE.2014.7020289,IEEE Xplore,1,,20150126,289,,Collaboration;Conferences;Context;Educational institutions;Electronic learning;Technological innovation,Internet;computer aided instruction;educational courses;human computer interaction;information analysis;learning (artificial intelligence);recommender systems,LMS;MOOC environments;RILS;Web;content recommendation;explicit feedback;generating personalized recommendation policies;implicit feedback;knowledge utilization;learning information analysis;learning management systems;massive open online course;reinforcement learning system,,,,19-20 Dec. 2014,,,,,,,,,,,,IEEE,14,,Electronic:978-1-4799-6876-3; POD:978-1-4799-6877-0,,285,IEEE Conferences,,,,,2014,2014,0,1,3,Author4
30,1,1,2,1,A Service Recommendation Using Reinforcement Learning for Network-based Robots in Ubiquitous Computing Environments,"Ubiquitous robotic companion (URC ) is a new concept for the network-based robot platform which can enable to be following its master wherever or whenever he/she be in order to provide necessary services. The robot platforms in present normally interest in providing services through the direct interaction in responding to the user's demands. On the other hand, URC services are required to be provided by the means of recognizing the circumstances and taking a user's preference into account. In this paper, we propose a service recommendation scheme for URC robots. The proposed service recommendation, developed based on the reinforcement learning, can be used to provide personalized services by learning users' preferences or tasks through the interaction with users. Using simulation for rapid testing, we evaluate of the proposed scheme under a variety of user modeling types and discount factors.","Ubiquitous robotic companion (URC ) is a new concept for the network-based robot platform which can enable to be following its master wherever or whenever he/she be in order to provide necessary services. The robot platforms in present normally interest in providing services through the direct interaction in responding to the user's demands. On the other hand, URC services are required to be provided by the means of recognizing the circumstances and taking a user's preference into account. In this paper, we propose a service recommendation scheme for URC robots. The proposed service recommendation, developed based on the reinforcement learning, can be used to provide personalized services by learning users' preferences or tasks through the interaction with users. Using simulation for rapid testing, we evaluate of the proposed scheme under a variety of user modeling types and discount factors.",,,,"Electronice and Telecommunication Research Institute, 161 Gajeong-dong, Yuseong-gu, Daejeon, 305-350, Korea. Email: akmoon@etri.re.kr",,Author3,Author4,a,a,a,a,a,a,other,a,domain: robotics,Could also say this is a personal assistant (in a physical form),a,a,RO-MAN 2007 - The 16th IEEE International Symposium on Robot and Human Interactive Communication,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4415198,,A. Moon; T. Kang; H. Kim; H. Kim,,2,,10.1109/ROMAN.2007.4415198,IEEE Xplore,1,,20080116,826,,Control systems;Human robot interaction;Intelligent robots;Inventory management;Machine learning;Moon;Robot control;Robot sensing systems;Service robots;Ubiquitous computing,computer networks;control engineering computing;learning (artificial intelligence);man-machine systems;robots;ubiquitous computing;user modelling,human-robot interaction;network-based robot;network-based robot platform;rapid testing;reinforcement learning;service recommendation scheme;ubiquitous computing environment;ubiquitous robotic companion;user modeling,,1944-9445;19449445,,26-29 Aug. 2007,,,,,,,,,,,,IEEE,15,,CD-ROM:978-1-4244-1635-6; POD:978-1-4244-1634-9,,821,IEEE Conferences,,,,,2007,2007,0,3,1,Author3
31,1,1,2,1,Reinforcement learning of context models for a ubiquitous personal assistant,"Ubiquitous environments may become a reality in a foreseeable future and research is aimed on making them more and more adapted and comfortable for users. Our work consists on applying reinforcement learning techniques in order to adapt services provided by a ubiquitous assistant to the user. The learning produces a context model, associating actions to perceived situations of the user. Associations are based on feedback given by the user as a reaction to the behavior of the assistant. Our method brings a solution to some of the problems encountered when applying reinforcement learning to systems where the user is in the loop. For instance, the behavior of the system is completely incoherent at the be-ginning and needs time to converge. The user does not accept to wait that long to train the system. The user's habits may change over time and the assistant needs to integrate these changes quickly. We study methods to accelerate the reinforced learning process. © 2009 Springer-Verlag Berlin Heidelberg.","Ubiquitous environments may become a reality in a foreseeable future and research is aimed on making them more and more adapted and comfortable for users. Our work consists on applying reinforcement learning techniques in order to adapt services provided by a ubiquitous assistant to the user. The learning produces a context model, associating actions to perceived situations of the user. Associations are based on feedback given by the user as a reaction to the behavior of the assistant. Our method brings a solution to some of the problems encountered when applying reinforcement learning to systems where the user is in the loop. For instance, the behavior of the system is completely incoherent at the be-ginning and needs time to converge. The user does not accept to wait that long to train the system. The user's habits may change over time and the assistant needs to integrate these changes quickly. We study methods to accelerate the reinforced learning process. © 2009 Springer-Verlag Berlin Heidelberg.",,"Laboratoire LIG, 681 rue de la Passerelle, St-Martin d'Hères 38402, France",,,,Author4,Author1,a,a,a,a,a,a,Personal Assistants,,Ambient intelligence/ubiquitous computing. Used PA to describe it.,,a,a,Advances in Soft Computing,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149109424&doi=10.1007%2f978-3-540-85867-6_30&partnerID=40&md5=7b74657e0245e5a528b0a4029e063cd7,"Zaidenberg S., Reignier P., Crowley J.L.","Zaidenberg, S., Laboratoire LIG, 681 rue de la Passerelle, St-Martin d'Hères 38402, France; Reignier, P., Laboratoire LIG, 681 rue de la Passerelle, St-Martin d'Hères 38402, France; Crowley, J.L., Laboratoire LIG, 681 rue de la Passerelle, St-Martin d'Hères 38402, France",6,,10.1007/978-3-540-85867-6_30,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-58149109424,,,,,264,254,,,,,Scopus,,,,Conference Paper,,,51,,2009,2009,0,3,1,Author3
32,2,2,0,2,Reinforcement learning strategies for clinical trials in nonsmall cell lung cancer,"Typical regimens for advanced metastatic stage IIIB/IV nonsmall cell lung cancer (NSCLC) consist of multiple lines of treatment. We present an adaptive reinforcement learning approach to discover optimal individualized treatment regimens from a specially designed clinical trial (a ""clinical reinforcement trial"") of an experimental treatment for patients with advanced NSCLC who have not been treated previously with systemic therapy. In addition to the complexity of the problem of selecting optimal compounds for first- and second-line treatments based on prognostic factors, another primary goal is to determine the optimal time to initiate second-line therapy, either immediately or delayed after induction therapy, yielding the longest overall survival time. A reinforcement learning method calledQ-learning is utilized, which involves learning an optimal regimen from patient data generated from the clinical reinforcement trial. Approximating theQ-function with time-indexed parameters can be achieved by using a modification of support vector regression that can utilize censored data. Within this framework, a simulation study shows that the procedure can extract optimal regimens for two lines of treatment directly from clinical data without prior knowledge of the treatment effect mechanism. In addition, we demonstrate that the design reliably selects the best initial time for second-line therapy while taking into account the heterogeneity of NSCLC across patients. © 2011, The International Biometric Society.","Typical regimens for advanced metastatic stage IIIB/IV nonsmall cell lung cancer (NSCLC) consist of multiple lines of treatment. We present an adaptive reinforcement learning approach to discover optimal individualized treatment regimens from a specially designed clinical trial (a ""clinical reinforcement trial"") of an experimental treatment for patients with advanced NSCLC who have not been treated previously with systemic therapy. In addition to the complexity of the problem of selecting optimal compounds for first- and second-line treatments based on prognostic factors, another primary goal is to determine the optimal time to initiate second-line therapy, either immediately or delayed after induction therapy, yielding the longest overall survival time. A reinforcement learning method calledQ-learning is utilized, which involves learning an optimal regimen from patient data generated from the clinical reinforcement trial. Approximating theQ-function with time-indexed parameters can be achieved by using a modification of support vector regression that can utilize censored data. Within this framework, a simulation study shows that the procedure can extract optimal regimens for two lines of treatment directly from clinical data without prior knowledge of the treatment effect mechanism. In addition, we demonstrate that the design reliably selects the best initial time for second-line therapy while taking into account the heterogeneity of NSCLC across patients. © 2011, The International Biometric Society.",,"Global Biostatistics and Epidemiology, Amgen Inc., One Amgen Center Drive, Thousand Oaks, CA 91320, United States; Department of Biostatistics, University of North Carolina at Chapel Hill, 3101 McGavran-Greenberg, CB 7420, Chapel Hill, NC 27599, United States; Department of Medicine, University of North Carolina at Chapel Hill, Physicians Office Building, 170 Manning Drive, Chapel Hill, NC 27599, United States",,,Adaptive design; Dynamic treatment regime; Individualized therapy; Multistage decision problems; Nonsmall cell lung cancer; Personalized medicine; Q-learning; Reinforcement learning; Support vector regression,Author1,Author2,a,a,a,a,a,a,Healthcare,,,,a,a,Biometrics,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-83655181241&doi=10.1111%2fj.1541-0420.2011.01572.x&partnerID=40&md5=64ad4676c2bde9eeed5337ac997d331a,"Zhao Y., Zeng D., Socinski M.A., Kosorok M.R.","Zhao, Y., Global Biostatistics and Epidemiology, Amgen Inc., One Amgen Center Drive, Thousand Oaks, CA 91320, United States; Zeng, D., Department of Biostatistics, University of North Carolina at Chapel Hill, 3101 McGavran-Greenberg, CB 7420, Chapel Hill, NC 27599, United States; Socinski, M.A., Department of Medicine, University of North Carolina at Chapel Hill, Physicians Office Building, 170 Manning Drive, Chapel Hill, NC 27599, United States; Kosorok, M.R., Department of Biostatistics, University of North Carolina at Chapel Hill, 3101 McGavran-Greenberg, CB 7420, Chapel Hill, NC 27599, United States",50,,10.1111/j.1541-0420.2011.01572.x,SCOPUS,1,,,,,,,,,,4,,,,2-s2.0-83655181241,,,,,1433,1422,,,,,Scopus,,,,Article,,,67,,2011,2011,1,0,3,Author2
33,2,0,0,0,Autonomous task sequencing for customized curriculum design in reinforcement learning,"Transfer learning is a method where an agent reuses knowledge learned in a source task to improve learning on a target task. Recent work has shown that transfer learning can be extended to the idea of curriculum learning, where the agent incrementally accumulates knowledge over a sequence of tasks (i.e. a curriculum). In most existing work, such curricula have been constructed manually. Furthermore, they are fixed ahead of time, and do not adapt to the progress or abilities of the agent. In this paper, we formulate the design of a curriculum as a Markov Decision Process, which directly models the accumulation of knowledge as an agent interacts with tasks, and propose a method that approximates an execution of an optimal policy in this MDP to produce an agent-specific curriculum. We use our approach to automatically sequence tasks for 3 agents with varying sensing and action capabilities in an experimental domain, and show that our method produces curricula customized for each agent that improve performance relative to learning from scratch or using a different agent's curriculum.","Transfer learning is a method where an agent reuses knowledge learned in a source task to improve learning on a target task. Recent work has shown that transfer learning can be extended to the idea of curriculum learning, where the agent incrementally accumulates knowledge over a sequence of tasks (i.e. a curriculum). In most existing work, such curricula have been constructed manually. Furthermore, they are fixed ahead of time, and do not adapt to the progress or abilities of the agent. In this paper, we formulate the design of a curriculum as a Markov Decision Process, which directly models the accumulation of knowledge as an agent interacts with tasks, and propose a method that approximates an execution of an optimal policy in this MDP to produce an agent-specific curriculum. We use our approach to automatically sequence tasks for 3 agents with varying sensing and action capabilities in an experimental domain, and show that our method produces curricula customized for each agent that improve performance relative to learning from scratch or using a different agent's curriculum.",,"Department of Computer Science, University of Texas, Austin, United States",,,,Author4,Author1,a,a,a,a,a,a,Personal Assistants,,Curriculum learning....,,a,a,IJCAI International Joint Conference on Artificial Intelligence,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031926206&partnerID=40&md5=999d71abd37b44b4bd9f668a6b94f8b8,"Narvekar S., Sinapov J., Stone P.","Narvekar, S., Department of Computer Science, University of Texas, Austin, United States; Sinapov, J., Department of Computer Science, University of Texas, Austin, United States; Stone, P., Department of Computer Science, University of Texas, Austin, United States",1,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-85031926206,,,,,2542,2536,,,,,Scopus,,,,Conference Paper,,,,,2017,2017,3,0,1,Author1
34,2,0,2,2,Curriculum learning in reinforcement learning,"Transfer learning in reinforcement learning is an area of research that seeks to speed up or improve learning of a complex target task, by leveraging knowledge from one or more source tasks. This thesis will extend the concept of transfer learning to curriculum learning, where the goal is to design a sequence of source tasks for an agent to train on, such that final performance or learning speed is improved. We discuss completed work on this topic, including methods for semi-automatically generating source tasks tailored to an agent and the characteristics of a target domain, and automatically sequencing such tasks into a curriculum. Finally, we also present ideas for future work.","Transfer learning in reinforcement learning is an area of research that seeks to speed up or improve learning of a complex target task, by leveraging knowledge from one or more source tasks. This thesis will extend the concept of transfer learning to curriculum learning, where the goal is to design a sequence of source tasks for an agent to train on, such that final performance or learning speed is improved. We discuss completed work on this topic, including methods for semi-automatically generating source tasks tailored to an agent and the characteristics of a target domain, and automatically sequencing such tasks into a curriculum. Finally, we also present ideas for future work.",,"Department of Computer Science, University of Texas at Austin, United States",,,,Author1,Author2,a,a,a,a,a,a,"Other (please in ""remark"")",,Generic,,a,a,IJCAI International Joint Conference on Artificial Intelligence,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031919399&partnerID=40&md5=bc1c2a4f7029255194de43ecc74eeb0d,Narvekar S.,"Narvekar, S., Department of Computer Science, University of Texas at Austin, United States",,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-85031919399,,,,,5196,5195,,,,,Scopus,,,,Conference Paper,,,,,2017,2017,1,0,3,Author4
35,2,2,2,2,Affective personalization of a social robot tutor for children's second language skills,"Though substantial research has been dedicated towards using technology to improve education, no current methods are as effective as one-on-one tutoring. A critical, though relatively understudied, aspect of effective tutoring is modulating the student's affective state throughout the tutoring session in order to maximize long-term learning gains. We developed an integrated experimental paradigm in which children play a second-language learning game on a tablet, in collaboration with a fully autonomous social robotic learning companion. As part of the system, we measured children's valence and engagement via an automatic facial expression analysis system. These signals were combined into a reward signal that fed into the robot's affective reinforcement learning algorithm. Over several sessions, the robot played the game and personalized its motivational strategies (using verbal and non-verbal actions) to each student. We evaluated this system with 34 children in preschool classrooms for a duration of two months. We saw that (1) children learned new words from the repeated tutoring sessions, (2) the affective policy personalized to students over the duration of the study, and (3) students who interacted with a robot that personalized its affective feedback strategy showed a significant increase in valence, as compared to students who interacted with a nonpersonalizing robot. This integrated system of tablet-based educational content, affective sensing, affective policy learning, and an autonomous social robot holds great promise for a more comprehensive approach to personalized tutoring. © Copyright 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","Though substantial research has been dedicated towards using technology to improve education, no current methods are as effective as one-on-one tutoring. A critical, though relatively understudied, aspect of effective tutoring is modulating the student's affective state throughout the tutoring session in order to maximize long-term learning gains. We developed an integrated experimental paradigm in which children play a second-language learning game on a tablet, in collaboration with a fully autonomous social robotic learning companion. As part of the system, we measured children's valence and engagement via an automatic facial expression analysis system. These signals were combined into a reward signal that fed into the robot's affective reinforcement learning algorithm. Over several sessions, the robot played the game and personalized its motivational strategies (using verbal and non-verbal actions) to each student. We evaluated this system with 34 children in preschool classrooms for a duration of two months. We saw that (1) children learned new words from the repeated tutoring sessions, (2) the affective policy personalized to students over the duration of the study, and (3) students who interacted with a robot that personalized its affective feedback strategy showed a significant increase in valence, as compared to students who interacted with a nonpersonalizing robot. This integrated system of tablet-based educational content, affective sensing, affective policy learning, and an autonomous social robot holds great promise for a more comprehensive approach to personalized tutoring. © Copyright 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Industrial Engineering Department, Curiosity Lab, Tel-Aviv Univerisity, Israel",,,,Author4,Author1,a,a,a,a,a,a,Intelligent Tutors,,,,a,a,"30th AAAI Conference on Artificial Intelligence, AAAI 2016",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007268746&partnerID=40&md5=1d1e6a4e8fcc81aca41b3aa559b142ac,"Gordon G., Spaulding S., Korywestlund J., Lee J.J., Plummer L., Martinez M., Das M., Breazeal C.","Gordon, G., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States, Industrial Engineering Department, Curiosity Lab, Tel-Aviv Univerisity, Israel; Spaulding, S., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Korywestlund, J., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Lee, J.J., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Plummer, L., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Martinez, M., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Das, M., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Breazeal, C., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States",16,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-85007268746,,,,,3957,3951,,,,,Scopus,,,,Conference Paper,,,,,2016,2016,0,0,4,Author2
36,1,1,2,1,Towards end-to-end reinforcement learning of dialogue agents for information access,"This paper proposes KB-InfoBot1 - a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. Such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge. Previous systems achieved this by issuing a symbolic query to the KB to retrieve entries based on their attributes. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced ""soft"" posterior distribution over the KB that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users. We also present a fully neural end-to-end agent, trained entirely from user feedback, and discuss its application towards personalized dialogue agents. © 2017 Association for Computational Linguistics.","This paper proposes KB-InfoBot1 - a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. Such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge. Previous systems achieved this by issuing a symbolic query to the KB to retrieve entries based on their attributes. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced ""soft"" posterior distribution over the KB that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users. We also present a fully neural end-to-end agent, trained entirely from user feedback, and discuss its application towards personalized dialogue agents. © 2017 Association for Computational Linguistics.",,"Carnegie Mellon University, Pittsburgh, PA, United States; Microsoft Research, Redmond, WA, United States; National, Taiwan University, Taipei, Taiwan",,,,Author2,Author3,a,a,u,a,a,a,Personal Assistants,a,,I think we can accept and see whether the system is made relevant to individuals in some sense.,u,a,"ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030087610&doi=10.18653%2fv1%2fP17-1045&partnerID=40&md5=c571cfd88d6caaedbd803bdfd2feb1df,"Dhingra B., Li L., Li X., Gao J., Chen Y.-N., Ahmed F., Deng L.","Dhingra, B., Carnegie Mellon University, Pittsburgh, PA, United States; Li, L., Microsoft Research, Redmond, WA, United States; Li, X., Microsoft Research, Redmond, WA, United States; Gao, J., Microsoft Research, Redmond, WA, United States; Chen, Y.-N., National, Taiwan University, Taipei, Taiwan; Ahmed, F., Microsoft Research, Redmond, WA, United States; Deng, L., Microsoft Research, Redmond, WA, United States",3,,10.18653/v1/P17-1045,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85030087610,,,,,495,484,,,,,Scopus,,,,Conference Paper,,,1,,,2017,0,3,1,Author3
37,1,0,2,1,Learning ontology for personalized video retrieval,"This paper proposes a new method for using implicit user feedback from clickthrough data to provide personalized ranking of results in a video retrieval system. The annotation based search is complemented with a feature based ranking in our approach. The ranking algorithm uses belief revision in a Bayesian Network, which is derived from a multimedia ontology that captures the probabilistic association of a concept with expected video features. We have developed a content model for videos using discrete feature states to enable Bayesian reasoning and to alleviate on-line feature processing overheads. We propose a reinforcement learning algorithm for the parameters of the Bayesian Network with the implicit feedback obtained from the clickthrough data. Copyright 2007 ACM.","This paper proposes a new method for using implicit user feedback from clickthrough data to provide personalized ranking of results in a video retrieval system. The annotation based search is complemented with a feature based ranking in our approach. The ranking algorithm uses belief revision in a Bayesian Network, which is derived from a multimedia ontology that captures the probabilistic association of a concept with expected video features. We have developed a content model for videos using discrete feature states to enable Bayesian reasoning and to alleviate on-line feature processing overheads. We propose a reinforcement learning algorithm for the parameters of the Bayesian Network with the implicit feedback obtained from the clickthrough data. Copyright 2007 ACM.",,"Innovation Labs, Delhi, TCS Limited, 249 D and E Udyog Vihar Phase 4, Gurgaon 122016, India; Electrical Engineering Deptt., Indian Institute of Technology, Delhi, New Delhi 110016, India",,,Bayesian network; Content modeling; Reinforcement learning; Video retrieval,Author2,Author3,a,a,a,a,a,a,Recommender Systems,a,,,a,a,Proceedings of the ACM International Multimedia Conference and Exhibition,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37849030482&doi=10.1145%2f1290067.1290075&partnerID=40&md5=f20e9386c1b589ba6b1a0af3b6d9de83,"Ghosh H., Poornachander P., Mallik A., Chaudhury S.","Ghosh, H., Innovation Labs, Delhi, TCS Limited, 249 D and E Udyog Vihar Phase 4, Gurgaon 122016, India; Poornachander, P., Electrical Engineering Deptt., Indian Institute of Technology, Delhi, New Delhi 110016, India; Mallik, A., Electrical Engineering Deptt., Indian Institute of Technology, Delhi, New Delhi 110016, India; Chaudhury, S., Electrical Engineering Deptt., Indian Institute of Technology, Delhi, New Delhi 110016, India",5,,10.1145/1290067.1290075,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-37849030482,,,,,46,39,,,,,Scopus,,,,Conference Paper,,,,,2007,2007,1,2,1,Author3
38,2,0,0,0,Multi-objective reinforcement learning algorithm and its improved convergency method,"This paper proposes a multi-objective reinforcement learning algorithm (MORLA) and uses simultaneous perturbation stochastic approximation (SPSA) to improve the convergence of it. Usually, reinforcement learning (RL) is used to design neurocontroller for control system with single objective. When facing multi-objective system, it is necessary to design the neurocontroller according to the personal preference. The MORLA can transform the multi-objective into synthetical objective and applies parallel genetic algorithm (PGA) to evolve the neurocontroller according to the synthetical objective. To establish the synthetical objective, the objective weight which represents the personal preference is calculated by solving the constrained optimization problem (COP) at the end of each generation. The COP requires not only the biggest variance of the synthetical objective in the population, but also requires the weight to fit the designer's preference. After acquiring the weights, the PGA can select the elitists from the population according to the designer's preference and design a satisfying neurocontroller by evolutionary operations. In addition, although GA has good global search ability, it descends slowly at local area. This paper applies SPSA algorithm to search optimal solution when GA is vibrating at local area. The SPSA converges fast by efficient gradient approximation that relies on measurements of the objective function. The hybrid algorithm accelerates the learning speed of reinforcement learning. At last, the MORLA is used to design neurocontroller for a speed-controlled induction motor drive with indirect vector control. With different personal preferences for the drive system, the simulation results show the feasibility and validity of the MORLA.","This paper proposes a multi-objective reinforcement learning algorithm (MORLA) and uses simultaneous perturbation stochastic approximation (SPSA) to improve the convergence of it. Usually, reinforcement learning (RL) is used to design neurocontroller for control system with single objective. When facing multi-objective system, it is necessary to design the neurocontroller according to the personal preference. The MORLA can transform the multi-objective into synthetical objective and applies parallel genetic algorithm (PGA) to evolve the neurocontroller according to the synthetical objective. To establish the synthetical objective, the objective weight which represents the personal preference is calculated by solving the constrained optimization problem (COP) at the end of each generation. The COP requires not only the biggest variance of the synthetical objective in the population, but also requires the weight to fit the designer's preference. After acquiring the weights, the PGA can select the elitists from the population according to the designer's preference and design a satisfying neurocontroller by evolutionary operations. In addition, although GA has good global search ability, it descends slowly at local area. This paper applies SPSA algorithm to search optimal solution when GA is vibrating at local area. The SPSA converges fast by efficient gradient approximation that relies on measurements of the objective function. The hybrid algorithm accelerates the learning speed of reinforcement learning. At last, the MORLA is used to design neurocontroller for a speed-controlled induction motor drive with indirect vector control. With different personal preferences for the drive system, the simulation results show the feasibility and validity of the MORLA.",,,,"Department of Control Science and Engineering, Huazhong University of Science and Technology, China",SPSA;multi-objective reinforcement learning;speed-controlled,Author3,Author4,a,a,a,a,a,a,other,,"This seems like a technical paper that does not describe an application of RL to the 'personalization problem' of weighting multiple objectives.

domain: unclear
",,a,a,2011 6th IEEE Conference on Industrial Electronics and Applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976002,,Z. Jin; Z. Huajun,,0,,10.1109/ICIEA.2011.5976002,IEEE Xplore,1,,20110804,2445,,Algorithm design and analysis;Approximation methods;Control systems;Convergence;Genetic algorithms;Learning;Neurocontrollers,genetic algorithms;learning (artificial intelligence);neurocontrollers,constrained optimization problem;indirect vector control;multiobjective reinforcement learning;multiobjective system;neurocontroller;objective function;parallel genetic algorithm;simultaneous perturbation stochastic approximation;speed-controlled induction motor drive,,2156-2318;21562318,,21-23 June 2011,,,,,,,,,,,,IEEE,47,,Electronic:978-1-4244-8756-1; POD:978-1-4244-8754-7,,2438,IEEE Conferences,,,,,2011,2011,3,0,1,Author1
39,2,2,0,2,A reinforcement learning approach for individualizing erythropoietin dosages in hemodialysis patients,"This paper presents a reinforcement learning (RL) approach for anemia management in patients undergoing chronic renal failure. Erythropoietin (EPO) is the treatment of choice for this kind of anemia but it is an expensive drug and with some dangerous side-effects that should be considered especially for patients who do not respond to the treatment. Therefore, an individualized treatment appears to be necessary. RL is a suitable approach to tackle this problem. Moreover, resulting policies are similar to medical protocols, and hence, they can easily be transferred to daily practice. A cohort of 64 patients are included in the study. An implementation of the Q-learning algorithm based on a state-aggregation table and another implementation using the multi-layer perceptron as a function approximator (Q-MLP) are compared with the protocols followed in the Nephrology Unit. The policy obtained by the Q-MLP approach outperforms the hospital policy in terms of the ratio of patients that are within the targeted range of hemoglobin (11.5-12.5 g/dl) at the end of the analyzed period, since an increase of 25% is observed. It ensures an improvement in patients' quality-of-life and considerable economic savings for the health care system due to both the expensiveness of EPO treatment and the costs incurred by the health care system in order to alleviate problems related to EPO over-dosing. It should be pointed out that the approach presented here is completely general, and therefore, it can be applied to any problem of drug dosage optimization. © 2009 Elsevier Ltd. All rights reserved.","This paper presents a reinforcement learning (RL) approach for anemia management in patients undergoing chronic renal failure. Erythropoietin (EPO) is the treatment of choice for this kind of anemia but it is an expensive drug and with some dangerous side-effects that should be considered especially for patients who do not respond to the treatment. Therefore, an individualized treatment appears to be necessary. RL is a suitable approach to tackle this problem. Moreover, resulting policies are similar to medical protocols, and hence, they can easily be transferred to daily practice. A cohort of 64 patients are included in the study. An implementation of the Q-learning algorithm based on a state-aggregation table and another implementation using the multi-layer perceptron as a function approximator (Q-MLP) are compared with the protocols followed in the Nephrology Unit. The policy obtained by the Q-MLP approach outperforms the hospital policy in terms of the ratio of patients that are within the targeted range of hemoglobin (11.5-12.5 g/dl) at the end of the analyzed period, since an increase of 25% is observed. It ensures an improvement in patients' quality-of-life and considerable economic savings for the health care system due to both the expensiveness of EPO treatment and the costs incurred by the health care system in order to alleviate problems related to EPO over-dosing. It should be pointed out that the approach presented here is completely general, and therefore, it can be applied to any problem of drug dosage optimization. © 2009 Elsevier Ltd. All rights reserved.",,"Department of Electronic Engineering, University of Valencia, CL. Dr. Moliner, 50, 46100 Burjassot, Valencia, Spain; Istituto Dalle Molle di Studi sull'Intelligenza Artificiale (IDSIA), Lugano, Switzerland; Pharmacy Unit, University Hospital, Dr. Peset, Valencia, Spain; Pharmacy and Pharmaceutical Technology Department, University of Valencia, Spain",,,Anemia; Chronic renal failure; Clinical pharmacokinetics; Erythropoietin; Reinforcement learning,Author2,Author3,a,a,a,a,a,a,Healthcare,a,,,a,a,Expert Systems with Applications,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-64049101720&doi=10.1016%2fj.eswa.2009.02.041&partnerID=40&md5=3f10a7edf6be7a31b5baf5d6be7218ef,"Martín-Guerrero J.D., Gomez F., Soria-Olivas E., Schmidhuber J., Climente-Martí M., Jiménez-Torres N.V.","Martín-Guerrero, J.D., Department of Electronic Engineering, University of Valencia, CL. Dr. Moliner, 50, 46100 Burjassot, Valencia, Spain; Gomez, F., Istituto Dalle Molle di Studi sull'Intelligenza Artificiale (IDSIA), Lugano, Switzerland; Soria-Olivas, E., Department of Electronic Engineering, University of Valencia, CL. Dr. Moliner, 50, 46100 Burjassot, Valencia, Spain; Schmidhuber, J., Istituto Dalle Molle di Studi sull'Intelligenza Artificiale (IDSIA), Lugano, Switzerland; Climente-Martí, M., Pharmacy Unit, University Hospital, Dr. Peset, Valencia, Spain; Jiménez-Torres, N.V., Pharmacy Unit, University Hospital, Dr. Peset, Valencia, Spain, Pharmacy and Pharmaceutical Technology Department, University of Valencia, Spain",17,,10.1016/j.eswa.2009.02.041,SCOPUS,1,,,,,,,,,,6,,,,2-s2.0-64049101720,,,,,9742,9737,,,,,Scopus,,,,Article,,,36,,2009,2009,1,0,3,Author2
40,0,0,0,0,Angle of arrival estimation in dynamic indoor THz channels with Bayesian filter and reinforcement learning,"This paper presents a novel algorithm to estimate the Angle of Arrival (AoA) in a dynamic indoor Terahertz channel. In a realistic application, the user equipment is often moved by the user during the data transmission and the AoA must be estimated periodically, such that the adaptive directional antenna can be adjusted to realize a high antenna gain. The Bayesian filter is applied to exploit continuity and smoothness of the channel dynamics for the AoA estimation. Reinforcement learning is introduced to adapt the prior transition probabilities between system states, in order to fit the variation of application scenarios and personal habits. The algorithm is validated using the ray launching channel simulator and realistic human movement models.","This paper presents a novel algorithm to estimate the Angle of Arrival (AoA) in a dynamic indoor Terahertz channel. In a realistic application, the user equipment is often moved by the user during the data transmission and the AoA must be estimated periodically, such that the adaptive directional antenna can be adjusted to realize a high antenna gain. The Bayesian filter is applied to exploit continuity and smoothness of the channel dynamics for the AoA estimation. Reinforcement learning is introduced to adapt the prior transition probabilities between system states, in order to fit the variation of application scenarios and personal habits. The algorithm is validated using the ray launching channel simulator and realistic human movement models.",,,,"Technische Universit&#x00E4;t Braunschweig Schleinitzstra&#x00DF;e 22, 38106 Braunschweig, Germany",Bayesian filter;Terahertz communication;angle of arrival estimation;dynamic channel;reinforcement learning,Author4,Author1,a,a,a,a,a,a,Personal Assistants,,Optimizes antenna position.,,a,a,2016 24th European Signal Processing Conference (EUSIPCO),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7760594,,B. Peng; Q. Jiao; T. Kürner,,1,,10.1109/EUSIPCO.2016.7760594,IEEE Xplore,1,,20161201,1979,,Azimuth;Bayes methods;Directive antennas;Estimation;Gain;Learning (artificial intelligence),belief networks;directive antennas;filtering theory;learning (artificial intelligence);probability,AoA estimation;Bayesian filter;adaptive directional antenna;angle of arrival estimation;dynamic indoor THz channels;high antenna gain;prior transition probabilities;ray launching channel simulator;reinforcement learning,,,,Aug. 29 2016-Sept. 2 2016,,,,,,,,,,,,IEEE,,,Electronic:978-0-9928-6265-7; POD:978-1-5090-1891-8,,1975,IEEE Conferences,,,,,2016,2016,4,0,0,Author1
41,0,0,0,0,A learning model for personalized adaptive cruise control,"This paper develops a learning model for personalized adaptive cruise control that can learn from human demonstration online and mimic a human driver's driving strategies in the dynamic traffic environment. Under the framework of the proposed model, reinforcement learning is used to capture the human-desired driving strategy, and the proportion-integration-differentiation controller is adopted to convert the learning strategy to low-level control commands. The performance of the learning model is tested in the simulation environment built in a driving simulator using PreScan. Experimental results show that the learning model can duplicate human driving strategies with acceptable errors. Moreover, compared with the traditional adaptive cruise control, the proposed model can provide better driving comfort and smoothness in the dynamic situation.","This paper develops a learning model for personalized adaptive cruise control that can learn from human demonstration online and mimic a human driver's driving strategies in the dynamic traffic environment. Under the framework of the proposed model, reinforcement learning is used to capture the human-desired driving strategy, and the proportion-integration-differentiation controller is adopted to convert the learning strategy to low-level control commands. The performance of the learning model is tested in the simulation environment built in a driving simulator using PreScan. Experimental results show that the learning model can duplicate human driving strategies with acceptable errors. Moreover, compared with the traditional adaptive cruise control, the proposed model can provide better driving comfort and smoothness in the dynamic situation.",,,,"Intelligent Vehicle Research Center, Beijing Institute of Technology, Beijing 100081 China",,Author4,Author1,a,a,a,a,a,a,Personal Assistants,,,,a,a,2017 IEEE Intelligent Vehicles Symposium (IV),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7995748,,X. Chen; Y. Zhai; C. Lu; J. Gong; G. Wang,,,,10.1109/IVS.2017.7995748,IEEE Xplore,1,,20170731,384,,Acceleration;Adaptation models;Cruise control;Data models;Learning (artificial intelligence);Vehicle dynamics;Vehicles,adaptive control;intelligent transportation systems;learning (artificial intelligence);three-term control,PreScan;driving simulator;human demonstration;human driver;human-desired driving strategy;intelligent driving systems;learning model;personalized adaptive cruise control;proportion-integration-differentiation controller;reinforcement learning,,,,11-14 June 2017,,,,,,,,,,,,IEEE,,,Electronic:978-1-5090-4804-5; POD:978-1-5090-4805-2; USB:978-1-5090-4803-8,,379,IEEE Conferences,,,,,2017,2017,4,0,0,Author2
42,1,2,0,1,Constructing evidence-based treatment strategies using methods from computer science,"This paper details a new methodology, instance-based reinforcement learning, for constructing adaptive treatment strategies from randomized trials. Adaptive treatment strategies are operationalized clinical guidelines which recommend the next best treatment for an individual based on his/her personal characteristics and response to earlier treatments. The instance-based reinforcement learning methodology comes from the computer science literature, where it was developed to optimize sequences of actions in an evolving, time varying system. When applied in the context of treatment design, this method provides the means to evaluate both the therapeutic and diagnostic effects of treatments in constructing an adaptive treatment strategy. The methodology is illustrated with data from the STAR*D trial, a multi-step randomized study of treatment alternatives for individuals with treatment-resistant major depressive disorder. © 2007 Elsevier Ireland Ltd. All rights reserved.","This paper details a new methodology, instance-based reinforcement learning, for constructing adaptive treatment strategies from randomized trials. Adaptive treatment strategies are operationalized clinical guidelines which recommend the next best treatment for an individual based on his/her personal characteristics and response to earlier treatments. The instance-based reinforcement learning methodology comes from the computer science literature, where it was developed to optimize sequences of actions in an evolving, time varying system. When applied in the context of treatment design, this method provides the means to evaluate both the therapeutic and diagnostic effects of treatments in constructing an adaptive treatment strategy. The methodology is illustrated with data from the STAR*D trial, a multi-step randomized study of treatment alternatives for individuals with treatment-resistant major depressive disorder. © 2007 Elsevier Ireland Ltd. All rights reserved.",,"McGill University, School of Computer Science, 3480 University St., Montreal, Que. H3A 2A7, Canada; University of Texas, Southwestern Medical Center, 323 Harry Hines Blvd, Dallas, TX 75390, United States; University of Michigan, Institute for Social Research, 426 Thompson St, Ann Arbor, MI 48106, United States",,,Clinical decision-making; Learning; Methodology; Sequential decisions; Treatment,Author1,Author2,a,a,a,a,a,a,Healthcare,,,,a,a,Drug and Alcohol Dependence,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047273906&doi=10.1016%2fj.drugalcdep.2007.01.005&partnerID=40&md5=636602c8d15aa0f10a7e2b9c0a82f70d,"Pineau J., Bellemare M.G., Rush A.J., Ghizaru A., Murphy S.A.","Pineau, J., McGill University, School of Computer Science, 3480 University St., Montreal, Que. H3A 2A7, Canada; Bellemare, M.G., McGill University, School of Computer Science, 3480 University St., Montreal, Que. H3A 2A7, Canada; Rush, A.J., University of Texas, Southwestern Medical Center, 323 Harry Hines Blvd, Dallas, TX 75390, United States; Ghizaru, A., McGill University, School of Computer Science, 3480 University St., Montreal, Que. H3A 2A7, Canada; Murphy, S.A., University of Michigan, Institute for Social Research, 426 Thompson St, Ann Arbor, MI 48106, United States",25,,10.1016/j.drugalcdep.2007.01.005,SCOPUS,1,,,,,,,,,,SUPPL. 2,,,,2-s2.0-34047273906,,,,,S60,S52,,,,,Scopus,,,,Article,,,88,,2007,2007,1,2,1,Author2
43,0,0,1,0,Smart Cable-Driven Camera Robotic Assistant,"This paper describes the mechanical design and a cognition system for a novel concept-of-camera robotic assistant. The system combines the advantages of intra-abdominal devices and autonomous camera navigation. The robotic assistant is composed of a magnetic intra-abdominal camera robot with two internal cable-driven degrees of freedom and an external robot that handles an external magnet. The intelligence of the robot is implemented in a cognitive architecture based on a long-term memory that stores the robot's knowledge and learning capabilities to improve the robot's behavior. The navigation strategy combines a reactive control based on instrument tracking and a proactive control based on predefined behaviors, depending on the actual state of the task. The robot's learning capabilities include a semantic customization, to adapt the camera's behavior to the surgeon's preferences, and reinforcement learning, to improve the camera navigation strategy. This paper details both the hardware implementation of the system and the software implementation in a robotic operating system architecture. The cognition system and the performance of the cable-driven mechanism have been validated with a set of in vitro experiments. Moreover, the camera robot has been evaluated through an in-vivo experiment in a pig.","This paper describes the mechanical design and a cognition system for a novel concept-of-camera robotic assistant. The system combines the advantages of intra-abdominal devices and autonomous camera navigation. The robotic assistant is composed of a magnetic intra-abdominal camera robot with two internal cable-driven degrees of freedom and an external robot that handles an external magnet. The intelligence of the robot is implemented in a cognitive architecture based on a long-term memory that stores the robot's knowledge and learning capabilities to improve the robot's behavior. The navigation strategy combines a reactive control based on instrument tracking and a proactive control based on predefined behaviors, depending on the actual state of the task. The robot's learning capabilities include a semantic customization, to adapt the camera's behavior to the surgeon's preferences, and reinforcement learning, to improve the camera navigation strategy. This paper details both the hardware implementation of the system and the software implementation in a robotic operating system architecture. The cognition system and the performance of the cable-driven mechanism have been validated with a set of in vitro experiments. Moreover, the camera robot has been evaluated through an in-vivo experiment in a pig.",,,,"Department of System Engineering and Automation, University of Malaga, Andaluc&#x00ED;a Tech, M&#x00E1;laga, Spain",Cognitive robotics;mechatronics;medical robotics;robot control;robot motion,Author2,Author3,a,a,u,a,a,a,Healthcare,a,,I think we can accept this for now and see whether the RL is used for customization ON TOP of the semantic customization or rather only for 'mere' autonomous control.,u,a,IEEE Transactions on Human-Machine Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8107576,,I. Rivas-Blanco; C. López-Casado; C. J. Pérez-del-Pulgar; F. García-Vacas; J. C. Fraile; V. F. Muñoz,,,,10.1109/THMS.2017.2767286,IEEE Xplore,1,,20180313,196,Spanish national projects; ,Cameras;Navigation;Robot vision systems;Surgery;Tools,cameras;control engineering computing;learning (artificial intelligence);medical computing;medical robotics;robot programming;robot vision;surgery,autonomous camera navigation;camera navigation strategy;cognition system;cognitive architecture;concept-of-camera robotic assistant;external magnet;external robot;intra-abdominal devices;learning capabilities;long-term memory;magnetic intra-abdominal camera robot;mechanical design;reinforcement learning;robotic operating system architecture;smart cable,,2168-2291;21682291,2,April 2018,,,,,,20171114,,,,,,IEEE,,,,,183,IEEE Journals & Magazines,,,48,,,2018,3,1,0,Author3
44,2,0,2,2,Reinforcement learning agent for personalized information filtering,"This paper describes a method for learning user's interests in the Web-based personalized information filtering system called WAIR. The proposed method analyzes user's reactions to the presented documents and learns from them the profiles for the individual users. Reinforcement learning is used to adapt the term weights in the user profile so that user's preferences are best represented. In contrast to conventional relevance feedback methods which require explicit user feedbacks, our approach learns user preferences implicitly from direct observations of user behaviors during interaction. Field tests have been made which involved 7 users reading a total of 7,700 HTML documents during 4 weeks. The proposed method showed superior performance in personalized information filtering compared to the existing relevance feedback methods.","This paper describes a method for learning user's interests in the Web-based personalized information filtering system called WAIR. The proposed method analyzes user's reactions to the presented documents and learns from them the profiles for the individual users. Reinforcement learning is used to adapt the term weights in the user profile so that user's preferences are best represented. In contrast to conventional relevance feedback methods which require explicit user feedbacks, our approach learns user preferences implicitly from direct observations of user behaviors during interaction. Field tests have been made which involved 7 users reading a total of 7,700 HTML documents during 4 weeks. The proposed method showed superior performance in personalized information filtering compared to the existing relevance feedback methods.",,"Seoul Natl Univ, Seoul, South Korea",,,,Author4,Author1,a,a,a,a,a,a,Recommender Systems,,Recommender or personal assistant?,,a,a,"International Conference on Intelligent User Interfaces, Proceedings IUI",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033688671&partnerID=40&md5=0bbfc79c2083cfd6cde5235144850154,"Seo Young-Woo, Zhang Byoung-Tak","Seo, Young-Woo, Seoul Natl Univ, Seoul, South Korea; Zhang, Byoung-Tak, Seoul Natl Univ, Seoul, South Korea",61,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-0033688671,,,,,251,248,,,,,Scopus,,,,Conference Paper,,,,,2000,2000,1,0,3,Author4
45,1,1,1,1,Learning user's preferences by analyzing Web-browsing behaviors,"This paper describes a method for an information filtering agent to learn user's preferences. The proposed method observes user's reactions to the filtered documents and learns from them the profiles for the individual users. Reinforcement learning is used to adapt the most significant terms that best represent user's interests. In contrast to conventional relevance feedback methods which require explicit user feedbacks, our approach learns user preferences implicitly from direct observations of browsing behaviors during interaction. Field tests have been made which involved 10 users reading a total of 18,750 HTML documents during 45 days. The proposed method showed superior performance in personalized information filtering compared to the existing relevance feedback methods.","This paper describes a method for an information filtering agent to learn user's preferences. The proposed method observes user's reactions to the filtered documents and learns from them the profiles for the individual users. Reinforcement learning is used to adapt the most significant terms that best represent user's interests. In contrast to conventional relevance feedback methods which require explicit user feedbacks, our approach learns user preferences implicitly from direct observations of browsing behaviors during interaction. Field tests have been made which involved 10 users reading a total of 18,750 HTML documents during 45 days. The proposed method showed superior performance in personalized information filtering compared to the existing relevance feedback methods.",,"Seoul Natl Univ, Seoul, South Korea",,,,Author2,Author3,a,a,a,a,a,a,Recommender Systems,a,,,a,a,Proceedings of the International Conference on Autonomous Agents,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033700745&partnerID=40&md5=e804803772de377c8c621ff60bb12c35,"Seo Young-Woo, Zhang Byoung-Tak","Seo, Young-Woo, Seoul Natl Univ, Seoul, South Korea; Zhang, Byoung-Tak, Seoul Natl Univ, Seoul, South Korea",47,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-0033700745,,,,,387,381,,,,,Scopus,,,,Conference Paper,,,,,2000,2000,0,4,0,Author2
46,1,0,2,1,A Learning interface agent for scheduling meetings,"This paper describes a Learning Interface Agent for a meeting scheduling application. The agent employs Machine Learning techniques to customize itself to the user's personal scheduling rules and preferences by observing the user's actions and receiving direct user-feedback. Our approach provides the user with sophisticated control over the gradual delegation of scheduling tasks to the agent, as a trust relationship is built. We report upon an experiment in which a collection of such assistants became gradually more helpful to their users through the use of memory-based and reinforcement learning. The experimental data reported upon demonstrate that the learning approach to building intelligent interface agents is a very promising one which has several advantages over more standard approaches. © 1992 ACM.","This paper describes a Learning Interface Agent for a meeting scheduling application. The agent employs Machine Learning techniques to customize itself to the user's personal scheduling rules and preferences by observing the user's actions and receiving direct user-feedback. Our approach provides the user with sophisticated control over the gradual delegation of scheduling tasks to the agent, as a trust relationship is built. We report upon an experiment in which a collection of such assistants became gradually more helpful to their users through the use of memory-based and reinforcement learning. The experimental data reported upon demonstrate that the learning approach to building intelligent interface agents is a very promising one which has several advantages over more standard approaches. © 1992 ACM.",,"MIT Media-Lab, 20 Ames Street, Cambridge, MA, United States",,,Interface agents; Learning interface agents; Machine learning; Personal assistants; Software agents,Author3,Author4,a,a,a,a,a,a,personal assistants,,"The domain 'HCI' could also apply. However, the authors focus on interacting with an agent, which makes 'personal assistant' more fitting.",,a,a,"International Conference on Intelligent User Interfaces, Proceedings IUI",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943177303&partnerID=40&md5=7e3e84b3f621b3d76bbe6069ae98fb28,"Kozierok R., Maes P.","Kozierok, R., MIT Media-Lab, 20 Ames Street, Cambridge, MA, United States; Maes, P., MIT Media-Lab, 20 Ames Street, Cambridge, MA, United States",57,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-84943177303,,,,,88,81,,,,,Scopus,,,,Conference Paper,,,Part F127502,,1993,1993,1,2,1,Author3
47,1,0,2,1,Automatic web content personalization through reinforcement learning,"This paper deals with the automatic adaptation of Web contents. It is recognized that quite often users need some personalized adaptations to access Web contents. This is more evident when we focus on people with some accessibility needs. Based on the user profile, it is possible to transcode or modify contents (e.g., adapt text fonts) so as to meet the user preferences. The problem is that applying such a kind of transformations to the whole content might significantly alter Web pages that might become unreadable, hence making matters worse. We present a system that employs Web intelligence to perform automatic adaptations on single elements composing a Web page. A reinforcement learning algorithm is utilized to manage user profiles. We evaluate our system through simulation and a real assessment where elderly users where asked to use for a time period our system prototype. Results confirm the feasibility of the proposal. © 2016 Elsevier Inc.","This paper deals with the automatic adaptation of Web contents. It is recognized that quite often users need some personalized adaptations to access Web contents. This is more evident when we focus on people with some accessibility needs. Based on the user profile, it is possible to transcode or modify contents (e.g., adapt text fonts) so as to meet the user preferences. The problem is that applying such a kind of transformations to the whole content might significantly alter Web pages that might become unreadable, hence making matters worse. We present a system that employs Web intelligence to perform automatic adaptations on single elements composing a Web page. A reinforcement learning algorithm is utilized to manage user profiles. We evaluate our system through simulation and a real assessment where elderly users where asked to use for a time period our system prototype. Results confirm the feasibility of the proposal. © 2016 Elsevier Inc.",,"Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy",,,Reinforcement learning; User profiling; Web personalization,Author4,Author1,a,a,a,a,a,a,Personal Assistants,,Website contents,,a,a,Journal of Systems and Software,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975757906&doi=10.1016%2fj.jss.2016.02.008&partnerID=40&md5=8d7c3661bc0d939a042f1b25211981a7,"Ferretti S., Mirri S., Prandi C., Salomoni P.","Ferretti, S., Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy; Mirri, S., Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy; Prandi, C., Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy; Salomoni, P., Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy",9,,10.1016/j.jss.2016.02.008,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84975757906,,,,,169,157,,,,,Scopus,,,,Article,,,121,,2016,2016,1,2,1,Author3
48,0,0,0,0,The route not taken: Driver-centric estimation of electric vehicle range,"This paper addresses the challenge of efficiently and accurately predicting an electric vehicle's attainable range. Specifically, our approach accounts for a driver's generalised route preferences to provide up-to-date, personalised information based on estimates of the energy required to reach every possible destination in a map. We frame this task in the context of sequential decision making and show that energy consumption in reaching a particular destination can be formulated as policy evaluation in a Markov Decision Process. In particular, we exploit the properties of the model adopted for predicting likely energy consumption to every possible destination in a realistically sized map in real-time. The policy to be evaluated is learned and, over time, refined using Inverse Reinforcement Learning to provide for a life-long adaptive system. Our approach is evaluated using a publicly available dataset providing real trajectory data of 50 individuals spanning approximately 10,000 miles of travel. We show that by accounting for driver specific route preferences our system significantly reduces the relative error in energy prediction compared to more common, driver-agnostic heuristics such as shortest-path or shortest-time routes. Copyright © 2014, Association for the Advancement of Artificial Intelligence.","This paper addresses the challenge of efficiently and accurately predicting an electric vehicle's attainable range. Specifically, our approach accounts for a driver's generalised route preferences to provide up-to-date, personalised information based on estimates of the energy required to reach every possible destination in a map. We frame this task in the context of sequential decision making and show that energy consumption in reaching a particular destination can be formulated as policy evaluation in a Markov Decision Process. In particular, we exploit the properties of the model adopted for predicting likely energy consumption to every possible destination in a realistically sized map in real-time. The policy to be evaluated is learned and, over time, refined using Inverse Reinforcement Learning to provide for a life-long adaptive system. Our approach is evaluated using a publicly available dataset providing real trajectory data of 50 individuals spanning approximately 10,000 miles of travel. We show that by accounting for driver specific route preferences our system significantly reduces the relative error in energy prediction compared to more common, driver-agnostic heuristics such as shortest-path or shortest-time routes. Copyright © 2014, Association for the Advancement of Artificial Intelligence.",,"Mobile Robotics Group, University of Oxford, United Kingdom",,,,Author2,Author3,a,a,a,a,a,a,Recommender Systems,d,,domain: autonomous vehicle,a,a,"Proceedings International Conference on Automated Planning and Scheduling, ICAPS",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933050809&partnerID=40&md5=46b4fa6e96f048605ef4708bb7d219f6,"Ondrúška P., Posner I.","Ondrúška, P., Mobile Robotics Group, University of Oxford, United Kingdom; Posner, I., Mobile Robotics Group, University of Oxford, United Kingdom",11,,,SCOPUS,0,,,,,,,,,,January,,,,2-s2.0-84933050809,,,,,420,413,,,,,Scopus,,,,Conference Paper,,,2014-January,,2014,2014,4,0,0,Author4
49,2,2,0,2,Machine learning in personalized anemia treatment,"This chapter presents application of reinforcement learning to drug dosing personalization in treatment of chronic conditions. Reinforcement learning is a machine learning paradigm that mimics the trialand-error skill acquisition typical for humans and animals. In treatment of chronic illnesses, finding the optimal dose amount for an individual is also a process that is usually based on trial-and-error. In this chapter, the author focuses on the challenge of personalized anemia treatment with recombinant human erythropoietin. The author demonstrates the application of a standard reinforcement learning method, called Q-learning, to guide the physician in selecting the optimal erythropoietin dose. The author further addresses the issue of random exploration in Q-learning from the drug dosing perspective and proposes a ""smart"" exploration method. Finally, the author performs computer simulations to compare the outcomes from reinforcement learning-based anemia treatment to those achieved by a standard dosing protocol used at a dialysis unit. © 2010, IGI Global.","This chapter presents application of reinforcement learning to drug dosing personalization in treatment of chronic conditions. Reinforcement learning is a machine learning paradigm that mimics the trialand-error skill acquisition typical for humans and animals. In treatment of chronic illnesses, finding the optimal dose amount for an individual is also a process that is usually based on trial-and-error. In this chapter, the author focuses on the challenge of personalized anemia treatment with recombinant human erythropoietin. The author demonstrates the application of a standard reinforcement learning method, called Q-learning, to guide the physician in selecting the optimal erythropoietin dose. The author further addresses the issue of random exploration in Q-learning from the drug dosing perspective and proposes a ""smart"" exploration method. Finally, the author performs computer simulations to compare the outcomes from reinforcement learning-based anemia treatment to those achieved by a standard dosing protocol used at a dialysis unit. © 2010, IGI Global.",,"Department of Medicine, Division of Nephrology, University of Louisville, United States",,,,Author3,Author4,a,a,a,a,u,a,,,"This is a book chapter which seems not to be a bundle of separate papers, e.g. this is not peer-reviewed.",I don't think we can say for sure whether this is peer-reviewed or not. Therefore I would be a favor of including it.,u,a,"Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898381465&doi=10.4018%2f978-1-60566-766-9.ch012&partnerID=40&md5=25913f90e423c1b3501bf0b18d7056c2,Gaweda A.E.,"Gaweda, A.E., Department of Medicine, Division of Nephrology, University of Louisville, United States",,,10.4018/978-1-60566-766-9.ch012,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84898381465,,,,,276,265,,,,,Scopus,,,,Book Chapter,,,,,,2009,1,0,3,Author2
50,1,1,2,1,An interactive multisensing framework for personalized human robot collaboration and assistive training using reinforcement learning,"There is a recent trend of research and applications of Cyber- Physical Systems (CPS) in manufacturing to enhance humanrobot collaboration and production. In this paper, we propose a CPS framework for personalized Human-Robot Collaboration and Training to promote safe human-robot collaboration in manufacturing environments. We propose a human-centric CPS approach that focuses on multimodal human behavior monitoring and assessment, to promote human worker safety and enable human training in Human- Robot Collaboration tasks. We present the architecture of our proposed system, our experimental testbed and our proposed methods for multimodal physiological sensing, human state monitoring and interactive robot adaptation, to enable personalized interaction. © 2017 ACM.","There is a recent trend of research and applications of Cyber- Physical Systems (CPS) in manufacturing to enhance humanrobot collaboration and production. In this paper, we propose a CPS framework for personalized Human-Robot Collaboration and Training to promote safe human-robot collaboration in manufacturing environments. We propose a human-centric CPS approach that focuses on multimodal human behavior monitoring and assessment, to promote human worker safety and enable human training in Human- Robot Collaboration tasks. We present the architecture of our proposed system, our experimental testbed and our proposed methods for multimodal physiological sensing, human state monitoring and interactive robot adaptation, to enable personalized interaction. © 2017 ACM.",,"HERACLEIA Human-Centered Computing Laboratory, CSE Dept., University of Texas, Arlington, United States; Industrial Engineering Dept., University of Texas, Arlington, United States; Dept. of Psychiatry, Yale School of Medicine, United States; CSE Dept., University of Michigan, United States; Mechanical Engineering Dept., University of Michigan-Flint, United States",,,Cyber Physical Systems; Human Robot Collaboration; Intelligent Manufacturing; Vocational Assessment and Training,Author1,Author2,a,a,a,a,a,a,Interfaces / HCI,,,,a,a,ACM International Conference Proceeding Series,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025128294&doi=10.1145%2f3056540.3076191&partnerID=40&md5=c676d60143fb0a043e53e429754aa90b,"Tsiakas K., Papakostas M., Papakostas M., Bell M., Mihalcea R., Wang S., Burzo M., Makedon F.","Tsiakas, K., HERACLEIA Human-Centered Computing Laboratory, CSE Dept., University of Texas, Arlington, United States; Papakostas, M., HERACLEIA Human-Centered Computing Laboratory, CSE Dept., University of Texas, Arlington, United States; Papakostas, M., HERACLEIA Human-Centered Computing Laboratory, CSE Dept., University of Texas, Arlington, United States; Bell, M., Dept. of Psychiatry, Yale School of Medicine, United States; Mihalcea, R., CSE Dept., University of Michigan, United States; Wang, S., Industrial Engineering Dept., University of Texas, Arlington, United States; Burzo, M., Mechanical Engineering Dept., University of Michigan-Flint, United States; Makedon, F., HERACLEIA Human-Centered Computing Laboratory, CSE Dept., University of Texas, Arlington, United States",,,10.1145/3056540.3076191,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85025128294,,,,,427,423,,,,,Scopus,,,,Conference Paper,,,Part F128530,,2017,2017,0,3,1,Author3
51,2,1,2,2,Hybrid-ε-greedy for mobile context-aware recommender system,"The wide development of mobile applications provides a considerable amount of data of all types. In this sense, Mobile Context-aware Recommender Systems (MCRS) suggest the user suitable information depending on her/his situation and interests. Our work consists in applying machine learning techniques and reasoning process in order to adapt dynamically the MCRS to the evolution of the user's interest. To achieve this goal, we propose to combine bandit algorithm and case-based reasoning in order to define a contextual recommendation process based on different context dimensions (social, temporal and location). This paper describes our ongoing work on the implementation of a MCRS based on a hybrid-ε-greedy algorithm. It also presents preliminary results by comparing the hybrid-ε-greedy and the standard ε-greedy algorithm. © 2012 Springer-Verlag.","The wide development of mobile applications provides a considerable amount of data of all types. In this sense, Mobile Context-aware Recommender Systems (MCRS) suggest the user suitable information depending on her/his situation and interests. Our work consists in applying machine learning techniques and reasoning process in order to adapt dynamically the MCRS to the evolution of the user's interest. To achieve this goal, we propose to combine bandit algorithm and case-based reasoning in order to define a contextual recommendation process based on different context dimensions (social, temporal and location). This paper describes our ongoing work on the implementation of a MCRS based on a hybrid-ε-greedy algorithm. It also presents preliminary results by comparing the hybrid-ε-greedy and the standard ε-greedy algorithm. © 2012 Springer-Verlag.",,"Department of Computer Science, Télécom SudParis, UMR CNRS Samovar, 91011 Evry Cedex, France",,,contextual bandit; exploration/exploitation dilemma; Machine learning; personalization; recommender systems,Author3,Author4,a,a,a,a,a,a,Recommender Systems,a,,,a,a,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861444817&doi=10.1007%2f978-3-642-30217-6_39&partnerID=40&md5=ad6ea70f5fbea80eaaa0f3864a7fd509,"Bouneffouf D., Bouzeghoub A., Gançarski A.L.","Bouneffouf, D., Department of Computer Science, Télécom SudParis, UMR CNRS Samovar, 91011 Evry Cedex, France; Bouzeghoub, A., Department of Computer Science, Télécom SudParis, UMR CNRS Samovar, 91011 Evry Cedex, France; Gançarski, A.L., Department of Computer Science, Télécom SudParis, UMR CNRS Samovar, 91011 Evry Cedex, France",11,,10.1007/978-3-642-30217-6_39,SCOPUS,1,,,,,,,,,,PART 1,,,,2-s2.0-84861444817,,,,,479,468,,,,,Scopus,,,,Conference Paper,,,7301 LNAI,,2012,2012,0,1,3,Author4
52,0,0,0,0,Optimal radio channel recommendations with explicit and implicit feedback,"The very large majority of recommender systems are running as server-side applications, and they are controlled by the content provider, i.e., who provides the recommended items. This paper focuses on a different scenario: the user is supposed to be able to access content from multiple providers, in our application they offer radio channels, and it is up to a personal recommender installed on the clients' side to decide which channel to select and recommend to the user. We exploit the implicit feedback derived from the user's listening behavior, and we model channel recommendation as a sequential decision making problem. We have implemented a personal RS that integrates reinforcement learning techniques to decide what channel to play every time the user asks for a new music track or the current track finishes playing. In a live user study we show that the proposed system can sequentially select the next channel to play such that the users listen to the streamed tracks for a larger fraction, and for more time, compared to a baseline system not exploiting implicit feedback. Copyright © 2012 by the Association for Computing Machinery, Inc. (ACM).","The very large majority of recommender systems are running as server-side applications, and they are controlled by the content provider, i.e., who provides the recommended items. This paper focuses on a different scenario: the user is supposed to be able to access content from multiple providers, in our application they offer radio channels, and it is up to a personal recommender installed on the clients' side to decide which channel to select and recommend to the user. We exploit the implicit feedback derived from the user's listening behavior, and we model channel recommendation as a sequential decision making problem. We have implemented a personal RS that integrates reinforcement learning techniques to decide what channel to play every time the user asks for a new music track or the current track finishes playing. In a live user study we show that the proposed system can sequentially select the next channel to play such that the users listen to the streamed tracks for a larger fraction, and for more time, compared to a baseline system not exploiting implicit feedback. Copyright © 2012 by the Association for Computing Machinery, Inc. (ACM).",,"Free University of Bozen-Bolzano, Piazza Domenicani 3, Bolzano, Italy; Telefonica Research, Plaza se E. Lluchi Martin 5, Barcelona, Spain",,,Implicit feedback; Reinforcement learning; Sequential music recommendations,Author4,Author1,a,a,a,a,a,a,Personal Assistants,,Difference between recommender system and PA very difficult to make.,,a,a,RecSys'12 - Proceedings of the 6th ACM Conference on Recommender Systems,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867388659&doi=10.1145%2f2365952.2365971&partnerID=40&md5=5a2334482844dda8d0e8d5a453984015,"Moling O., Baltrunas L., Ricci F.","Moling, O., Free University of Bozen-Bolzano, Piazza Domenicani 3, Bolzano, Italy; Baltrunas, L., Telefonica Research, Plaza se E. Lluchi Martin 5, Barcelona, Spain; Ricci, F., Free University of Bozen-Bolzano, Piazza Domenicani 3, Bolzano, Italy",6,,10.1145/2365952.2365971,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84867388659,,,,,82,75,,,,,Scopus,,,,Conference Paper,,,,,2012,2012,4,0,0,Author1
53,0,0,1,0,Recipe tuning by reinforcement learning in the SandS ecosystem,"The Social and Smart (SandS) project ecosystem is compounded of household appliance users sharing recipes for the used of appliances, an intermediate control layer, and an intelligent social layer which aims to optimize the appliance recipes maximizing user satisfaction. We consider two aspects of the social intelligence, the innovation producing new recipes for unkown user tasks, and the adaptation to personalize the recipe to an individual user on the basis of his/her specific feedback. The second aspect is proposed to be dealt with by Reinforcement Learning approach, thus user feedback becomes the system reward. In this paper we discuss such an architecture based on the actor-critic approach, providing some experimental results on synthetic datasets that demonstrate the feasibility of the approach, previous to real life implementations.","The Social and Smart (SandS) project ecosystem is compounded of household appliance users sharing recipes for the used of appliances, an intermediate control layer, and an intelligent social layer which aims to optimize the appliance recipes maximizing user satisfaction. We consider two aspects of the social intelligence, the innovation producing new recipes for unkown user tasks, and the adaptation to personalize the recipe to an individual user on the basis of his/her specific feedback. The second aspect is proposed to be dealt with by Reinforcement Learning approach, thus user feedback becomes the system reward. In this paper we discuss such an architecture based on the actor-critic approach, providing some experimental results on synthetic datasets that demonstrate the feasibility of the approach, previous to real life implementations.",,,,"Computational Intelligence Group, University of the Basque Country, UPV/EHU, San Sebastian, Spain",Reinforcement Learning;Social computing;Social networks;subconscious social intelligence,Author1,Author2,a,a,a,a,a,a,Recommender Systems,,,,a,a,2014 6th International Conference on Computational Aspects of Social Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6920422,,B. Fernandez-Gauna; M. Graña,,0,,10.1109/CASoN.2014.6920422,IEEE Xplore,1,,20141013,60,,Biological system modeling;Computational modeling;Computer architecture;Robots;Service-oriented architecture,domestic appliances;learning (artificial intelligence);social sciences computing;user interfaces,SandS ecosystem;actor-critic approach;household appliance;intelligent social layer;recipe tuning;reinforcement learning;social and smart project ecosystem;social intelligence;user satisfaction,,,,July 30 2014-Aug. 1 2014,,,,,,,,,,,,IEEE,15,,Electronic:978-1-4799-5940-2; POD:978-1-4799-5941-9,,55,IEEE Conferences,,,,,2014,2014,3,1,0,Author3
54,1,0,0,0,GongBroker: A Broker Model for Power Trading in Smart Grid Markets,"The Smart Grid market is dynamic and complex, and brokers are widely introduced to better manage this market. This paper proposes an intelligent broker model - GongBroker, with smart trading strategies to cope with the dynamics and complexity. GongBroker first predicts the short-term demands of various consumers, and then buys energy from the wholesale market through auctions, and sells energy to various consumers in the retail market. In order to predict the customer demand, a data-driven method is proposed. All the consumers are hierarchically clustered according to their historical energy consumptions, and different prediction methods are tailored for different customer clusters to predict one-day-ahead hourly energy demand. Based on the predicted demand, the GongBroker employs a Markov Decision Process for the one-day-ahead auction in the wholesale market. To compete with other brokers, GongBroker uses independent reinforcement learning processes to optimize prices for different types of consumers. Experimental results demonstrate that GongBroker not only is competitive in making profit, but also keeps a good supply-demand balance.","The Smart Grid market is dynamic and complex, and brokers are widely introduced to better manage this market. This paper proposes an intelligent broker model - GongBroker, with smart trading strategies to cope with the dynamics and complexity. GongBroker first predicts the short-term demands of various consumers, and then buys energy from the wholesale market through auctions, and sells energy to various consumers in the retail market. In order to predict the customer demand, a data-driven method is proposed. All the consumers are hierarchically clustered according to their historical energy consumptions, and different prediction methods are tailored for different customer clusters to predict one-day-ahead hourly energy demand. Based on the predicted demand, the GongBroker employs a Markov Decision Process for the one-day-ahead auction in the wholesale market. To compete with other brokers, GongBroker uses independent reinforcement learning processes to optimize prices for different types of consumers. Experimental results demonstrate that GongBroker not only is competitive in making profit, but also keeps a good supply-demand balance.",,,,"Sch. of Comput. & Inf. Technol., Univ. of Wollongong, Wollongong, NSW, Australia",Broker Model;Data-driven;Reinforcement Learning;Smart Grid Market,Author2,Author3,a,a,u,a,a,a,Personal Assistants,d,domain: energy,,u,a,2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7397309,,X. Wang; M. Zhang; F. Ren; T. Ito,,1,,10.1109/WI-IAT.2015.108,IEEE Xplore,1,,20160204,24,,Adaptation models;Electronic mail;Energy consumption;Mathematical model;Prediction algorithms;Prediction methods;Smart grids,Markov processes;demand side management;learning (artificial intelligence);power markets;smart power grids;supply and demand,GongBroker;Markov decision process;consumers short-term demands;data-driven method;day-ahead hourly energy demand;independent reinforcement learning processes;intelligent broker model;power trading;prediction methods;smart grid markets;smart trading strategies;supply-demand balance,,,,6-9 Dec. 2015,,,,,,,,,,,,IEEE,6,,Electronic:978-1-4673-9618-9; POD:978-1-4673-9619-6,,21,IEEE Conferences,,,2,,,2015,3,1,0,Author1
55,0,0,0,0,Dynamic Class of Service mapping for Quality of Experience control in future networks,"The present work introduces a novel approach to cope with some key limitations of the present communication networks. In particular, the need for effectively managing heterogeneous resources over heterogeneous networks while guaranteeing personalized Quality of Experience (QoE) requirements to the applications, claims for a full cognitive approach which is realized through the introduction of an appropriate Future Internet architecture. The paper, taking this architecture in mind, introduces the innovative concept of dynamic association between applications and Classes of Services, performed by means of proper Reinforcement Learning algorithms. The resulting procedure guarantees QoE personalization, requires low processing capabilities and entails limited signalling overhead.","The present work introduces a novel approach to cope with some key limitations of the present communication networks. In particular, the need for effectively managing heterogeneous resources over heterogeneous networks while guaranteeing personalized Quality of Experience (QoE) requirements to the applications, claims for a full cognitive approach which is realized through the introduction of an appropriate Future Internet architecture. The paper, taking this architecture in mind, introduces the innovative concept of dynamic association between applications and Classes of Services, performed by means of proper Reinforcement Learning algorithms. The resulting procedure guarantees QoE personalization, requires low processing capabilities and entails limited signalling overhead.",,,,,,Author1,Author2,a,a,a,a,a,a,"Other (please in ""remark"")",,Software/systems and network engineering,,a,a,WTC 2014; World Telecommunications Congress 2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6840010,,F. D. Priscoli; L. Fogliati; A. Palo; A. Pietrabissa,,0,,,IEEE Xplore,0,,20140624,6,,,,,,,,1-3 June 2014,,,,,,,,,,,,VDE,,,Paper:978-3-8007-3602-7,,1,VDE Conferences,,,,,2014,2014,4,0,0,Author2
56,1,1,1,1,User centered and context dependent personalization through experiential transcoding,"The pervasive presence of devices exploited to use and deliver entertainment text-based content can make reading easier to get but more difficult to enjoy, in particular for people with reading-related disabilities. The main solutions that allow overcoming some of the difficulties experienced by users with specific and special needs are based on content adaptation and user profiling. This paper presents a system that aims to improve content legibility by exploiting experiential transcoding techniques. The system we propose tracks users' behaviors so as to provide a tailored adaptation of textual content, in order to fit the needs of a specific user on the several different devices she/he actually uses.","The pervasive presence of devices exploited to use and deliver entertainment text-based content can make reading easier to get but more difficult to enjoy, in particular for people with reading-related disabilities. The main solutions that allow overcoming some of the difficulties experienced by users with specific and special needs are based on content adaptation and user profiling. This paper presents a system that aims to improve content legibility by exploiting experiential transcoding techniques. The system we propose tracks users' behaviors so as to provide a tailored adaptation of textual content, in order to fit the needs of a specific user on the several different devices she/he actually uses.",,,,"Department of Computer Science and Engineering University of Bologna Bologna, Italy",content adaptation;device capabilities;legibility;reinforcement learning;user profiling,Author2,Author3,u,a,a,a,a,a,Intelligent Tutors,a,,"I propose to accept based on having ""Reinforcement Learning"" in the title. The domain is not really clear, we'll have to see whether it's really about tutoring or about content adaptation for improved legibility in general",u,a,2014 IEEE 11th Consumer Communications and Networking Conference (CCNC),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6940520,,S. Ferretti; S. Mirri; C. Prandi; P. Salomoni,,4,,10.1109/CCNC.2014.6940520,IEEE Xplore,1,,20141103,491,,Adaptation models;Context;Entertainment industry;Multimedia communication;Prototypes;Transcoding;Web pages,Web sites;assisted living;human factors;text analysis;transcoding;ubiquitous computing,content adaptation;content legibility improvement;context dependent personalization;entertainment text-based content;transcoding techniques;user centered personalization;user profiling,,2331-9852;23319852,,10-13 Jan. 2014,,,,,,,,,,,,IEEE,23,,Electronic:978-1-4799-2355-7; POD:978-1-4799-2357-1,,486,IEEE Conferences,,,,,,2014,0,4,0,Author4
57,1,0,0,0,Personalized galaxies of information,"The Personalized Galaxies of Information demonstration presents a new interface approach for visualizing, navigating and accessing information objects in a large body of unstructured information, such as on-line new stories, photographs and video clips available via Clarinews; electronic mail; and World Wide Web documents. The system provides mechanisms to analyze the relationships between information objects and builds a representation of the underlying structure of the entire body of information. This relational structure is used to construct a visual information space with which the user interacts to explore the contents of the information base. The system also uses a learning algorithm to adaptively customize the presentation of information to a particular user's interests. This dynamic, personalized structuring of information helps users perform directed searches while simultaneously affording general browsing in a fluid and seamless environment.","The Personalized Galaxies of Information demonstration presents a new interface approach for visualizing, navigating and accessing information objects in a large body of unstructured information, such as on-line new stories, photographs and video clips available via Clarinews; electronic mail; and World Wide Web documents. The system provides mechanisms to analyze the relationships between information objects and builds a representation of the underlying structure of the entire body of information. This relational structure is used to construct a visual information space with which the user interacts to explore the contents of the information base. The system also uses a learning algorithm to adaptively customize the presentation of information to a particular user's interests. This dynamic, personalized structuring of information helps users perform directed searches while simultaneously affording general browsing in a fluid and seamless environment.",,"MIT Media Lab, Cambridge, United States",,,"Information visualization, abstracted information spaces, 3D interactive graphics, user interest models, reinforcement learning",Author3,Author4,a,a,a,a,a,a,Interfaces / HCI,a,"It's a bit unclear which learning algorithm is used, but it s",Keywords at least list reinforcement learning.,a,a,Conference on Human Factors in Computing Systems - Proceedings,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029204727&partnerID=40&md5=46e64663194ee5b069580c8532f337c5,Rennison Earl,"Rennison, Earl, MIT Media Lab, Cambridge, United States",7,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-0029204727,,,,,32,31,,,,,Scopus,,,,Conference Paper,,,2,,1995,1995,3,1,0,Author1
58,2,2,0,2,Active learning for personalizing treatment,"The personalization of treatment via genetic biomarkers and other risk categories has drawn increasing interest among clinical researchers and scientists. A major challenge here is to construct individualized treatment rules (ITR), which recommend the best treatment for each of the different categories of individuals. In general, ITRs can be constructed using data from clinical trials, however these are generally very costly to run. In order to reduce the cost of learning an ITR, we explore active learning techniques designed to carefully decide whom to recruit, and which treatment to assign, throughout the online conduct of the clinical trial. As an initial investigation, we focus on simple ITRs that utilize a small number of subpopulation categories to personalize treatment. To minimize the maximal uncertainty regarding the treatment effects for each subpopulation, we propose the use of a minimax bandit model and provide an active learning policy for solving it. We evaluate our active learning policy using simulated data and data modeled after a clinical trial involving treatments for depressed individuals. We contrast this policy with other plausible active learning policies. The techniques presented in the paper may be generalized to tackle problems of efficient exploration in other domains.","The personalization of treatment via genetic biomarkers and other risk categories has drawn increasing interest among clinical researchers and scientists. A major challenge here is to construct individualized treatment rules (ITR), which recommend the best treatment for each of the different categories of individuals. In general, ITRs can be constructed using data from clinical trials, however these are generally very costly to run. In order to reduce the cost of learning an ITR, we explore active learning techniques designed to carefully decide whom to recruit, and which treatment to assign, throughout the online conduct of the clinical trial. As an initial investigation, we focus on simple ITRs that utilize a small number of subpopulation categories to personalize treatment. To minimize the maximal uncertainty regarding the treatment effects for each subpopulation, we propose the use of a minimax bandit model and provide an active learning policy for solving it. We evaluate our active learning policy using simulated data and data modeled after a clinical trial involving treatments for depressed individuals. We contrast this policy with other plausible active learning policies. The techniques presented in the paper may be generalized to tackle problems of efficient exploration in other domains.",,,,"Department of Statistics, University of Michigan, USA",,Author4,Author1,a,a,a,a,a,a,Healthcare,,,,a,a,2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967348,,K. Deng; J. Pineau; S. Murphy,,1,,10.1109/ADPRL.2011.5967348,IEEE Xplore,1,,20110728,39,,Clinical trials;Learning systems;Loss measurement;Machine learning;Recruitment;Resource management;Uncertainty,learning (artificial intelligence);medical computing;minimax techniques;patient treatment,active learning;clinical research;genetic biomarkers;individualized treatment rules;minimax bandit model;risk category;treatment personalization,,2325-1824;23251824,,11-15 April 2011,,,,,,,,,,,,IEEE,32,,Electronic:978-1-4244-9888-8; POD:978-1-4244-9887-1,,32,IEEE Conferences,,,,,2011,2011,1,0,3,Author2
59,0,1,0,0,A markov-based decision process model for wireless access agent,"The Personal Router is a mobile personal user agent whose task is to dynamically model the user, update its knowledge of a market of wireless service providers and select providers that satisfies the user's expected preferences. In this paper, we show how the user modeling problem can be represented as a Markov-based Decision Process (MDP) and suggest reinforcement learning and collaborative filtering as two candidate solution mechanisms for the information problem in the user modeling. © 2009 Binary Information Press.","The Personal Router is a mobile personal user agent whose task is to dynamically model the user, update its knowledge of a market of wireless service providers and select providers that satisfies the user's expected preferences. In this paper, we show how the user modeling problem can be represented as a Markov-based Decision Process (MDP) and suggest reinforcement learning and collaborative filtering as two candidate solution mechanisms for the information problem in the user modeling. © 2009 Binary Information Press.",,"Business School, Renmin University of China, Beijing 100872, China",,,Agent; Markov-based decision process; Personal router; Wireless access,Author1,Author2,a,a,a,a,a,a,Recommender Systems,,,,a,a,Journal of Information and Computational Science,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66149128446&partnerID=40&md5=ad9493ff0bfd400ce2d780a8a277aa4f,Chen H.,"Chen, H., Business School, Renmin University of China, Beijing 100872, China",,,,SCOPUS,0,,,,,,,,,,1,,,,2-s2.0-66149128446,,,,,422,415,,,,,Scopus,,,,Article,,,6,,2009,2009,3,1,0,Author2
60,0,0,0,0,Emergency Navigation in Confined Spaces Using Dynamic Grouping,"The performance of Emergency Management Systems (EMS) in confined spaces is highly dependent on the decision algorithm employed for the safe navigation of the evacuees to the available exits. In the algorithm proposed in this paper, we have considered evacuees under two groups, based on their age and physical condition, and we tailor two routing metrics, one for each group, in finding suitable paths for the evacuees. A dynamic grouping mechanism that can adjust an evacuee's group, and therefore routing metric, according to its on-going health condition is employed during the evacuation. To implement the routing metrics, we have used the Cognitive Packet Network (CPN) with random neural networks (RNN) and reinforcement learning. The CPN is an adaptive routing protocol that is loop-free at all times and easily handles multiple quality of service (QoS) metrics. Simulation results show that allowing the navigation system to be sensitive to the on-going health conditions and mobility of the evacuees, using our proposed dynamic grouping, can achieve higher survival rates.","The performance of Emergency Management Systems (EMS) in confined spaces is highly dependent on the decision algorithm employed for the safe navigation of the evacuees to the available exits. In the algorithm proposed in this paper, we have considered evacuees under two groups, based on their age and physical condition, and we tailor two routing metrics, one for each group, in finding suitable paths for the evacuees. A dynamic grouping mechanism that can adjust an evacuee's group, and therefore routing metric, according to its on-going health condition is employed during the evacuation. To implement the routing metrics, we have used the Cognitive Packet Network (CPN) with random neural networks (RNN) and reinforcement learning. The CPN is an adaptive routing protocol that is loop-free at all times and easily handles multiple quality of service (QoS) metrics. Simulation results show that allowing the navigation system to be sensitive to the on-going health conditions and mobility of the evacuees, using our proposed dynamic grouping, can achieve higher survival rates.",,,,"Dept. of Electr. & Electron. Eng., Imperial Coll. London, London, UK",Cognitive Packet Network;Dynamic Grouping;Emergency navigation;QoS driven protocol,Author2,Author3,a,a,u,a,a,a,"Other (please in ""remark"")",d,Dynamic grouping,"From the abstract it seems that RL is not used to achieve personalization.
domain: path planning (in rescue)",u,a,"2015 9th International Conference on Next Generation Mobile Applications, Services and Technologies",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7373229,,H. Bi; O. J. Akinwande; E. Gelenbe,,,,10.1109/NGMAST.2015.12,IEEE Xplore,1,,20160107,125,,Buildings;Hazards;Heuristic algorithms;Measurement;Navigation;Quality of service;Routing,cognitive radio;emergency management;learning (artificial intelligence);navigation;neural nets;packet radio networks;quality of service;routing protocols;safety,CPN;EMS;QoS metrics;RNN;adaptive routing protocol;cognitive packet network;confined spaces;decision algorithm;dynamic grouping mechanism;emergency management systems;emergency navigation;health conditions;navigation safety;navigation system;physical condition;quality of service metrics;random neural networks;reinforcement learning;routing metrics,,,,9-11 Sept. 2015,,,,,,,,,,,,IEEE,22,,CD-ROM:978-1-4799-8659-0; Electronic:978-1-4799-8660-6; POD:978-1-4799-8661-3,,120,IEEE Conferences,,,,,,2015,4,0,0,Author4
61,0,0,0,0,Application of reinforcement learning to admission control in CDMA network,The paper describes an admission control algorithm for the CDMA networks which is able to adapt to the operating environment. The algorithm is based on the principle of reinforcement learning and it achieves near-optimal performance for various radio propagation conditions and network operator's objectives. The performance evaluation results for different state space alternatives and algorithm parameters are presented and compared with the conventional admission control based on the power thresholds,The paper describes an admission control algorithm for the CDMA networks which is able to adapt to the operating environment. The algorithm is based on the principle of reinforcement learning and it achieves near-optimal performance for various radio propagation conditions and network operator's objectives. The performance evaluation results for different state space alternatives and algorithm parameters are presented and compared with the conventional admission control based on the power thresholds,,,,"Commun. Lab., Helsinki Univ. of Technol., Espoo, Finland",,Author4,Author1,a,a,a,a,a,a,"Other (please in ""remark"")",,Network control,,a,a,11th IEEE International Symposium on Personal Indoor and Mobile Radio Communications. PIMRC 2000. Proceedings (Cat. No.00TH8525),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=881639,,B. Makarevitch,,3,,10.1109/PIMRC.2000.881639,IEEE Xplore,1,,20020806,1357 vol.2,,Admission control;Base stations;Communication system control;Degradation;Downlink;Intelligent networks;Interference;Learning;Multiaccess communication;Paper technology,Markov processes;cellular radio;code division multiple access;learning (artificial intelligence);multiuser channels;radio networks;telecommunication congestion control,CDMA network;Markov decision process;admission control algorithm;algorithm parameters;cellular radio;near-optimal performance;network operator objectives;performance evaluation results;power thresholds;radio propagation conditions;reinforcement learning;state space alternatives,,,,2000,,,,18 Sep 2000-21 Sep 2000,,,,,,,1,IEEE,4,,POD:0-7803-6463-5,,1353,IEEE Conferences,,,2,,2000,2000,4,0,0,Author1
62,2,2,1,2,Adaptive Learning Based on Exercises Fitness Degree,"The paper considers the e-learning systems that provide personalized content to their users and that permanently adapt to the evolution of the users during their learning stages. Such systems help the students to consolidate their knowledge faster than other methods. The main contribution is the proposal of a mathematical model of an adaptive learning system with the mentioned characteristics. The model involves a multi step process where, at each stage, the performances of the student are measured and the system is adapting accordingly.","The paper considers the e-learning systems that provide personalized content to their users and that permanently adapt to the evolution of the users during their learning stages. Such systems help the students to consolidate their knowledge faster than other methods. The main contribution is the proposal of a mathematical model of an adaptive learning system with the mentioned characteristics. The model involves a multi step process where, at each stage, the performances of the student are measured and the system is adapting accordingly.",,,,,adaptive control;adaptive learning environments;learning systems;personalized learning;reinforcement learning,Author1,Author2,a,a,a,a,a,a,Recommender Systems,,,,a,a,2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5284965,,A. M. Mirea; M. C. Preda,,0,,10.1109/WI-IAT.2009.266,IEEE Xplore,1,,20091009,218,,Adaptive systems;Computer science;Electronic learning;Intelligent agent;Learning systems;Mathematical model;Paper technology;Performance evaluation;Proposals;Testing,,,,,,15-18 Sept. 2009,,,,,,,,,,,,IEEE,5,,POD:978-0-7695-3801-3,,215,IEEE Conferences,,,3,,2009,2009,0,1,3,Author4
63,1,0,2,1,Personalized Web recommendations: supporting epistemic information about end-users,The online recommendations are a popular presence in the Web sites world due to their potential to increase the customers' satisfaction. The ability to represent epistemic information about the clients' beliefs is important to understand their needs. This paper presents a recommender system based on reinforcement learning. The system represents concepts presented on a Web site by epistemic logical programs and uses a similarity measure between programs in order to facilitate generalization. A prototype of this system and experiments are presented.,The online recommendations are a popular presence in the Web sites world due to their potential to increase the customers' satisfaction. The ability to represent epistemic information about the clients' beliefs is important to understand their needs. This paper presents a recommender system based on reinforcement learning. The system represents concepts presented on a Web site by epistemic logical programs and uses a similarity measure between programs in order to facilitate generalization. A prototype of this system and experiments are presented.,,,,"Dept. of Comput. Sci., Craiova Univ., Romania",,Author4,Author1,a,a,a,a,a,a,Recommender Systems,,,,a,a,The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1517935,,M. Preda; D. Popescu,,0,,10.1109/WI.2005.115,IEEE Xplore,1,,20051017,695,,Automation;Computer science;Feedback;Function approximation;Humans;Learning;Logic;Ontologies;Prototypes;Recommender systems,Web sites;customer satisfaction;information filters;learning (artificial intelligence);logic programming,Web site;customer satisfaction;end-user;epistemic logical program;online recommendation;personalized Web recommendation;program similarity measure;reinforcement learning,,,,19-22 Sept. 2005,,,,,,,,,,,,IEEE,8,,POD:0-7695-2415-X,,692,IEEE Conferences,,,,,2005,2005,1,2,1,Author3
64,0,0,0,0,Reducing Delay during Vertical Handover,"The next generation of wireless networks will provide heterogeneous wireless technologies to a mobile user, in which they can move using terminals with multiple access interfaces and non-real-time or real-time services. The most important issue in such environment is the Always Best Connected (ABC) concept allowing the best connectivity to applications anywhere at any time. With the growing of communication world lot of research is carried out in the field of wireless networks specifically next generation networks. The integration and interoperability of various wireless networks will lead to number of challenges. One of the challenges is the handovers across various/heterogeneous wireless networks as well as reduction of call drop probability. Number of researches has proposed solutions for this challenge. This paper proposes a solution to this problem for the improvement of end-to-end Quality of Service supporting high data rates, real time transmission over wide area and enabling users to specify their personal preferences using Markov Decision Process (MDP). The proposed method uses delay as its basic parameter to select a network during handovers. The selection of the network amongst the existing wireless network considers the ongoing application of the user during the handover. Through simulations we show that our algorithm reduces call drop probability and satisfies user's requirements.","The next generation of wireless networks will provide heterogeneous wireless technologies to a mobile user, in which they can move using terminals with multiple access interfaces and non-real-time or real-time services. The most important issue in such environment is the Always Best Connected (ABC) concept allowing the best connectivity to applications anywhere at any time. With the growing of communication world lot of research is carried out in the field of wireless networks specifically next generation networks. The integration and interoperability of various wireless networks will lead to number of challenges. One of the challenges is the handovers across various/heterogeneous wireless networks as well as reduction of call drop probability. Number of researches has proposed solutions for this challenge. This paper proposes a solution to this problem for the improvement of end-to-end Quality of Service supporting high data rates, real time transmission over wide area and enabling users to specify their personal preferences using Markov Decision Process (MDP). The proposed method uses delay as its basic parameter to select a network during handovers. The selection of the network amongst the existing wireless network considers the ongoing application of the user during the handover. Through simulations we show that our algorithm reduces call drop probability and satisfies user's requirements.",,,,"Dept. of Electron. Eng., Pad. Dr. D.Y.P.I.E.T., Pune, India",Markov Decision process;Reinforcement Learning;Reward;Transition Probability;Vertical Handover,Author1,Author2,a,a,a,a,a,a,Interfaces / HCI,,,,a,a,2015 International Conference on Computing Communication Control and Automation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155834,,N. Bagdure; B. Ambudkar,,0,,10.1109/ICCUBEA.2015.44,IEEE Xplore,1,,20150716,204,,Bandwidth;Delays;Handover;Mobile communication;Quality of service;Wireless networks,delays;mobility management (mobile radio);next generation networks;probability;quality of service,call drop probability reduction;delay reduction;end-to-end quality of service;handover;heterogeneous wireless networks;mobile user;next generation networks;wireless network integration;wireless network interoperability,,,,26-27 Feb. 2015,,,,,,,,,,,,IEEE,13,,Electronic:978-1-4799-6892-3; POD:978-1-4799-6893-0,,200,IEEE Conferences,,,,,2015,2015,4,0,0,Author2
65,2,2,0,2,A reinforcement learning approach to weaning of mechanical ventilation in intensive care units,"The management of invasive mechanical ventilation, and the regulation of sedation and analgesia during ventilation, constitutes a major part of the care of patients admitted to intensive care units. Both prolonged dependence on mechanical ventilation and premature extubation are associated with increased risk of complications and higher hospital costs, but clinical opinion on the best protocol for weaning patients off of a ventilator varies. This work aims to develop a decision support tool that uses available patient information to predict time-to-extubation readiness and to recommend a personalized regime of sedation dosage and ventilator support. To this end, we use off-policy reinforcement learning algorithms to determine the best action at a given patient state from sub-optimal historical ICU data. We compare treatment policies from fitted Qiteration with extremely randomized trees and with feedforward neural networks, and demonstrate that the policies learnt show promise in recommending weaning protocols with improved outcomes, in terms of minimizing rates of reintubation and regulating physiological stability.","The management of invasive mechanical ventilation, and the regulation of sedation and analgesia during ventilation, constitutes a major part of the care of patients admitted to intensive care units. Both prolonged dependence on mechanical ventilation and premature extubation are associated with increased risk of complications and higher hospital costs, but clinical opinion on the best protocol for weaning patients off of a ventilator varies. This work aims to develop a decision support tool that uses available patient information to predict time-to-extubation readiness and to recommend a personalized regime of sedation dosage and ventilator support. To this end, we use off-policy reinforcement learning algorithms to determine the best action at a given patient state from sub-optimal historical ICU data. We compare treatment policies from fitted Qiteration with extremely randomized trees and with feedforward neural networks, and demonstrate that the policies learnt show promise in recommending weaning protocols with improved outcomes, in terms of minimizing rates of reintubation and regulating physiological stability.",,"Computer Science, Princeton University, United States; Electrical Engineering, Princeton University, United States; Penn Medicine, United States",,,,Author2,Author3,a,a,a,a,a,a,Healthcare,a,,,a,a,"Uncertainty in Artificial Intelligence - Proceedings of the 33rd Conference, UAI 2017",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031120880&partnerID=40&md5=e33338ea1b60efc1a559dcfffb00b4f9,"Prasad N., Cheng L.-F., Chivers C., Draugelis M., Engelhardt B.E.","Prasad, N., Computer Science, Princeton University, United States; Cheng, L.-F., Electrical Engineering, Princeton University, United States; Chivers, C., Penn Medicine, United States; Draugelis, M., Penn Medicine, United States; Engelhardt, B.E., Computer Science, Princeton University, United States",,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-85031120880,,,,,,,,,,,Scopus,,,,Conference Paper,,,,,2017,2017,1,0,3,Author2
66,2,0,1,1,Smart Lifelong Learning System Based on Q-Learning,"The majority of current web-based learning systems are closed learning environments where courses and learning materials are fixed and the only dynamic aspect is the organization of the material that can be adapted to allow a relatively individualized learning environment. In this paper, we propose an evolving web-based learning system which can adapt itself to its learners. More specifically, the novelty with respect to the system lies in its ability to find relevant content on the web, and its ability to personalize and adapt this content based on the system's observation of its learners and the accumulated ratings given by the learners. Hence, although learners do not have direct interaction with the open Web, the system can retrieve relevant information related to them and their situated learning characteristics. Lifelong learning scenarios have particular differences in their need for personalized recommendations that make not possible reusing existing general approaches of recommender systems. The paper describes those challenges and we propose a hybrid technique based on machine learning to recognize learner preferences and predict theirs required contents with high accuracy.","The majority of current web-based learning systems are closed learning environments where courses and learning materials are fixed and the only dynamic aspect is the organization of the material that can be adapted to allow a relatively individualized learning environment. In this paper, we propose an evolving web-based learning system which can adapt itself to its learners. More specifically, the novelty with respect to the system lies in its ability to find relevant content on the web, and its ability to personalize and adapt this content based on the system's observation of its learners and the accumulated ratings given by the learners. Hence, although learners do not have direct interaction with the open Web, the system can retrieve relevant information related to them and their situated learning characteristics. Lifelong learning scenarios have particular differences in their need for personalized recommendations that make not possible reusing existing general approaches of recommender systems. The paper describes those challenges and we propose a hybrid technique based on machine learning to recognize learner preferences and predict theirs required contents with high accuracy.",,,,"Adv. E-Learning Technol. Lab., AmirKabir Univ. of Technol., Tehran, Iran",Learning Promotion;Lifelong Learning;Machine Learning;Q Learning;Recommender Systems;Reinforcement Learning,Author4,Author1,a,a,a,a,a,a,Intelligent Tutors,,,,a,a,2010 Seventh International Conference on Information Technology: New Generations,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501486,,A. A. Kardan; O. R. B. Speily,,0,,10.1109/ITNG.2010.140,IEEE Xplore,1,,20100701,1091,,Adaptive systems;Electronic learning;Information filtering;Information retrieval;Information technology;Learning systems;Least squares approximation;Machine learning;Multitasking;Recommender systems,Internet;computer aided instruction;continuing professional development;information filters;learning (artificial intelligence),Q-Learning;Web-based learning systems;learning materials;machine learning;personalized recommendations;recommender systems;smart lifelong learning system,,,,12-14 April 2010,,,,,,,,,,,,IEEE,12,,Electronic:978-1-4244-6271-1; POD:978-1-4244-6270-4,,1086,IEEE Conferences,,,,,2010,2010,1,2,1,Author1
67,2,1,0,1,Genomics and artificial intelligence working together in drug discovery and repositioning: The advent of adaptive pharmacogenomics in glioblastoma and chronic arterial inflammation therapies,"The field of pharmacogenomics investigates how genomics may modulate pathological trends using information on both genotype and phenotype, with the aim of designing personalised healthcare. Homoeostasis is partially regulated through the expression of core protein groups whose functionality is determined at gene level and modulated by environmental factors. Harmful changes in physiology may promote several dis-functionalities. In prior work gene expression was used as a biomarker to assess both pathological propensity and disease progression. A growing body of pharmacogenomics research has developed new compounds, on one hand, and on the other, it has proposed novel therapeutic applications for the existing ones. Over the past decades, collective efforts have significantly increased the number of omics information available. However, efficient and deterministic in silico mechanisms that efficiently analyse and detect trends on the basis of often unknown and limited physiological information responding to challenging clinical questions are still lacking. In this context, computational automation via artificial intelligence methodologies has proven to be accurate, robust to noise, cost efficient, and dynamic dealing with massive databases and forecasting on the basis of the available information. Moreover, this set of computational techniques, based on well-established mathematical models, provide efficient ways of determining trends based on both a priori knowledge and dynamically acquired information, working successfully on incomplete datasets. Therefore, in this chapter we assess developmental similarities between two major causes of worldwide death: glioblastoma and chronic arterial inflammation; and discuss the potential applicability of two artificial intelligence approaches for drug discovery and repositioning. According to the World Health Organization (WHO) a glioblastoma multiform is the most malignant glial-type tumour (graded level IV in the WHO scale); and inflammatory diseases affecting the cardiovascular network are the cause of high mortality. As suggested, these two pathologies have several developmental similarities and share common genetic variants. Therefore, we additionally seek to discuss the main promoters presented in the current literature, aiming at benefiting from their similarities in drug discovery and repositioning, via automatic artificial intelligence pattern recognition, forecasting, and computational design. © Springer International Publishing AG 2017.","The field of pharmacogenomics investigates how genomics may modulate pathological trends using information on both genotype and phenotype, with the aim of designing personalised healthcare. Homoeostasis is partially regulated through the expression of core protein groups whose functionality is determined at gene level and modulated by environmental factors. Harmful changes in physiology may promote several dis-functionalities. In prior work gene expression was used as a biomarker to assess both pathological propensity and disease progression. A growing body of pharmacogenomics research has developed new compounds, on one hand, and on the other, it has proposed novel therapeutic applications for the existing ones. Over the past decades, collective efforts have significantly increased the number of omics information available. However, efficient and deterministic in silico mechanisms that efficiently analyse and detect trends on the basis of often unknown and limited physiological information responding to challenging clinical questions are still lacking. In this context, computational automation via artificial intelligence methodologies has proven to be accurate, robust to noise, cost efficient, and dynamic dealing with massive databases and forecasting on the basis of the available information. Moreover, this set of computational techniques, based on well-established mathematical models, provide efficient ways of determining trends based on both a priori knowledge and dynamically acquired information, working successfully on incomplete datasets. Therefore, in this chapter we assess developmental similarities between two major causes of worldwide death: glioblastoma and chronic arterial inflammation; and discuss the potential applicability of two artificial intelligence approaches for drug discovery and repositioning. According to the World Health Organization (WHO) a glioblastoma multiform is the most malignant glial-type tumour (graded level IV in the WHO scale); and inflammatory diseases affecting the cardiovascular network are the cause of high mortality. As suggested, these two pathologies have several developmental similarities and share common genetic variants. Therefore, we additionally seek to discuss the main promoters presented in the current literature, aiming at benefiting from their similarities in drug discovery and repositioning, via automatic artificial intelligence pattern recognition, forecasting, and computational design. © Springer International Publishing AG 2017.",,"Biotechnology, Icelandic Institute for Intelligent Machines, Reykjavik, Iceland; Department of Computer Sciences, Polytechnic Institute, University Autonoma of Madrid, Madrid, Spain; Department of Bioengineering, Imperial College London, London, United Kingdom",,,Adaptive pharmacogenomics; Artificial intelligence; Chronic arterial inflammation; Cytokines; Deep neural networks; Drug discovery; Drug repositioning; Genetic fingerprints; Genomics; Glioblastoma; Inflammatory signalling cascades; Reinforcement learning; Transcription factors,Author3,Author4,a,a,a,a,u,a,,,"This is a book chapter which seems not to be a bundle of separate papers, e.g. this is not peer-reviewed.","I think we need to decide on how to proceed with such books, because they could be peer reviewed.",u,a,Biotechnology and Production of Anti-Cancer Compounds,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034400377&doi=10.1007%2f978-3-319-53880-8_11&partnerID=40&md5=fde62df000191d3078169ff2d8da4bd5,Pereira G.C.,"Pereira, G.C., Biotechnology, Icelandic Institute for Intelligent Machines, Reykjavik, Iceland, Department of Computer Sciences, Polytechnic Institute, University Autonoma of Madrid, Madrid, Spain, Department of Bioengineering, Imperial College London, London, United Kingdom",,,10.1007/978-3-319-53880-8_11,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85034400377,,,,,281,253,,,,,Scopus,,,,Book Chapter,,,,,,2017,1,2,1,Author1
68,0,0,1,0,A learning strategy for paging in mobile environments,"The essence of designing a good paging strategy is to incorporate user mobility characteristics in a predictive mechanism that reduces the average paging cost with as little computational effort as possible. We introduce a novel paging scheme based on the concept of reinforcement learning. Learning endows the paging mechanism with the predictive power necessary to determine a mobile terminal's position, without having to extract a location probability distribution for each specific user. The proposed algorithm is compared against a heuristic randomized learning strategy akin to reinforcement learning, that we invented for this purpose and performs better than the case where no learning is used at all. It is shown that if the user normally moves only among a fraction of cells in the location area, significant savings can be achieved over the randomized strategy, without excessive time to train the network.","The essence of designing a good paging strategy is to incorporate user mobility characteristics in a predictive mechanism that reduces the average paging cost with as little computational effort as possible. We introduce a novel paging scheme based on the concept of reinforcement learning. Learning endows the paging mechanism with the predictive power necessary to determine a mobile terminal's position, without having to extract a location probability distribution for each specific user. The proposed algorithm is compared against a heuristic randomized learning strategy akin to reinforcement learning, that we invented for this purpose and performs better than the case where no learning is used at all. It is shown that if the user normally moves only among a fraction of cells in the location area, significant savings can be achieved over the randomized strategy, without excessive time to train the network.",,,,"Sch. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Greece",,Author4,Author1,a,a,a,a,a,a,"Other (please in ""remark"")",,"Uses user chars to provide more efficient routing, is that personalization?",,a,a,2003 5th European Personal Mobile Communications Conference (Conf. Publ. No. 492),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1350260,,I. Koukoutsidis; P. Demestichas; M. Theologou,,0,,10.1049/cp:20030322,IEEE Xplore,1,,20041101,590,,,cellular radio;learning (artificial intelligence);paging communication;software agents;telecommunication computing,heuristic randomized learning strategy;intelligent agent;learning strategy;location probability distribution;mobile environments;predictive mechanism;reinforcement learning;terminal paging;user mobility characteristics,,0537-9989;05379989,,22-25 April 2003,,,,,,,,,,,,IET,,,Paper:0-85296-753-5,,585,IET Conferences,,,,,2003,2003,3,1,0,Author3
69,2,2,0,2,On-line policy learning and adaptation for real-time personalization of an artificial pancreas,"The dynamic complexity of the glucose-insulin metabolism in diabetic patients is the main obstacle towards widespread use of an artificial pancreas. The significant level of subject-specific glycemic variability requires continuously adapting the control policy to successfully face daily changes in patient's metabolism and lifestyle. In this paper, an on-line selective reinforcement learning algorithm that enables real-time adaptation of a control policy based on ongoing interactions with the patient so as to tailor the artificial pancreas is proposed. Adaptation includes two online procedures: on-line sparsification and parameter updating of the Gaussian process used to approximate the control policy. With the proposed sparsification method, the support data dictionary for on-line learning is modified by checking if in the arriving data stream there exists novel information to be added to the dictionary in order to personalize the policy. Results obtained in silico experiments demonstrate that on-line policy learning is both safe and efficient for maintaining blood glucose variability within the normoglycemic range. © 2014 Elsevier Ltd. All rights reserved.","The dynamic complexity of the glucose-insulin metabolism in diabetic patients is the main obstacle towards widespread use of an artificial pancreas. The significant level of subject-specific glycemic variability requires continuously adapting the control policy to successfully face daily changes in patient's metabolism and lifestyle. In this paper, an on-line selective reinforcement learning algorithm that enables real-time adaptation of a control policy based on ongoing interactions with the patient so as to tailor the artificial pancreas is proposed. Adaptation includes two online procedures: on-line sparsification and parameter updating of the Gaussian process used to approximate the control policy. With the proposed sparsification method, the support data dictionary for on-line learning is modified by checking if in the arriving data stream there exists novel information to be added to the dictionary in order to personalize the policy. Results obtained in silico experiments demonstrate that on-line policy learning is both safe and efficient for maintaining blood glucose variability within the normoglycemic range. © 2014 Elsevier Ltd. All rights reserved.",,"INTELYMEC, Centro de Investigaciones en Física e Ingeniería del Centro - CIFICEN, CONICET, Av. del Valle 5737, Olavarría, Argentina; UNCPBA-Facultad de Ingeniería, Universidad Nacional del Centro de la Provincia de Buenos Aires, Av. del Valle 5737, Olavarría, Argentina; INGAR (CONICET-UTN), Avellaneda 3657, Santa Fe, Argentina",,,Diabetes; Gaussian processes; Glycemic variability; On-line sparsification; Policy learning; Reinforcement learning,Author3,Author4,a,a,a,a,a,a,Healthcare,a,,,a,a,Expert Systems with Applications,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910646834&doi=10.1016%2fj.eswa.2014.10.038&partnerID=40&md5=0b2a6928c88bfd78c305b765e1fa2e2b,"De Paula M., Acosta G.G., Martínez E.C.","De Paula, M., INTELYMEC, Centro de Investigaciones en Física e Ingeniería del Centro - CIFICEN, CONICET, Av. del Valle 5737, Olavarría, Argentina; Acosta, G.G., UNCPBA-Facultad de Ingeniería, Universidad Nacional del Centro de la Provincia de Buenos Aires, Av. del Valle 5737, Olavarría, Argentina; Martínez, E.C., INGAR (CONICET-UTN), Avellaneda 3657, Santa Fe, Argentina",4,,10.1016/j.eswa.2014.10.038,SCOPUS,1,,,,,,,,,,4,,,,2-s2.0-84910646834,,,,,2255,2234,,,,,Scopus,,,,Article,,,42,,2015,2015,1,0,3,Author4
70,2,1,2,2,Contextual recommender problems,"The contextual recommender task is the problem of making useful offers, e.g., placing ads or related links on a web page, based on the context information, e.g., contents of the page and information about the user visiting, and information on the available alternatives, i.e., the advertisements or relevant links. In the case of ads for example, the goal is to select ads that result in high click rates, where the (ad) click rate is some unknown function of the attributes of the context and ad. We describe the task and make connections to related problems including recommender and multi-armed bandit problems. Copyright 2005 ACM.","The contextual recommender task is the problem of making useful offers, e.g., placing ads or related links on a web page, based on the context information, e.g., contents of the page and information about the user visiting, and information on the available alternatives, i.e., the advertisements or relevant links. In the case of ads for example, the goal is to select ads that result in high click rates, where the (ad) click rate is some unknown function of the attributes of the context and ad. We describe the task and make connections to related problems including recommender and multi-armed bandit problems. Copyright 2005 ACM.",,"Yahoo Research, 74 N. Pasadena Ave, Pasadena, CA 91103, United States",,,data mining; exploration-exploitation; multi-armed bandit; personalization; recommenders; regression; reinforcement learning; utility,Author1,Author2,a,a,u,a,a,a,Recommender Systems,,,,u,a,"Proceedings of the 1st International Workshop on Utility-Based Data Mining, UBDM '05",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751025354&doi=10.1145%2f1089827.1089838&partnerID=40&md5=02bb7c496352dcb4e276b940c4c3ea5f,"Madani O., DeCoste D.","Madani, O., Yahoo Research, 74 N. Pasadena Ave, Pasadena, CA 91103, United States; DeCoste, D., Yahoo Research, 74 N. Pasadena Ave, Pasadena, CA 91103, United States",9,,10.1145/1089827.1089838,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-33751025354,,,,,89,86,,,,,Scopus,,,,Conference Paper,,,,,,2005,0,1,3,Author4
71,2,1,2,2,Ensemble contextual bandits for personalized recommendation,"The cold-start problem has attracted extensive attention among various online services that provide personalized recommendation. Many online vendors employ contextual bandit strategies to tackle the so-called exploration/exploitation dilemma rooted from the cold-start problem. However, due to high-dimensional user/item features and the underlying characteristics of bandit policies, it is often difficult for service providers to obtain and deploy an appropriate algorithm to achieve acceptable and robust economic profit. In this paper, we explore ensemble strategies of contextual bandit algorithms to obtain robust predicted click-through rate (CTR) of web objects. The ensemble is acquired by aggregating different pulling policies of bandit algorithms, rather than forcing the agreement of prediction results or learning a unified predictive model. To this end, we employ a meta-bandit paradigm that places a hyper bandit over the base bandits, to explicitly explore/exploit the relative importance of base bandits based on user feedbacks. Extensive empirical experiments on two real-world data sets (news recommendation and online advertising) demonstrate the effectiveness of our proposed approach in terms of CTR. Copyright © 2014 ACM.","The cold-start problem has attracted extensive attention among various online services that provide personalized recommendation. Many online vendors employ contextual bandit strategies to tackle the so-called exploration/exploitation dilemma rooted from the cold-start problem. However, due to high-dimensional user/item features and the underlying characteristics of bandit policies, it is often difficult for service providers to obtain and deploy an appropriate algorithm to achieve acceptable and robust economic profit. In this paper, we explore ensemble strategies of contextual bandit algorithms to obtain robust predicted click-through rate (CTR) of web objects. The ensemble is acquired by aggregating different pulling policies of bandit algorithms, rather than forcing the agreement of prediction results or learning a unified predictive model. To this end, we employ a meta-bandit paradigm that places a hyper bandit over the base bandits, to explicitly explore/exploit the relative importance of base bandits based on user feedbacks. Extensive empirical experiments on two real-world data sets (news recommendation and online advertising) demonstrate the effectiveness of our proposed approach in terms of CTR. Copyright © 2014 ACM.",,"School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States",,,Contextual bandit; CTR prediction; Ensemble recommendation; Meta learning; Personalized recommendation,Author2,Author3,a,a,u,a,a,a,Recommender Systems,a,,"I think we can accept for now, based on 'personalization' being in the title",u,a,RecSys 2014 - Proceedings of the 8th ACM Conference on Recommender Systems,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908877655&doi=10.1145%2f2645710.2645732&partnerID=40&md5=ba570d5a0e504e972c6b955345f4d614,"Tang L., Jiang Y., Li L., Li T.","Tang, L., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Jiang, Y., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Li, L., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Li, T., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States",17,,10.1145/2645710.2645732,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84908877655,,,,,80,73,,,,,Scopus,,,,Conference Paper,,,,,,2014,0,1,3,Author4
72,1,0,2,1,Learning conversational systems that interleave task and non-task content,"Task-oriented dialog systems have been applied in various tasks, such as automated personal assistants, customer service providers and tutors. These systems work well when users have clear and explicit intentions that are well-aligned to the systems' capabilities. However, they fail if users intentions are not explicit. To address this shortcoming, we propose a framework to interleave nontask content (i.e. everyday social conversation) into task conversations. When the task content fails, the system can still keep the user engaged with the non-task content. We trained a policy using reinforcement learning algorithms to promote long-turn conversation coherence and consistency, so that the system can have smooth transitions between task and non-task content. To test the effectiveness of the proposed framework, we developed a movie promotion dialog system. Experiments with human users indicate that a system that interleaves social and task content achieves a better task success rate and is also rated as more engaging compared to a pure task-oriented system.","Task-oriented dialog systems have been applied in various tasks, such as automated personal assistants, customer service providers and tutors. These systems work well when users have clear and explicit intentions that are well-aligned to the systems' capabilities. However, they fail if users intentions are not explicit. To address this shortcoming, we propose a framework to interleave nontask content (i.e. everyday social conversation) into task conversations. When the task content fails, the system can still keep the user engaged with the non-task content. We trained a policy using reinforcement learning algorithms to promote long-turn conversation coherence and consistency, so that the system can have smooth transitions between task and non-task content. To test the effectiveness of the proposed framework, we developed a movie promotion dialog system. Experiments with human users indicate that a system that interleaves social and task content achieves a better task success rate and is also rated as more engaging compared to a pure task-oriented system.",,"Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA, United States",,,,Author1,Author2,a,a,u,a,a,a,Personal Assistants,,,,u,a,IJCAI International Joint Conference on Artificial Intelligence,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031939839&partnerID=40&md5=35012a2acb00bcb4b13740192de24e3f,"Yu Z., Black A.W., Rudnicky A.I.","Yu, Z., Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA, United States; Black, A.W., Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA, United States; Rudnicky, A.I., Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA, United States",,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-85031939839,,,,,4220,4214,,,,,Scopus,,,,Conference Paper,,,,,,2017,1,2,1,Author3
73,1,1,1,1,Closed-loop task difficulty adaptation during virtual reality reach-to-grasp training assisted with an exoskeleton for stroke rehabilitation,"Stroke patients with severe motor deficits of the upper extremity may practice rehabilitation exercises with the assistance of a multi-joint exoskeleton. Although this technology enables intensive task-oriented training, it may also lead to slacking when the assistance is too supportive. Preserving the engagement of the patients while providing ""assistance-as-needed"" during the exercises, therefore remains an ongoing challenge. We applied a commercially available seven degree-of-freedom arm exoskeleton to provide passive gravity compensation during task-oriented training in a virtual environment. During this 4-week pilot study, five severely affected chronic stroke patients performed reach-to-grasp exercises resembling activities of daily living. The subjects received virtual reality feedback from their three-dimensional movements. The level of difficulty for the exercise was adjusted by a performance-dependent real-time adaptation algorithm. The goal of this algorithm was the automated improvement of the range of motion. In the course of 20 training and feedback sessions, this unsupervised adaptive training concept led to a progressive increase of the virtual training space (p < 0.001) in accordance with the subjects' abilities. This learning curve was paralleled by a concurrent improvement of real world kinematic parameters, i.e., range of motion (p = 0.008), accuracy of movement (p = 0.01), and movement velocity (p < 0.001). Notably, these kinematic gains were paralleled by motor improvements such as increased elbow movement (p = 0.001), grip force (p < 0.001), and upper extremity Fugl-Meyer-Assessment score from 14.3 ï¿½ 5 to 16.9 ï¿½ 6.1 (p = 0.026). Combining gravity-compensating assistance with adaptive closed-loop feedback in virtual reality provides customized rehabilitation environments for severely affected stroke patients. This approach may facilitate motor learning by progressively challenging the subject in accordance with the individual capacity for functional restoration. It might be necessary to apply concurrent restorative interventions to translate these improvements into relevant functional gains of severely motor impaired patients in activities of daily living. ï¿½ 2016 Grimm, Naros and Gharabaghi.","Stroke patients with severe motor deficits of the upper extremity may practice rehabilitation exercises with the assistance of a multi-joint exoskeleton. Although this technology enables intensive task-oriented training, it may also lead to slacking when the assistance is too supportive. Preserving the engagement of the patients while providing ""assistance-as-needed"" during the exercises, therefore remains an ongoing challenge. We applied a commercially available seven degree-of-freedom arm exoskeleton to provide passive gravity compensation during task-oriented training in a virtual environment. During this 4-week pilot study, five severely affected chronic stroke patients performed reach-to-grasp exercises resembling activities of daily living. The subjects received virtual reality feedback from their three-dimensional movements. The level of difficulty for the exercise was adjusted by a performance-dependent real-time adaptation algorithm. The goal of this algorithm was the automated improvement of the range of motion. In the course of 20 training and feedback sessions, this unsupervised adaptive training concept led to a progressive increase of the virtual training space (p < 0.001) in accordance with the subjects' abilities. This learning curve was paralleled by a concurrent improvement of real world kinematic parameters, i.e., range of motion (p = 0.008), accuracy of movement (p = 0.01), and movement velocity (p < 0.001). Notably, these kinematic gains were paralleled by motor improvements such as increased elbow movement (p = 0.001), grip force (p < 0.001), and upper extremity Fugl-Meyer-Assessment score from 14.3 ï¿½ 5 to 16.9 ï¿½ 6.1 (p = 0.026). Combining gravity-compensating assistance with adaptive closed-loop feedback in virtual reality provides customized rehabilitation environments for severely affected stroke patients. This approach may facilitate motor learning by progressively challenging the subject in accordance with the individual capacity for functional restoration. It might be necessary to apply concurrent restorative interventions to translate these improvements into relevant functional gains of severely motor impaired patients in activities of daily living. ï¿½ 2016 Grimm, Naros and Gharabaghi.",,"Division of Functional and Restorative Neurosurgery, Centre for Integrative Neuroscience, Eberhard Karls University of Tuebingen, Tuebingen, Germany",518,,Hemiparesis; Individualized therapy; Motor recovery; Reinforcement learning; Robot-assisted rehabilitation; Robotic rehabilitation; Upper-limb assistance,Author3,Author4,a,a,a,a,a,a,Healthcare,a,"This work resides at the edge of healthcare and robotics, however, since robotics are used for rehabilitation, I think the domain should be Healthcare.",,a,a,Frontiers in Neuroscience,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009810403&doi=10.3389%2ffnins.2016.00518&partnerID=40&md5=81fb7c532c89357ddb73a579d4d5fd91,"Grimm F., Naros G., Gharabaghi A.","Grimm, F., Division of Functional and Restorative Neurosurgery, Centre for Integrative Neuroscience, Eberhard Karls University of Tuebingen, Tuebingen, Germany; Naros, G., Division of Functional and Restorative Neurosurgery, Centre for Integrative Neuroscience, Eberhard Karls University of Tuebingen, Tuebingen, Germany; Gharabaghi, A., Division of Functional and Restorative Neurosurgery, Centre for Integrative Neuroscience, Eberhard Karls University of Tuebingen, Tuebingen, Germany",8,,10.3389/fnins.2016.00518,SCOPUS,1,,,,,,,,,,NOV,,,,2-s2.0-85009810403,,,,,,,,,,,Scopus,,,,Article,,,10,,2016,2016,0,4,0,Author1
74,1,0,2,1,A unified learning paradigm for large-scale personalized information management,"Statistical-learning approaches such as unsupervised learning, supervised learning, active learning, and reinforcement learning have generally been separately studied and applied to solve application problems. In this paper, we provide an overview of our newly proposed unified learning paradigm (ULP), which combines these approaches into one synergistic framework. We outline the architecture and the algorithm of ULP, and explain benefits of employing this unified learning paradigm on personalizing information management.","Statistical-learning approaches such as unsupervised learning, supervised learning, active learning, and reinforcement learning have generally been separately studied and applied to solve application problems. In this paper, we provide an overview of our newly proposed unified learning paradigm (ULP), which combines these approaches into one synergistic framework. We outline the architecture and the algorithm of ULP, and explain benefits of employing this unified learning paradigm on personalizing information management.",,,,"Electr. & Comput. Eng., California Univ., Santa Barbara, CA, USA",,Author4,Author1,a,a,u,a,a,a,Personal Assistants,,,,u,a,"Conference, Emerging Information Technology 2005.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544372,,E. Y. Chang; S. C. H. Hop; Xinjing Wang; Wei-Ying Max; M. R. Lyu,,1,,10.1109/EITC.2005.1544372,IEEE Xplore,1,,20051205,,,Clustering algorithms;Convergence;Humans;Information management;Kernel;Large-scale systems;Machine learning;Stability;Supervised learning;Unsupervised learning,information management;learning (artificial intelligence),large-scale personalized information management;statistical-learning approach;unified learning paradigm,,,,15-16 Aug. 2005,,,,,,,,,,,,IEEE,9,,POD:0-7803-9328-7,,4 pp.,IEEE Conferences,,,,,,2005,1,2,1,Author3
75,0,0,0,0,OWLS: Observational wireless life-enhancing system,"Socially assistive robotics technologies for individuals, who have been affected by age-related disabilities and similar types of disorders, have become popular options for facilitating natural independence and uninterrupted mobility. Wireless wearable sensor systems enable proactive personal health management and the ubiquitous monitoring of vital signs to keep an active watch on immediate health conditions. In this paper, we develop a system, called OWLS, where multiple wearable sensors, software agents, robots and health analysis technology, have been integrated into a single personal therapy solution (SPTS). Our system uses a reinforcement learning algorithm to make decisions about the user's current health conditions, and to take appropriate actions, as necessary (i.e, contacting outside parties). We show that the approach of non-invasive monitoring, when combined with an alert system, makes this a desirable SPTS in future health care. Copyright © 2016, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.","Socially assistive robotics technologies for individuals, who have been affected by age-related disabilities and similar types of disorders, have become popular options for facilitating natural independence and uninterrupted mobility. Wireless wearable sensor systems enable proactive personal health management and the ubiquitous monitoring of vital signs to keep an active watch on immediate health conditions. In this paper, we develop a system, called OWLS, where multiple wearable sensors, software agents, robots and health analysis technology, have been integrated into a single personal therapy solution (SPTS). Our system uses a reinforcement learning algorithm to make decisions about the user's current health conditions, and to take appropriate actions, as necessary (i.e, contacting outside parties). We show that the approach of non-invasive monitoring, when combined with an alert system, makes this a desirable SPTS in future health care. Copyright © 2016, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",,"Allegheny College, 520 North Main Street, Allegheny, PA, United States",,,,Author4,Author1,a,a,u,a,a,a,Personal Assistants,,,,u,a,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014134480&partnerID=40&md5=26e364abd3feeb0a7316192d9b9e02f8,"Zheng H., Jumadinova J.","Zheng, H., Allegheny College, 520 North Main Street, Allegheny, PA, United States; Jumadinova, J., Allegheny College, 520 North Main Street, Allegheny, PA, United States",,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-85014134480,,,,,1426,1425,,,,,Scopus,,,,Conference Paper,,,,,,2016,4,0,0,Author4
76,0,0,0,0,Adaptive Behavior Generation for Child-Robot Interaction,"Social robots are increasingly applied in assistive settings where they interact with human users to support them in their daily life. There, abilities for a robust and reliable social interaction are required, especially for robots that interact autonomously with humans. Apart from challenges regarding safety and trust, the complexity and difficulty of attaining mutual understanding, engagement or assistance in social interactions that comprise spoken languages and non-verbal behaviors need to be taken into account. In addition, different users or user groups have inter-individual differences with respect to their personal preferences, skills and limitations. This makes it more difficult to develop reliable and understandable robots that work well in different situations or for different users. © 2018 Authors.","Social robots are increasingly applied in assistive settings where they interact with human users to support them in their daily life. There, abilities for a robust and reliable social interaction are required, especially for robots that interact autonomously with humans. Apart from challenges regarding safety and trust, the complexity and difficulty of attaining mutual understanding, engagement or assistance in social interactions that comprise spoken languages and non-verbal behaviors need to be taken into account. In addition, different users or user groups have inter-individual differences with respect to their personal preferences, skills and limitations. This makes it more difficult to develop reliable and understandable robots that work well in different situations or for different users. © 2018 Authors.",,"CITEC, Bielefeld University, Bielefeld, Germany",,,Assistive Robotics; Child-Robot-Interaction; Multimodal Social Behavior; Reinforcement Learning,Author1,Author2,a,a,a,a,a,a,Interfaces / HCI,,,,a,a,ACM/IEEE International Conference on Human-Robot Interaction,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045246276&doi=10.1145%2f3173386.3176916&partnerID=40&md5=5d60132774050efaff259e83b239d95d,"Hemminghaus J., Kopp S.","Hemminghaus, J., CITEC, Bielefeld University, Bielefeld, Germany; Kopp, S., CITEC, Bielefeld University, Bielefeld, Germany",,,10.1145/3173386.3176916,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85045246276,,,,,296,295,,,,,Scopus,,,,Conference Paper,,,,,2018,2018,4,0,0,Author1
77,1,1,1,1,Learning algorithms For intelligent agents based e-learning system,"Requirements are the basic entity which makes the project to be successfully implemented. In the development of the software, theses requirements must be measured accurately and correctly. The requirements of the end users may be changed with respect to different individual personality. As different requirements are given by different customers, all of them cannot be processed by single software system. In such a case, it is essential to make the changes in the software systems automatically which fulfills all the requirements of the user. Such condition is met by the systems with intelligent agents. In this paper, algorithms of intelligent agents adviser agent, personalization agent and content managing agent are proposed to automatically gather the requirements of the students and fulfill them. The algorithms are structured using reinforcement learning. Theses algorithm makes the agents to sense the needs of the students and evolve in the course of their operation. These intelligent agents are applied after the deployment of the e-learning software. All the algorithms have been implemented successfully.","Requirements are the basic entity which makes the project to be successfully implemented. In the development of the software, theses requirements must be measured accurately and correctly. The requirements of the end users may be changed with respect to different individual personality. As different requirements are given by different customers, all of them cannot be processed by single software system. In such a case, it is essential to make the changes in the software systems automatically which fulfills all the requirements of the user. Such condition is met by the systems with intelligent agents. In this paper, algorithms of intelligent agents adviser agent, personalization agent and content managing agent are proposed to automatically gather the requirements of the students and fulfill them. The algorithms are structured using reinforcement learning. Theses algorithm makes the agents to sense the needs of the students and evolve in the course of their operation. These intelligent agents are applied after the deployment of the e-learning software. All the algorithms have been implemented successfully.",,,,"Department of Computer Science & Engineering, Ajay Kumar Garg Engineering College, Ghaziabad. India",E-Learning;Intelligent Agent;Reinforcement Learnin;Requirement Engineering,Author3,Author4,a,a,a,a,a,a,,,The website for this years' conference indicates they have a peer review process and proceedings are published by IEEE. The writing in the abstract seems very poor though. The website also indicates that only Word document submissions are accepted and inquires are to a gmail address. Is IEEE + website mentioning peer reviews enough?,"I would include this one, if it is peer reviewed then we shouldn't start judging the quality ourselves.",a,a,2013 3rd IEEE International Advance Computing Conference (IACC),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514369,,N. Pandey; R. K. Tyagi; S. Sahu; A. Dwivedi,,1,,10.1109/IAdCC.2013.6514369,IEEE Xplore,1,,20130513,1039,,Databases;Electronic learning;Intelligent agents;Negative feedback;Software algorithms;Standards,computer aided instruction;learning (artificial intelligence);multi-agent systems,adviser agent;content managing agent;e-learning system;electronic learning;intelligent agent;learning algorithm;personalization agent;reinforcement learning;software development;student requirement;user requirement,,,,22-23 Feb. 2013,,,,,,,,,,,,IEEE,8,,Electronic:978-1-4673-4529-3; POD:978-1-4673-4527-9,,1034,IEEE Conferences,,,,,2013,2013,0,4,0,Author2
78,1,0,1,1,AgentX: Using Reinforcement Learning to Improve the Effectiveness of Intelligent Tutoring Systems,"Reinforcement Learning (RL) can be used to train an agent to comply with the needs of a student using an intelligent tutoring system. In this paper, we introduce a method of increasing efficiency by way of customization of the hints provided by a tutoring system, by applying techniques from RL to gain knowledge about the usefulness of hints leading to the exclusion or introduction of other helpful hints. Students are clustered into learning levels and can influence the agents method of selecting actions in each state in their cluster of affect. In addition, students can change learning levels based on their performance within the tutoring system and continue to affect the entire student population. The RL agent, AgentX, then uses the cluster information to create one optimal policy for all students in the cluster and begin to customize the help given to the cluster based on that optimal policy. © Springer-Verlag 2004 References.","Reinforcement Learning (RL) can be used to train an agent to comply with the needs of a student using an intelligent tutoring system. In this paper, we introduce a method of increasing efficiency by way of customization of the hints provided by a tutoring system, by applying techniques from RL to gain knowledge about the usefulness of hints leading to the exclusion or introduction of other helpful hints. Students are clustered into learning levels and can influence the agents method of selecting actions in each state in their cluster of affect. In addition, students can change learning levels based on their performance within the tutoring system and continue to affect the entire student population. The RL agent, AgentX, then uses the cluster information to create one optimal policy for all students in the cluster and begin to customize the help given to the cluster based on that optimal policy. © Springer-Verlag 2004 References.",,"Department of Computer Science, University of Massachusetts, 140 Governors Drive, Amherst, MA 01003, United States",,,,Author1,Author2,a,a,a,a,a,a,Intelligent Tutors,,,,a,a,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048874714&partnerID=40&md5=c80fd6957e1ff113fd93a7e0ef37fab6,"Martin K.N., Arroyo I.","Martin, K.N., Department of Computer Science, University of Massachusetts, 140 Governors Drive, Amherst, MA 01003, United States; Arroyo, I., Department of Computer Science, University of Massachusetts, 140 Governors Drive, Amherst, MA 01003, United States",17,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-35048874714,,,,,572,564,,,,,Scopus,,,,Article,,,3220,,2004,2004,1,3,0,Author4
79,2,1,2,2,Locality-sensitive linear bandit model for online social recommendation,"Recommender systems provide personalized suggestions by learning users’ preference based on their historical feedback. To alleviate the heavy relying on historical data, several online recommendation methods are recently proposed and have shown the effectiveness in solving data sparsity and cold start problems in recommender systems. However, existing online recommendation methods neglect the use of social connections among users, which has been proven as an effective way to improve recommendation accuracy in offline settings. In this paper, we investigate how to leverage social connections to improve online recommendation performance. In particular, we formulate the online social recommendation task as a contextual bandit problem and propose a Locality-sensitive Linear Bandit (LS.Lin) method to solve it. The proposed model incorporates users’ local social relations into a linear contextual bandit model and is capable to deal with the dynamic changes of user preference and the network structure. We provide a theoretical analysis to the proposed LS.Lin method and then demonstrate its improved performance for online social recommendation in empirical studies compared with baseline methods. © Springer International Publishing AG 2016.","Recommender systems provide personalized suggestions by learning users’ preference based on their historical feedback. To alleviate the heavy relying on historical data, several online recommendation methods are recently proposed and have shown the effectiveness in solving data sparsity and cold start problems in recommender systems. However, existing online recommendation methods neglect the use of social connections among users, which has been proven as an effective way to improve recommendation accuracy in offline settings. In this paper, we investigate how to leverage social connections to improve online recommendation performance. In particular, we formulate the online social recommendation task as a contextual bandit problem and propose a Locality-sensitive Linear Bandit (LS.Lin) method to solve it. The proposed model incorporates users’ local social relations into a linear contextual bandit model and is capable to deal with the dynamic changes of user preference and the network structure. We provide a theoretical analysis to the proposed LS.Lin method and then demonstrate its improved performance for online social recommendation in empirical studies compared with baseline methods. © Springer International Publishing AG 2016.",,"Shenzhen Key Laboratory of Rich Media Big Data Analytics and Applications, Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong",,,Linear bandits; Online learning; Social recommendation,Author3,Author4,a,a,a,a,a,a,Recommender Systems,a,,,a,a,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992623509&doi=10.1007%2f978-3-319-46687-3_9&partnerID=40&md5=9050adb209e548fd851f01c9385bc3ba,"Zhao T., King I.","Zhao, T., Shenzhen Key Laboratory of Rich Media Big Data Analytics and Applications, Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China, Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong; King, I., Shenzhen Key Laboratory of Rich Media Big Data Analytics and Applications, Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China, Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong",1,,10.1007/978-3-319-46687-3_9,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84992623509,,,,,90,80,,,,,Scopus,,,,Conference Paper,,,9947 LNCS,,2016,2016,0,1,3,Author4
80,2,1,2,2,Learning and adaptivity in interactive recommender systems,"Recommender systems are intelligent E-commerce applications that assist users in a decision-making process by offering personalized product recommendations during an interaction session. Quite recently, conversational approaches have been introduced in order to support more interactive recommendation sessions. Notwithstanding the increased interactivity offered by these approaches, the system employs an interaction strategy that is specified apriori (at design time) and followed quite rigidly during the interaction. In this paper, we present a new type of recommender system which is capable of learning autonomously an adaptive interaction strategy for assisting the users in acquiring their interaction goals. We view the recommendation process as a sequential decision problem and we model it as a Markov Decision Process (MDP). We learn a model of the user behavior, and use it to acquire the adaptive strategy using Reinforcement Learning (RL) techniques. In this context, the system learns the optimal strategy by observing the consequences of its actions on the users and also on the final outcome of the recommendation session. We apply our approach within an existing travel recommender system which uses a rigid, non-adaptive support strategy for advising a user in refining a query to a travel product catalogue. The initial results demonstrate the value of our approach and show that our system is able to improve the non-adaptive strategy in order to learn an optimal (adaptive) recommendation strategy. Copyright 2007 ACM.","Recommender systems are intelligent E-commerce applications that assist users in a decision-making process by offering personalized product recommendations during an interaction session. Quite recently, conversational approaches have been introduced in order to support more interactive recommendation sessions. Notwithstanding the increased interactivity offered by these approaches, the system employs an interaction strategy that is specified apriori (at design time) and followed quite rigidly during the interaction. In this paper, we present a new type of recommender system which is capable of learning autonomously an adaptive interaction strategy for assisting the users in acquiring their interaction goals. We view the recommendation process as a sequential decision problem and we model it as a Markov Decision Process (MDP). We learn a model of the user behavior, and use it to acquire the adaptive strategy using Reinforcement Learning (RL) techniques. In this context, the system learns the optimal strategy by observing the consequences of its actions on the users and also on the final outcome of the recommendation session. We apply our approach within an existing travel recommender system which uses a rigid, non-adaptive support strategy for advising a user in refining a query to a travel product catalogue. The initial results demonstrate the value of our approach and show that our system is able to improve the non-adaptive strategy in order to learn an optimal (adaptive) recommendation strategy. Copyright 2007 ACM.",,"University of Trento, Trento, Italy; Free University of Bozen-Bolzano, Bolzano, Italy",,,Adaptivity; Conversational recommender systems; Markov decision process; Reinforcement learning,Author4,Author1,a,a,a,a,a,a,Recommender Systems,,,,a,a,ACM International Conference Proceeding Series,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36849060222&doi=10.1145%2f1282100.1282114&partnerID=40&md5=3eb06672fc6e015b5b7949df5d904559,"Mahmood T., Ricci F.","Mahmood, T., University of Trento, Trento, Italy; Ricci, F., Free University of Bozen-Bolzano, Bolzano, Italy",25,,10.1145/1282100.1282114,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-36849060222,,,,,84,75,,,,,Scopus,,,,Conference Paper,,,258,,2007,2007,0,1,3,Author4
81,2,0,2,2,Learning to rank for recommender systems,"Recommender system aim at providing a personalized list of items ranked according to the preferences of the user, as such ranking methods are at the core of many recommendation algorithms. The topic of this tutorial focuses on the cutting-edge algorithmic development in the area of recommender systems. This tutorial will provide an in depth picture of the progress of ranking models in the field, summarizing the strengths and weaknesses of existing methods, and discussing open issues that could be promising for future research in the community. A qualitative and quantitative comparison between different models will be provided while we will also highlight recent developments in the areas of Reinforcement Learning. © 2013 ACM.","Recommender system aim at providing a personalized list of items ranked according to the preferences of the user, as such ranking methods are at the core of many recommendation algorithms. The topic of this tutorial focuses on the cutting-edge algorithmic development in the area of recommender systems. This tutorial will provide an in depth picture of the progress of ranking models in the field, summarizing the strengths and weaknesses of existing methods, and discussing open issues that could be promising for future research in the community. A qualitative and quantitative comparison between different models will be provided while we will also highlight recent developments in the areas of Reinforcement Learning. © 2013 ACM.",,"Telefonica Research, Spain; Delft University of Technology, Netherlands",,,Collaborative filtering; Learning to rank; Ranking; Recommender systems,Author1,Author2,a,a,a,a,a,a,Recommender Systems,,,,a,a,RecSys 2013 - Proceedings of the 7th ACM Conference on Recommender Systems,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887600751&doi=10.1145%2f2507157.2508063&partnerID=40&md5=6fb877b463ecc336e9a285a543b4ae18,"Karatzoglou A., Baltrunas L., Shi Y.","Karatzoglou, A., Telefonica Research, Spain; Baltrunas, L., Telefonica Research, Spain; Shi, Y., Delft University of Technology, Netherlands",18,,10.1145/2507157.2508063,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84887600751,,,,,494,493,,,,,Scopus,,,,Conference Paper,,,,,2013,2013,1,0,3,Author4
82,2,1,1,1,A design proposal of a game-based professional training system for highly dangerous professions,"Recently, modern society frequently faces local wars, terrorism, earthquakes, fire accidents, epidemics and coal mining accidents. Members of highly dangerous professions must obtain rigorous training so that they could bear the great historical mission. Generally speaking, these professions include armed forces, special police force, fire department, astronauts and mine disaster rescue troop. The game-based professional training systems for highly dangerous professions have their own distinct requirements. The aim of game-based learning systems is not only the study of declarative knowledge, but also entraining procedural knowledge through repeated practice until it becomes an automatic skill. The result of highly dangerous professional training is extremely important, since if a trainee dose not master the basic knowledge and skill, they could be in grave danger; the trainee's mental qualities should be continuously prompted by the training system so that they could be act intuitively under the most execrable circumstance. Based on requirements analysis and taking the case of mining rescue into account, we divide the whole training system into three parts: machine learning subsystem, brain information subsystem and credit-assignment subsystem. The machine learning subsystem (as know as serious game subsystem), contains the audio-visual coherency analysis, semantic annotation of a scene based on association memory, cooperating management of audio-visual cross-modal signals, personalization rendering of a scene. The brain information subsystem includes functions for receiving, storing and analyzing trainee's trial data based on visual and auditory signals from EEG, sEMG and psychological tests. The credit-assignment subsystem involves trainee's profiles and effect evaluation which are sent from brain information subsystem to machine learning subsystem, while the plan of knowledge learning, the result of skills training and consequence of the desensitization trial are sent as the feedback to brain information subsystem. Therefore, the whole framework works as a reinforcement learning system. The kernel of this system is the cooperating learning schema of audio-visual cross-modal signals. Furthermore, in this system the main visual signals contain scene textures, 3D character animation, 3D scene animation, while the main auditory signals contain the realistic sound, the on-the-spot orders, the on-the-spot yells and background music. In the light of cognitive principles, the following factors should be considered when a game-based leaning system is designed: (1)The working memory including phonological loop and visuo-spatial sketchpad act as two slave systems, play the role of dual sensory channels so that semantic coherency of the visual and the auditory data could be combined with the prior knowledge to be formed as long-term memory; (2) A goal of cooperative learning for audio-visual cross-modal signals is to create an approach which can process verbal information(like the realistic sound and the on-the-spot orders) and non-verbal information(such as 3D character animation as well as 3D scene animation) from the two separate subsystems; (3) Schema acquisition (based on Theory of Cognitive Load -TCL) should be a primary means of learning, and the automation of cognitive process (including declarative knowledge procedural knowledge) will be used to reduce working memory load.","Recently, modern society frequently faces local wars, terrorism, earthquakes, fire accidents, epidemics and coal mining accidents. Members of highly dangerous professions must obtain rigorous training so that they could bear the great historical mission. Generally speaking, these professions include armed forces, special police force, fire department, astronauts and mine disaster rescue troop. The game-based professional training systems for highly dangerous professions have their own distinct requirements. The aim of game-based learning systems is not only the study of declarative knowledge, but also entraining procedural knowledge through repeated practice until it becomes an automatic skill. The result of highly dangerous professional training is extremely important, since if a trainee dose not master the basic knowledge and skill, they could be in grave danger; the trainee's mental qualities should be continuously prompted by the training system so that they could be act intuitively under the most execrable circumstance. Based on requirements analysis and taking the case of mining rescue into account, we divide the whole training system into three parts: machine learning subsystem, brain information subsystem and credit-assignment subsystem. The machine learning subsystem (as know as serious game subsystem), contains the audio-visual coherency analysis, semantic annotation of a scene based on association memory, cooperating management of audio-visual cross-modal signals, personalization rendering of a scene. The brain information subsystem includes functions for receiving, storing and analyzing trainee's trial data based on visual and auditory signals from EEG, sEMG and psychological tests. The credit-assignment subsystem involves trainee's profiles and effect evaluation which are sent from brain information subsystem to machine learning subsystem, while the plan of knowledge learning, the result of skills training and consequence of the desensitization trial are sent as the feedback to brain information subsystem. Therefore, the whole framework works as a reinforcement learning system. The kernel of this system is the cooperating learning schema of audio-visual cross-modal signals. Furthermore, in this system the main visual signals contain scene textures, 3D character animation, 3D scene animation, while the main auditory signals contain the realistic sound, the on-the-spot orders, the on-the-spot yells and background music. In the light of cognitive principles, the following factors should be considered when a game-based leaning system is designed: (1)The working memory including phonological loop and visuo-spatial sketchpad act as two slave systems, play the role of dual sensory channels so that semantic coherency of the visual and the auditory data could be combined with the prior knowledge to be formed as long-term memory; (2) A goal of cooperative learning for audio-visual cross-modal signals is to create an approach which can process verbal information(like the realistic sound and the on-the-spot orders) and non-verbal information(such as 3D character animation as well as 3D scene animation) from the two separate subsystems; (3) Schema acquisition (based on Theory of Cognitive Load -TCL) should be a primary means of learning, and the automation of cognitive process (including declarative knowledge procedural knowledge) will be used to reduce working memory load.",,"College of Computer and Software, Taiyuan University of Technology, China",,,Audio-visual coherency; Game-based professional training; Highly dangerous professions; Theory of cognitive load; Working memory,Author3,Author4,u,a,u,a,a,a,Intelligent Tutors,a,,,u,a,Proceedings of the European Conference on Games-based Learning,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938575896&partnerID=40&md5=6cd3db1e9a9c7a1b9429e5e93b4e8cab,"Xueli y., Zhi L., Changneng Z., Guangping Z., Zengrong L.","Xueli, y., College of Computer and Software, Taiyuan University of Technology, China; Zhi, L., College of Computer and Software, Taiyuan University of Technology, China; Changneng, Z., College of Computer and Software, Taiyuan University of Technology, China; Guangping, Z., College of Computer and Software, Taiyuan University of Technology, China; Zengrong, L., College of Computer and Software, Taiyuan University of Technology, China",,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-84938575896,,,,,394,388,,,,,Scopus,,,,Conference Paper,,,2009-January,,,2009,0,3,1,Author1
83,0,0,1,0,Decomposing drama management in educational interactive narrative: A modular reinforcement learning approach,"Recent years have seen growing interest in data-driven approaches to personalized interactive narrative generation and drama management. Reinforcement learning (RL) shows particular promise for training policies to dynamically shape interactive narratives based on corpora of player-interaction data. An important open question is how to design reinforcement learning-based drama managers in order to make effective use of player interaction data, which is often expensive to gather and sparse relative to the vast state and action spaces required by drama management. We investigate an offline optimization framework for training modular reinforcement learning-based drama managers in an educational interactive narrative, CRYSTAL ISLAND. We leverage importance sampling to evaluate drama manager policies derived from different decompositional representations of the interactive narrative. Empirical results show significant improvements in drama manager quality from adopting an optimized modular RL decomposition compared to competing representations. © Springer International Publishing AG 2016.","Recent years have seen growing interest in data-driven approaches to personalized interactive narrative generation and drama management. Reinforcement learning (RL) shows particular promise for training policies to dynamically shape interactive narratives based on corpora of player-interaction data. An important open question is how to design reinforcement learning-based drama managers in order to make effective use of player interaction data, which is often expensive to gather and sparse relative to the vast state and action spaces required by drama management. We investigate an offline optimization framework for training modular reinforcement learning-based drama managers in an educational interactive narrative, CRYSTAL ISLAND. We leverage importance sampling to evaluate drama manager policies derived from different decompositional representations of the interactive narrative. Empirical results show significant improvements in drama manager quality from adopting an optimized modular RL decomposition compared to competing representations. © Springer International Publishing AG 2016.",,"North Carolina State University, Raleigh, NC, United States",,,Drama management; Educational interactive narrative; Intelligent narrative technologies; Modular reinforcement learning,Author1,Author2,a,a,a,a,a,a,Intelligent Tutors,,,,a,a,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996879693&doi=10.1007%2f978-3-319-48279-8_24&partnerID=40&md5=3e9d8ada18e9ba06d96af278df3be0e5,"Wang P., Rowe J., Mott B., Lester J.","Wang, P., North Carolina State University, Raleigh, NC, United States; Rowe, J., North Carolina State University, Raleigh, NC, United States; Mott, B., North Carolina State University, Raleigh, NC, United States; Lester, J., North Carolina State University, Raleigh, NC, United States",1,,10.1007/978-3-319-48279-8_24,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84996879693,,,,,282,270,,,,,Scopus,,,,Conference Paper,,,10045 LNCS,,2016,2016,3,1,0,Author3
84,0,2,1,1,A learning-based control architecture for an assistive robot providing social engagement during cognitively stimulating activities,"Recent studies have shown that sustained engagement in cognitively stimulating activities has had positive effects on the cognitive functioning of humans. The objective of our work is to develop an intelligent socially assistive robot that can engage individuals in person-centered cognitively stimulating activities. In this paper, we present the design of a novel learning-based control architecture that enables the robot to act as a social motivator by providing assistance, encouragement and celebration during the course of an activity. A hierarchical reinforcement learning (HRL) approach is used to provide the robot with the ability to: (i) learn appropriate assistive behaviors based on the structure of the activity and (ii) personalize the interaction based on the person's affective state during the activity. Preliminary experiments show that the proposed learning-based control architecture is effective in determining the optimal assistive behaviors of the robot during a memory game interaction.","Recent studies have shown that sustained engagement in cognitively stimulating activities has had positive effects on the cognitive functioning of humans. The objective of our work is to develop an intelligent socially assistive robot that can engage individuals in person-centered cognitively stimulating activities. In this paper, we present the design of a novel learning-based control architecture that enables the robot to act as a social motivator by providing assistance, encouragement and celebration during the course of an activity. A hierarchical reinforcement learning (HRL) approach is used to provide the robot with the ability to: (i) learn appropriate assistive behaviors based on the structure of the activity and (ii) personalize the interaction based on the person's affective state during the activity. Preliminary experiments show that the proposed learning-based control architecture is effective in determining the optimal assistive behaviors of the robot during a memory game interaction.",,,,"Autonomous Systems and Biomechatronics Laboratory in the Department of Mechanical and Industrial Engineering at the University of Toronto, 5 King's College Road, ON, M5S 3G8 Canada",,Author3,Author4,a,a,a,a,a,a,"Other (please in ""remark"")",a,Domain: robotics,Could also say personal assistants.,a,a,2011 IEEE International Conference on Robotics and Automation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5980426,,J. Chan; G. Nejat,,4,,10.1109/ICRA.2011.5980426,IEEE Xplore,1,,20110818,3933,,Games;Heart rate;Humans;Robot sensing systems;Speech recognition;Training,cognition;intelligent robots;learning (artificial intelligence);service robots;social sciences,cognitively stimulating activity;hierarchical reinforcement learning;human cognitive function;intelligent socially assistive robot;learning-based control architecture;memory game interaction,,1050-4729;10504729,,9-13 May 2011,,,,,,,,,,,,IEEE,18,,CD:978-1-61284-380-3; Electronic:978-1-61284-385-8; Paper:978-1-61284-386-5,,3928,IEEE Conferences,,,,,2011,2011,1,2,1,Author2
85,0,0,0,0,A personalized QoE-aware handover decision based on distributed reinforcement learning,"Recent developments in heterogeneous mobile networks and growing demands for variety of real-time and multimedia applications have emphasized the necessity of more intelligent handover decisions. Addressing the context knowledge of mobile devices, users, applications, and networks is the subject of context-aware handoff decision as a recent effort to this aim. However, user perception has not been attended adequately in the area of context-aware handover decision making. Mobile users may have different judgments about the Quality of Service (QoS) depending on their environmental conditions, and personal and psychological characteristics. This reality has been exploited in this paper to introduce a personalized user-centric handoff decision method to decide about the time and target of handover based on User Perceived Quality (UPQ) feedbacks. The UPQ degradations are mainly for the sake of (1) exiting the coverage of the serving Point of Attachment (PoA) or (2) QoS degradation of serving access network. Using UPQ metric, the proposed method obviates the necessity of being aware about rapidly varying network QoS parameters and overcomes the complexity and overhead of gathering and managing some other context information. Moreover, considering the underlying network and geographical map, the proposed method is able to inherently exploit the trajectory information of mobile users for handover decision. UPQ degradation is not only due to the user behaviour, but also due to the behaviours of others users. As such, multi-agent reinforcement learning paradigm has been considered for target PoA selection. The employed decision algorithm is based on WoLF-PHC learning method where UPQ is used as a delayed reward for training. The proposed handoff decision has been implemented under IEEE 802.21 framework using NS2 network simulator. The results have shown better performance of the proposed method comparing to conventional methods assuming regular movement of mobile users. © 2013 Springer Science+Business Media New York.","Recent developments in heterogeneous mobile networks and growing demands for variety of real-time and multimedia applications have emphasized the necessity of more intelligent handover decisions. Addressing the context knowledge of mobile devices, users, applications, and networks is the subject of context-aware handoff decision as a recent effort to this aim. However, user perception has not been attended adequately in the area of context-aware handover decision making. Mobile users may have different judgments about the Quality of Service (QoS) depending on their environmental conditions, and personal and psychological characteristics. This reality has been exploited in this paper to introduce a personalized user-centric handoff decision method to decide about the time and target of handover based on User Perceived Quality (UPQ) feedbacks. The UPQ degradations are mainly for the sake of (1) exiting the coverage of the serving Point of Attachment (PoA) or (2) QoS degradation of serving access network. Using UPQ metric, the proposed method obviates the necessity of being aware about rapidly varying network QoS parameters and overcomes the complexity and overhead of gathering and managing some other context information. Moreover, considering the underlying network and geographical map, the proposed method is able to inherently exploit the trajectory information of mobile users for handover decision. UPQ degradation is not only due to the user behaviour, but also due to the behaviours of others users. As such, multi-agent reinforcement learning paradigm has been considered for target PoA selection. The employed decision algorithm is based on WoLF-PHC learning method where UPQ is used as a delayed reward for training. The proposed handoff decision has been implemented under IEEE 802.21 framework using NS2 network simulator. The results have shown better performance of the proposed method comparing to conventional methods assuming regular movement of mobile users. © 2013 Springer Science+Business Media New York.",,"Department of Information Technology Engineering, University of Isfahan, Isfahan, Iran; Department of Computer Engineering, University of Isfahan, Isfahan, Iran",,,Context-aware handover; Distributed reinforcement learning; QoE-aware handover; User perceived quality,Author1,Author2,a,a,a,a,a,a,"Other (please in ""remark"")",,Software/systems and network engineering,,a,a,Wireless Networks,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886583003&doi=10.1007%2fs11276-013-0572-2&partnerID=40&md5=163b8778ea5599a29885ace91718c438,"Ghahfarokhi B.S., Movahhedinia N.","Ghahfarokhi, B.S., Department of Information Technology Engineering, University of Isfahan, Isfahan, Iran; Movahhedinia, N., Department of Computer Engineering, University of Isfahan, Isfahan, Iran",5,,10.1007/s11276-013-0572-2,SCOPUS,1,,,,,,,,,,8,,,,2-s2.0-84886583003,,,,,1828,1807,,,,,Scopus,,,,Article,,,19,,2013,2013,4,0,0,Author2
86,0,0,0,0,Framework for control and deep reinforcement learning in traffic,"Recent advances in deep reinforcement learning (RL) offer an opportunity to revisit complex traffic control problems at the level of vehicle dynamics, with the aim of learning locally optimal policies (with respect to the policy parameterization) for a variety of objectives such as matching a target velocity or minimizing fuel consumption. In this article, we present a framework called CISTAR (Customized Interface for SUMO, TraCI, and RLLab) that integrates the widely used traffic simulator SUMO with a standard deep reinforcement learning library RLLab. We create an interface allowing for easy customization of SUMO, allowing users to easily implement new controllers, heterogeneous experiments, and user-defined cost functions that depend on arbitrary state variables. We demonstrate the usage of CISTAR with several benchmark control and RL examples.","Recent advances in deep reinforcement learning (RL) offer an opportunity to revisit complex traffic control problems at the level of vehicle dynamics, with the aim of learning locally optimal policies (with respect to the policy parameterization) for a variety of objectives such as matching a target velocity or minimizing fuel consumption. In this article, we present a framework called CISTAR (Customized Interface for SUMO, TraCI, and RLLab) that integrates the widely used traffic simulator SUMO with a standard deep reinforcement learning library RLLab. We create an interface allowing for easy customization of SUMO, allowing users to easily implement new controllers, heterogeneous experiments, and user-defined cost functions that depend on arbitrary state variables. We demonstrate the usage of CISTAR with several benchmark control and RL examples.",,,,"UC Berkeley, Electrical Engineering and Computer Science",Simulation;control;deep reinforcement learning;vehicle dynamics,Author2,Author3,a,a,u,a,a,a,"Other (please in ""remark"")",a,,"This paper seems to introduce a framework, rather than apply RL to something",u,a,2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8317694,,C. Wu; K. Parvate; N. Kheterpal; L. Dickstein; A. Mehta; E. Vinitsky; A. M. Bayen,,,,10.1109/ITSC.2017.8317694,IEEE Xplore,1,,20180315,8,,Acceleration;Automobiles;Learning (artificial intelligence);Libraries;Machine learning;Roads;Vehicle dynamics,control engineering computing;learning (artificial intelligence);optimisation;road traffic control;road vehicles;traffic engineering computing;vehicle dynamics,Customized Interface;SUMO;benchmark control;fuel consumption minimization;locally optimal policies;policy parameterization;standard deep reinforcement learning library RLLab;traffic control;vehicle dynamics,,,,16-19 Oct. 2017,,,,,,,,,,,,IEEE,,,Electronic:978-1-5386-1526-3; POD:978-1-5386-1527-0; USB:978-1-5386-1525-6,,1,IEEE Conferences,,,,,,2017,4,0,0,Author3
87,0,0,0,0,MOOClets: A framework for dynamic experimentation and personalization,"Randomized experiments in online educational environments are ubiquitous as a scientific method for investigating learning and motivation, but they rarely improve educational resources and produce practical benefits for learners. We suggest that tools for experimentally comparing resources are designed primarily through the lens of experiments as a scientific methodology, and therefore miss a tremendous opportunity for online experiments to serve as engines for dynamic improvement and personalization. We present the MOOClet requirements specification to guide the implementation of software tools for experiments to ensure that whenever alternative versions of a resource can be experimentally compared (by randomly assigning versions), the resource can also be dynamically improved (by changing which versions are presented), and personalized (by presenting different versions to different people). The MOOClet specification was used to implement DEXPER, a proof-of-concept web service backend that enables dynamic experimentation and personalization of resources embedded in frontend educational platforms. We describe three use cases of MOOClets for dynamic experimentation and personalization of motivational emails, explanations, and problems. © 2017 ACM.","Randomized experiments in online educational environments are ubiquitous as a scientific method for investigating learning and motivation, but they rarely improve educational resources and produce practical benefits for learners. We suggest that tools for experimentally comparing resources are designed primarily through the lens of experiments as a scientific methodology, and therefore miss a tremendous opportunity for online experiments to serve as engines for dynamic improvement and personalization. We present the MOOClet requirements specification to guide the implementation of software tools for experiments to ensure that whenever alternative versions of a resource can be experimentally compared (by randomly assigning versions), the resource can also be dynamically improved (by changing which versions are presented), and personalized (by presenting different versions to different people). The MOOClet specification was used to implement DEXPER, a proof-of-concept web service backend that enables dynamic experimentation and personalization of resources embedded in frontend educational platforms. We describe three use cases of MOOClets for dynamic experimentation and personalization of motivational emails, explanations, and problems. © 2017 ACM.",,"Harvard University, Cambridge, MA, United States; Carleton College, Northfield, MN, United States; San Jose State University, San Jose, CA, United States; KAIST, Daejeon, South Korea",,,A/B experiment; Adaptive learning; Dynamic experimentation; MOOCLet; Multi-Armed bandit; Personalization; Reinforcement learning; Statistical machine learning,Author3,Author4,a,a,a,a,a,a,"Other (please in ""remark"")",,"Domain: education (maybe change ""intelligent tutors"" to education)",I would think this falls under the intelligent tutors indeed.,a,a,L@S 2017 - Proceedings of the 4th (2017) ACM Conference on Learning at Scale,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018407094&doi=10.1145%2f3051457.3054006&partnerID=40&md5=e7315bb93f2d39de7391ea6a6ad5617a,"Jaywilliams J., Rafferty A.N., Maldonado S., Ang A., Tingley D., Kim J.","Jaywilliams, J., Harvard University, Cambridge, MA, United States; Rafferty, A.N., Carleton College, Northfield, MN, United States; Maldonado, S., San Jose State University, San Jose, CA, United States; Ang, A., Harvard University, Cambridge, MA, United States; Tingley, D., Harvard University, Cambridge, MA, United States; Kim, J., KAIST, Daejeon, South Korea",,,10.1145/3051457.3054006,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85018407094,,,,,290,287,,,,,Scopus,,,,Conference Paper,,,,,2017,2017,4,0,0,Author4
88,2,2,0,2,Tools for the Precision Medicine Era: How to Develop Highly Personalized Treatment Recommendations from Cohort and Registry Data Using Q-Learning,"Q-Learning is a method of reinforcement learning that employs backwards stagewise estimation to identify sequences of actions that maximize some long-term reward. The method can be applied to sequential multipleassignment randomized trials to develop personalized adaptive treatment strategies (ATSs)-longitudinal practice guidelines highly tailored to time-varying attributes of individual patients. Sometimes, the basis for choosing which ATSs to include in a sequential multiple-assignment randomized trial (or randomized controlled trial) may be inadequate. Nonrandomized data sources may inform the initial design of ATSs, which could later be prospectively validated. In this paper, we illustrate challenges involved in using nonrandomized data for this purpose with a case study from the Center for International Blood and Marrow Transplant Research registry (1995-2007) aimed at 1) determining whether the sequence of therapeutic classes used in graft-versus-host disease prophylaxis and in refractory graft-versus-host disease is associated with improved survival and 2) identifying donor and patient factors with which to guide individualized immunosuppressant selections over time. We discuss how to communicate the potential benefit derived from following an ATS at the population and subgroup levels and how to evaluate its robustness to modeling assumptions. This worked example may serve as a model for developing ATSs from registries and cohorts in oncology and other fields requiring sequential treatment decisions. © 2017 The Author(s).","Q-Learning is a method of reinforcement learning that employs backwards stagewise estimation to identify sequences of actions that maximize some long-term reward. The method can be applied to sequential multipleassignment randomized trials to develop personalized adaptive treatment strategies (ATSs)-longitudinal practice guidelines highly tailored to time-varying attributes of individual patients. Sometimes, the basis for choosing which ATSs to include in a sequential multiple-assignment randomized trial (or randomized controlled trial) may be inadequate. Nonrandomized data sources may inform the initial design of ATSs, which could later be prospectively validated. In this paper, we illustrate challenges involved in using nonrandomized data for this purpose with a case study from the Center for International Blood and Marrow Transplant Research registry (1995-2007) aimed at 1) determining whether the sequence of therapeutic classes used in graft-versus-host disease prophylaxis and in refractory graft-versus-host disease is associated with improved survival and 2) identifying donor and patient factors with which to guide individualized immunosuppressant selections over time. We discuss how to communicate the potential benefit derived from following an ATS at the population and subgroup levels and how to evaluate its robustness to modeling assumptions. This worked example may serve as a model for developing ATSs from registries and cohorts in oncology and other fields requiring sequential treatment decisions. © 2017 The Author(s).",,"Department of Epidemiology Biostatistics and Occupational Health, Faculty of Medicine, McGill University, 1020 Pine Avenue West, Montreal, QC, Canada; Division of Hematology and Oncology, Department of Medicine, Hopital MaisonneuveRosemont Research Center, University of Montreal, Montreal, QC, Canada; Center for International Blood and Marrow Transplant Research, Department of Medicine, Medical College of Wisconsin, Milwaukee, WI, United States; Division of Biostatistics, Institute for Health and Society, Medical College of Wisconsin, Milwaukee, WI, United States; Division of Hematology, Oncology, and Transplantation, Department of Medicine, University of Minnesota Medical Center, Minneapolis, MN, United States; Center for International Blood and Marrow Transplant Research, National Marrow Donor Program/Be the Match, Minneapolis, MN, United States; Division of Hematology/BMT, Department of Internal Medicine, University of Utah Health Care and Huntsman Cancer Center, University of Utah, Salt Lake City, UT, United States; Division of Cancer Medicine, Department of Stem Cell Transplantation, University of Texas M.D. Anderson Cancer Center, Houston, TX, United States; Department of Blood and Marrow Transplantation, Moffitt Cancer Center, Tampa, FL, United States; US Department of Defense, Fort Meade, MD, United States",,,Adaptive treatment strategies; Dynamic treatment regimes; Graft-versus-host disease; Machine learning; Personalized medicine; Prediction; Q-learning; Registry data,Author4,Author1,a,a,a,a,a,a,Healthcare,,,,a,a,American Journal of Epidemiology,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029668317&doi=10.1093%2faje%2fkwx027&partnerID=40&md5=1c867a9b4398186b0bd9c0698bd7be80,"Krakow E.F., Hemmer M., Wang T., Logan B., Arora M., Spellman S., Couriel D., Alousi A., Pidala J., Last M., Lachance S., Moodie E.E.M.","Krakow, E.F., Department of Epidemiology Biostatistics and Occupational Health, Faculty of Medicine, McGill University, 1020 Pine Avenue West, Montreal, QC, Canada, Division of Hematology and Oncology, Department of Medicine, Hopital MaisonneuveRosemont Research Center, University of Montreal, Montreal, QC, Canada; Hemmer, M., Department of Epidemiology Biostatistics and Occupational Health, Faculty of Medicine, McGill University, 1020 Pine Avenue West, Montreal, QC, Canada; Wang, T., Center for International Blood and Marrow Transplant Research, Department of Medicine, Medical College of Wisconsin, Milwaukee, WI, United States, Division of Biostatistics, Institute for Health and Society, Medical College of Wisconsin, Milwaukee, WI, United States; Logan, B., Center for International Blood and Marrow Transplant Research, Department of Medicine, Medical College of Wisconsin, Milwaukee, WI, United States, Division of Biostatistics, Institute for Health and Society, Medical College of Wisconsin, Milwaukee, WI, United States; Arora, M., Division of Hematology, Oncology, and Transplantation, Department of Medicine, University of Minnesota Medical Center, Minneapolis, MN, United States; Spellman, S., Center for International Blood and Marrow Transplant Research, National Marrow Donor Program/Be the Match, Minneapolis, MN, United States; Couriel, D., Division of Hematology/BMT, Department of Internal Medicine, University of Utah Health Care and Huntsman Cancer Center, University of Utah, Salt Lake City, UT, United States; Alousi, A., Division of Cancer Medicine, Department of Stem Cell Transplantation, University of Texas M.D. Anderson Cancer Center, Houston, TX, United States; Pidala, J., Department of Blood and Marrow Transplantation, Moffitt Cancer Center, Tampa, FL, United States; Last, M., Center for International Blood and Marrow Transplant Research, Department of Medicine, Medical College of Wisconsin, Milwaukee, WI, United States, US Department of Defense, Fort Meade, MD, United States; Lachance, S., Division of Hematology and Oncology, Department of Medicine, Hopital MaisonneuveRosemont Research Center, University of Montreal, Montreal, QC, Canada; Moodie, E.E.M., Department of Epidemiology Biostatistics and Occupational Health, Faculty of Medicine, McGill University, 1020 Pine Avenue West, Montreal, QC, Canada",1,,10.1093/aje/kwx027,SCOPUS,1,,,,,,,,,,2,,,,2-s2.0-85029668317,,,,,172,160,,,,,Scopus,,,,Article,,,186,,2017,2017,1,0,3,Author2
89,2,2,0,2,Deep reinforcement learning for automated radiation adaptation in lung cancer:,"Purpose: To investigate deep reinforcement learning (DRL) based on historical treatment plans for developing automated radiation adaptation protocols for nonsmall cell lung cancer (NSCLC) patients that aim to maximize tumor local control at reduced rates of radiation pneumonitis grade 2 (RP2). Methods: In a retrospective population of 114 NSCLC patients who received radiotherapy, a three-component neural networks framework was developed for deep reinforcement learning (DRL) of dose fractionation adaptation. Large-scale patient characteristics included clinical, genetic, and imaging radiomics features in addition to tumor and lung dosimetric variables. First, a generative adversarial network (GAN) was employed to learn patient population characteristics necessary for DRL training from a relatively limited sample size. Second, a radiotherapy artificial environment (RAE) was reconstructed by a deep neural network (DNN) utilizing both original and synthetic data (by GAN) to estimate the transition probabilities for adaptation of personalized radiotherapy patients' treatment courses. Third, a deep Q-network (DQN) was applied to the RAE for choosing the optimal dose in a response-adapted treatment setting. This multicomponent reinforcement learning approach was benchmarked against real clinical decisions that were applied in an adaptive dose escalation clinical protocol. In which, 34 patients were treated based on avid PET signal in the tumor and constrained by a 17.2% normal tissue complication probability (NTCP) limit for RP2. The uncomplicated cure probability (P+) was used as a baseline reward function in the DRL. Results: Taking our adaptive dose escalation protocol as a blueprint for the proposed DRL (GAN + RAE + DQN) architecture, we obtained an automated dose adaptation estimate for use at ∼2/3 of the way into the radiotherapy treatment course. By letting the DQN component freely control the estimated adaptive dose per fraction (ranging from 1-5 Gy), the DRL automatically favored dose escalation/de-escalation between 1.5 and 3.8 Gy, a range similar to that used in the clinical protocol. The same DQN yielded two patterns of dose escalation for the 34 test patients, but with different reward variants. First, using the baseline P+ reward function, individual adaptive fraction doses of the DQN had similar tendencies to the clinical data with an RMSE = 0.76 Gy; but adaptations suggested by the DQN were generally lower in magnitude (less aggressive). Second, by adjusting the P+ reward function with higher emphasis on mitigating local failure, better matching of doses between the DQN and the clinical protocol was achieved with an RMSE = 0.5 Gy. Moreover, the decisions selected by the DQN seemed to have better concordance with patients eventual outcomes. In comparison, the traditional temporal difference (TD) algorithm for reinforcement learning yielded an RMSE = 3.3 Gy due to numerical instabilities and lack of sufficient learning. Conclusion: We demonstrated that automated dose adaptation by DRL is a feasible and a promising approach for achieving similar results to those chosen by clinicians. The process may require customization of the reward function if individual cases were to be considered. However, development of this framework into a fully credible autonomous system for clinical decision support would require further validation on larger multi-institutional datasets. © 2017 American Association of Physicists in Medicine.","Purpose: To investigate deep reinforcement learning (DRL) based on historical treatment plans for developing automated radiation adaptation protocols for nonsmall cell lung cancer (NSCLC) patients that aim to maximize tumor local control at reduced rates of radiation pneumonitis grade 2 (RP2). Methods: In a retrospective population of 114 NSCLC patients who received radiotherapy, a three-component neural networks framework was developed for deep reinforcement learning (DRL) of dose fractionation adaptation. Large-scale patient characteristics included clinical, genetic, and imaging radiomics features in addition to tumor and lung dosimetric variables. First, a generative adversarial network (GAN) was employed to learn patient population characteristics necessary for DRL training from a relatively limited sample size. Second, a radiotherapy artificial environment (RAE) was reconstructed by a deep neural network (DNN) utilizing both original and synthetic data (by GAN) to estimate the transition probabilities for adaptation of personalized radiotherapy patients' treatment courses. Third, a deep Q-network (DQN) was applied to the RAE for choosing the optimal dose in a response-adapted treatment setting. This multicomponent reinforcement learning approach was benchmarked against real clinical decisions that were applied in an adaptive dose escalation clinical protocol. In which, 34 patients were treated based on avid PET signal in the tumor and constrained by a 17.2% normal tissue complication probability (NTCP) limit for RP2. The uncomplicated cure probability (P+) was used as a baseline reward function in the DRL. Results: Taking our adaptive dose escalation protocol as a blueprint for the proposed DRL (GAN + RAE + DQN) architecture, we obtained an automated dose adaptation estimate for use at ∼2/3 of the way into the radiotherapy treatment course. By letting the DQN component freely control the estimated adaptive dose per fraction (ranging from 1-5 Gy), the DRL automatically favored dose escalation/de-escalation between 1.5 and 3.8 Gy, a range similar to that used in the clinical protocol. The same DQN yielded two patterns of dose escalation for the 34 test patients, but with different reward variants. First, using the baseline P+ reward function, individual adaptive fraction doses of the DQN had similar tendencies to the clinical data with an RMSE = 0.76 Gy; but adaptations suggested by the DQN were generally lower in magnitude (less aggressive). Second, by adjusting the P+ reward function with higher emphasis on mitigating local failure, better matching of doses between the DQN and the clinical protocol was achieved with an RMSE = 0.5 Gy. Moreover, the decisions selected by the DQN seemed to have better concordance with patients eventual outcomes. In comparison, the traditional temporal difference (TD) algorithm for reinforcement learning yielded an RMSE = 3.3 Gy due to numerical instabilities and lack of sufficient learning. Conclusion: We demonstrated that automated dose adaptation by DRL is a feasible and a promising approach for achieving similar results to those chosen by clinicians. The process may require customization of the reward function if individual cases were to be considered. However, development of this framework into a fully credible autonomous system for clinical decision support would require further validation on larger multi-institutional datasets. © 2017 American Association of Physicists in Medicine.",,"Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States; Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan",,,adaptive radiotherapy; deep learning; lung cancer; reinforcement learning,Author1,Author2,a,a,a,a,a,a,Healthcare,,,,a,a,Medical Physics,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037829216&doi=10.1002%2fmp.12625&partnerID=40&md5=64aadbb97a2f33ad182430a659f43b97,"Tseng H.-H., Luo Y., Cui S., Chien J.-T., Ten Haken R.K., Naqa I.E.","Tseng, H.-H., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States; Luo, Y., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States; Cui, S., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States; Chien, J.-T., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States, Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan; Ten Haken, R.K., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States; Naqa, I.E., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States",2,,10.1002/mp.12625,SCOPUS,1,,,,,,,,,,12,,,,2-s2.0-85037829216,,,,,6705,6690,,,,,Scopus,,,,Article,,,44,,2017,2017,1,0,3,Author2
90,0,1,1,1,An MARL-Based Distributed Learning Scheme for Capturing User Preferences in a Smart Environment,"Providing a personalized service to a user in a smart environment has been one of the key goals in the area of pervasive computing. The proliferation of individually developed smart devices in the name of Internet of Things opens up a possibility of providing personalized services to a user in an autonomous and distributed manner. As a user's task often involves services supported by multiple devices, capturing a device-specific service preference is not enough to maximize a user's comfort. In this paper, we propose a distributed learning scheme for capturing multiple device service preferences in a smart environment. We exploit multi-agent reinforcement learning (MARL) method where each smart device acts as a reinforcement learning agent to incrementally and cooperatively capture a user specific preference of a task. Experiments confirm that smart devices with the proposed scheme are able to capture multiple device service preferences from a small number of interactions with a user and an environment. Also, the proposed transfer learning method improves learning performance for a new task.","Providing a personalized service to a user in a smart environment has been one of the key goals in the area of pervasive computing. The proliferation of individually developed smart devices in the name of Internet of Things opens up a possibility of providing personalized services to a user in an autonomous and distributed manner. As a user's task often involves services supported by multiple devices, capturing a device-specific service preference is not enough to maximize a user's comfort. In this paper, we propose a distributed learning scheme for capturing multiple device service preferences in a smart environment. We exploit multi-agent reinforcement learning (MARL) method where each smart device acts as a reinforcement learning agent to incrementally and cooperatively capture a user specific preference of a task. Experiments confirm that smart devices with the proposed scheme are able to capture multiple device service preferences from a small number of interactions with a user and an environment. Also, the proposed transfer learning method improves learning performance for a new task.",,,,"Sch. of Comput., KAIST, Daejeon, South Korea",context aware;distributed learning;personalization;smart device;user preference,Author2,Author3,a,a,a,a,a,a,Interfaces / HCI,a,,,a,a,2017 IEEE International Conference on Services Computing (SCC),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8034977,,J. Lim; H. Son; D. Lee; D. Lee,,,,10.1109/SCC.2017.24,IEEE Xplore,1,,20170914,139,,Brightness;Learning (artificial intelligence);Ontologies;Performance evaluation;Servers;Smart devices;TV,learning (artificial intelligence);multi-agent systems;ubiquitous computing,Internet of Things;MARL;autonomous distributed manner;capturing user preferences;device-specific service preference;distributed learning scheme;individually developed smart devices;multiagent reinforcement learning method;multiple device service preferences;personalized service;pervasive computing;reinforcement learning agent;smart environment;transfer learning method;user specific preference,,,,25-30 June 2017,,,,,,,,,,,,IEEE,,,Electronic:978-1-5386-2005-2; POD:978-1-5386-2006-9; USB:978-1-5386-2004-5,,132,IEEE Conferences,,,,,2017,2017,1,3,0,Author4
91,0,1,0,0,Learning from demonstration: Teaching a myoelectric prosthesis with an intact limb via reinforcement learning,"Prosthetic arms should restore and extend the capabilities of someone with an amputation. They should move naturally and be able to perform elegant, coordinated movements that approximate those of a biological arm. Despite these objectives, the control of modern-day prostheses is often nonintuitive and taxing. Existing devices and control approaches do not yet give users the ability to effect highly synergistic movements during their daily-life control of a prosthetic device. As a step towards improving the control of prosthetic arms and hands, we introduce an intuitive approach to training a prosthetic control system that helps a user achieve hard-to-engineer control behaviours. Specifically, we present an actor-critic reinforcement learning method that for the first time promises to allow someone with an amputation to use their non-amputated arm to teach their prosthetic arm how to move through a wide range of coordinated motions and grasp patterns. We evaluate our method during the myoelectric control of a multi-joint robot arm by non-amputee users, and demonstrate that by using our approach a user can train their arm to perform simultaneous gestures and movements in all three degrees of freedom in the robot's hand and wrist based only on information sampled from the robot and the user's above-elbow myoelectric signals. Our results indicate that this learning-from-demonstration paradigm may be well suited to use by both patients and clinicians with minimal technical knowledge, as it allows a user to personalize the control of his or her prosthesis without having to know the underlying mechanics of the prosthetic limb. These preliminary results also suggest that our approach may extend in a straightforward way to next-generation prostheses with precise finger and wrist control, such that these devices may someday allow users to perform fluid and intuitive movements like playing the piano, catching a ball, and comfortably shaking hands.","Prosthetic arms should restore and extend the capabilities of someone with an amputation. They should move naturally and be able to perform elegant, coordinated movements that approximate those of a biological arm. Despite these objectives, the control of modern-day prostheses is often nonintuitive and taxing. Existing devices and control approaches do not yet give users the ability to effect highly synergistic movements during their daily-life control of a prosthetic device. As a step towards improving the control of prosthetic arms and hands, we introduce an intuitive approach to training a prosthetic control system that helps a user achieve hard-to-engineer control behaviours. Specifically, we present an actor-critic reinforcement learning method that for the first time promises to allow someone with an amputation to use their non-amputated arm to teach their prosthetic arm how to move through a wide range of coordinated motions and grasp patterns. We evaluate our method during the myoelectric control of a multi-joint robot arm by non-amputee users, and demonstrate that by using our approach a user can train their arm to perform simultaneous gestures and movements in all three degrees of freedom in the robot's hand and wrist based only on information sampled from the robot and the user's above-elbow myoelectric signals. Our results indicate that this learning-from-demonstration paradigm may be well suited to use by both patients and clinicians with minimal technical knowledge, as it allows a user to personalize the control of his or her prosthesis without having to know the underlying mechanics of the prosthetic limb. These preliminary results also suggest that our approach may extend in a straightforward way to next-generation prostheses with precise finger and wrist control, such that these devices may someday allow users to perform fluid and intuitive movements like playing the piano, catching a ball, and comfortably shaking hands.",,,,"Department of Computing Science and the Department of Medicine, University of Alberta, Edmonton, AB T6G 2E1, Canada",,Author3,Author4,a,a,a,a,a,a,Healthcare,a,,,a,a,2017 International Conference on Rehabilitation Robotics (ICORR),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8009453,,G. Vasan; P. M. Pilarski,,,,10.1109/ICORR.2017.8009453,IEEE Xplore,1,,20170814,1464,,Elbow;Manipulators;Muscles;Prosthetics;Training;Wrist,electromyography;gait analysis;medical robotics;medical signal processing;prosthetics;signal sampling;student experiments,actor-critic reinforcement learning method;amputation;biological arm;control approaches;coordinated motions;daily-life control;effect highly synergistic movements;elegant coordinated movements;fluid movements;grasp patterns;hard-to-engineer control behaviours;information sampling;intact limb;intuitive movements;learning-from-demonstration paradigm;minimal technical knowledge;modern-day prostheses;multijoint robot arm;myoelectric prosthesis teaching;next-generation prostheses;nonamputated arm;prosthetic arms;prosthetic control system;prosthetic device;prosthetic limb;reinforcement learning;robot hand;three degrees-of-freedom;user above-elbow myoelectric signals;wrist,,,,17-20 July 2017,,,,,,,,,,,,IEEE,,,Electronic:978-1-5386-2296-4; POD:978-1-5386-2297-1; USB:978-1-5386-2295-7,,1457,IEEE Conferences,,,,,2017,2017,3,1,0,Author2
92,0,0,0,0,Emotion-driven learning agent for setting rich presence in mobile telephony,"Presence or personal status information is going to be an integral part of human life in the near future. With the possibility of personalizing user preferences in a fine grained way, mobile presence will appeal to most users. Among all the advantages, one of the most annoying problems is to set the presence status manually each time. This paper discusses the development of an intelligent agent based presence client that will learn and make decisions on behalf of the user about his or her presence status. The decision is emotion driven and the learning depends on real world experience. The proposed system utilizes a neural network (NN) based emotion-driven agent to learn user preferences. As a NN learning algorithm, two approaches based on Differential Evolution and Reinforcement have been proposed, of which either one can be used. Rich presence status is set using a scripting language named Call Processing Language; and SIP is used for publishing the presence to others.","Presence or personal status information is going to be an integral part of human life in the near future. With the possibility of personalizing user preferences in a fine grained way, mobile presence will appeal to most users. Among all the advantages, one of the most annoying problems is to set the presence status manually each time. This paper discusses the development of an intelligent agent based presence client that will learn and make decisions on behalf of the user about his or her presence status. The decision is emotion driven and the learning depends on real world experience. The proposed system utilizes a neural network (NN) based emotion-driven agent to learn user preferences. As a NN learning algorithm, two approaches based on Differential Evolution and Reinforcement have been proposed, of which either one can be used. Rich presence status is set using a scripting language named Call Processing Language; and SIP is used for publishing the presence to others.",,,,"The Royal Institute of Technology (KTH), Sweden and University of Dhaka, Bangladesh",ANN;CPL;DE;Presence;SIP;context aware;learning agent;neural network;reinforcement,Author1,Author2,u,a,a,a,a,a,Personal Assistants,,,,u,a,2008 11th International Conference on Computer and Information Technology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4803023,,S. Saha; R. Quazi,,0,,10.1109/ICCITECHN.2008.4803023,IEEE Xplore,1,,20090321,126,,Artificial neural networks;Context awareness;Humans;Information technology;Intelligent agent;Mobile computing;Mobile radio mobility management;Neural networks;Publishing;Telephony,authoring languages;learning (artificial intelligence);mobile computing;neural nets;software agents;telephony,call processing language;differential evolution;emotion-driven agent;emotion-driven learning agent;intelligent agent;mobile telephony;neural network;personalizing user preferences;reinforcement;rich presence;scripting language,,,,24-27 Dec. 2008,,,,,,,,,,,,IEEE,21,,CD-ROM:978-1-4244-2136-7; POD:978-1-4244-2135-0,,121,IEEE Conferences,,,,,,2008,4,0,0,Author1
93,1,0,0,0,Vito – a generic agent for multi-physics model personalization: Application to heart modeling,"Precise estimation of computational physiological model parameters from patient data is one of the main hurdles towards their clinical applicability. Designing robust estimation algorithms is often a tedious and model-specific process. We propose to use, for the first time to our knowledge, artificial intelligence (AI) concepts to learn how to personalize a computational model, inspired by how an expert manually personalizes. We reformulate the parameter estimation problem in terms of Markov decision process and reinforcement learning. In an off-line phase, the artificial agent, called Vito, automatically learns a representative state-action-state model through data-driven exploration of the computational model under consideration. In other words, Vito learns how the model behaves under change of parameters and how to personalize it. Vito then controls the on-line personalization by exploiting its automatically derived action policy. Because the algorithm is model-independent, personalizing a completely new model would require only adjusting some simple parameters of the agent and defining the observations to match, without the full knowledge of the model itself. Vito was evaluated on two challenging problems: the inverse problem of cardiac electrophysiology and the personalization of a lumped-parameter whole-body circulation model. Obtained results suggested that Vito could achieve equivalent goodness of fit than standard methods, while being more robust (up to 25% higher success rates) and with faster (up to three times) convergence rate. Our AI approach could thus make model personalization algorithms generalizable and self-adaptable to any patient, like a human operator. © Springer International Publishing Switzerland 2015.","Precise estimation of computational physiological model parameters from patient data is one of the main hurdles towards their clinical applicability. Designing robust estimation algorithms is often a tedious and model-specific process. We propose to use, for the first time to our knowledge, artificial intelligence (AI) concepts to learn how to personalize a computational model, inspired by how an expert manually personalizes. We reformulate the parameter estimation problem in terms of Markov decision process and reinforcement learning. In an off-line phase, the artificial agent, called Vito, automatically learns a representative state-action-state model through data-driven exploration of the computational model under consideration. In other words, Vito learns how the model behaves under change of parameters and how to personalize it. Vito then controls the on-line personalization by exploiting its automatically derived action policy. Because the algorithm is model-independent, personalizing a completely new model would require only adjusting some simple parameters of the agent and defining the observations to match, without the full knowledge of the model itself. Vito was evaluated on two challenging problems: the inverse problem of cardiac electrophysiology and the personalization of a lumped-parameter whole-body circulation model. Obtained results suggested that Vito could achieve equivalent goodness of fit than standard methods, while being more robust (up to 25% higher success rates) and with faster (up to three times) convergence rate. Our AI approach could thus make model personalization algorithms generalizable and self-adaptable to any patient, like a human operator. © Springer International Publishing Switzerland 2015.",,"Imaging and Computer Vision, Siemens Corporate Technology, Princeton, NJ, Romania; Pattern Recognition Lab, FAU Erlangen-Nürnberg, Germany; Imaging and Computer Vision, Siemens Corporate Technology, Romania; Department of Internal Medicine III, University Hospital Heidelberg, Germany",,,,Author2,Author3,a,a,a,a,a,a,Healthcare,a,,"I would argue that the parameters of a physical computational model are fit to an individual using RL, therefore RL is used for personalization.
",a,a,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950986600&doi=10.1007%2f978-3-319-24571-3_53&partnerID=40&md5=c5628b61c47dad9007b6e8161dc760f4,"Neumann D., Mansi T., Itu L., Georgescu B., Kayvanpour E., Sedaghat-Hamedani F., Haas J., Katus H., Meder B., Steidl S., Hornegger J., Comaniciu D.","Neumann, D., Imaging and Computer Vision, Siemens Corporate Technology, Princeton, NJ, Romania, Pattern Recognition Lab, FAU Erlangen-Nürnberg, Germany; Mansi, T., Imaging and Computer Vision, Siemens Corporate Technology, Princeton, NJ, Romania; Itu, L., Imaging and Computer Vision, Siemens Corporate Technology, Romania; Georgescu, B., Imaging and Computer Vision, Siemens Corporate Technology, Princeton, NJ, Romania; Kayvanpour, E., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Sedaghat-Hamedani, F., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Haas, J., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Katus, H., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Meder, B., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Steidl, S., Pattern Recognition Lab, FAU Erlangen-Nürnberg, Germany; Hornegger, J., Pattern Recognition Lab, FAU Erlangen-Nürnberg, Germany; Comaniciu, D., Imaging and Computer Vision, Siemens Corporate Technology, Princeton, NJ, Romania",1,,10.1007/978-3-319-24571-3_53,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84950986600,,,,,449,442,,,,,Scopus,,,,Conference Paper,,,9350,,2015,2015,3,1,0,Author1
94,2,2,0,2,A multimodal adaptive session manager for physical rehabilitation exercising,"Physical exercising is an essential part of any rehabilitation plan. The subject must be committed to a daily exercising routine, as well as to a frequent contact with the therapist. Rehabilitation plans can be quite expensive and time-consuming. On the other hand, tele-rehabilitation systems can be really helpful and efficient for both subjects and therapists. In this paper, we present ReAdapt, an adaptive module for a tele-rehabilitation system that takes into consideration the progress and performance of the exercising utilizing multisensing data and adjusts the session difficulty resulting to a personalized session. Multimodal data such as speech, facial expressions and body motion are being collected during the exercising and feed the system to decide on the exercise and session difficulty. We formulate the problem as a Markov Decision Process and apply a Reinforcement Learning algorithm to train and evaluate the system on simulated data. © 2015 ACM.","Physical exercising is an essential part of any rehabilitation plan. The subject must be committed to a daily exercising routine, as well as to a frequent contact with the therapist. Rehabilitation plans can be quite expensive and time-consuming. On the other hand, tele-rehabilitation systems can be really helpful and efficient for both subjects and therapists. In this paper, we present ReAdapt, an adaptive module for a tele-rehabilitation system that takes into consideration the progress and performance of the exercising utilizing multisensing data and adjusts the session difficulty resulting to a personalized session. Multimodal data such as speech, facial expressions and body motion are being collected during the exercising and feed the system to decide on the exercise and session difficulty. We formulate the problem as a Markov Decision Process and apply a Reinforcement Learning algorithm to train and evaluate the system on simulated data. © 2015 ACM.",,"HERACLEIA Lab, Computer Science and Engineering Department, University of Texas, Arlington, United States; Computer Science and Engineering Department, University of Texas, Arlington, United States", a33,,Markov Decision Process; Multimodal adaptive systems; Personalized rehabilitation systems; Reinforcement Learning,Author3,Author4,a,a,a,a,a,a,Healthcare,a,,,a,a,"8th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2015 - Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956977207&doi=10.1145%2f2769493.2769507&partnerID=40&md5=0f798bcebf296279e4bbce0786ed18c2,"Tsiakas K., Huber M., Makedon F.","Tsiakas, K., HERACLEIA Lab, Computer Science and Engineering Department, University of Texas, Arlington, United States; Huber, M., Computer Science and Engineering Department, University of Texas, Arlington, United States; Makedon, F., HERACLEIA Lab, Computer Science and Engineering Department, University of Texas, Arlington, United States",6,,10.1145/2769493.2769507,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84956977207,,,,,,,,,,,Scopus,,,,Conference Paper,,,,,2015,2015,1,0,3,Author2
95,1,0,2,1,A contextual-bandit approach to personalized news article recommendation,"Personalized web services strive to adapt their services (advertisements, news articles, etc.) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation. In this work, we model personalized recommendation of news articles as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks. The contributions of this work are three-fold. First, we propose a new, general contextual bandit algorithm that is computationally efficient and well motivated from learning theory. Second, we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic. Finally, using this offline evaluation method, we successfully applied our new algorithm to a Yahoo Front Page Today Module dataset containing over 33 million events. Results showed a 12.5% click lift compared to a standard context-free bandit algorithm, and the advantage becomes even greater when data gets more scarce. © 2010 International World Wide Web Conference Committee (IW3C2).","Personalized web services strive to adapt their services (advertisements, news articles, etc.) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation. In this work, we model personalized recommendation of news articles as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks. The contributions of this work are three-fold. First, we propose a new, general contextual bandit algorithm that is computationally efficient and well motivated from learning theory. Second, we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic. Finally, using this offline evaluation method, we successfully applied our new algorithm to a Yahoo Front Page Today Module dataset containing over 33 million events. Results showed a 12.5% click lift compared to a standard context-free bandit algorithm, and the advantage becomes even greater when data gets more scarce. © 2010 International World Wide Web Conference Committee (IW3C2).",,"Yahoo Labs., United States; Dept. of Computer Science, Princeton University, United States",,,contextual bandit; exploration/exploitation dilemma; personalization; recommender systems; web service,Author1,Author2,a,a,a,a,a,a,Recommender Systems,,,,a,a,"Proceedings of the 19th International Conference on World Wide Web, WWW '10",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954641643&doi=10.1145%2f1772690.1772758&partnerID=40&md5=0deba3e304c09a3db001bfeeca14cd3f,"Li L., Chu W., Langford J., Schapire R.E.","Li, L., Yahoo Labs., United States; Chu, W., Yahoo Labs., United States; Langford, J., Yahoo Labs., United States; Schapire, R.E., Dept. of Computer Science, Princeton University, United States",350,,10.1145/1772690.1772758,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-77954641643,,,,,670,661,,,,,Scopus,,,,Conference Paper,,,,,2010,2010,1,2,1,Author3
96,1,0,2,1,Latent contextual bandits and their application to personalized recommendations for new users,"Personalized recommendations for new users, also known as the cold-start problem, can be formulated as a contextual bandit problem. Existing contextual bandit algorithms generally rely on features alone to capture user variability. Such methods are inefficient in learning new users' interests. In this paper we propose Latent Contextual Bandits. We consider both the benefit of leveraging a set of learned latent user classes for new users, and how we can learn such latent classes from prior users. We show that our approach achieves a better regret bound than existing algorithms. We also demonstrate the benefit of our approach using a large real world dataset and a preliminary user study.","Personalized recommendations for new users, also known as the cold-start problem, can be formulated as a contextual bandit problem. Existing contextual bandit algorithms generally rely on features alone to capture user variability. Such methods are inefficient in learning new users' interests. In this paper we propose Latent Contextual Bandits. We consider both the benefit of leveraging a set of learned latent user classes for new users, and how we can learn such latent classes from prior users. We show that our approach achieves a better regret bound than existing algorithms. We also demonstrate the benefit of our approach using a large real world dataset and a preliminary user study.",,"Computer Science Department, Carnegie Mellon University, United States",,,,Author2,Author3,a,a,a,a,a,a,Recommender Systems,a,,,a,a,IJCAI International Joint Conference on Artificial Intelligence,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006132706&partnerID=40&md5=85ce893966ccab257cc8d48bc9141540,"Zhou L., Brunskill E.","Zhou, L., Computer Science Department, Carnegie Mellon University, United States; Brunskill, E., Computer Science Department, Carnegie Mellon University, United States",1,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-85006132706,,,,,3653,3646,,,,,Scopus,,,,Conference Paper,,,2016-January,,2016,2016,1,2,1,Author3
97,1,0,2,1,Personalized recommendation via parameter-free contextual bandits,"Personalized recommendation services have gained increasing popularity and attention in recent years as most useful information can be accessed online in real-time. Most online recommender systems try to address the information needs of users by virtue of both user and content information. Despite extensive recent advances, the problem of personalized recommendation remains challenging for at least two reasons. First, the user and item repositories undergo frequent changes, which makes traditional recommendation algorithms ineffective. Second, the so-called cold-start problem is difficult to address, as the information for learning a recommendation model is limited for new items or new users. Both challenges are formed by the dilemma of exploration and exploitation. In this paper, we formulate personalized recommendation as a contextual bandit problem to solve the exploration/exploitation dilemma. Specifically in our work, we propose a parameter-free bandit strategy, which employs a principled resampling approach called online bootstrap, to derive the distribution of estimated models in an online manner. Under the paradigm of probability matching, the proposed algorithm randomly samples a model from the derived distribution for every recommendation. Extensive empirical experiments on two real-world collections of web data (including online advertising and news recommendation) demonstrate the effectiveness of the proposed algorithm in terms of the click-through rate. The experimental results also show that this proposed algorithm is robust in the cold-start situation, in which there is no sufficient data or knowledge to tune the parameters. © 2015 ACM.","Personalized recommendation services have gained increasing popularity and attention in recent years as most useful information can be accessed online in real-time. Most online recommender systems try to address the information needs of users by virtue of both user and content information. Despite extensive recent advances, the problem of personalized recommendation remains challenging for at least two reasons. First, the user and item repositories undergo frequent changes, which makes traditional recommendation algorithms ineffective. Second, the so-called cold-start problem is difficult to address, as the information for learning a recommendation model is limited for new items or new users. Both challenges are formed by the dilemma of exploration and exploitation. In this paper, we formulate personalized recommendation as a contextual bandit problem to solve the exploration/exploitation dilemma. Specifically in our work, we propose a parameter-free bandit strategy, which employs a principled resampling approach called online bootstrap, to derive the distribution of estimated models in an online manner. Under the paradigm of probability matching, the proposed algorithm randomly samples a model from the derived distribution for every recommendation. Extensive empirical experiments on two real-world collections of web data (including online advertising and news recommendation) demonstrate the effectiveness of the proposed algorithm in terms of the click-through rate. The experimental results also show that this proposed algorithm is robust in the cold-start situation, in which there is no sufficient data or knowledge to tune the parameters. © 2015 ACM.",,"School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States",,,Bootstrapping; Contextual bandit; Personalization; Probability matching; Recommender systems,Author3,Author4,a,a,a,a,a,a,Recommender Systems,a,,,a,a,SIGIR 2015 - Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953732355&doi=10.1145%2f2766462.2767707&partnerID=40&md5=10663d72e54cf9a059b4485b18011be2,"Tang L., Jiang Y., Li L., Zeng C., Li T.","Tang, L., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Jiang, Y., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Li, L., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Zeng, C., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Li, T., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States",8,,10.1145/2766462.2767707,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84953732355,,,,,332,323,,,,,Scopus,,,,Conference Paper,,,,,2015,2015,1,2,1,Author3
98,2,0,0,0,Adaptive treatment allocation using sub-sampled Gaussian processes,"Personalized medicine targets the customization of treatment strategies to patients' individual characteristics. Here we consider the problem of optimizing personalized pharmacological treatment strategies for cancer. We focus primarily on developing effective strategies to collect the data necessary for the construction of personalized treatments. We formulate this problem as a contextual bandit and present a new algorithm based on repeated sub-sampling for robust data collection in this framework. We present a case study showing experiments on a simulation setting, built from real data collected in a previous animal experiments. Promising results in this case study have since lead us to deploy this strategy in a partner wet lab to allocate treatments for the next phase of animal experiments. Copyright © 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","Personalized medicine targets the customization of treatment strategies to patients' individual characteristics. Here we consider the problem of optimizing personalized pharmacological treatment strategies for cancer. We focus primarily on developing effective strategies to collect the data necessary for the construction of personalized treatments. We formulate this problem as a contextual bandit and present a new algorithm based on repeated sub-sampling for robust data collection in this framework. We present a case study showing experiments on a simulation setting, built from real data collected in a previous animal experiments. Promising results in this case study have since lead us to deploy this strategy in a partner wet lab to allocate treatments for the next phase of animal experiments. Copyright © 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Department of Electrical Engineering and Computer Engineering Universite, Laval, QC, Canada; School of Computer Science, McGill University, Montreal, Canada",,,,Author4,Author1,a,a,a,a,a,a,Healthcare,,,,a,a,AAAI Fall Symposium - Technical Report,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964573656&partnerID=40&md5=534281ed1c527f1c0b6fa9b0c046b90e,"Durand A., Pineau J.","Durand, A., Department of Electrical Engineering and Computer Engineering Universite, Laval, QC, Canada; Pineau, J., School of Computer Science, McGill University, Montreal, Canada",,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-84964573656,,,,,11,9,,,,,Scopus,,,,Conference Paper,,,FS-15-04,,2015,2015,3,0,1,Author1
99,0,0,0,0,A self-taught artificial agent for multi-physics computational model personalization,"Personalization is the process of fitting a model to patient data, a critical step towards application of multi-physics computational models in clinical practice. Designing robust personalization algorithms is often a tedious, time-consuming, model- and data-specific process. We propose to use artificial intelligence concepts to learn this task, inspired by how human experts manually perform it. The problem is reformulated in terms of reinforcement learning. In an off-line phase, Vito, our self-taught artificial agent, learns a representative decision process model through exploration of the computational model: it learns how the model behaves under change of parameters. The agent then automatically learns an optimal strategy for on-line personalization. The algorithm is model-independent; applying it to a new model requires only adjusting few hyper-parameters of the agent and defining the observations to match. The full knowledge of the model itself is not required. Vito was tested in a synthetic scenario, showing that it could learn how to optimize cost functions generically. Then Vito was applied to the inverse problem of cardiac electrophysiology and the personalization of a whole-body circulation model. The obtained results suggested that Vito could achieve equivalent, if not better goodness of fit than standard methods, while being more robust (up to 11% higher success rates) and with faster (up to seven times) convergence rate. Our artificial intelligence approach could thus make personalization algorithms generalizable and self-adaptable to any patient and any model. © 2016","Personalization is the process of fitting a model to patient data, a critical step towards application of multi-physics computational models in clinical practice. Designing robust personalization algorithms is often a tedious, time-consuming, model- and data-specific process. We propose to use artificial intelligence concepts to learn this task, inspired by how human experts manually perform it. The problem is reformulated in terms of reinforcement learning. In an off-line phase, Vito, our self-taught artificial agent, learns a representative decision process model through exploration of the computational model: it learns how the model behaves under change of parameters. The agent then automatically learns an optimal strategy for on-line personalization. The algorithm is model-independent; applying it to a new model requires only adjusting few hyper-parameters of the agent and defining the observations to match. The full knowledge of the model itself is not required. Vito was tested in a synthetic scenario, showing that it could learn how to optimize cost functions generically. Then Vito was applied to the inverse problem of cardiac electrophysiology and the personalization of a whole-body circulation model. The obtained results suggested that Vito could achieve equivalent, if not better goodness of fit than standard methods, while being more robust (up to 11% higher success rates) and with faster (up to seven times) convergence rate. Our artificial intelligence approach could thus make personalization algorithms generalizable and self-adaptable to any patient and any model. © 2016",,"Medical Imaging Technologies, Siemens Healthcare GmbH, Erlangen, Germany; Medical Imaging Technologies, Siemens Healthcare, Princeton, United States; Pattern Recognition Lab, FAU Erlangen-Nürnberg, Erlangen, Germany; Siemens Corporate Technology, Siemens SRL, Brasov, Romania; Transilvania University of Brasov, Brasov, Romania; Department of Internal Medicine III, University Hospital Heidelberg, Germany",,,Artificial intelligence; Computational modeling; Model personalization; Reinforcement learning,Author1,Author2,a,a,a,a,a,a,Healthcare,,,,a,a,Medical Image Analysis,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964690557&doi=10.1016%2fj.media.2016.04.003&partnerID=40&md5=b1d426957966100eec637cc992aa1ff5,"Neumann D., Mansi T., Itu L., Georgescu B., Kayvanpour E., Sedaghat-Hamedani F., Amr A., Haas J., Katus H., Meder B., Steidl S., Hornegger J., Comaniciu D.","Neumann, D., Medical Imaging Technologies, Siemens Healthcare GmbH, Erlangen, Germany, Pattern Recognition Lab, FAU Erlangen-Nürnberg, Erlangen, Germany; Mansi, T., Medical Imaging Technologies, Siemens Healthcare, Princeton, United States; Itu, L., Siemens Corporate Technology, Siemens SRL, Brasov, Romania, Transilvania University of Brasov, Brasov, Romania; Georgescu, B., Medical Imaging Technologies, Siemens Healthcare, Princeton, United States; Kayvanpour, E., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Sedaghat-Hamedani, F., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Amr, A., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Haas, J., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Katus, H., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Meder, B., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Steidl, S., Pattern Recognition Lab, FAU Erlangen-Nürnberg, Erlangen, Germany; Hornegger, J., Pattern Recognition Lab, FAU Erlangen-Nürnberg, Erlangen, Germany; Comaniciu, D., Medical Imaging Technologies, Siemens Healthcare, Princeton, United States",5,,10.1016/j.media.2016.04.003,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84964690557,,,,,64,52,,,,,Scopus,,,,Article,,,34,,2016,2016,4,0,0,Author2
100,1,1,1,1,A Bayesian reinforcement learning approach for customizing human-robot interfaces,"Personal robots are becoming increasingly prevalent, which raises a number of interesting issues regarding the design and customization of interfaces to such platforms. The particular problem addressed by this paper is the use of learning methods to improve the quality and effectiveness of human-machine interaction onboard a robotic wheelchair. In support of this, we present a method for learning and adapting probabilistic models with the aid of a human operator. We use a Bayesian reinforcement learning framework, that allows us to mix learning and execution, as well as take advantage of prior information about the world. We address the problems of learning, handling a partially observable environment, and limiting the number of action requests. We demonstrate empirical feasibility of our approach on an interface for an autonomous wheelchair. Copyright 2009 ACM.","Personal robots are becoming increasingly prevalent, which raises a number of interesting issues regarding the design and customization of interfaces to such platforms. The particular problem addressed by this paper is the use of learning methods to improve the quality and effectiveness of human-machine interaction onboard a robotic wheelchair. In support of this, we present a method for learning and adapting probabilistic models with the aid of a human operator. We use a Bayesian reinforcement learning framework, that allows us to mix learning and execution, as well as take advantage of prior information about the world. We address the problems of learning, handling a partially observable environment, and limiting the number of action requests. We demonstrate empirical feasibility of our approach on an interface for an autonomous wheelchair. Copyright 2009 ACM.",,"School of Computer Science, McGill University, Montreal, QC H3A 2A7, Canada",,,Activity & plan recognition; Intelligent assistants; Intelligent interfaces for ubiquitous computing,Author3,Author4,a,a,u,a,a,a,Interfaces / HCI,a,It's not clear whether the goal of IRL is personalized interfaces or interfaces in general.,I agree that the goal is unclear. Could also be a personal assistant btw.,u,a,"International Conference on Intelligent User Interfaces, Proceedings IUI",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953871643&doi=10.1145%2f1502650.1502700&partnerID=40&md5=265764ecab4726718eea0e4605b2a094,"Atrash A., Pineau J.","Atrash, A., School of Computer Science, McGill University, Montreal, QC H3A 2A7, Canada; Pineau, J., School of Computer Science, McGill University, Montreal, QC H3A 2A7, Canada",10,,10.1145/1502650.1502700,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-77953871643,,,,,359,355,,,,,Scopus,,,,Conference Paper,,,,,,2009,0,4,0,Author4
101,0,0,1,0,Personalizing robot behavior for interruption in social human-robot interaction,"People engaging in an activity usually has individual tolerance to be interrupted [1], [2]. Humans subconsciously adapt their behaviors to draw other one's attention and to get into a conversation based on their historical experiences, but robots often fail to be aware of humans' feeling and thus interrupt their users repeatedly. To endow service robots with such socially acceptable ability, we propose an online human-aware interactive learning framework in this paper, under which the robot personalizes its behaviors according to both observed user's attention and its conjecture about user's awareness of itself. To this purpose, the correlation between the robot's theory of awareness, user's attention and robot behavior are explored through reinforcement learning techniques. The conducted experiment shows that the robot can personalize its interruption strategy, and the optimal policies converged for at least 26 episodes.","People engaging in an activity usually has individual tolerance to be interrupted [1], [2]. Humans subconsciously adapt their behaviors to draw other one's attention and to get into a conversation based on their historical experiences, but robots often fail to be aware of humans' feeling and thus interrupt their users repeatedly. To endow service robots with such socially acceptable ability, we propose an online human-aware interactive learning framework in this paper, under which the robot personalizes its behaviors according to both observed user's attention and its conjecture about user's awareness of itself. To this purpose, the correlation between the robot's theory of awareness, user's attention and robot behavior are explored through reinforcement learning techniques. The conducted experiment shows that the robot can personalize its interruption strategy, and the optimal policies converged for at least 26 episodes.",,,,"Department of Computer Science and Information Engineering, National Taiwan University, Taipei 10617, Taiwan",,Author1,Author2,a,a,a,a,a,a,"Other (please in ""remark"")",,Robotics,,a,a,2014 IEEE International Workshop on Advanced Robotics and its Social Impacts,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7020978,,Y. S. Chiang; T. S. Chu; C. D. Lim; T. Y. Wu; S. H. Tseng; L. C. Fu,,0,,10.1109/ARSO.2014.7020978,IEEE Xplore,1,,20150126,49,,Face;Hidden Markov models;Interrupters;Learning (artificial intelligence);Markov processes;Robot sensing systems,human-robot interaction;learning (artificial intelligence);service robots;social sciences,interruption strategy;online human-aware interactive learning framework;reinforcement learning techniques;robot behavior personalization;robot theory of awareness;service robots;social human-robot interaction;user attention;user awareness,,2162-7568;21627568,,11-13 Sept. 2014,,,,,,,,,,,,IEEE,23,,Electronic:978-1-4799-6968-5; POD:978-1-4799-6969-2; USB:978-1-4799-6967-8,,44,IEEE Conferences,,,,,2014,2014,3,1,0,Author3
102,1,0,1,1,High-order feature-based mixture models of classification learning predict individual learning curves and enable personalized teaching,"Pattern classification learning tasks are commonly used to explore learning strategies in human subjects. The universal and individual traits of learning such tasks reflect our cognitive abilities and have been of interest both psychophysically and clinically. From a computational perspective, these tasks are hard, because the number of patterns and rules one could consider even in simple cases is exponentially large. Thus, when we learn to classify we must use simplifying assumptions and generalize. Studies of human behavior in probabilistic learning tasks have focused on rules in which pattern cues are independent, and also described individual behavior in terms of simple, single-cue, feature-based models. Here, we conducted psychophysical experiments in which people learned to classify binary sequences according to deterministic rules of different complexity, including high-order, multicue-dependent rules. We show that human performance on such tasks is very diverse, but that a class of reinforcement learning-like models that use a mixture of features captures individual learning behavior surprisinglywell. Thesemodels reflect the important role of subjects' priors, and their reliance on high-order features even when learning a low-order rule. Further, we show that these models predict future individual answers to a high degree of accuracy. We then use these models to build personally optimized teaching sessions and boost learning.","Pattern classification learning tasks are commonly used to explore learning strategies in human subjects. The universal and individual traits of learning such tasks reflect our cognitive abilities and have been of interest both psychophysically and clinically. From a computational perspective, these tasks are hard, because the number of patterns and rules one could consider even in simple cases is exponentially large. Thus, when we learn to classify we must use simplifying assumptions and generalize. Studies of human behavior in probabilistic learning tasks have focused on rules in which pattern cues are independent, and also described individual behavior in terms of simple, single-cue, feature-based models. Here, we conducted psychophysical experiments in which people learned to classify binary sequences according to deterministic rules of different complexity, including high-order, multicue-dependent rules. We show that human performance on such tasks is very diverse, but that a class of reinforcement learning-like models that use a mixture of features captures individual learning behavior surprisinglywell. Thesemodels reflect the important role of subjects' priors, and their reliance on high-order features even when learning a low-order rule. Further, we show that these models predict future individual answers to a high degree of accuracy. We then use these models to build personally optimized teaching sessions and boost learning.",,"Department of Neurobiology, Weizmann Institute of Science, Rehovot 76100, Israel",,,Inference; Information; Maximum entropy,Author3,Author4,a,a,a,a,a,a,Intelligent Tutors,,We'll have to assess whether RL-like model entails an RL algo,"I would estimate this is not a computational model, but we cannot derive this. If I would have been the first reviewer I would probably have given an unknown.",a,a,Proceedings of the National Academy of Sciences of the United States of America,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872186192&doi=10.1073%2fpnas.1211606110&partnerID=40&md5=6691bc98916c6238b952fd11589c72bd,"Cohen Y., Schneidman E.","Cohen, Y., Department of Neurobiology, Weizmann Institute of Science, Rehovot 76100, Israel; Schneidman, E., Department of Neurobiology, Weizmann Institute of Science, Rehovot 76100, Israel",4,,10.1073/pnas.1211606110,SCOPUS,1,,,,,,,,,,2,,,,2-s2.0-84872186192,,,,,689,684,,,,,Scopus,,,,Article,,,110,,2013,2013,1,3,0,Author4
103,2,1,1,1,Using Options to Accelerate Learning of New Tasks According to Human Preferences,"Over the years, people need to incorporate a wider range of information and multiple objectives for their decision making. Nowadays, humans are dependent on computer systems to interpret and take profit from the huge amount of available data on the Internet. Hence, varied services, such as location- based systems, must combine a huge quantity of raw data to give the desired response to the user. However, as humans have different preferences, the optimal answer is different for each user profile, and few systems offer the service of solving tasks in a customized manner for each user. Reinforcement Learning (RL) has been used to autonomously train systems to solve (or assist on) decision-making tasks according to user preferences. However, the learning process is very slow and require many interactions with the environment. Therefore, we here propose to reuse knowledge from previous tasks to accelerate the learning process in a new task. Our proposal, called Multiobjective Options, accelerates learning while providing a customized solution according to the current user preferences. Our experiments in the Tourist World Domain show that our proposal learns faster and better than regular learning, and that the achieved solutions follow user preferences. © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","Over the years, people need to incorporate a wider range of information and multiple objectives for their decision making. Nowadays, humans are dependent on computer systems to interpret and take profit from the huge amount of available data on the Internet. Hence, varied services, such as location- based systems, must combine a huge quantity of raw data to give the desired response to the user. However, as humans have different preferences, the optimal answer is different for each user profile, and few systems offer the service of solving tasks in a customized manner for each user. Reinforcement Learning (RL) has been used to autonomously train systems to solve (or assist on) decision-making tasks according to user preferences. However, the learning process is very slow and require many interactions with the environment. Therefore, we here propose to reuse knowledge from previous tasks to accelerate the learning process in a new task. Our proposal, called Multiobjective Options, accelerates learning while providing a customized solution according to the current user preferences. Our experiments in the Tourist World Domain show that our proposal learns faster and better than regular learning, and that the achieved solutions follow user preferences. © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Escola Politdcnica da Universidade de São Paulo, São Paulo, Brazil",,,,Author1,Author2,a,a,a,a,a,a,Personal Assistants,,,,a,a,AAAI Workshop - Technical Report,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046100452&partnerID=40&md5=5301426c1524d266fb75276e43518ac7,"Bonini R.C., Da Silva F.L., Spina E., Costa A.H.R.","Bonini, R.C., Escola Politdcnica da Universidade de São Paulo, São Paulo, Brazil; Da Silva, F.L., Escola Politdcnica da Universidade de São Paulo, São Paulo, Brazil; Spina, E., Escola Politdcnica da Universidade de São Paulo, São Paulo, Brazil; Costa, A.H.R., Escola Politdcnica da Universidade de São Paulo, São Paulo, Brazil",,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-85046100452,,,,,650,643,,,,,Scopus,,,,Conference Paper,,,WS-17-01 - WS-17-15,,2017,2017,0,3,1,Author1
104,2,0,1,1,Contextual multi-armed bandit algorithms for personalized learning action selection,"Optimizing the selection of learning resources and practice questions to address each individual student's needs has the potential to improve students' learning efficiency. In this paper, we study the problem of selecting a personalized learning action for each student (e.g. watching a lecture video, working on a practice question, etc.), based on their prior performance, in order to maximize their learning outcome. We formulate this problem using the contextual multi-armed bandits framework, where students' prior concept knowledge states (estimated from their responses to questions in previous assessments) correspond to contexts, the personalized learning actions correspond to arms, and their performance on future assessments correspond to rewards. We propose three new Bayesian policies to select personalized learning actions for students that each exhibits advantages over prior work, and experimentally validate them using real-world datasets.","Optimizing the selection of learning resources and practice questions to address each individual student's needs has the potential to improve students' learning efficiency. In this paper, we study the problem of selecting a personalized learning action for each student (e.g. watching a lecture video, working on a practice question, etc.), based on their prior performance, in order to maximize their learning outcome. We formulate this problem using the contextual multi-armed bandits framework, where students' prior concept knowledge states (estimated from their responses to questions in previous assessments) correspond to contexts, the personalized learning actions correspond to arms, and their performance on future assessments correspond to rewards. We propose three new Bayesian policies to select personalized learning actions for students that each exhibits advantages over prior work, and experimentally validate them using real-world datasets.",,,,"Rice University, United States of America",contextual bandits;personalized learning,Author1,Author2,a,a,a,a,a,a,Intelligent Tutors,,,,a,a,"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7953377,,I. Manickam; A. S. Lan; R. G. Baraniuk,,,,10.1109/ICASSP.2017.7953377,IEEE Xplore,1,,20170619,6348,,Approximation algorithms;Bayes methods;Context;Programmable logic arrays;Random variables;Schedules;Uncertainty,educational institutions,Bayesian policies;contextual multi-armed bandit algorithms;learning resources;personalized learning action selection;prior concept knowledge states;student learning efficiency,,,,5-9 March 2017,,,,,,,,,,,,IEEE,,,Electronic:978-1-5090-4117-6; POD:978-1-5090-4118-3; USB:978-1-5090-4116-9,,6344,IEEE Conferences,,,,,2017,2017,1,2,1,Author1
105,1,0,2,1,Personalized response generation by Dual-learning based domain adaptation,"Open-domain conversation is one of the most challenging artificial intelligence problems, which involves language understanding, reasoning, and the utilization of common sense knowledge. The goal of this paper is to further improve the response generation, using personalization criteria. We propose a novel method called PRGDDA (Personalized Response Generation by Dual-learning based Domain Adaptation) which is a personalized response generation model based on theories of domain adaptation and dual learning. During the training procedure, PRGDDA first learns the human responding style from large general data (without user-specific information), and then fine-tunes the model on a small size of personalized data to generate personalized conversations with a dual learning mechanism. We conduct experiments to verify the effectiveness of the proposed model on two real-world datasets in both English and Chinese. Experimental results show that our model can generate better personalized responses for different users. © 2018 Elsevier Ltd","Open-domain conversation is one of the most challenging artificial intelligence problems, which involves language understanding, reasoning, and the utilization of common sense knowledge. The goal of this paper is to further improve the response generation, using personalization criteria. We propose a novel method called PRGDDA (Personalized Response Generation by Dual-learning based Domain Adaptation) which is a personalized response generation model based on theories of domain adaptation and dual learning. During the training procedure, PRGDDA first learns the human responding style from large general data (without user-specific information), and then fine-tunes the model on a small size of personalized data to generate personalized conversations with a dual learning mechanism. We conduct experiments to verify the effectiveness of the proposed model on two real-world datasets in both English and Chinese. Experimental results show that our model can generate better personalized responses for different users. © 2018 Elsevier Ltd",,"Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; School of Information Management and Engineering, Shanghai University of Finance and Economics, Shanghai, China; School of Computing Science, Zhejiang University, Hangzhou, China; College of Computer Science and Software, Shenzhen University, Shenzhen, China; School of Computer Science, South China Normal University, Guangzhou, China",,,Deep reinforcement learning; Domain adaptation; Dual learning; Personalized response generation,Author3,Author4,a,a,a,a,a,a,"Other (please in ""remark"")",a,domain: dialog planning,,a,a,Neural Networks,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045705590&doi=10.1016%2fj.neunet.2018.03.009&partnerID=40&md5=62c0e7468a872e17eeb031613314833a,"Yang M., Tu W., Qu Q., Zhao Z., Chen X., Zhu J.","Yang, M., Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Tu, W., School of Information Management and Engineering, Shanghai University of Finance and Economics, Shanghai, China; Qu, Q., Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Zhao, Z., School of Computing Science, Zhejiang University, Hangzhou, China; Chen, X., College of Computer Science and Software, Shenzhen University, Shenzhen, China; Zhu, J., School of Computer Science, South China Normal University, Guangzhou, China",,,10.1016/j.neunet.2018.03.009,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85045705590,,,,,82,72,,,,,Scopus,,,,Article,,,103,,2018,2018,1,2,1,Author3
106,2,0,1,1,Trust and privacy correlations in social networks: A deep learning framework,"Online Social Networks (OSNs) remain the focal point of Internet usage. Since the beginning, networking sites tried best to have right privacy mechanisms in place for users, enabling them to share the right content with the right audience. With all these efforts, privacy customizations remain hard for users across the sites. Existing research that address this problem mainly focus on semi-supervised strategies that introduce extra complexity by requiring the user to manually specify initial privacy preferences for their friends. In this work, we suggest an adaptive solution that can dynamically generate privacy labels for users in OSNs. To this end, we introduce a deep reinforcement learning framework that targets two key problems in OSNs like Facebook: the exposure of users' interactions through the network to less trusted direct friends, and the possibility of propagating user updates through direct friends' interactions to indirect friends. By implementing this framework, we aim at understanding how social trust and privacy could be correlated, specifically in a dynamic fashion. We report the ranked dependence between the generated privacy labels and the estimated user trust values, which indicate the ability of the framework to identify the highly trusted users and share with them higher percentages of data. © 2016 IEEE.","Online Social Networks (OSNs) remain the focal point of Internet usage. Since the beginning, networking sites tried best to have right privacy mechanisms in place for users, enabling them to share the right content with the right audience. With all these efforts, privacy customizations remain hard for users across the sites. Existing research that address this problem mainly focus on semi-supervised strategies that introduce extra complexity by requiring the user to manually specify initial privacy preferences for their friends. In this work, we suggest an adaptive solution that can dynamically generate privacy labels for users in OSNs. To this end, we introduce a deep reinforcement learning framework that targets two key problems in OSNs like Facebook: the exposure of users' interactions through the network to less trusted direct friends, and the possibility of propagating user updates through direct friends' interactions to indirect friends. By implementing this framework, we aim at understanding how social trust and privacy could be correlated, specifically in a dynamic fashion. We report the ranked dependence between the generated privacy labels and the estimated user trust values, which indicate the ability of the framework to identify the highly trusted users and share with them higher percentages of data. © 2016 IEEE.",,"KTH, Royal Institute of Technology, Sweden; University of Insubria, Italy",7752236,,,Author4,Author1,a,a,a,a,a,a,Recommender Systems,,"Recommends privacy settings. Personalization not superclear, but seems to be implied.",,a,a,"Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2016",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006765626&doi=10.1109%2fASONAM.2016.7752236&partnerID=40&md5=fb08bc9dfc0d971eb728bd1856cff9d7,"Jaradat S., Dokoohaki N., Matskin M., Ferrari E.","Jaradat, S., KTH, Royal Institute of Technology, Sweden; Dokoohaki, N., KTH, Royal Institute of Technology, Sweden; Matskin, M., KTH, Royal Institute of Technology, Sweden; Ferrari, E., University of Insubria, Italy",1,,10.1109/ASONAM.2016.7752236,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85006765626,,,,,206,203,,,,,Scopus,,,,Conference Paper,,,,,2016,2016,1,2,1,Author1
107,1,1,1,1,A self-adaptive system for improving autonomy and public spaces accessibility for elderly,"Nowadays, there is an increasing need to provide a safe and independent living for cognitively deficient population. Notably, we have to improve seniors’ autonomy and their public spaces accessibility. Giving these observations, the aim of this paper is to provide a personalized adaptive assisting system for elderly. More precisely, this paper presents the specification and implementation of a self-organizing multi-agent system able to abstract the different distributed components involved in user’s environment. This system is able to detect different possible situations that a user could face in his daily outdoors activities and propose accordingly appropriate actions. This system not only learns user’s habits from its perceptions but also improves its recommendations thanks to feedbacks provided by stakeholders (family, doctors …) following a reinforcement learning reasoning. Finally, we present our system evaluation specially its learning capabilities through different scenarios that have been generated automatically. © Springer International Publishing AG 2018.","Nowadays, there is an increasing need to provide a safe and independent living for cognitively deficient population. Notably, we have to improve seniors’ autonomy and their public spaces accessibility. Giving these observations, the aim of this paper is to provide a personalized adaptive assisting system for elderly. More precisely, this paper presents the specification and implementation of a self-organizing multi-agent system able to abstract the different distributed components involved in user’s environment. This system is able to detect different possible situations that a user could face in his daily outdoors activities and propose accordingly appropriate actions. This system not only learns user’s habits from its perceptions but also improves its recommendations thanks to feedbacks provided by stakeholders (family, doctors …) following a reinforcement learning reasoning. Finally, we present our system evaluation specially its learning capabilities through different scenarios that have been generated automatically. © Springer International Publishing AG 2018.",,"118 Route de Narbonne, Toulouse, France; 2 Rue du Doyen-Gabriel-Marty, Toulouse, France",,,AMAS theory; Assisted living system; Multi-agent system; Reinforcement learning,Author1,Author2,a,a,a,a,a,a,Personal Assistants,,,,a,a,"Smart Innovation, Systems and Technologies",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020375682&doi=10.1007%2f978-3-319-59394-4_6&partnerID=40&md5=e06d5cd071339abef3f12d85ab89af53,"Triki S., Hanachi C.","Triki, S., 118 Route de Narbonne, Toulouse, France; Hanachi, C., 2 Rue du Doyen-Gabriel-Marty, Toulouse, France",,,10.1007/978-3-319-59394-4_6,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85020375682,,,,,66,53,,,,,Scopus,,,,Conference Paper,,,74,,2018,2018,0,4,0,Author1
108,1,0,0,0,Adaptive interventions treatment modelling and regimen optimization using Sequential Multiple Assignment Randomized Trials (SMART) and Q-learning,"Nowadays, pharmacological practices are focused on a single best treatment to treat a disease which sounds impractical as the same treatment may not work the same way for every patient. Thus, there is a need of shift towards more patient-centric rather than disease-centric approach, in which personal characteristics of a patient or biomarkers are used to determine the tailored optimal treatment. The ""one size fits all"" concept is contradicted by research area of personalized medicine. The Sequential Multiple Assignment Randomized Trial (SMART) is a multi-stage trials to inform the development of dynamic treatment regimens (DTR's). In SMART, a subject is randomized through different stages of treatment where each stage corresponds to a treatment decision. These types of adaptive interventions are individualized and are repeatedly adjusted across time based on patient's individual clinical characteristics and ongoing performance. The reinforcement learning (Q-learning), a computational algorithm for optimization of treatment regimens to maximize desired clinical outcome is used in optimizing the sequence of treatments. This statistical model contains regression analysis for function approximation of data from clinical trials. The model will predict a series of regimens across time, depending on the biomarkers of a new participant for optimizing the weight management decision rules.","Nowadays, pharmacological practices are focused on a single best treatment to treat a disease which sounds impractical as the same treatment may not work the same way for every patient. Thus, there is a need of shift towards more patient-centric rather than disease-centric approach, in which personal characteristics of a patient or biomarkers are used to determine the tailored optimal treatment. The ""one size fits all"" concept is contradicted by research area of personalized medicine. The Sequential Multiple Assignment Randomized Trial (SMART) is a multi-stage trials to inform the development of dynamic treatment regimens (DTR's). In SMART, a subject is randomized through different stages of treatment where each stage corresponds to a treatment decision. These types of adaptive interventions are individualized and are repeatedly adjusted across time based on patient's individual clinical characteristics and ongoing performance. The reinforcement learning (Q-learning), a computational algorithm for optimization of treatment regimens to maximize desired clinical outcome is used in optimizing the sequence of treatments. This statistical model contains regression analysis for function approximation of data from clinical trials. The model will predict a series of regimens across time, depending on the biomarkers of a new participant for optimizing the weight management decision rules.",,"Department of Electrical Engineering and Computer Science, United States; Sanford Health Research SD, United States; Department of Construction and Operations Management, South Dakota State University, United States",,,Dynamic treatment regimens (DTR); Personalized medicine; Q-learning algorithm; Regression analysis; Sequential Multiple Assignment Randomized Trial (SMART),Author2,Author3,a,a,a,a,a,a,Healthcare,a,,,a,a,67th Annual Conference and Expo of the Institute of Industrial Engineers 2017,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031025743&partnerID=40&md5=80f6536d12d6f4028e4e66ec08b8128d,"Baniya A., Herrmann S., Qiao Q., Lu H.","Baniya, A., Department of Electrical Engineering and Computer Science, United States, Department of Construction and Operations Management, South Dakota State University, United States; Herrmann, S., Sanford Health Research SD, United States; Qiao, Q., Department of Electrical Engineering and Computer Science, United States; Lu, H., Department of Construction and Operations Management, South Dakota State University, United States",,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-85031025743,,,,,1192,1187,,,,,Scopus,,,,Conference Paper,,,,,2017,2017,3,1,0,Author1
109,1,0,2,1,On personalizing Web content through reinforcement learning,"Nowadays, a large use of personalization techniques is used to adapt Web content to users’ habits, mainly with the aim of offering appropriate products and services. This paper presents a system that uses personalization and adaptation techniques, in order to transcode or modify contents (e.g., adapt text fonts) so as to meet preferences and needs of elderly users and users with disabilities. Such an adaptation can have a positive effect, in particular for users with some reading-related disabilities (i.e., people with dyslexia, users with low vision, users with color blindness, elderly people.). To avoid issues arising from applying transformations to the whole content, the proposed system uses Web intelligence to perform automatic adaptations on single elements composing a Web page. The transformation is applied on the basis of a reinforcement learning algorithm which manages a user profile. The system is evaluated through simulations and a real assessment, where elderly users where asked to use the system prototype for a time period and to perform some specific tasks. Results of the qualitative evaluation confirm the feasibility of the proposal, showing its validity and the users’ appreciation. © 2016, Springer-Verlag Berlin Heidelberg.","Nowadays, a large use of personalization techniques is used to adapt Web content to users’ habits, mainly with the aim of offering appropriate products and services. This paper presents a system that uses personalization and adaptation techniques, in order to transcode or modify contents (e.g., adapt text fonts) so as to meet preferences and needs of elderly users and users with disabilities. Such an adaptation can have a positive effect, in particular for users with some reading-related disabilities (i.e., people with dyslexia, users with low vision, users with color blindness, elderly people.). To avoid issues arising from applying transformations to the whole content, the proposed system uses Web intelligence to perform automatic adaptations on single elements composing a Web page. The transformation is applied on the basis of a reinforcement learning algorithm which manages a user profile. The system is evaluated through simulations and a real assessment, where elderly users where asked to use the system prototype for a time period and to perform some specific tasks. Results of the qualitative evaluation confirm the feasibility of the proposal, showing its validity and the users’ appreciation. © 2016, Springer-Verlag Berlin Heidelberg.",,"Department of Computer Science and Engineering, Università di Bologna, Via Mura Anteo Zamboni 7, Bologna, Italy",,,Content adaptation; Legibility; Reinforcement learning; User profiling; Web personalization,Author3,Author4,a,a,a,a,a,a,Interfaces / HCI,a,,"Hmmm, difficult to judge domain.",a,a,Universal Access in the Information Society,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960114893&doi=10.1007%2fs10209-016-0463-2&partnerID=40&md5=5261867eed4eafe7188ba50052e7410c,"Ferretti S., Mirri S., Prandi C., Salomoni P.","Ferretti, S., Department of Computer Science and Engineering, Università di Bologna, Via Mura Anteo Zamboni 7, Bologna, Italy; Mirri, S., Department of Computer Science and Engineering, Università di Bologna, Via Mura Anteo Zamboni 7, Bologna, Italy; Prandi, C., Department of Computer Science and Engineering, Università di Bologna, Via Mura Anteo Zamboni 7, Bologna, Italy; Salomoni, P., Department of Computer Science and Engineering, Università di Bologna, Via Mura Anteo Zamboni 7, Bologna, Italy",1,,10.1007/s10209-016-0463-2,SCOPUS,1,,,,,,,,,,2,,,,2-s2.0-84960114893,,,,,410,395,,,,,Scopus,,,,Article,,,16,,2017,2017,1,2,1,Author3
110,1,1,1,1,Reinforcement Learning of User Preferences for a Ubiquitous Personal Assistant,"New technologies bring a multiplicity of new possibilities for users to work with computers. Not only are spaces more and more equipped with stationary computers or notebooks, but more and more users carry mobile devices with them (smart-phones, personal digital …","New technologies bring a multiplicity of new possibilities for users to work with computers. Not only are spaces more and more equipped with stationary computers or notebooks, but more and more users carry mobile devices with them (smart-phones, personal digital …",,,,,,Author4,Author1,a,a,a,a,a,a,Personal Assistants,,Not 100% sure about peer review. Cannot find it.,,a,a,,"http://scholar.google.com/scholar?cluster=8925224382166895311&hl=en&as_sdt=0,5",http://scholar.google.com/https://www.intechopen.com/download/pdf/13053,,,2,,,Google Scholar,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2011,2011,0,4,0,Author2
111,0,0,0,0,Hello! Mr. Sage,"Mr. Sage, an ideal Professor, can motivate, evoke involvement, and supply rewards and reinforcement, for each individual student all within the same class period. Although his minimum standards require mastery of concepts at all levels of learning, from simple memory to creativity, he does not expect the same performance of all students. His grades are directly related to the breadth of subject matter and depth of mastery. He personalizes his instruction within the classroom and in homework assignments by a range in the difficulty of illustration, thereby challenging each student to his capacity. He uses tests and examinations as learning experiences. To achieve this he combines the excellent feature of the conventional system and the current views on personalized system of instruction.","Mr. Sage, an ideal Professor, can motivate, evoke involvement, and supply rewards and reinforcement, for each individual student all within the same class period. Although his minimum standards require mastery of concepts at all levels of learning, from simple memory to creativity, he does not expect the same performance of all students. His grades are directly related to the breadth of subject matter and depth of mastery. He personalizes his instruction within the classroom and in homework assignments by a range in the difficulty of illustration, thereby challenging each student to his capacity. He uses tests and examinations as learning experiences. To achieve this he combines the excellent feature of the conventional system and the current views on personalized system of instruction.",,,,,,Author1,Author2,a,a,a,a,a,a,Intelligent Tutors,,,,a,a,IEEE Transactions on Education,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4321037,,O. E. Lancaster,,0,,10.1109/TE.1976.4321037,IEEE Xplore,1,,20071112,58,,Engineering education;Eyes;History;Testing;Vehicle dynamics;Vehicles,,,,0018-9359;00189359,2,May 1976,,,,,,,,,,,,IEEE,15,,,,54,IEEE Journals & Magazines,,,19,,1976,1976,4,0,0,Author3
112,2,2,0,2,Encouraging physical activity in patients with diabetes through automatic personalized feedback via reinforcement learning improves glycemic control,"Most patients with type 2 diabetes are sedentary despite the clear benefit of regular physical activity, including better glucose control and improvement in quality of life (1). Smartphones could potentially improve patient care by continual communication with patients and sensors …","Most patients with type 2 diabetes are sedentary despite the clear benefit of regular physical activity, including better glucose control and improvement in quality of life (1). Smartphones could potentially improve patient care by continual communication with patients and sensors …",,"Rambam Health Care Campus, Haifa, Israel; Faculty of Medicine, Technion-Israel Institute of Technology, Haifa, Israel; Faculty of Electrical Engineering, Technion-Israel Institute of Technology, Haifa, Israel; Faculty of Industrial Engineering and Management, Technion-Israel Institute of Technology, Haifa, Israel; Microsoft Research, Herzliya, Israel",,,,Author4,Author1,a,a,a,a,a,a,Healthcare,,,,a,a,Diabetes Care,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962053103&doi=10.2337%2fdc15-2340&partnerID=40&md5=6a9f3b67dee5f793093684afee87229a,"Hochberg I., Feraru G., Kozdoba M., Mannor S., Tennenholtz M., Yom-Tov E.","Hochberg, I., Rambam Health Care Campus, Haifa, Israel; Feraru, G., Faculty of Medicine, Technion-Israel Institute of Technology, Haifa, Israel; Kozdoba, M., Faculty of Electrical Engineering, Technion-Israel Institute of Technology, Haifa, Israel; Mannor, S., Faculty of Electrical Engineering, Technion-Israel Institute of Technology, Haifa, Israel; Tennenholtz, M., Faculty of Industrial Engineering and Management, Technion-Israel Institute of Technology, Haifa, Israel; Yom-Tov, E., Microsoft Research, Herzliya, Israel",6,,10.2337/dc15-2340,SCOPUS,1,,,,,,,,,,4,,,,2-s2.0-84962053103,,,,,e60,e59,,,,,Scopus,,,,Letter,,,39,,2016,2016,1,0,3,Author2
113,1,0,2,1,Gaussian processes for fast policy optimisation of POMDP-based dialogue managers,"Modelling dialogue as a Partially Observable Markov Decision Process (POMDP) enables a dialogue policy robust to speech understanding errors to be learnt. However, a major challenge in POMDP policy learning is to maintain tractability, so the use of approximation is inevitable. We propose applying Gaussian Processes in Reinforcement learning of optimal POMDP dialogue policies, in order (1) to make the learning process faster and (2) to obtain an estimate of the uncertainty of the approximation. We first demonstrate the idea on a simple voice mail dialogue task and then apply this method to a real-world tourist information dialogue task. © 2010 Association for Computational Linguistics.","Modelling dialogue as a Partially Observable Markov Decision Process (POMDP) enables a dialogue policy robust to speech understanding errors to be learnt. However, a major challenge in POMDP policy learning is to maintain tractability, so the use of approximation is inevitable. We propose applying Gaussian Processes in Reinforcement learning of optimal POMDP dialogue policies, in order (1) to make the learning process faster and (2) to obtain an estimate of the uncertainty of the approximation. We first demonstrate the idea on a simple voice mail dialogue task and then apply this method to a real-world tourist information dialogue task. © 2010 Association for Computational Linguistics.",,"Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom",,,,Author1,Author2,a,a,u,a,a,a,Interfaces / HCI,,,,u,a,Proceedings of the SIGDIAL 2010 Conference: 11th Annual Meeting of the Special Interest Group onDiscourse and Dialogue,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857755225&partnerID=40&md5=53959c32cfe4d3fe63d3cc1543d0da65,"Gasic M., Jurčíček F., Keizer S., Mairesse F., Thomson B., Yu K., Young S.","Gasic, M., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom; Jurčíček, F., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom; Keizer, S., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom; Mairesse, F., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom; Thomson, B., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom; Yu, K., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom; Young, S., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom",33,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-84857755225,,,,,204,201,,,,,Scopus,,,,Conference Paper,,,,,,2010,1,2,1,Author3
114,1,0,2,1,Knowledge transfer between speakers for personalised dialogue management,"Model-free reinforcement learning has been shown to be a promising data driven approach for automatic dialogue policy optimization, but a relatively large amount of dialogue interactions is needed before the system reaches reasonable performance. Recently, Gaussian process based reinforcement learning methods have been shown to reduce the number of dialogues needed to reach optimal performance, and pre-training the policy with data gathered from different dialogue systems has further reduced this amount. Following this idea, a dialogue system designed for a single speaker can be initialised with data from other speakers, but if the dynamics of the speakers are very different the model will have a poor performance. When data gathered from different speakers is available, selecting the data from the most similar ones might improve the performance. We propose a method which automatically selects the data to transfer by defining a similarity measure between speakers, and uses this measure to weight the influence of the data from each speaker in the policy model. The methods are tested by simulating users with different severities of dysarthria interacting with a voice enabled environmental control system.. © 2015 Association for Computational Linguistics.","Model-free reinforcement learning has been shown to be a promising data driven approach for automatic dialogue policy optimization, but a relatively large amount of dialogue interactions is needed before the system reaches reasonable performance. Recently, Gaussian process based reinforcement learning methods have been shown to reduce the number of dialogues needed to reach optimal performance, and pre-training the policy with data gathered from different dialogue systems has further reduced this amount. Following this idea, a dialogue system designed for a single speaker can be initialised with data from other speakers, but if the dynamics of the speakers are very different the model will have a poor performance. When data gathered from different speakers is available, selecting the data from the most similar ones might improve the performance. We propose a method which automatically selects the data to transfer by defining a similarity measure between speakers, and uses this measure to weight the influence of the data from each speaker in the policy model. The methods are tested by simulating users with different severities of dysarthria interacting with a voice enabled environmental control system.. © 2015 Association for Computational Linguistics.",,"Department of Computer Science, University of Sheffield, United Kingdom",,,,Author2,Author3,a,a,u,a,a,a,"Other (please in ""remark"")",d,Knowledge transfer,domain: dialog systems,u,a,"SIGDIAL 2015 - 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Proceedings of the Conference",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988391769&partnerID=40&md5=dfddd45f3d809c20ad6bebb66483451d,"Casanueva I., Hain T., Christensen H., Marxer R., Green P.","Casanueva, I., Department of Computer Science, University of Sheffield, United Kingdom; Hain, T., Department of Computer Science, University of Sheffield, United Kingdom; Christensen, H., Department of Computer Science, University of Sheffield, United Kingdom; Marxer, R., Department of Computer Science, University of Sheffield, United Kingdom; Green, P., Department of Computer Science, University of Sheffield, United Kingdom",5,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-84988391769,,,,,21,12,,,,,Scopus,,,,Conference Paper,,,,,,2015,1,2,1,Author3
115,2,2,1,2,Optimal medication dosing from suboptimal clinical examples: A deep reinforcement learning approach,"Misdosing medications with sensitive therapeutic windows, such as heparin, can place patients at unnecessary risk, increase length of hospital stay, and lead to wasted hospital resources. In this work, we present a clinician-in-the-loop sequential decision making framework, which provides an individualized dosing policy adapted to each patient's evolving clinical phenotype. We employed retrospective data from the publicly available MIMIC II intensive care unit database, and developed a deep reinforcement learning algorithm that learns an optimal heparin dosing policy from sample dosing trails and their associated outcomes in large electronic medical records. Using separate training and testing datasets, our model was observed to be effective in proposing heparin doses that resulted in better expected outcomes than the clinical guidelines. Our results demonstrate that a sequential modeling approach, learned from retrospective data, could potentially be used at the bedside to derive individualized patient dosing policies.","Misdosing medications with sensitive therapeutic windows, such as heparin, can place patients at unnecessary risk, increase length of hospital stay, and lead to wasted hospital resources. In this work, we present a clinician-in-the-loop sequential decision making framework, which provides an individualized dosing policy adapted to each patient's evolving clinical phenotype. We employed retrospective data from the publicly available MIMIC II intensive care unit database, and developed a deep reinforcement learning algorithm that learns an optimal heparin dosing policy from sample dosing trails and their associated outcomes in large electronic medical records. Using separate training and testing datasets, our model was observed to be effective in proposing heparin doses that resulted in better expected outcomes than the clinical guidelines. Our results demonstrate that a sequential modeling approach, learned from retrospective data, could potentially be used at the bedside to derive individualized patient dosing policies.",,,,"Dept. of Biomedical Informatics, Emory University, Atlanta, GA 30322",,Author1,Author2,a,a,a,a,a,a,Healthcare,,,,a,a,2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7591355,,S. Nemati; M. M. Ghassemi; G. D. Clifford,,1,,10.1109/EMBC.2016.7591355,IEEE Xplore,1,,20161018,2981,,Cost function;Drugs;Feature extraction;Hospitals;Learning (artificial intelligence);Time series analysis;Training,decision making;drugs;electronic health records;learning (artificial intelligence);patient treatment,MIMIC II intensive care unit database;clinical guidelines;clinical phenotype;clinician-in-the-loop sequential decision making framework;deep reinforcement learning algorithm;electronic medical records;hospital stay length;individualized dosing policy;individualized patient dosing policies;optimal heparin dosing policy;optimal medication dosing;retrospective data;sample dosing trails;sensitive therapeutic windows;sequential modeling approach;suboptimal clinical examples;testing datasets;training datasets,,1557-170X;1557170X,,16-20 Aug. 2016,,,,,"Algorithms;Databases, Factual;Dose-Response Relationship, Drug;Heparin;Humans;Learning;Length of Stay;Markov Chains;Reinforcement (Psychology);Retrospective Studies",,,,,,1,IEEE,,,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,,2978,IEEE Conferences,,,,,2016,2016,0,1,3,Author2
116,1,0,2,1,Learning to Ground in Spoken Dialogue Systems,"Machine learning methods such as reinforcement learning applied to dialogue strategy optimization has become a leading subject of researches since the mid 90's. Indeed, the great variability of factors to take into account makes the design of a spoken dialogue system a tailoring task and reusability of previous work is very difficult. Yet, techniques such as reinforcement learning are very demanding in training data while obtaining a substantial amount of data in the particular case of spoken dialogues is time-consuming and therefore expansive. In order to expand existing data sets, dialogue simulation techniques are becoming a standard solution. In this paper, we present a user model for realistic spoken dialogue simulation and a method for using this model so as to simulate the grounding process. This allows including grounding subdialogues as actions in the reinforcement learning process and learning adapted strategy.","Machine learning methods such as reinforcement learning applied to dialogue strategy optimization has become a leading subject of researches since the mid 90's. Indeed, the great variability of factors to take into account makes the design of a spoken dialogue system a tailoring task and reusability of previous work is very difficult. Yet, techniques such as reinforcement learning are very demanding in training data while obtaining a substantial amount of data in the particular case of spoken dialogues is time-consuming and therefore expansive. In order to expand existing data sets, dialogue simulation techniques are becoming a standard solution. In this paper, we present a user model for realistic spoken dialogue simulation and a method for using this model so as to simulate the grounding process. This allows including grounding subdialogues as actions in the reinforcement learning process and learning adapted strategy.",,,,"&#201;cole Sup&#233;rieure d'&#201;lectricit&#233; - SUP&#201;LEC, Metz Campus - IMS Team, 2 rue &#201;douard Belin - F-57070 Metz - FRANCE. e-mail: olivier.pietquin@supelec.fr",Speech Communication;Unsupervised Learning;User Modelling,Author3,Author4,a,a,a,a,a,a,"Other (please in ""remark"")",domain: dialog systems,It is unclear whether a RL is used for user modeling,Agreed that RL is not clear.,a,a,"2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218063,,O. Pietquin,,2,,10.1109/ICASSP.2007.367189,IEEE Xplore,1,,20070604,IV-168,,Automatic speech recognition;Grounding;Learning systems;Machine learning;Man machine systems;Optimization methods;Space exploration;Speech processing;Speech synthesis;Stochastic processes,interactive systems;speech-based user interfaces;unsupervised learning,dialogue simulation techniques;grounding process;realistic spoken dialogue simulation;reinforcement learning;spoken dialogue systems,,1520-6149;15206149,,15-20 April 2007,,,,,,,,,,,,IEEE,15,,CD-ROM:1-4244-0728-1; POD:1-4244-0727-3,,IV-165,IEEE Conferences,,,4,,2007,2007,1,2,1,Author3
117,0,0,0,0,Automatic navigation among mobile DTV services,"Limited number of input buttons on a mobile device, such as mobile phones and PDAs, restricts people's access to digital broadcast services. In this paper, we present a reinforcement learning approach to automatically navigating among services in mobile digital television systems. Our approach uses standard Q-learning algorithm as a theory basis to predict next button for the user by learning usage patterns from interaction experiences. We did the experiment using a modified algorithm in test system. The experimental results demonstrate that the performance is good and the method is feasible and appropriate in practice.","Limited number of input buttons on a mobile device, such as mobile phones and PDAs, restricts people's access to digital broadcast services. In this paper, we present a reinforcement learning approach to automatically navigating among services in mobile digital television systems. Our approach uses standard Q-learning algorithm as a theory basis to predict next button for the user by learning usage patterns from interaction experiences. We did the experiment using a modified algorithm in test system. The experimental results demonstrate that the performance is good and the method is feasible and appropriate in practice.",,"Telecom. Software Multimedia Lab., Dept. of Compter Sci. and Eng., Helsinki University of Technology, P.O. Box 5400, FIN-02015, Finland",,,Automatic navigation; Button prediction; Exploration; Intelligent user interface; Reinforcement learning,Author4,Author1,a,a,a,a,a,a,Personal Assistants,,,,a,a,ICEIS 2004 - Proceedings of the Sixth International Conference on Enterprise Information Systems,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-8444233936&partnerID=40&md5=61d5a01b971e0282eec404d1fbe43bb2,"Peng C., Vuorimaa P.","Peng, C., Telecom. Software Multimedia Lab., Dept. of Compter Sci. and Eng., Helsinki University of Technology, P.O. Box 5400, FIN-02015, Finland; Vuorimaa, P., Telecom. Software Multimedia Lab., Dept. of Compter Sci. and Eng., Helsinki University of Technology, P.O. Box 5400, FIN-02015, Finland",,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-8444233936,,,,,145,140,,,,,Scopus,,,,Conference Paper,,,,,2004,2004,4,0,0,Author4
118,1,2,2,2,A Recursive Dialogue Game for Personalized Computer-Aided Pronunciation Training,"Learning languages in addition to the native language is very important for all people in the globalized world today, and computer-aided pronunciation training (CAPT) is attractive since the software can be used anywhere at any time, and repeated as many times as desired. In this paper, we introduce the immersive interaction scenario offered by spoken dialogues to CAPT by proposing a recursive dialogue game to make CAPT personalized. A number of tree-structured sub-dialogues are linked sequentially and recursively as the script for the game. The system policy at each dialogue turn is to select in real-time along the dialogue the best training sentence for each specific individual learner within the dialogue script, considering the learner's learning status and the future possible dialogue paths in the script, such that the learner can have the scores for all pronunciation units considered reaching a predefined standard in a minimum number of turns. The purpose here is that those pronunciation units poorly produced by the specific learner can be offered with more practice opportunities in the future sentences along the dialogue, which enables the learner to improve the pronunciation without having to repeat the same training sentences many times. This makes the learning process for each learner completely personalized. The dialogue policy is modeled by Markov decision process (MDP) with high-dimensional continuous state space, and trained with fitted value iteration using a huge number of simulated learners. These simulated leaners have the behavior similar to real learners, and were generated from a corpus of real learner data. The experiments demonstrated very promising results and a real cloud-based system is also successfully implemented.","Learning languages in addition to the native language is very important for all people in the globalized world today, and computer-aided pronunciation training (CAPT) is attractive since the software can be used anywhere at any time, and repeated as many times as desired. In this paper, we introduce the immersive interaction scenario offered by spoken dialogues to CAPT by proposing a recursive dialogue game to make CAPT personalized. A number of tree-structured sub-dialogues are linked sequentially and recursively as the script for the game. The system policy at each dialogue turn is to select in real-time along the dialogue the best training sentence for each specific individual learner within the dialogue script, considering the learner's learning status and the future possible dialogue paths in the script, such that the learner can have the scores for all pronunciation units considered reaching a predefined standard in a minimum number of turns. The purpose here is that those pronunciation units poorly produced by the specific learner can be offered with more practice opportunities in the future sentences along the dialogue, which enables the learner to improve the pronunciation without having to repeat the same training sentences many times. This makes the learning process for each learner completely personalized. The dialogue policy is modeled by Markov decision process (MDP) with high-dimensional continuous state space, and trained with fitted value iteration using a huge number of simulated learners. These simulated leaners have the behavior similar to real learners, and were generated from a corpus of real learner data. The experiments demonstrated very promising results and a real cloud-based system is also successfully implemented.",,,,"Department of Engineering, University of Cambridge, Cambridge, United Kingdom",Computer-aided pronunciation training (CAPT);Markov decision process;computer-assisted language learning;dialogue game;reinforcement learning,Author2,Author3,a,a,a,a,a,a,Games,d,"domain: intelligent tutors
as the task here is to teach pronounciation to people.",,a,a,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6971195,,P. h. Su; C. h. Wu; L. s. Lee,,2,,10.1109/TASLP.2014.2375572,IEEE Xplore,1,,20150114,141,,Computers;Games;Markov processes;Software;Speech;Speech processing;Training,Markov processes;cloud computing;computer based training;computer games;interactive systems;linguistics;real-time systems,MDP;Markov decision process;dialogue paths;dialogue policy;dialogue script;fitted value iteration;game script;high-dimensional continuous state space;immersive interaction scenario;language learning;learner learning status;native language;personalized CAPT;personalized computer-aided pronunciation training;pronunciation improvement;pronunciation unit scores;real cloud-based system;real learners;recursive dialogue game;sequentially-recursively linked tree-structured subdialogues;simulated leaners;spoken dialogues;system policy;training sentence,,2329-9290;23299290,1,Jan. 2015,,,,,,20141202,,,,,,IEEE,52,,,,127,IEEE Journals & Magazines,,,23,,2015,2015,0,1,3,Author2
119,1,1,1,1,Learning games for the cognitively impaired people,"Learning environments have been profoundly reshaped by pervasive technology. New educational methodologies take full advantage of ICT in a mobile customized user-friendly environment, to support learning and stimulate individuals'potential. Unfortunately, technology-enhanced learning tools are not often designed with accessibility in mind, although they can greatly benefit the personal empowerment and inclusion of special-needs people. To address this gap, a Web platform has been created for delivering accessible games to people with Down syndrome. Since personalization, orderliness and positive reinforcement are crucial to learning in these subjects, the platform offers a personalized safe environment for learning, conforming to behavioral analysis principles. Learning analytics are incorporated in the platform for easy monitoring of student progress via Web interfaces. The participatory design driving the development of the learning platform allowed the customization of the games'discriminative stimuli, difficulty levels and reinforcement, as well as the creation of a game ""engine"" to easily set up new personalized exercises. These customization features make the game platform usable by a larger audience, including individuals with learning difficulties and autism. © 2016 ACM.","Learning environments have been profoundly reshaped by pervasive technology. New educational methodologies take full advantage of ICT in a mobile customized user-friendly environment, to support learning and stimulate individuals'potential. Unfortunately, technology-enhanced learning tools are not often designed with accessibility in mind, although they can greatly benefit the personal empowerment and inclusion of special-needs people. To address this gap, a Web platform has been created for delivering accessible games to people with Down syndrome. Since personalization, orderliness and positive reinforcement are crucial to learning in these subjects, the platform offers a personalized safe environment for learning, conforming to behavioral analysis principles. Learning analytics are incorporated in the platform for easy monitoring of student progress via Web interfaces. The participatory design driving the development of the learning platform allowed the customization of the games'discriminative stimuli, difficulty levels and reinforcement, as well as the creation of a game ""engine"" to easily set up new personalized exercises. These customization features make the game platform usable by a larger audience, including individuals with learning difficulties and autism. © 2016 ACM.",,"CNR-IIT, Via Moruzzi, 1, Pisa, Italy", a30,,Cognitive games; Computer-enhanced learning; People with special needs; Web applications,Author3,Author4,u,a,a,a,a,a,Intelligent Tutors,a,,,u,a,W4A 2016 - 13th Web for All Conference,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983554779&doi=10.1145%2f2899475.2899487&partnerID=40&md5=a718dc59778be489c52825e92098f1e0,"Buzzi M.C., Buzzi M., Perrone E., Rapisarda B., Senette C.","Buzzi, M.C., CNR-IIT, Via Moruzzi, 1, Pisa, Italy; Buzzi, M., CNR-IIT, Via Moruzzi, 1, Pisa, Italy; Perrone, E., CNR-IIT, Via Moruzzi, 1, Pisa, Italy; Rapisarda, B., CNR-IIT, Via Moruzzi, 1, Pisa, Italy; Senette, C., CNR-IIT, Via Moruzzi, 1, Pisa, Italy",,,10.1145/2899475.2899487,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84983554779,,,,,,,,,,,Scopus,,,,Conference Paper,,,,,,2016,0,4,0,Author4
120,2,0,1,1,Self-learning system for personalized e-learning,"Knowledge is a basic requirement in the era of globalization. The act of acquiring knowledge is learning and with the advent of technology, learning has become easier. E-Learning is a popular learning method which uses the web or other technologies to promote learning. A very traditional learning method is discussed and some new methods with the integration of modern technology have been discussed. Although technologies like artificial intelligence, cloud computing, ontology, fuzzy logic etc. have been used in learning systems they are still less automated, not personalized and do not pose the capability to self-learn. A method is proposed to increase the automation and self-learning in the e-learning management systems. Further, the personalization of learning for the user so that every type of learner can learn from the type and level of the content the learner seems fit is the main aim. The proposed system is feasible, economical and scalable which makes it suitable for every level.","Knowledge is a basic requirement in the era of globalization. The act of acquiring knowledge is learning and with the advent of technology, learning has become easier. E-Learning is a popular learning method which uses the web or other technologies to promote learning. A very traditional learning method is discussed and some new methods with the integration of modern technology have been discussed. Although technologies like artificial intelligence, cloud computing, ontology, fuzzy logic etc. have been used in learning systems they are still less automated, not personalized and do not pose the capability to self-learn. A method is proposed to increase the automation and self-learning in the e-learning management systems. Further, the personalization of learning for the user so that every type of learner can learn from the type and level of the content the learner seems fit is the main aim. The proposed system is feasible, economical and scalable which makes it suitable for every level.",,,,"Dept. of Computer Science & Engineering, Graphic Era University, Dehradun, India",Machine learning;e-learning system;learning management system;reinforcement learning,Author1,Author2,a,a,a,a,a,a,Intelligent Tutors,,,,a,a,2017 International Conference on Emerging Trends in Computing and Communication Technologies (ICETCCT),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8280344,,V. Pant; S. Bhasin; S. Jain,,,,10.1109/ICETCCT.2017.8280344,IEEE Xplore,1,,20180205,6,,Crawlers;Electronic learning;Learning management systems;Semantics;Task analysis;Videos,cloud computing;computer aided instruction;fuzzy logic;ontologies (artificial intelligence),artificial intelligence;cloud computing;e-learning management systems;fuzzy logic;learning systems;ontology;personalized e-learning;self-learning system,,,,17-18 Nov. 2017,,,,,,,,,,,,IEEE,,,CD:978-1-5386-1146-3; Electronic:978-1-5386-1147-0; POD:978-1-5386-1148-7,,1,IEEE Conferences,,,,,2017,2017,1,2,1,Author1
121,0,0,1,0,Intelligent feedback polarity and timing selection in the Shufti Intelligent Tutoring System,"It is well known that the training of medical students is a long and arduous process. Students master many areas of knowledge in a relatively short amount of time in order to become experts in their chosen field. The Socratic Method used in the latter stages of medical education, where a physician directly monitors a group of students, is inherently restrictive due to the limited number of cases and length of the students' rotations. Innovative Intelligent Tutoring techniques offer a solution to this problem. This paper outlines the overall structure and design of Shufti, an Intelligent Tutoring System (ITS) focused on mammography and medical imaging. Shufti's aim is to provide medical students with an improved learning environment, exposing them to a broad range of examples supported by customized feedback and hints driven by an adaptive Reinforcement Learning system and Clustering Techniques.","It is well known that the training of medical students is a long and arduous process. Students master many areas of knowledge in a relatively short amount of time in order to become experts in their chosen field. The Socratic Method used in the latter stages of medical education, where a physician directly monitors a group of students, is inherently restrictive due to the limited number of cases and length of the students' rotations. Innovative Intelligent Tutoring techniques offer a solution to this problem. This paper outlines the overall structure and design of Shufti, an Intelligent Tutoring System (ITS) focused on mammography and medical imaging. Shufti's aim is to provide medical students with an improved learning environment, exposing them to a broad range of examples supported by customized feedback and hints driven by an adaptive Reinforcement Learning system and Clustering Techniques.",,"Alberta Innovates Center for Machine Learning, University of Alberta, Canada",,,Breast cancer; Data mining; Feedback; Hints; Intelligent Tutoring System; Machine learning; Reinforcement learning; Serious games,Author2,Author3,a,a,a,a,a,a,Intelligent Tutors,a,,,a,a,"Proceedings of the 20th International Conference on Computers in Education, ICCE 2012",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896328874&partnerID=40&md5=2f69d64ebf1e2474954ed1c9e97c26e4,"Johnson S., Zaiane O.","Johnson, S., Alberta Innovates Center for Machine Learning, University of Alberta, Canada; Zaiane, O., Alberta Innovates Center for Machine Learning, University of Alberta, Canada",1,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-84896328874,,,,,60,56,,,,,Scopus,,,,Conference Paper,,,,,2012,2012,3,1,0,Author3
122,2,2,0,2,Recent development on statistical methods for personalized medicine discovery,"It is well documented that patients can show significant heterogeneous responses to treatments so the best treatment strategies may require adaptation over individuals and time. Recently, a number of new statistical methods have been developed to tackle the important problem of estimating personalized treatment rules using single-stage or multiple-stage clinical data. In this paper, we provide an overview of these methods and list a number of challenges. © 2013 Higher Education Press and Springer-Verlag Berlin Heidelberg.","It is well documented that patients can show significant heterogeneous responses to treatments so the best treatment strategies may require adaptation over individuals and time. Recently, a number of new statistical methods have been developed to tackle the important problem of estimating personalized treatment rules using single-stage or multiple-stage clinical data. In this paper, we provide an overview of these methods and list a number of challenges. © 2013 Higher Education Press and Springer-Verlag Berlin Heidelberg.",,"Department of Biostatistics and Medical Informatics, University of Wisconsin-Madison, 600 Highland Ave., Madison, WI 53792, United States; Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States",,,dynamic treatment regimes; personalized medicine; Q-learning; reinforcement learning,Author4,Author1,a,a,a,a,a,a,Healthcare,,More overview paper.,,a,a,Frontiers of Medicine in China,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874810512&doi=10.1007%2fs11684-013-0245-7&partnerID=40&md5=57299256c0f308af89fd3cc6054d33dd,"Zhao Y., Zeng D.","Zhao, Y., Department of Biostatistics and Medical Informatics, University of Wisconsin-Madison, 600 Highland Ave., Madison, WI 53792, United States; Zeng, D., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States",10,,10.1007/s11684-013-0245-7,SCOPUS,1,,,,,,,,,,1,,,,2-s2.0-84874810512,,,,,110,102,,,,,Scopus,,,,Review,,,7,,2013,2013,1,0,3,Author4
123,0,1,1,1,"Robot self-preservation and adaptation to user preferences in game play, a preliminary study","It is expected that in a near future, personal robots will be endowed with enough autonomy to function and live in an individual's home. This is while commercial robots are designed with default configuration and factory settings which may often be different to an individual's operating preferences. This paper presents how reinforcement learning is applied and utilised towards personalisation of a robot's behaviour. Two-level reinforcement learning has been implemented: first level is in charge of energy autonomy, i.e. how to survive, and second level is involved in adapting robot's behaviour to user's preferences. In both levels Q-learning algorithm has been applied. First level actions have been learnt in a simulated environment and then the results have been transferred to the real robot. Second level has been fully implemented in the real robot and learnt by human-robot interaction. Finally, experiments showing the performance of the system are presented.","It is expected that in a near future, personal robots will be endowed with enough autonomy to function and live in an individual's home. This is while commercial robots are designed with default configuration and factory settings which may often be different to an individual's operating preferences. This paper presents how reinforcement learning is applied and utilised towards personalisation of a robot's behaviour. Two-level reinforcement learning has been implemented: first level is in charge of energy autonomy, i.e. how to survive, and second level is involved in adapting robot's behaviour to user's preferences. In both levels Q-learning algorithm has been applied. First level actions have been learnt in a simulated environment and then the results have been transferred to the real robot. Second level has been fully implemented in the real robot and learnt by human-robot interaction. Finally, experiments showing the performance of the system are presented.",,,,"RoboticsLab at the Carlos III University of Madrid, 28911, Legan&#x00E9;s, Madrid, Spain",,Author3,Author4,a,a,a,a,a,a,"Other (please in ""remark"")",a,domain: robotics,Could also be personal assistants :),a,a,2011 IEEE International Conference on Robotics and Biomimetics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6181679,,Á. Castro-González; F. Amirabdollahian; D. Polani; M. Malfaz; M. A. Salichs,,2,,10.1109/ROBIO.2011.6181679,IEEE Xplore,1,,20120412,2498,,Batteries;Games;Humans;Learning;Machine learning;Machine learning algorithms;Robots,human-robot interaction;learning (artificial intelligence),Q-learning algorithm;commercial robots;game play;human-robot interaction;personal robots;reinforcement learning;robot behaviour personalisation;robot self-preservation;user preferences adaptation,,,,7-11 Dec. 2011,,,,,,,,,,,,IEEE,28,,DVD:978-1-4577-2137-3; Electronic:978-1-4577-2138-0; POD:978-1-4577-2136-6,,2491,IEEE Conferences,,,,,2011,2011,1,3,0,Author4
124,1,0,2,1,Learning interface agents,"Interface agents are computer programs that employ Artificial Intelligence techniques in order to provide assistance to a user dealing with a particular computer application. The paper discusses an interface agent which has been modelled closely after the metaphor of a personal assistant. The agent learns how to assist the user by (i) observing the user's actions and imitating them, (ii) receiving user feedback when it takes wrong actions and (iii) being trained by the user on the basis of hypothetical examples. The paper discusses how this learning agent was implemented using memory-based learning and reinforcement learning techniques. It presents actual results from two proto-type agents built using these techniques: one for a meeting scheduling application and one for electronic mail. It argues that the machine learning approach to building interface agents is a feasible one which has several advantages over other approaches: it provides a customized and adaptive solution which is less costly and ensures better user acceptability. The paper also argues what the advantages are of the particular learning techniques used.","Interface agents are computer programs that employ Artificial Intelligence techniques in order to provide assistance to a user dealing with a particular computer application. The paper discusses an interface agent which has been modelled closely after the metaphor of a personal assistant. The agent learns how to assist the user by (i) observing the user's actions and imitating them, (ii) receiving user feedback when it takes wrong actions and (iii) being trained by the user on the basis of hypothetical examples. The paper discusses how this learning agent was implemented using memory-based learning and reinforcement learning techniques. It presents actual results from two proto-type agents built using these techniques: one for a meeting scheduling application and one for electronic mail. It argues that the machine learning approach to building interface agents is a feasible one which has several advantages over other approaches: it provides a customized and adaptive solution which is less costly and ensures better user acceptability. The paper also argues what the advantages are of the particular learning techniques used.",,"MIT Media Lab, Cambridge, United States",,,,Author1,Author2,a,a,a,a,a,a,Personal Assistants,,,,a,a,Proceedings of the National Conference on Artificial Intelligence,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027708859&partnerID=40&md5=df7028dac1a1660a147b9a020dc8489d,"Maes Pattie, Kozierok Robyn","Maes, Pattie, MIT Media Lab, Cambridge, United States; Kozierok, Robyn, MIT Media Lab, Cambridge, United States",112,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-0027708859,,,,,465,459,,,,,Scopus,,,,Conference Paper,,,,,1993,1993,1,2,1,Author3
125,0,0,0,0,A unified control framework of HVAC system for thermal and acoustic comforts in office building,"Intelligent building system attracts more and more attention in both academic and industrial communities. Learning human comfort requirements and incorporating it into building control system is one of the important issues. In the traditional HVAC control system, the thermal comfort and the acoustic comfort are often conflicted and we lack of a scheme to trade off them well. In this paper, we propose a unified control framework based on reinforcement learning to balance the multiple dimension comforts, including the thermal and acoustic comforts. We utilize the user's complaints in thermal and acoustic sensations as feedback and combine the current environment and devices information to learn the personalized optimal control policy using online Q-learning. The challenge caused by the complaints is coped with an incorporated perception estimation scheme in the Q-learning reward design. Both simulation results and the field experimental results demonstrate the effectiveness of the algorithm, especially in the adaptivity to the individual tradeoff between thermal and acoustic comfort.","Intelligent building system attracts more and more attention in both academic and industrial communities. Learning human comfort requirements and incorporating it into building control system is one of the important issues. In the traditional HVAC control system, the thermal comfort and the acoustic comfort are often conflicted and we lack of a scheme to trade off them well. In this paper, we propose a unified control framework based on reinforcement learning to balance the multiple dimension comforts, including the thermal and acoustic comforts. We utilize the user's complaints in thermal and acoustic sensations as feedback and combine the current environment and devices information to learn the personalized optimal control policy using online Q-learning. The challenge caused by the complaints is coped with an incorporated perception estimation scheme in the Q-learning reward design. Both simulation results and the field experimental results demonstrate the effectiveness of the algorithm, especially in the adaptivity to the individual tradeoff between thermal and acoustic comfort.",,,,"Dept. of Autom., Tsinghua Univ., Beijing, China",,Author4,Author1,a,a,a,a,a,a,Personal Assistants,,Control climate in building.,,a,a,2013 IEEE International Conference on Automation Science and Engineering (CASE),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6653964,,Y. Zhao; Q. Zhao; L. Xia; Z. Cheng; F. Wang; F. Song,,1,,10.1109/CoASE.2013.6653964,IEEE Xplore,1,,20131107,421,,Acoustics;Buildings;Estimation;Humidity;Noise;Temperature sensors;Upper bound,HVAC;architectural acoustics;building management systems;control engineering computing;learning (artificial intelligence);optimal control,HVAC system;Q-learning reward design;acoustic comforts;acoustic sensations;office building;online Q-learning;perception estimation scheme;personalized optimal control policy;reinforcement learning;thermal comforts;thermal sensations;unified control framework,,2161-8070;21618070,,17-20 Aug. 2013,,,,,,,,,,,,IEEE,17,,Electronic:978-1-4799-1515-6; POD:978-1-4799-1513-2; USB:978-1-4799-1514-9,,416,IEEE Conferences,,,,,2013,2013,4,0,0,Author1
126,2,0,2,2,Usage-based web recommendations: A reinforcement learning approach,"Information overload is no longer news; the explosive growth of the Internet has made this issue increasingly serious for Web users. Users are very often overwhelmed by the huge amount of information and are faced with a big challenge to find the most relevant information in the right time. Recommender systems aim at pruning this information space and directing users toward the items that best meet their needs and interests. Web Recommendation has been an active application area in Web Mining and Machine Learning research. In this paper we propose a novel machine learning perspective toward the problem, based on reinforcement learning. Unlike other recommender systems, our system does not use the static patterns discovered from web usage data, instead it learns to make recommendations as the actions it performs in each situation. We model the problem as Q-Learning while employing concepts and techniques commonly applied in the web usage mining domain. We propose that the reinforcement learning paradigm provides an appropriate model for the recommendation problem, as well as a framework in which the system constantly interacts with the user and learns from her behavior. Our experimental evaluations support our claims and demonstrate how this approach can improve the quality of web recommendations. Copyright 2007 ACM.","Information overload is no longer news; the explosive growth of the Internet has made this issue increasingly serious for Web users. Users are very often overwhelmed by the huge amount of information and are faced with a big challenge to find the most relevant information in the right time. Recommender systems aim at pruning this information space and directing users toward the items that best meet their needs and interests. Web Recommendation has been an active application area in Web Mining and Machine Learning research. In this paper we propose a novel machine learning perspective toward the problem, based on reinforcement learning. Unlike other recommender systems, our system does not use the static patterns discovered from web usage data, instead it learns to make recommendations as the actions it performs in each situation. We model the problem as Q-Learning while employing concepts and techniques commonly applied in the web usage mining domain. We propose that the reinforcement learning paradigm provides an appropriate model for the recommendation problem, as well as a framework in which the system constantly interacts with the user and learns from her behavior. Our experimental evaluations support our claims and demonstrate how this approach can improve the quality of web recommendations. Copyright 2007 ACM.",,"Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran",,,Machine learning; Personalization; Recommender systems; Reinforcement learning; Web usage mining,Author4,Author1,a,a,a,a,a,a,Recommender Systems,,,,a,a,RecSys'07: Proceedings of the 2007 ACM Conference on Recommender Systems,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149177829&doi=10.1145%2f1297231.1297250&partnerID=40&md5=0344c9539560bc03572a0e0f62a61992,"Taghipour N., Kardan A., Ghidary S.S.","Taghipour, N., Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran; Kardan, A., Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran; Ghidary, S.S., Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran",31,,10.1145/1297231.1297250,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-42149177829,,,,,120,113,,,,,Scopus,,,,Conference Paper,,,,,2007,2007,1,0,3,Author4
127,2,0,2,2,Incorporating prior knowledge into Q-learning for drug delivery individualization,"Individualization of drug delivery in treatment of chronic ailments is a challenge to the physician. Variability of response across patient population requires tailoring the dosing strategies to individual's needs. We have previously demonstrated the potential of reinforcement learning methods to support the physician in the management of anemia. In this paper, we propose the incorporation of prior knowledge into the learning mechanism to further improve the outcomes of the treatment.","Individualization of drug delivery in treatment of chronic ailments is a challenge to the physician. Variability of response across patient population requires tailoring the dosing strategies to individual's needs. We have previously demonstrated the potential of reinforcement learning methods to support the physician in the management of anemia. In this paper, we propose the incorporation of prior knowledge into the learning mechanism to further improve the outcomes of the treatment.",,,,"Louisville Univ., KY, USA",,Author2,Author3,a,a,a,a,a,a,Healthcare,a,,,a,a,Fourth International Conference on Machine Learning and Applications (ICMLA'05),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607452,,A. E. Gaweda; M. K. Muezzinoglu; G. R. Aronoff; A. A. Jacobs; J. M. Zurada; M. E. Brier,,2,,10.1109/ICMLA.2005.40,IEEE Xplore,1,,20060320,,,Animals;Bones;Decision making;Drug delivery;Humans;Jacobian matrices;Learning systems;Production;Protocols;Red blood cells,diseases;drug delivery systems;drugs;learning (artificial intelligence);medical computing,Q-learning;anemia;chronic ailment treatment;drug delivery individualization;reinforcement learning,,,,15-17 Dec. 2005,,,,,,,,,,,,IEEE,8,,POD:0-7695-2495-8,,6 pp.,IEEE Conferences,,,,,2005,2005,1,0,3,Author4
128,1,1,1,1,Optimal learning control of oxygen saturation using a policy iteration algorithm and a proof-of-concept in an interconnecting three-tank system,"In this work, “policy iteration algorithm” (PIA) is applied for controlling arterial oxygen saturation that does not require mathematical models of the plant. This technique is based on nonlinear optimal control to solve the Hamilton–Jacobi–Bellman equation. The controller is synthesized using a state feedback configuration based on an unidentified model of complex pathophysiology of pulmonary system in order to control gas exchange in ventilated patients, as under some circumstances (like emergency situations), there may not be a proper and individualized model for designing and tuning controllers available in time. The simulation results demonstrate the optimal control of oxygenation based on the proposed PIA by iteratively evaluating the Hamiltonian cost functions and synthesizing the control actions until achieving the converged optimal criteria. Furthermore, as a practical example, we examined the performance of this control strategy using an interconnecting three-tank system as a real nonlinear system. © 2016 Elsevier Ltd","In this work, “policy iteration algorithm” (PIA) is applied for controlling arterial oxygen saturation that does not require mathematical models of the plant. This technique is based on nonlinear optimal control to solve the Hamilton–Jacobi–Bellman equation. The controller is synthesized using a state feedback configuration based on an unidentified model of complex pathophysiology of pulmonary system in order to control gas exchange in ventilated patients, as under some circumstances (like emergency situations), there may not be a proper and individualized model for designing and tuning controllers available in time. The simulation results demonstrate the optimal control of oxygenation based on the proposed PIA by iteratively evaluating the Hamiltonian cost functions and synthesizing the control actions until achieving the converged optimal criteria. Furthermore, as a practical example, we examined the performance of this control strategy using an interconnecting three-tank system as a real nonlinear system. © 2016 Elsevier Ltd",,"Philips Chair for Medical Information Technology, RWTH Aachen University, Aachen, Germany",,,Biomedical control system; Closed-loop ventilation; Control of oxygen saturation; Optimal control; Policy iteration algorithm; Reinforcement learning,Author3,Author4,a,a,a,a,a,a,Healthcare,a,,,a,a,Control Engineering Practice,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995900104&doi=10.1016%2fj.conengprac.2016.07.014&partnerID=40&md5=8b494bd2a9f229c0bc0edde6e57e0fde,"Pomprapa A., Leonhardt S., Misgeld B.J.E.","Pomprapa, A., Philips Chair for Medical Information Technology, RWTH Aachen University, Aachen, Germany; Leonhardt, S., Philips Chair for Medical Information Technology, RWTH Aachen University, Aachen, Germany; Misgeld, B.J.E., Philips Chair for Medical Information Technology, RWTH Aachen University, Aachen, Germany",,,10.1016/j.conengprac.2016.07.014,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84995900104,,,,,203,194,,,,,Scopus,,,,Article,,,59,,2017,2017,0,4,0,Author1
129,1,0,2,1,Rapid simulation-driven reinforcement learning of multimodal dialog strategies in human-robot interaction,"In this work we propose a procedure model for rapid automatic strategy learning in multimodal dialogs. Our approach is tailored for typical task-oriented human-robot dialog interactions, with no prior knowledge about the expected user and system dynamics being present. For such scenarios, we propose the use of stochastic dialog simulation for strategy learning, where the user and system error models are solely trained through the initial execution of an inexpensive Wizard-of-Oz experiment. We argue that for the addressed dialogs, already a small data corpus combined with a low-conditioned simulation model facilitates learning of strong and complex dialog strategies. To validate our overall approach, we empirically show the supremacy of the learned strategy over a hand-crafted strategy for a concrete human-robot dialog scenario. To the authors' knowledge, this work is the first to perform strategy learning from multimodal dialog simulation.","In this work we propose a procedure model for rapid automatic strategy learning in multimodal dialogs. Our approach is tailored for typical task-oriented human-robot dialog interactions, with no prior knowledge about the expected user and system dynamics being present. For such scenarios, we propose the use of stochastic dialog simulation for strategy learning, where the user and system error models are solely trained through the initial execution of an inexpensive Wizard-of-Oz experiment. We argue that for the addressed dialogs, already a small data corpus combined with a low-conditioned simulation model facilitates learning of strong and complex dialog strategies. To validate our overall approach, we empirically show the supremacy of the learned strategy over a hand-crafted strategy for a concrete human-robot dialog scenario. To the authors' knowledge, this work is the first to perform strategy learning from multimodal dialog simulation.",,"Interactive Systems Labs., Universität Karlsruhe (TH), Germany",,,Multimodal human-robot dialogs; Strategy learning,Author4,Author1,a,a,a,a,a,a,"Other (please in ""remark"")",,Robots,,a,a,"INTERSPEECH 2006 and 9th International Conference on Spoken Language Processing, INTERSPEECH 2006 - ICSLP",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44949181000&partnerID=40&md5=b035fc070c6373c68c94bedfd9b4194d,"Prommer T., Holzapfel H., Waibel A.","Prommer, T., Interactive Systems Labs., Universität Karlsruhe (TH), Germany; Holzapfel, H., Interactive Systems Labs., Universität Karlsruhe (TH), Germany; Waibel, A., Interactive Systems Labs., Universität Karlsruhe (TH), Germany",12,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-44949181000,,,,,1921,1918,,,,,Scopus,,,,Conference Paper,,,4,,2006,2006,1,2,1,Author3
130,2,0,0,0,Deep Reinforcement Learning for Dynamic Treatment Regimes on Medical Registry Data,"In this paper, we propose the first deep reinforce-ment learning framework to estimate the optimal Dynamic Treat-ment Regimes from observational medical data. This framework is more flexible and adaptive for high dimensional action and state spaces than existing reinforcement learning methods to model real life complexity in heterogeneous disease progression and treatment choices, with the goal to provide doctor and patients the data-driven personalized decision recommendations. The proposed deep reinforcement learning framework contains a supervised learning step to predict the most possible expert actions; and a deep reinforcement learning step to estimate the long term value function of Dynamic Treatment Regimes. We motivated and implemented the proposed framework on a data set from the Center for International Bone Marrow Transplant Research (CIBMTR) registry database, focusing on the sequence of prevention and treatments for acute and chronic graft versus host disease. We showed results of the initial implementation that demonstrates promising accuracy in predicting human expert decisions and initial implementation for the reinforcement learning step.","In this paper, we propose the first deep reinforce-ment learning framework to estimate the optimal Dynamic Treat-ment Regimes from observational medical data. This framework is more flexible and adaptive for high dimensional action and state spaces than existing reinforcement learning methods to model real life complexity in heterogeneous disease progression and treatment choices, with the goal to provide doctor and patients the data-driven personalized decision recommendations. The proposed deep reinforcement learning framework contains a supervised learning step to predict the most possible expert actions; and a deep reinforcement learning step to estimate the long term value function of Dynamic Treatment Regimes. We motivated and implemented the proposed framework on a data set from the Center for International Bone Marrow Transplant Research (CIBMTR) registry database, focusing on the sequence of prevention and treatments for acute and chronic graft versus host disease. We showed results of the initial implementation that demonstrates promising accuracy in predicting human expert decisions and initial implementation for the reinforcement learning step.",,,,"Div. of Biostat., Med. Coll. of Wisconsin, Milwaukee, WI, USA",,Author1,Author2,a,a,a,a,a,a,Healthcare,,,,a,a,2017 IEEE International Conference on Healthcare Informatics (ICHI),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031178,,Y. Liu; B. Logan; N. Liu; Z. Xu; J. Tang; Y. Wang,,,,10.1109/ICHI.2017.45,IEEE Xplore,1,,20170914,385,,Biomedical imaging;Decision making;Diseases;Games;Learning (artificial intelligence);Machine learning,bone;data analysis;diseases;learning (artificial intelligence);patient treatment,CIBMTR;Dynamic Treatment Regimes;International Bone Marrow Transplant Research registry database;chronic graft versus host disease;deep reinforcement learning framework;deep reinforcement learning step;heterogeneous disease progression;medical registry data;observational medical data;personalized decision recommendations;reinforcement learning framework;supervised learning step,,,,23-26 Aug. 2017,,,,,,,,,,,,IEEE,,,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,,380,IEEE Conferences,,,,,2017,2017,3,0,1,Author1
131,2,0,2,2,Investigating Deep Reinforcement Learning Techniques in Personalized Dialogue Generation,"In this paper, we propose a personalized dialogue generation system, which combines reinforcement learning techniques with an attention-based hierarchical recurrent encoderdecoder model. Firstly, we incorporate user-specific information into the decoder to capture user's background information and speaking style. Secondly, we employ reinforcement learning techniques to maximize future reward in dialogue, which enables our system to generate topic-coherent, informative and grammatical responses. Moreover, we propose three types of rewards to characterize good conversations. Finally, we compare the performance of the following reinforcement learning methods in dialogue generation: policy gradient, Q-learning, and actor-critic algorithms. We conduct experiments to verify the effectiveness of the proposed model on two dialogue datasets. Experimental results demonstrate that our model can generate better personalized dialogues for different users. Quantitatively, our method achieves better performance than the state-of-the-art dialogue systems in terms of BLEU score, perplexity, and human evaluation.
","In this paper, we propose a personalized dialogue generation system, which combines reinforcement learning techniques with an attention-based hierarchical recurrent encoderdecoder model. Firstly, we incorporate user-specific information into the decoder to …",,,,,,Author2,Author3,a,a,u,a,u,a,"Other (please in ""remark"")",a,Chat bot,"It is unclear how the decoder RNN and RL are combined. I think we should accept for now and investigate further in the eligibility phase. I can image the RNN being integrated into the RL algo somehow, in which case we would have to include this paper.",u,a,,,http://scholar.google.com/https://epubs.siam.org/doi/abs/10.1137/1.9781611975321.71,,,0,,,Google Scholar,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2018,1,0,3,Author4
132,1,0,2,1,Personalized response generation via domain adaptation,"In this paper, we propose a novel personalized response generation model via domain adaptation (PRG-DM). First, we learn the human responding style from large general data (without user-specific information). Second, we fine tune the model on a small size of personalized data to generate personalized responses with a dual learning mechanism. Moreover, we propose three new rewards to characterize good conversations that are personalized, informative and grammatical. We employ the policy gradient method to generate highly rewarded responses. Experimental results show that our model can generate better personalized responses for different users. © 2017 Copyright held by the owner/author(s).","In this paper, we propose a novel personalized response generation model via domain adaptation (PRG-DM). First, we learn the human responding style from large general data (without user-specific information). Second, we fine tune the model on a small size of personalized data to generate personalized responses with a dual learning mechanism. Moreover, we propose three new rewards to characterize good conversations that are personalized, informative and grammatical. We employ the policy gradient method to generate highly rewarded responses. Experimental results show that our model can generate better personalized responses for different users. © 2017 Copyright held by the owner/author(s).",,"Tencent AI Lab, United States; Zhejiang University, China; Tencent, China; Normal University, China; IIE, Chinese Academy of Sciences, China; South China Normal University, China",,,Domain adaptation; Reinforcement learning; Response generation,Author3,Author4,a,a,a,a,a,a,"Other (please in ""remark"")",a,domain: dialog systems,,a,a,SIGIR 2017 - Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029357642&doi=10.1145%2f3077136.3080706&partnerID=40&md5=7dbab8245c15ee222478932988b1f01b,"Yang M., Zhao Z., Zhao W., Chen X., Zhu J., Zhou L., Cao Z.","Yang, M., Tencent AI Lab, United States; Zhao, Z., Zhejiang University, China; Zhao, W., Tencent, China; Chen, X., Normal University, China; Zhu, J., IIE, Chinese Academy of Sciences, China; Zhou, L., South China Normal University, China; Cao, Z., Tencent AI Lab, United States",,,10.1145/3077136.3080706,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85029357642,,,,,1024,1021,,,,,Scopus,,,,Conference Paper,,,,,2017,2017,1,2,1,Author3
133,2,0,2,2,Learning cooperative persuasive dialogue policies using framing,"In this paper, we propose a new framework of cooperative persuasive dialogue, where a dialogue system simultaneously attempts to achieve user satisfaction while persuading the user to take some action that achieves a pre-defined system goal. Within this framework, we describe a method for reinforcement learning of cooperative persuasive dialogue policies by defining a reward function that reflects both the system and user goal, and using framing, the use of emotionally charged statements common in persuasive dialogue between humans. In order to construct the various components necessary for reinforcement learning, we first describe a corpus of persuasive dialogues between human interlocutors, then propose a method to construct user simulators and reward functions specifically tailored to persuasive dialogue based on this corpus. Then, we implement a fully automatic text-based dialogue system for evaluating the learned policies. Using the implemented dialogue system, we evaluate the learned policy and the effect of framing through experiments both with a user simulator and with real users. The experimental evaluation indicates that the proposed method is effective for construction of cooperative persuasive dialogue systems. © 2016 Elsevier B.V.","In this paper, we propose a new framework of cooperative persuasive dialogue, where a dialogue system simultaneously attempts to achieve user satisfaction while persuading the user to take some action that achieves a pre-defined system goal. Within this framework, we describe a method for reinforcement learning of cooperative persuasive dialogue policies by defining a reward function that reflects both the system and user goal, and using framing, the use of emotionally charged statements common in persuasive dialogue between humans. In order to construct the various components necessary for reinforcement learning, we first describe a corpus of persuasive dialogues between human interlocutors, then propose a method to construct user simulators and reward functions specifically tailored to persuasive dialogue based on this corpus. Then, we implement a fully automatic text-based dialogue system for evaluating the learned policies. Using the implemented dialogue system, we evaluate the learned policy and the effect of framing through experiments both with a user simulator and with real users. The experimental evaluation indicates that the proposed method is effective for construction of cooperative persuasive dialogue systems. © 2016 Elsevier B.V.",,"Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan; Nagoya University, Furo-cho, Chikusa-kuNagoya, Japan",,,Cooperative persuasive dialogue; Dialogue modeling; Dialogue system; Framing; Reinforcement learning,Author4,Author1,a,a,a,a,a,a,Personal Assistants,,Dialogue system,,a,a,Speech Communication,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989201964&doi=10.1016%2fj.specom.2016.09.002&partnerID=40&md5=6d48db88d86248c3e2ce1aa7891e78c2,"Hiraoka T., Neubig G., Sakti S., Toda T., Nakamura S.","Hiraoka, T., Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan; Neubig, G., Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan; Sakti, S., Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan; Toda, T., Nagoya University, Furo-cho, Chikusa-kuNagoya, Japan; Nakamura, S., Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan",3,,10.1016/j.specom.2016.09.002,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84989201964,,,,,96,83,,,,,Scopus,,,,Article,,,84,,2016,2016,1,0,3,Author4
134,1,0,1,1,Improving user satisfaction in agent-based electronic marketplaces by reputation modelling and adjustable product quality,"In this paper, we propose a market model and learning algorithms for buying and selling agents in electronic marketplaces. We take into account the fact that multiple selling agents may offer the same good with different qualities, and that selling agents may alter the quality of their goods. We also consider the possible existence of dishonest selling agents in the market. In our approach, buying agents learn to maximize their expected value of goods using reinforcement learning. In addition, they model and exploit the reputation of selling agents to avoid interaction with the disreputable ones, and therefore to reduce the risk of purchasing low value goods. Our selling agents learn to maximize their expected profits by using reinforcement learning to adjust product prices, and also by altering product quality to provide more customized value to their goods. This paper focuses on presenting results from experiments investigating the behaviours of buying and selling agents in large-sized electronic marketplaces. Our results confirm that buying and selling agents following the proposed algorithms obtain greater satisfaction than buying and selling agents who only use reinforcement learning, with the buying agents not modelling sellers' reputation and the selling agents not adjusting product quality.","In this paper, we propose a market model and learning algorithms for buying and selling agents in electronic marketplaces. We take into account the fact that multiple selling agents may offer the same good with different qualities, and that selling agents may alter the quality of their goods. We also consider the possible existence of dishonest selling agents in the market. In our approach, buying agents learn to maximize their expected value of goods using reinforcement learning. In addition, they model and exploit the reputation of selling agents to avoid interaction with the disreputable ones, and therefore to reduce the risk of purchasing low value goods. Our selling agents learn to maximize their expected profits by using reinforcement learning to adjust product prices, and also by altering product quality to provide more customized value to their goods. This paper focuses on presenting results from experiments investigating the behaviours of buying and selling agents in large-sized electronic marketplaces. Our results confirm that buying and selling agents following the proposed algorithms obtain greater satisfaction than buying and selling agents who only use reinforcement learning, with the buying agents not modelling sellers' reputation and the selling agents not adjusting product quality.",,"School of Computer Science, University of Waterloo, Waterloo, Ont. N2L 3G1, Canada",,,,Author1,Author2,a,a,u,a,a,a,Games,,,,u,a,"Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS 2004",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4544220420&partnerID=40&md5=d1231e95154e8761409eedfa69f28d67,"Tran T., Cohen R.","Tran, T., School of Computer Science, University of Waterloo, Waterloo, Ont. N2L 3G1, Canada; Cohen, R., School of Computer Science, University of Waterloo, Waterloo, Ont. N2L 3G1, Canada",42,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-4544220420,,,,,835,828,,,,,Scopus,,,,Conference Paper,,,2,,,2004,1,3,0,Author4
135,2,2,2,2,Facilitating safe adaptation of learning agents using interactive reinforcement learning,"In this paper, we propose a learning framework for the adaptation of an interactive agent to a new user. We focus on applications where safety and personalization are essential, as Rehabilitation Systems and Robot Assisted Therapy. We argue that interactive learning methods can be utilised and combined into the Reinforcement Learning framework, aiming at a safe and tailored interaction.","In this paper, we propose a learning framework for the adaptation of an interactive agent to a new user. We focus on applications where safety and personalization are essential, as Rehabilitation Systems and Robot Assisted Therapy. We argue that interactive learning methods can be utilised and combined into the Reinforcement Learning framework, aiming at a safe and tailored interaction.",,"Computer Science and Engineering Dept., University of Texas, Arlington, United States; Institute of Informatics and Telecommunications, National Centre for Scientific Research Demokritos, Greece",,,Adaptation; Interaction management; Interactive reinforcement learning; Learning agents,Author2,Author3,a,a,a,a,a,a,Interfaces / HCI,a,,,a,a,"International Conference on Intelligent User Interfaces, Proceedings IUI",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014170682&doi=10.1145%2f2876456.2876457&partnerID=40&md5=9691ccc76385d92e9a7f6aefae9ddc6c,Tsiakas K.,"Tsiakas, K., Computer Science and Engineering Dept., University of Texas, Arlington, United States, Institute of Informatics and Telecommunications, National Centre for Scientific Research Demokritos, Greece",4,,10.1145/2876456.2876457,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85014170682,,,,,109,106,,,,,Scopus,,,,Conference Paper,,,,,2016,2016,0,0,4,Author2
136,2,0,2,2,Personalized ad recommendation systems for life-time value optimization with guarantees,"In this paper, we propose a framework for using reinforcement learning (RL) algorithms to learn good policies for personalized ad recommendation (PAR) systems. The RL algorithms take into account the long-term effect of an action, and thus, could be more suitable than myopic techniques like supervised learning and contextual bandit, for modern PAR systems in which the number of returning visitors is rapidly growing. However, while myopic techniques have been well-studied in PAR systems, the RL approach is still in its infancy, mainly due to two fundamental challenges: how to compute a good RL strategy and how to evaluate a solution using historical data to ensure its ""safety"" before deployment. In this paper, we propose to use a family of off-policy evaluation techniques with statistical guarantees to tackle both these challenges. We apply these methods to a real PAR problem, both for evaluating the final performance and for optimizing the parameters of the RL algorithm. Our results show that a RL algorithm equipped with these offpolicy evaluation techniques outperforms the myopic approaches. Our results also give fundamental insights on the difference between the click through rate (CTR) and life-time value (LTV) metrics for evaluating the performance of a PAR algorithm.","In this paper, we propose a framework for using reinforcement learning (RL) algorithms to learn good policies for personalized ad recommendation (PAR) systems. The RL algorithms take into account the long-term effect of an action, and thus, could be more suitable than myopic techniques like supervised learning and contextual bandit, for modern PAR systems in which the number of returning visitors is rapidly growing. However, while myopic techniques have been well-studied in PAR systems, the RL approach is still in its infancy, mainly due to two fundamental challenges: how to compute a good RL strategy and how to evaluate a solution using historical data to ensure its ""safety"" before deployment. In this paper, we propose to use a family of off-policy evaluation techniques with statistical guarantees to tackle both these challenges. We apply these methods to a real PAR problem, both for evaluating the final performance and for optimizing the parameters of the RL algorithm. Our results show that a RL algorithm equipped with these offpolicy evaluation techniques outperforms the myopic approaches. Our results also give fundamental insights on the difference between the click through rate (CTR) and life-time value (LTV) metrics for evaluating the performance of a PAR algorithm.",,"Adobe Research, United States; UMassAmherst, United States; INRIA, France",,,,Author3,Author4,a,a,a,a,a,a,Recommender Systems,a,,,a,a,IJCAI International Joint Conference on Artificial Intelligence,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949758918&partnerID=40&md5=7653e3556c282c0be9fdbdfaf07f4fb5,"Theocharous G., Thomas P.S., Ghavamzadeh M.","Theocharous, G., Adobe Research, United States; Thomas, P.S., Adobe Research, United States, UMassAmherst, United States; Ghavamzadeh, M., Adobe Research, United States, INRIA, France",7,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-84949758918,,,,,1812,1806,,,,,Scopus,,,,Conference Paper,,,2015-January,,2015,2015,1,0,3,Author4
137,2,0,2,2,Use of reinforcement learning in two real applications,"In this paper, we present two sucessful applications of Reinforcement Learning (RL) in real life. First, the optimization of anemia management in patients undergoing Chronic Renal Failure is presented. The aim is to individualize the treatment (Erythropoietin dosages) in order to stabilize patients within a targeted range of Hemoglobin (Hb). Results show that the use of RL increases the ratio of patients within the desired range of Hb. Thus, patients' quality of life is increased, and additionally, Health Care System reduces its expenses in anemia management. Second, RL is applied to modify a marketing campaign in order to maximize long-term profits. RL obtains an individualized policy depending on customer characteristics that increases long-term profits at the end of the campaign. Results in both problems show the robustness of the obtained policies and suggest their use in other real-life problems. © 2008 Springer Berlin Heidelberg.","In this paper, we present two sucessful applications of Reinforcement Learning (RL) in real life. First, the optimization of anemia management in patients undergoing Chronic Renal Failure is presented. The aim is to individualize the treatment (Erythropoietin dosages) in order to stabilize patients within a targeted range of Hemoglobin (Hb). Results show that the use of RL increases the ratio of patients within the desired range of Hb. Thus, patients' quality of life is increased, and additionally, Health Care System reduces its expenses in anemia management. Second, RL is applied to modify a marketing campaign in order to maximize long-term profits. RL obtains an individualized policy depending on customer characteristics that increases long-term profits at the end of the campaign. Results in both problems show the robustness of the obtained policies and suggest their use in other real-life problems. © 2008 Springer Berlin Heidelberg.",,"Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain",,,,Author4,Author1,a,a,a,a,a,a,Healthcare,,Also marketing (two domains),,a,a,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58449123856&doi=10.1007%2f978-3-540-89722-4_15&partnerID=40&md5=fb2f4ac752eba3eca8189bfc230bd0e6,"Martín-Guerrero J.D., Soria-Olivas E., Martínez-Sober M., Serrrano-López A.J., Magdalena-Benedito R., Gómez-Sanchis J.","Martín-Guerrero, J.D., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain; Soria-Olivas, E., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain; Martínez-Sober, M., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain; Serrrano-López, A.J., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain; Magdalena-Benedito, R., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain; Gómez-Sanchis, J., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain",,,10.1007/978-3-540-89722-4_15,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-58449123856,,,,,204,191,,,,,Scopus,,,,Conference Paper,,,5323 LNAI,,2008,2008,1,0,3,Author4
138,0,1,1,1,An interactive learning and adaptation framework for adaptive robot assisted therapy,"In this paper, we present an interactive learning and adaptation framework. The framework combines Interactive Reinforcement Learning methods to effectively adapt and refine a learned policy to cope with new users. We argue that implicit feedback provided by the primary user and guidance from a secondary user can be integrated to the adaptation mechanism, resulting at a tailored and safe interaction. We illustrate this framework with a use case in Robot Assisted Therapy, presenting a Robot Yoga Trainer that monitors a yoga training session, adjusting the session parameters based on human motion activity recognition and evaluation through depth data, to assist the user complete the session, following a Reinforcement Learning approach. Copyright 2016 is held by the owner/author(s).","In this paper, we present an interactive learning and adaptation framework. The framework combines Interactive Reinforcement Learning methods to effectively adapt and refine a learned policy to cope with new users. We argue that implicit feedback provided by the primary user and guidance from a secondary user can be integrated to the adaptation mechanism, resulting at a tailored and safe interaction. We illustrate this framework with a use case in Robot Assisted Therapy, presenting a Robot Yoga Trainer that monitors a yoga training session, adjusting the session parameters based on human motion activity recognition and evaluation through depth data, to assist the user complete the session, following a Reinforcement Learning approach. Copyright 2016 is held by the owner/author(s).",,"CSE Department, University of Texas at Arlington, United States; Department of Psychology, HERACLEIA Lab, University of Texas at Arlington, United States; National Center for Scientific Research Demokritos, Athens, Greece",,,Adaptive Robot Assisted Therapy; Interactive Reinforcement Learning; Policy Adaptation,Author1,Author2,a,a,a,a,a,a,Personal Assistants,,,,a,a,ACM International Conference Proceeding Series,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005990789&doi=10.1145%2f2910674.2935857&partnerID=40&md5=cc7971a64b3859d4fd701146e04a14f9,"Tsiakas K., Papakostas M., Chebaa B., Ebert D., Karkaletsis V., Makedon F.","Tsiakas, K., CSE Department, University of Texas at Arlington, United States; Papakostas, M., CSE Department, University of Texas at Arlington, United States; Chebaa, B., Department of Psychology, HERACLEIA Lab, University of Texas at Arlington, United States; Ebert, D., CSE Department, University of Texas at Arlington, United States; Karkaletsis, V., National Center for Scientific Research Demokritos, Athens, Greece; Makedon, F., CSE Department, University of Texas at Arlington, United States",1,,10.1145/2910674.2935857,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85005990789,,,,,,,,,,,Scopus,,,,Conference Paper,,,29-June-2016,,2016,2016,1,3,0,Author4
139,1,1,1,1,Interactive learning and adaptation for robot assisted therapy for people with dementia,"In this paper, we present an adaptive cognitive music game designed to monitor and improve the attention levels of peo- ple with dementia. The goal of this game is to provide a customized protocol based on user needs and preferences, following the Reinforcement Learning (RL) framework. The game adjusts its parameters (e.g., diffculty level) so as to help the user complete the task successfully, while keeping them engaged. The main contribution of this paper is an interactive learning and adaptation framework that enables and facilitates the adaptation of the robot behavior towards new users, providing a safe, tailored and effcient interac- tion. Copyright 2016 is held by the owner/author(s).","In this paper, we present an adaptive cognitive music game designed to monitor and improve the attention levels of peo- ple with dementia. The goal of this game is to provide a customized protocol based on user needs and preferences, following the Reinforcement Learning (RL) framework. The game adjusts its parameters (e.g., diffculty level) so as to help the user complete the task successfully, while keeping them engaged. The main contribution of this paper is an interactive learning and adaptation framework that enables and facilitates the adaptation of the robot behavior towards new users, providing a safe, tailored and effcient interac- tion. Copyright 2016 is held by the owner/author(s).",,"CSE Department, HERACLEIA Lab, University of Texas at Arlington, United States; Department of Psychology, Cognitive Neuroscience of Memory Lab, University of Texas at Arlington, United States",,,Interactive Reinforcement Learning; Music Therapy; Policy Adaptation; Robot Assisted Therapy; Robot Learning and Behavior Adaptation,Author2,Author3,a,a,a,a,a,a,Games,a,,,a,a,ACM International Conference Proceeding Series,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006062180&doi=10.1145%2f2910674.2935849&partnerID=40&md5=ec3f5c09483c1d6eab071621bde732a1,"Tsiakas K., Abellanoza C., Makedon F.","Tsiakas, K., CSE Department, HERACLEIA Lab, University of Texas at Arlington, United States; Abellanoza, C., Department of Psychology, Cognitive Neuroscience of Memory Lab, University of Texas at Arlington, United States; Makedon, F., CSE Department, HERACLEIA Lab, University of Texas at Arlington, United States",,,10.1145/2910674.2935849,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85006062180,,,,,,,,,,,Scopus,,,,Conference Paper,,,29-June-2016,,2016,2016,0,4,0,Author2
140,1,0,2,1,Exploiting Reinforcement Learning to Profile Users and Personalize Web Pages,"In this paper, we present a Web content adaptation system that is able to automatically adapt textual elements of Web pages, based on the user profile and preferences. The system employs Web intelligence to perform these automatic adaptations on single elements composing a Web page. In particular, a reinforcement learning algorithm, i.e. Q-learning, based on the idea of reward/punishment is utilized as the machine learning system that manages the user profile. Based on it, the user profile is updated, so that automatic adaptations can be effectively performed while surfing the Web. We created a simulation scenario to test our approach over different users with specific preferences and/or different kinds of disabilities. Simulation results confirm the viability of the proposal.","In this paper, we present a Web content adaptation system that is able to automatically adapt textual elements of Web pages, based on the user profile and preferences. The system employs Web intelligence to perform these automatic adaptations on single elements composing a Web page. In particular, a reinforcement learning algorithm, i.e. Q-learning, based on the idea of reward/punishment is utilized as the machine learning system that manages the user profile. Based on it, the user profile is updated, so that automatic adaptations can be effectively performed while surfing the Web. We created a simulation scenario to test our approach over different users with specific preferences and/or different kinds of disabilities. Simulation results confirm the viability of the proposal.",,,,"Dept. of Comput. Sci. & Eng., Univ. di Bologna, Bologna, Italy",Web personalization;content adaptation;legibility;reinforcement learning;user profiling,Author3,Author4,a,a,a,a,a,a,Interfaces / HCI,a,,,a,a,2014 IEEE 38th International Computer Software and Applications Conference Workshops,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903138,,S. Ferretti; S. Mirri; C. Prandi; P. Salomoni,,1,,10.1109/COMPSACW.2014.45,IEEE Xplore,1,,20140922,257,,Adaptation models;Context;Learning (artificial intelligence);Learning systems;Prototypes;Senior citizens;Web pages,Internet;learning (artificial intelligence);user interfaces,Q-learning;Web content adaptation system;Web intelligence;Web page personalization;machine learning system;reinforcement learning;simulation scenario;textual elements;user preference;user profile,,,,21-25 July 2014,,,,,,,,,,,,IEEE,24,,Electronic:978-1-4799-3578-9; POD:978-1-4799-3579-6,,252,IEEE Conferences,,,,,2014,2014,1,2,1,Author3
141,1,0,1,1,Personalized intelligent tutoring system using reinforcement learning,"In this paper, we present a Personalized Intelligent Tutoring System that uses Reinforcement Learning techniques to implicitly learn teaching rules and provide instructions to students based on their needs. The system works on coarsely labeled data with minimum expert knowledge to ease extension to newer domains. Copyright © 2011, Association for the Advancement of Artificial Intelligence. All rights reserved.","In this paper, we present a Personalized Intelligent Tutoring System that uses Reinforcement Learning techniques to implicitly learn teaching rules and provide instructions to students based on their needs. The system works on coarsely labeled data with minimum expert knowledge to ease extension to newer domains. Copyright © 2011, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Microsoft Corporation, India; Department of Computer Science and Engineering, IIT Madras, India",,,,Author4,Author1,a,a,a,a,a,a,Intelligent Tutors,,,,a,a,"Proceedings of the 24th International Florida Artificial Intelligence Research Society, FLAIRS - 24",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052392052&partnerID=40&md5=6dc8d7ca7906b00d34f9338e4ba9a6bf,"Malpani A., Ravindran B., Murthy H.","Malpani, A., Microsoft Corporation, India; Ravindran, B., Department of Computer Science and Engineering, IIT Madras, India; Murthy, H., Department of Computer Science and Engineering, IIT Madras, India",5,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-80052392052,,,,,562,561,,,,,Scopus,,,,Conference Paper,,,,,2011,2011,1,3,0,Author4
142,0,0,1,0,Constructing an intelligent behavior avatar in a virtual world: a self-learning model based on reinforcement,"In this paper, a novel method for personal intelligent behavior avatar (IBA) is proposed to acquire autonomous behavior based on the interactions between user and smart objects in the virtual environment. In this method, the behavior decision model and the self-learning model are integrated by Bayesian networks and reinforcement learning. The Bayesian networks can treat interaction experiences using statistical processes, and the sureness of decision making is represented by certainty factors using stochastic reasoning. The reinforcement learning is implemented by learning experimentation or trial and error mechanisms to improve the performance of IBA through feedback. Therefore, the IBA makes a strategic decision that is approximated and appropriate to the user through the self-learning process by reinforcement learning. Finally, the feasibility of this method is investigated by imitating user's behavior and the results of self-learning process. The results of simulation show that the method is successful in imitating user's behavior and improving the performance of IBA.","In this paper, a novel method for personal intelligent behavior avatar (IBA) is proposed to acquire autonomous behavior based on the interactions between user and smart objects in the virtual environment. In this method, the behavior decision model and the self-learning model are integrated by Bayesian networks and reinforcement learning. The Bayesian networks can treat interaction experiences using statistical processes, and the sureness of decision making is represented by certainty factors using stochastic reasoning. The reinforcement learning is implemented by learning experimentation or trial and error mechanisms to improve the performance of IBA through feedback. Therefore, the IBA makes a strategic decision that is approximated and appropriate to the user through the self-learning process by reinforcement learning. Finally, the feasibility of this method is investigated by imitating user's behavior and the results of self-learning process. The results of simulation show that the method is successful in imitating user's behavior and improving the performance of IBA.",,,,"Dept. of Inf. Eng., Tamkang Univ., Tamsui, Taiwan",,Author2,Author3,a,a,u,a,a,a,Personal Assistants,a,,I think we should accept and see in the eligibility phase whether the application of RL is to achieve personalization,u,a,"IRI -2005 IEEE International Conference on Information Reuse and Integration, Conf, 2005.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1506510,,Jui-Fa Chen; Wei-Chuan Lin; Hua-Sheng Bai; Chia-Che Yang; Hsiao-Chuan Chao,,1,,10.1109/IRI-05.2005.1506510,IEEE Xplore,1,,20050912,426,,Artificial intelligence;Avatars;Bayesian methods;Chaos;Decision making;Educational institutions;Information technology;Intelligent agent;Learning;Virtual environment,avatars;belief networks;decision making;human computer interaction;inference mechanisms;stochastic processes;unsupervised learning,Bayesian networks;behavior decision model;decision making;personal intelligent behavior avatar;reinforcement learning;self-learning model;statistical process;stochastic reasoning;user behavior;virtual world,,,,15-17 Aug. 2005,,,,,,,,,,,,IEEE,9,,POD:0-7803-9093-8,,421,IEEE Conferences,,,,,,2005,3,1,0,Author3
143,1,2,0,1,A Fast Interactive Search System for Healthcare Services,"In this paper we describe the design, development, and evaluation of a general human-machine interaction search system, and its potential and use in the context of a collaboration project with SAP and Saffron. The objective of a specialized version of the system is to provide medical and healthcare information services to users via interactive search for personalized patient needs. Patients usually have questions regarding healthcare, including those which concern illness symptoms, duration and types of treatment, possible drug effects, and more. Authorized personnel would often be ideal in responding to such needs; however they could potentially be very expensive, and not easy to support and maintain. If patients could have access to information at their home, by means of i-phone or online access, this could save time, doctor office visit expenses, as well as valuable and restricted medical time. What is more, information concerning other anonymized and similar patient cases provides knowledge and perspective on a wide range of patient issues. From the doctors' perspective, they typically need to spend time on differential analysis about new patient cases: study symptoms, research possible causes, rank results by emergency priority and treat them accordingly. A search system that would direct a doctor (or patient/user) to similar patient cases would save significant amount of manual search time. The powerful new feature of this system is the storage and mining of past patient cases knowledge, to create metadata to be used in the subsequent retrieval of relevant documents. Finally, the interactive search system would speed up identification of rare cases; for instance, symptoms that do not appear commonly in past cases may require special treatment or expert referral. We build a model which dynamically learns medical needs of interacting MDs and patients. The model works on free or unstructured text, allowing disambiguation of vague words and flexibility in descri- ing medical needs. In addition, both experts with an advanced knowledge of medical terminology, and beginning users using basic medical terms, can achieve high search relevance. Furthermore, our approach obviates the need for the assignment of tags or labels, such as treatment, symptoms, causes, to documents, to respond effectively to user queries. In particular, we build a temporal difference algorithm to predict user's information needs by incorporating both current and predicted knowledge into learning the user profile. Our source of information about the user consists of submitted queries and feedback on the returned results. We tested our system on publicly available medical data (OhsuMed TREC dataset 2002) and we achieved a significant improvement in retrieval accuracy, compared to the literature. We provide quantitative results as well as demonstration screenshots which illustrate a) the value of interaction (user time spent with system versus results accuracy), b) the value of using medical terminology understanding, when compared with simple general words, and c) the value of allowing the maximum number of feedback submissions to vary.","In this paper we describe the design, development, and evaluation of a general human-machine interaction search system, and its potential and use in the context of a collaboration project with SAP and Saffron. The objective of a specialized version of the system is to provide medical and healthcare information services to users via interactive search for personalized patient needs. Patients usually have questions regarding healthcare, including those which concern illness symptoms, duration and types of treatment, possible drug effects, and more. Authorized personnel would often be ideal in responding to such needs; however they could potentially be very expensive, and not easy to support and maintain. If patients could have access to information at their home, by means of i-phone or online access, this could save time, doctor office visit expenses, as well as valuable and restricted medical time. What is more, information concerning other anonymized and similar patient cases provides knowledge and perspective on a wide range of patient issues. From the doctors' perspective, they typically need to spend time on differential analysis about new patient cases: study symptoms, research possible causes, rank results by emergency priority and treat them accordingly. A search system that would direct a doctor (or patient/user) to similar patient cases would save significant amount of manual search time. The powerful new feature of this system is the storage and mining of past patient cases knowledge, to create metadata to be used in the subsequent retrieval of relevant documents. Finally, the interactive search system would speed up identification of rare cases; for instance, symptoms that do not appear commonly in past cases may require special treatment or expert referral. We build a model which dynamically learns medical needs of interacting MDs and patients. The model works on free or unstructured text, allowing disambiguation of vague words and flexibility in descri- ing medical needs. In addition, both experts with an advanced knowledge of medical terminology, and beginning users using basic medical terms, can achieve high search relevance. Furthermore, our approach obviates the need for the assignment of tags or labels, such as treatment, symptoms, causes, to documents, to respond effectively to user queries. In particular, we build a temporal difference algorithm to predict user's information needs by incorporating both current and predicted knowledge into learning the user profile. Our source of information about the user consists of submitted queries and feedback on the returned results. We tested our system on publicly available medical data (OhsuMed TREC dataset 2002) and we achieved a significant improvement in retrieval accuracy, compared to the literature. We provide quantitative results as well as demonstration screenshots which illustrate a) the value of interaction (user time spent with system versus results accuracy), b) the value of using medical terminology understanding, when compared with simple general words, and c) the value of allowing the maximum number of feedback submissions to vary.",,,,"Technol. & Inf. Manage., Univ. of California Santa Cruz, Santa Cruz, CA, USA",healthcare information services;interactive retrieval;medical data retrieval;reinforcement learning;temporal difference,Author3,Author4,a,a,a,a,a,a,Healthcare,a,,,a,a,2012 Annual SRII Global Conference,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6311035,,M. Daltayanni; C. Wang; R. Akella,,0,,10.1109/SRII.2012.65,IEEE Xplore,1,,20120924,534,,Diseases;Information services;Medical diagnostic imaging;Senior citizens;Terminology;Unified modeling language,data mining;health care;human computer interaction;information needs;information retrieval;interactive systems;medical information systems;text analysis,SAP;Saffron;basic medical terms;collaboration project;data mining;data storage;differential analysis;document retrieval;fast interactive search system;free or unstructured text;healthcare information services;human-machine interaction search system;information needs;medical information services;medical terminology;metadata;patient cases knowledge;patient issues;personalized patient needs;publicly available medical data;rare cases;submitted queries;temporal difference algorithm;unstructured text,,2166-0778;21660778,,24-27 July 2012,,,,,,,,,,,,IEEE,25,,Electronic:978-1-5090-5643-9; POD:978-1-4673-2318-5; USB:978-0-7695-4770-1,,525,IEEE Conferences,,,,,2012,2012,1,2,1,Author2
144,1,0,1,1,An immersive learning model using evolutionary learning,"In this article, we have proposed an educational model using virtual reality on a mobile platform by personalizing the simulated environments as per user actions. We have also proposed an evolutionary learning algorithm based on which the user learning path is designed and the corresponding simulated learning environment is modified. The main objective of this study is to create a personalized learning path for each student as per their calibre and make the learning immersive and retainable using virtual reality. Our proposed model emulates the innate natural learning process in humans and uses that to customize the virtual simulations of the lessons by applying the evolutionary learning technique. A quasi-experimental study is conducted by taking different case studies to establish the effectiveness of our learning model. The results show that our learning model is immersive and gives long term retention while enhancing creativity through reinforced customization of the simulations. © 2017 Elsevier Ltd","In this article, we have proposed an educational model using virtual reality on a mobile platform by personalizing the simulated environments as per user actions. We have also proposed an evolutionary learning algorithm based on which the user learning path is designed and the corresponding simulated learning environment is modified. The main objective of this study is to create a personalized learning path for each student as per their calibre and make the learning immersive and retainable using virtual reality. Our proposed model emulates the innate natural learning process in humans and uses that to customize the virtual simulations of the lessons by applying the evolutionary learning technique. A quasi-experimental study is conducted by taking different case studies to establish the effectiveness of our learning model. The results show that our learning model is immersive and gives long term retention while enhancing creativity through reinforced customization of the simulations. © 2017 Elsevier Ltd",,"Department of Computer Science and Engineering, Kyungpook National University, Daegu, South Korea; Department of Electronics and Telecommunication Engineering, Karpagam College of Engineering, Coimbatore, TN, India",,,Education; Evolutionary learning; Immersive learning; Immersive virtual reality; m-learning; Personalized learning; Reinforcement learning,Author2,Author3,u,a,a,a,a,a,Personal Assistants,d,domain: intelligent tutors,Considering the keywords I think we should accept,u,a,Computers and Electrical Engineering,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032829128&doi=10.1016%2fj.compeleceng.2017.08.023&partnerID=40&md5=e276a65771f4540bb5bcd68a3cc6b0e1,"Bhattacharjee D., Paul A., Kim J.H., Karthigaikumar P.","Bhattacharjee, D., Department of Computer Science and Engineering, Kyungpook National University, Daegu, South Korea; Paul, A., Department of Computer Science and Engineering, Kyungpook National University, Daegu, South Korea; Kim, J.H., Department of Computer Science and Engineering, Kyungpook National University, Daegu, South Korea; Karthigaikumar, P., Department of Electronics and Telecommunication Engineering, Karpagam College of Engineering, Coimbatore, TN, India",1,,10.1016/j.compeleceng.2017.08.023,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85032829128,,,,,249,236,,,,,Scopus,,,,Article,,,65,,,2018,1,3,0,Author4
145,2,0,1,1,Towards learning reward functions from user interactions,"In the physical world, people have dynamic preferences, e.g., the same situation can lead to satisfaction for some humans and to frustration for others. Personalization is called for. The same observation holds for online behavior with interactive systems. It is natural to represent the behavior of users who are engaging with interactive systems such as a search engine or a recommender system, as a sequence of actions where each next action depends on the current situation and the user reward of taking a particular action. By and large, current online evaluation metrics for interactive systems such as search engines or recommender systems, are static and do not reflect differences in user behavior. They rarely capture or model the reward experienced by a user while interacting with an interactive system.We argue that knowing a user's reward function is essential for an interactive system as both for learning and evaluation. We propose to learn users' reward functions directly from observed interaction traces. In particular, we present how users' reward functions can be uncovered directly using inverse reinforcement learning techniques. We also show how to incorporate user features into the learning process. Our main contribution is a novel and dynamic approach to restore a user's reward function. We present an analytic approach to this problem and complement it with initial experiments using the interaction logs of a cultural heritage institution that demonstrate the feasibility of the approach by uncovering different reward functions for different user groups. © 2017 Copyright held by the owner/author(s).","In the physical world, people have dynamic preferences, e.g., the same situation can lead to satisfaction for some humans and to frustration for others. Personalization is called for. The same observation holds for online behavior with interactive systems. It is natural to represent the behavior of users who are engaging with interactive systems such as a search engine or a recommender system, as a sequence of actions where each next action depends on the current situation and the user reward of taking a particular action. By and large, current online evaluation metrics for interactive systems such as search engines or recommender systems, are static and do not reflect differences in user behavior. They rarely capture or model the reward experienced by a user while interacting with an interactive system.We argue that knowing a user's reward function is essential for an interactive system as both for learning and evaluation. We propose to learn users' reward functions directly from observed interaction traces. In particular, we present how users' reward functions can be uncovered directly using inverse reinforcement learning techniques. We also show how to incorporate user features into the learning process. Our main contribution is a novel and dynamic approach to restore a user's reward function. We present an analytic approach to this problem and complement it with initial experiments using the interaction logs of a cultural heritage institution that demonstrate the feasibility of the approach by uncovering different reward functions for different user groups. © 2017 Copyright held by the owner/author(s).",,"University of Amsterdam, Amsterdam, Netherlands; UserSat.com, University of Amsterdam, Amsterdam, Netherlands",,,Interactive systems; Inverse reinforcement learning; Online evaluation,Author3,Author4,a,a,a,a,a,a,Interfaces / HCI,a,,,a,a,ICTIR 2017 - Proceedings of the 2017 ACM SIGIR International Conference on the Theory of Information Retrieval,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033215376&doi=10.1145%2f3121050.3121098&partnerID=40&md5=ddbe414389c116035f0fe3ceee854a01,"Li Z., Kiseleva J., De Rijke M., Grotov A.","Li, Z., University of Amsterdam, Amsterdam, Netherlands; Kiseleva, J., UserSat.com, University of Amsterdam, Amsterdam, Netherlands; De Rijke, M., University of Amsterdam, Amsterdam, Netherlands; Grotov, A., University of Amsterdam, Amsterdam, Netherlands",,,10.1145/3121050.3121098,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85033215376,,,,,292,289,,,,,Scopus,,,,Conference Paper,,,,,2017,2017,1,2,1,Author1
146,1,1,1,1,Personalised Human-Robot Co-Adaptation in Instructional Settings using Reinforcement Learning,"In the domain of robotic tutors, personalised tutoring has startedto receive scientists’ attention, but is still relatively underexplored. Previ-ous work using reinforcement learning (RL) has addressed personalisedtutoring from the perspective of affective policy learning. In this paperwe build on previous work on affective policy learning that used RL tolearn  what  robot’s  supportive  behaviours  are  preferred  by  users  in  aneducational  scenario.  We  propose  a  RL  framework  for  personalisationthat selects a robot’s supportive behaviours to maximize user’s task per-formance in a learning scenario where a Pepper robot acting as a tutorhelps people learning how to solve grid-based logic puzzles. This workis relevant for the development of persuasive embodied agents and so-cial robots used to support users in different scenarios. In particular, thispaper makes a contribution towards the development of algorithms forhuman-robot co-adaptation that enable robots and agents to select effec-tive strategies to establish long-term relationships with human users","In the domain of robotic tutors, personalised tutoring has startedto receive scientists’ attention, but is still relatively underexplored. Previ-ous work using reinforcement learning (RL) has addressed personalisedtutoring from the perspective of affective policy learning. In this paperwe build on previous work on affective policy learning that used RL tolearn  what  robot’s  supportive  behaviours  are  preferred  by  users  in  aneducational  scenario.  We  propose  a  RL  framework  for  personalisationthat selects a robot’s supportive behaviours to maximize user’s task per-formance in a learning scenario where a Pepper robot acting as a tutorhelps people learning how to solve grid-based logic puzzles. This workis relevant for the development of persuasive embodied agents and so-cial robots used to support users in different scenarios. In particular, thispaper makes a contribution towards the development of algorithms forhuman-robot co-adaptation that enable robots and agents to select effec-tive strategies to establish long-term relationships with human users",,,,,,Author3,Author4,a,a,a,a,a,a,Intelligent Tutors,a,,,a,a,Persuasive Embodied Agents for Behavior Change (PEACH2017) Workshop at the International conference on Intelligent Virtual Agents (IVA2017),"http://scholar.google.com/scholar?cluster=14209507154038331625&hl=en&as_sdt=0,5",http://www.diva-portal.org/smash/get/diva2:1162389/FULLTEXT01.pdf,,,1,,,Google Scholar,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2017,2017,0,4,0,Author4
147,1,0,0,0,Automated negotiation in multiple e-Marketplaces by using Learning Personalized Mobile Shopping Agents,"In the B2C- or C2C- based e-Marketplaces, the same merchandise is sold by different sellers in different e-Marketplaces. The trading platforms nowadays has resulted in information explosion that makes buyers unable to retrieve and analyze entire merchandise information easily and, therefore, decreases their negotiation power. Moreover, without the records of buyers ' transaction preference, it 's not easy for most agent systems to help common buyers to increase their negotiation power. In this paper, we propose the Learning Personalized Mobile Shopping Agent (LPMSA) and apply it to three e-Marketplace architectures: alliance, broker, noncooperation. The buyer can dispatch mobile agents to multiple e-Marketplaces for collecting merchandise information, negotiating with sellers, and buying merchandise from the above architectures. Furthermore, the agent can learn more about buyer's preference. As a result, the proposed architecture can not only find suitable trading partners for buyers but also help them to get better deals.","In the B2C- or C2C- based e-Marketplaces, the same merchandise is sold by different sellers in different e-Marketplaces. The trading platforms nowadays has resulted in information explosion that makes buyers unable to retrieve and analyze entire merchandise information easily and, therefore, decreases their negotiation power. Moreover, without the records of buyers ' transaction preference, it 's not easy for most agent systems to help common buyers to increase their negotiation power. In this paper, we propose the Learning Personalized Mobile Shopping Agent (LPMSA) and apply it to three e-Marketplace architectures: alliance, broker, noncooperation. The buyer can dispatch mobile agents to multiple e-Marketplaces for collecting merchandise information, negotiating with sellers, and buying merchandise from the above architectures. Furthermore, the agent can learn more about buyer's preference. As a result, the proposed architecture can not only find suitable trading partners for buyers but also help them to get better deals.",,"Department of Information Management, Chaoyang University of Technology, 168, GiFeng East Road, Wufeng, Taichung County, Taiwan",,,Automated Negotiation; E-Marketplace; Mobile Agent; Reinforcement Learning,Author4,Author1,a,a,a,a,a,a,Personal Assistants,,,,a,a,"Proceedings of the International Conference on Artificial Intelligence, IC-AI'04",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12744281483&partnerID=40&md5=d315d9774254985d85ee436efabd63d8,"Lee F.-M., Li L.-H., Liu Y.-C.","Lee, F.-M., Department of Information Management, Chaoyang University of Technology, 168, GiFeng East Road, Wufeng, Taichung County, Taiwan; Li, L.-H., Department of Information Management, Chaoyang University of Technology, 168, GiFeng East Road, Wufeng, Taichung County, Taiwan; Liu, Y.-C., Department of Information Management, Chaoyang University of Technology, 168, GiFeng East Road, Wufeng, Taichung County, Taiwan",,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-12744281483,,,,,783,777,,,,,Scopus,,,,Conference Paper,,,2,,2004,2004,3,1,0,Author1
148,2,0,1,1,Optimistic bayesian sampling in contextual-bandit problems,"In sequential decision problems in an unknown environment, the decision maker often faces a dilemma over whether to explore to discover more about the environment, or to exploit current knowledge. We address the exploration-exploitation dilemma in a general setting encompassing both standard and contextualised bandit problems. The contextual bandit problem has recently resurfaced in attempts to maximise click-through rates in web based applications, a task with significant commercial interest. In this article we consider an approach of Thompson (1933) which makes use of samples from the posterior distributions for the instantaneous value of each action. We extend the approach by introducing a new algorithm, Optimistic Bayesian Sampling (OBS), in which the probability of playing an action increases with the uncertainty in the estimate of the action value. This results in better directed exploratory behaviour. We prove that, under unrestrictive assumptions, both approaches result in optimal behaviour with respect to the average reward criterion of Yang and Zhu (2002). We implement OBS and measure its performance in simulated Bernoulli bandit and linear regression domains, and also when tested with the task of personalised news article recommendation on a Yahoo! Front Page Today Module data set. We find that OBS performs competitively when compared to recently proposed benchmark algorithms and outperforms Thompson's method throughout. © 2012 Benedict C. May, Nathan Korda, Anthony Lee and David S. Leslie.","In sequential decision problems in an unknown environment, the decision maker often faces a dilemma over whether to explore to discover more about the environment, or to exploit current knowledge. We address the exploration-exploitation dilemma in a general setting encompassing both standard and contextualised bandit problems. The contextual bandit problem has recently resurfaced in attempts to maximise click-through rates in web based applications, a task with significant commercial interest. In this article we consider an approach of Thompson (1933) which makes use of samples from the posterior distributions for the instantaneous value of each action. We extend the approach by introducing a new algorithm, Optimistic Bayesian Sampling (OBS), in which the probability of playing an action increases with the uncertainty in the estimate of the action value. This results in better directed exploratory behaviour. We prove that, under unrestrictive assumptions, both approaches result in optimal behaviour with respect to the average reward criterion of Yang and Zhu (2002). We implement OBS and measure its performance in simulated Bernoulli bandit and linear regression domains, and also when tested with the task of personalised news article recommendation on a Yahoo! Front Page Today Module data set. We find that OBS performs competitively when compared to recently proposed benchmark algorithms and outperforms Thompson's method throughout. © 2012 Benedict C. May, Nathan Korda, Anthony Lee and David S. Leslie.",,"School of Mathematics, University of Bristol, Bristol, BS8 1TW, United Kingdom; Oxford-Man Institute, University of Oxford, Walton Well Road, Oxford, OX2 6ED, United Kingdom",,,Contextual bandits; Exploration-exploitation; Multi-armed bandits; Sequential allocation; Thompson sampling,Author2,Author3,u,a,a,a,a,a,Recommender Systems,a,,"Seem to use Contextual Bandit algorithms, which are in scope for this survey",u,a,Journal of Machine Learning Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864939787&partnerID=40&md5=7096900388c4f8aba3fc7a7151cd9980,"May B.C., Korda N., Lee A., Leslie D.S.","May, B.C., School of Mathematics, University of Bristol, Bristol, BS8 1TW, United Kingdom; Korda, N., Oxford-Man Institute, University of Oxford, Walton Well Road, Oxford, OX2 6ED, United Kingdom; Lee, A., Oxford-Man Institute, University of Oxford, Walton Well Road, Oxford, OX2 6ED, United Kingdom; Leslie, D.S., School of Mathematics, University of Bristol, Bristol, BS8 1TW, United Kingdom",40,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-84864939787,,,,,2106,2069,,,,,Scopus,,,,Article,,,13,,,2012,1,2,1,Author1
149,1,0,2,1,Q-LDA: Uncovering latent patterns in text-based sequential decision processes,"In sequential decision making, it is often important and useful for end users to understand the underlying patterns or causes that lead to the corresponding decisions. However, typical deep reinforcement learning algorithms seldom provide such information due to their black-box nature. In this paper, we present a probabilistic model, Q-LDA, to uncover latent patterns in text-based sequential decision processes. The model can be understood as a variant of latent topic models that are tailored to maximize total rewards; we further draw an interesting connection between an approximate maximum-likelihood estimation of Q-LDA and the celebrated Q-learning algorithm. We demonstrate in the text-game domain that our proposed method not only provides a viable mechanism to uncover latent patterns in decision processes, but also obtains state-of-the-art rewards in these games. © 2017 Neural information processing systems foundation. All rights reserved.","In sequential decision making, it is often important and useful for end users to understand the underlying patterns or causes that lead to the corresponding decisions. However, typical deep reinforcement learning algorithms seldom provide such information due to their black-box nature. In this paper, we present a probabilistic model, Q-LDA, to uncover latent patterns in text-based sequential decision processes. The model can be understood as a variant of latent topic models that are tailored to maximize total rewards; we further draw an interesting connection between an approximate maximum-likelihood estimation of Q-LDA and the celebrated Q-learning algorithm. We demonstrate in the text-game domain that our proposed method not only provides a viable mechanism to uncover latent patterns in decision processes, but also obtains state-of-the-art rewards in these games. © 2017 Neural information processing systems foundation. All rights reserved.",,"Microsoft Research, Redmond, WA, United States; Google Inc., Kirkland, WA, United States; Citadel LLC, Seattle/Chicago, United States",,,"Social robots, Reinforcement learning, Personalisation, Tutoring sys-
tems",Author3,Author4,a,a,u,a,a,a,Intelligent Tutors,a,,Can we see that there is personalization?,u,a,Advances in Neural Information Processing Systems,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047005909&partnerID=40&md5=f243dfada6ff6acc567438af82c954c9,"Chen J., Wang C., Xiao L., He J., Li L., Deng L.","Chen, J., Microsoft Research, Redmond, WA, United States; Wang, C., Google Inc., Kirkland, WA, United States; Xiao, L., Microsoft Research, Redmond, WA, United States; He, J., Citadel LLC, Seattle/Chicago, United States; Li, L., Google Inc., Kirkland, WA, United States; Deng, L., Citadel LLC, Seattle/Chicago, United States",,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-85047005909,,,,,4987,4978,,,,,Scopus,,,,Conference Paper,,,2017-December,,,2017,1,2,1,Author3
150,2,0,2,2,Making better recommendations with online profiling agents,"In recent years, we have witnessed the success of autonomous agents applying machine learning techniques across a wide range of applications. However, agents applying the same machine learning techniques in online applications have not been so successful. Even agent-based hybrid recommender systems that combine information filtering techniques with collaborative filtering techniques have only been applied with considerable success to simple consumer goods such as movies, books, clothing and food. Complex, adaptive autonomous agent systems that can handle complex goods such as real estate, vacation plans, insurance, mutual funds, and mortgage have yet emerged. To a large extent, the reinforcement learning methods developed to aid agents in learning have been more successfully deployed in offline applications. The inherent limitations in these methods have rendered them somewhat ineffective in online applications. In this paper, we postulate that a small amount of prior knowledge and human-provided input can dramatically speed up online learning. We will demonstrate that our agent HumanE - with its prior knowledge or ""experiences"" about the real estate domain -can effectively assist users in identifying requirements, especially unstated ones, quickly and unobtrusively.","In recent years, we have witnessed the success of autonomous agents applying machine learning techniques across a wide range of applications. However, agents applying the same machine learning techniques in online applications have not been so successful. Even agent-based hybrid recommender systems that combine information filtering techniques with collaborative filtering techniques have only been applied with considerable success to simple consumer goods such as movies, books, clothing and food. Complex, adaptive autonomous agent systems that can handle complex goods such as real estate, vacation plans, insurance, mutual funds, and mortgage have yet emerged. To a large extent, the reinforcement learning methods developed to aid agents in learning have been more successfully deployed in offline applications. The inherent limitations in these methods have rendered them somewhat ineffective in online applications. In this paper, we postulate that a small amount of prior knowledge and human-provided input can dramatically speed up online learning. We will demonstrate that our agent HumanE - with its prior knowledge or ""experiences"" about the real estate domain -can effectively assist users in identifying requirements, especially unstated ones, quickly and unobtrusively.",,"School of Computing, National University of Singapore, 3 Science Drive 2, Singapore 117543, Singapore",,,Electronic profiling; Experience; Inference; Intelligent agents; Interactive learning; Personalization; Reinforcement learning; User preferences,Author1,Author2,a,a,u,a,a,a,Recommender Systems,,,,u,a,Proceedings of the National Conference on Artificial Intelligence,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-9444285517&partnerID=40&md5=52bdd57d2094350dc0f993039c1ff5df,"Oh D., Tan C.L.","Oh, D., School of Computing, National University of Singapore, 3 Science Drive 2, Singapore 117543, Singapore; Tan, C.L., School of Computing, National University of Singapore, 3 Science Drive 2, Singapore 117543, Singapore",,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-9444285517,,,,,792,785,,,,,Scopus,,,,Conference Paper,,,,,,2004,1,0,3,Author4
151,0,0,0,0,Design and Evaluation of a Self-Learning HTTP Adaptive Video Streaming Client,"HTTP Adaptive Streaming (HAS) is becoming the de facto standard for Over-The-Top (OTT)-based video streaming services such as YouTube and Netflix. By splitting a video into multiple segments of a couple of seconds and encoding each of these at multiple quality levels, HAS allows a video client to dynamically adapt the requested quality during the playout to react to network changes. However, state-of-the-art quality selection heuristics are deterministic and tailored to specific network configurations. Therefore, they are unable to cope with a vast range of highly dynamic network settings. In this letter, a novel Reinforcement Learning (RL)-based HAS client is presented and evaluated. The self-learning HAS client dynamically adapts its behaviour by interacting with the environment to optimize the Quality of Experience (QoE), the quality as perceived by the end-user. The proposed client has been thoroughly evaluated using a network-based simulator and is shown to outperform traditional HAS clients by up to 13% in a mobile network environment.","HTTP Adaptive Streaming (HAS) is becoming the de facto standard for Over-The-Top (OTT)-based video streaming services such as YouTube and Netflix. By splitting a video into multiple segments of a couple of seconds and encoding each of these at multiple quality levels, HAS allows a video client to dynamically adapt the requested quality during the playout to react to network changes. However, state-of-the-art quality selection heuristics are deterministic and tailored to specific network configurations. Therefore, they are unable to cope with a vast range of highly dynamic network settings. In this letter, a novel Reinforcement Learning (RL)-based HAS client is presented and evaluated. The self-learning HAS client dynamically adapts its behaviour by interacting with the environment to optimize the Quality of Experience (QoE), the quality as perceived by the end-user. The proposed client has been thoroughly evaluated using a network-based simulator and is shown to outperform traditional HAS clients by up to 13% in a mobile network environment.",,,,"Dept. of Inf. Tech., Ghent Univ., Ghent, Belgium",Streaming media;intelligent agent;learning systems;quality of service,Author4,Author1,a,a,a,a,a,a,"Other (please in ""remark"")",,"Perceived quality seems to point towards personalization, that's why I accept is as personalization.",,a,a,IEEE Communications Letters,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6746772,,M. Claeys; S. Latre; J. Famaey; F. De Turck,,27,,10.1109/LCOMM.2014.020414.132649,IEEE Xplore,1,,20140416,719,,Adaptive systems;Bandwidth;Bit rate;Convergence;Standards;Streaming media;Video sequences,hypermedia;learning (artificial intelligence);quality of experience;transport protocols;video streaming,OTT media streaming;QoE;mobile network environment;network-based simulator;novel reinforcement learning based HAS client;over-the-top based video streaming services;quality of experience;self-learning HAS client;self-learning HTTP adaptive video streaming client,,1089-7798;10897798,4,April 2014,,,,,,20140221,,,,,,IEEE,11,,,,716,IEEE Journals & Magazines,,,18,,2014,2014,4,0,0,Author2
152,1,1,1,1,Reinforcement learning for game personalization on edge devices,"Good progress has been shown recently in the area of active learning, specifically, Reinforcement learning (RL). In this paper, the authors show how RL can be used to personalize games based on user-interaction with the game. The work uses Deep Q network models (DQN) and the open source framework OpenAI to build an RL model that is able to optimize the gamer's engagement level in a game. The authors define an example quantitative measure of gamer engagement and incorporate that into the DQN learning reward function. The gamer experience optimization is empirically demonstrated using a game of Pong. Simulation testing and analysis of results indicate adapted RL models increase engagement reward values, thus enhancing gamer experience. The contribution of this paper is twofold: (1) using RL, it paves the path for wider adaptation to user-behavior, starting with gaming, and (2) it shows analysis and feasibility of an RL algorithm on an edge device (Personal Computer) in real-time.","Good progress has been shown recently in the area of active learning, specifically, Reinforcement learning (RL). In this paper, the authors show how RL can be used to personalize games based on user-interaction with the game. The work uses Deep Q network models (DQN) and the open source framework OpenAI to build an RL model that is able to optimize the gamer's engagement level in a game. The authors define an example quantitative measure of gamer engagement and incorporate that into the DQN learning reward function. The gamer experience optimization is empirically demonstrated using a game of Pong. Simulation testing and analysis of results indicate adapted RL models increase engagement reward values, thus enhancing gamer experience. The contribution of this paper is twofold: (1) using RL, it paves the path for wider adaptation to user-behavior, starting with gaming, and (2) it shows analysis and feasibility of an RL algorithm on an edge device (Personal Computer) in real-time.",,,,Intel Corporation,artificial intelligence;computer games;edge computing;game personalization;reinforcement learning,Author4,Author1,a,a,a,a,a,a,Games,,,,a,a,2018 International Conference on Information and Computer Technologies (ICICT),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8356853,,A. Bodas; B. Upadhyay; C. Nadiger; S. Abdelhak,,,,10.1109/INFOCT.2018.8356853,IEEE Xplore,1,,20180510,122,,Adaptation models;Buildings;Games;Learning (artificial intelligence);Mathematical model;Training,computer games;learning (artificial intelligence);mobile computing,DQN learning reward function;RL algorithm;RL model;active learning;deep Q network models;edge device;engagement reward values;game personalization;gamer engagement;gamer experience;open source framework OpenAI;personal computer;reinforcement learning,,,,23-25 March 2018,,,,,,,,,,,,IEEE,,,Electronic:978-1-5386-5384-5; POD:978-1-5386-5385-2; Paper:978-1-5386-5382-1; USB:978-1-5386-5383-8,,119,IEEE Conferences,,,,,2018,2018,0,4,0,Author1
153,1,0,2,1,Personalized Course Sequence Recommendations,"Given the variability in student learning, it is becoming increasingly important to tailor courses as well as course sequences to student needs. This paper presents a systematic methodology for offering personalized course sequence recommendations to students. First, a forward-search backward-induction algorithm is developed that can optimally select course sequences to decrease the time required for a student to graduate. The algorithm accounts for prerequisite requirements (typically present in higher level education) and course availability. Second, using the tools of multiarmed bandits, an algorithm is developed that can optimally recommend a course sequence that both reduces the time to graduate while also increasing the overall GPA of the student. The algorithm dynamically learns how students with different contextual backgrounds perform for given course sequences and, then, recommends an optimal course sequence for new students. Using real-world student data from the UCLA Mechanical and Aerospace Engineering Department, we illustrate how the proposed algorithms outperform other methods that do not include student contextual information when making course sequence recommendations.","Given the variability in student learning, it is becoming increasingly important to tailor courses as well as course sequences to student needs. This paper presents a systematic methodology for offering personalized course sequence recommendations to students. First, a forward-search backward-induction algorithm is developed that can optimally select course sequences to decrease the time required for a student to graduate. The algorithm accounts for prerequisite requirements (typically present in higher level education) and course availability. Second, using the tools of multiarmed bandits, an algorithm is developed that can optimally recommend a course sequence that both reduces the time to graduate while also increasing the overall GPA of the student. The algorithm dynamically learns how students with different contextual backgrounds perform for given course sequences and, then, recommends an optimal course sequence for new students. Using real-world student data from the UCLA Mechanical and Aerospace Engineering Department, we illustrate how the proposed algorithms outperform other methods that do not include student contextual information when making course sequence recommendations.",,,,"Department of Electrical and Computer Engineering, University of Miami, Coral Gables, FL, USA",Personalized education;contextual bandits;course sequence recommendation;dynamic programming,Author1,Author2,a,a,a,a,a,a,Intelligent Tutors,,,,a,a,IEEE Transactions on Signal Processing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7524023,,J. Xu; T. Xing; M. van der Schaar,,2,,10.1109/TSP.2016.2595495,IEEE Xplore,1,,20160824,5352,10.13039/100000001 - National Science Foundation; ,Adaptive systems;Aerospace engineering;Complexity theory;Education;Electronic mail;Heuristic algorithms;Signal processing algorithms,aerospace computing;computer aided instruction;educational courses;engineering education;inference mechanisms;mechanical engineering computing;recommender systems,GPA;UCLA Mechanical and Aerospace Engineering Department;contextual backgrounds;course availability;forward-search backward-induction algorithm;multiarmed bandit tool;personalized course sequence recommendations;prerequisite requirements;student learning;systematic methodology,,1053-587X;1053587X,20,"Oct.15, 15 2016",,,,,,20160727,,,,,,IEEE,,,,,5340,IEEE Journals & Magazines,,,64,,2016,2016,1,2,1,Author3
154,1,2,0,1,MPRL: Multiple-Periodic Reinforcement Learning for difficulty adjustment in rehabilitation games,"Generally, the difficulty level of a therapeutic game is regulated manually by a therapist. However, home-based rehabilitation games require a technique for automatic difficulty adjustment. This paper proposes a personalized difficulty adjustment technique for a rehabilitation game that automatically regulates difficulty settings based on a patient's skills in real-time. To this end, ideas from reinforcement learning are used to dynamically adjust the difficulty of a game. We show that difficulty adjustment is a multiple-objective problem, in which some objectives might be evaluated at different periods. To address this problem, we propose and use Multiple-Periodic Reinforcement Learning (MPRL) that makes it possible to evaluate different objectives of difficulty adjustment in separate periods. The results of experiments show that MPRL outperforms traditional Multiple-Objective Reinforcement Learning (MORL) in terms of user satisfaction parameters as well as improving the movement skills of patients.","Generally, the difficulty level of a therapeutic game is regulated manually by a therapist. However, home-based rehabilitation games require a technique for automatic difficulty adjustment. This paper proposes a personalized difficulty adjustment technique for a rehabilitation game that automatically regulates difficulty settings based on a patient's skills in real-time. To this end, ideas from reinforcement learning are used to dynamically adjust the difficulty of a game. We show that difficulty adjustment is a multiple-objective problem, in which some objectives might be evaluated at different periods. To address this problem, we propose and use Multiple-Periodic Reinforcement Learning (MPRL) that makes it possible to evaluate different objectives of difficulty adjustment in separate periods. The results of experiments show that MPRL outperforms traditional Multiple-Objective Reinforcement Learning (MORL) in terms of user satisfaction parameters as well as improving the movement skills of patients.",,,,"Faculty of Multimedia, Tabriz Islamic Art University, Iran",,Author2,Author3,a,a,a,a,a,a,Games,a,,,a,a,2017 IEEE 5th International Conference on Serious Games and Applications for Health (SeGAH),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7939260,,Y. A. Sekhavat,,,,10.1109/SeGAH.2017.7939260,IEEE Xplore,1,,20170608,7,,Biological cells;Silicon,learning (artificial intelligence);medical computing;patient rehabilitation;serious games (computing),home-based rehabilitation games;multiple-objective problem;multiple-periodic reinforcement learning;patient movement skills;therapeutic game,,,,2-4 April 2017,,,,,,,,,,,,IEEE,,,Electronic:978-1-5090-5482-4; POD:978-1-5090-5483-1,,1,IEEE Conferences,,,,,2017,2017,1,2,1,Author2
155,1,0,0,0,Multi-objective reinforcement learning algorithm and its application in drive system,"Generally, reinforcement learning (RL) is used to design neurocontroller for control system with single objective. When facing multi-objective system, it is necessary to design the neurocontroller according to the personal preference. This paper proposed a multi-objective reinforcement learning algorithm (MORLA) to design neurocontroller with the personal preference. It transformed the multi-objective into synthetical objective and applied parallel genetic algorithm (PGA) to evolve the neurocontroller according to the synthetical objective. To establish the synthetical objective, the objective weight which represents the personal preference is calculated by solving the constrained optimization problem (COP) at the end of each generation. The COP requires not only the biggest variance of the synthetical objective in the population, but also requires the weight to fit the designerpsilas preference. After acquiring the weights, the PGA can select the elitists from the population according to the designerpsilas preference and design a satisfying neurocontroller by evolutionary operations. At last, the MORLA is used to design neurocontroller for a speed-controlled induction motor drive with indirect vector control. This paper designed several neurocontrollers with different personal preferences for the drive system. The simulation results show the feasibility and validity of the MORLA.","Generally, reinforcement learning (RL) is used to design neurocontroller for control system with single objective. When facing multi-objective system, it is necessary to design the neurocontroller according to the personal preference. This paper proposed a multi-objective reinforcement learning algorithm (MORLA) to design neurocontroller with the personal preference. It transformed the multi-objective into synthetical objective and applied parallel genetic algorithm (PGA) to evolve the neurocontroller according to the synthetical objective. To establish the synthetical objective, the objective weight which represents the personal preference is calculated by solving the constrained optimization problem (COP) at the end of each generation. The COP requires not only the biggest variance of the synthetical objective in the population, but also requires the weight to fit the designerpsilas preference. After acquiring the weights, the PGA can select the elitists from the population according to the designerpsilas preference and design a satisfying neurocontroller by evolutionary operations. At last, the MORLA is used to design neurocontroller for a speed-controlled induction motor drive with indirect vector control. This paper designed several neurocontrollers with different personal preferences for the drive system. The simulation results show the feasibility and validity of the MORLA.",,,,"Department of Control Science and Engineering, Huazhong University of Science and Technology, China",,Author3,Author4,a,a,a,a,a,a,"Other (please in ""remark"")",a,domain: automated driving,,a,a,2008 34th Annual Conference of IEEE Industrial Electronics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4757965,,Zhang Huajun; Zhao Jin; Wang Rui; Ma Tan,,1,,10.1109/IECON.2008.4757965,IEEE Xplore,1,,20090123,279,,Algorithm design and analysis;Constraint optimization;Control systems;Convergence;Design engineering;Design optimization;Electronics packaging;Genetic algorithms;Learning;Neurocontrollers,control system synthesis;genetic algorithms;induction motor drives;learning (artificial intelligence);machine control;neurocontrollers;velocity control,MORLA;constrained optimization problem;control system;drive system;indirect vector control;multiobjective reinforcement learning algorithm;neurocontroller;parallel genetic algorithm;speed-controlled induction motor drive,,1553-572X;1553572X,,10-13 Nov. 2008,,,,,,,,,,,,IEEE,32,,CD-ROM:978-1-4244-1766-7; POD:978-1-4244-1767-4,,274,IEEE Conferences,,,,,2008,2008,3,1,0,Author1
156,1,1,1,1,Dynamic Game Difficulty Scaling Using Adaptive Behavior-Based AI,"Games are played by a wide variety of audiences. Different individuals will play with different gaming styles and employ different strategic approaches. This often involves interacting with nonplayer characters that are controlled by the game AI. From a developer's standpoint, it is important to design a game AI that is able to satisfy the variety of players that will interact with the game. Thus, an adaptive game AI that can scale the difficulty of the game according to the proficiency of the player has greater potential to customize a personalized and entertaining game experience compared to a static game AI. In particular, dynamic game difficulty scaling refers to the use of an adaptive game AI that performs game adaptations in real time during the game session. This paper presents two adaptive algorithms that use ideas from reinforcement learning and evolutionary computation to improve player satisfaction by scaling the difficulty of the game AI while the game is being played. The effects of varying the learning and mutation rates are examined and a general rule of thumb for the parameters is proposed. The proposed algorithms are demonstrated to be capable of matching its opponents in terms of mean scores and winning percentages. Both algorithms are able to generalize well to a variety of opponents.","Games are played by a wide variety of audiences. Different individuals will play with different gaming styles and employ different strategic approaches. This often involves interacting with nonplayer characters that are controlled by the game AI. From a developer's standpoint, it is important to design a game AI that is able to satisfy the variety of players that will interact with the game. Thus, an adaptive game AI that can scale the difficulty of the game according to the proficiency of the player has greater potential to customize a personalized and entertaining game experience compared to a static game AI. In particular, dynamic game difficulty scaling refers to the use of an adaptive game AI that performs game adaptations in real time during the game session. This paper presents two adaptive algorithms that use ideas from reinforcement learning and evolutionary computation to improve player satisfaction by scaling the difficulty of the game AI while the game is being played. The effects of varying the learning and mutation rates are examined and a general rule of thumb for the parameters is proposed. The proposed algorithms are demonstrated to be capable of matching its opponents in terms of mean scores and winning percentages. Both algorithms are able to generalize well to a variety of opponents.",,,,"Institute for Infocomm Research, Agency for Science Technology and Research (A*STAR), Singapore, Singapore",Artificial intelligence;behavior based;car racing simulation;game AI;player satisfaction;real-time adaptation,Author4,Author1,a,a,a,a,a,a,Games,,,,a,a,IEEE Transactions on Computational Intelligence and AI in Games,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783334,,C. H. Tan; K. C. Tan; A. Tay,,15,,10.1109/TCIAIG.2011.2158434,IEEE Xplore,1,,20111212,301,,Adaptation model;Artificial intelligence;Games;Humans;Pixel;Real time systems;Vehicles,computer games;evolutionary computation;learning (artificial intelligence),adaptive behavior-based artificial intelligence;dynamic game difficulty scaling;evolutionary computation;game adaptation;gaming styles;mutation rates;reinforcement learning,,1943-068X;1943068X,4,Dec. 2011,,,,,,20110602,,,,,1,IEEE,37,,,,289,IEEE Journals & Magazines,,,3,,2011,2011,0,4,0,Author2
157,1,1,2,1,Learning to give route directions from human demonstrations,"For several applications, robots and other computer systems must provide route descriptions to humans. These descriptions should be natural and intuitive for the human users. In this paper, we present an algorithm that learns how to provide good route descriptions from a corpus of human-written directions. Using inverse reinforcement learning, our algorithm learns how to select the information for the description depending on the context of the route segment. The algorithm then uses the learned policy to generate directions that imitate the style of the descriptions provided by humans, thus taking into account personal as well as cultural preferences and special requirements of the particular user group providing the learning demonstrations. We evaluate our approach in a user study and show that the directions generated by our policy sound similar to human-given directions and substantially more natural than directions provided by commercial web services.","For several applications, robots and other computer systems must provide route descriptions to humans. These descriptions should be natural and intuitive for the human users. In this paper, we present an algorithm that learns how to provide good route descriptions from a corpus of human-written directions. Using inverse reinforcement learning, our algorithm learns how to select the information for the description depending on the context of the route segment. The algorithm then uses the learned policy to generate directions that imitate the style of the descriptions provided by humans, thus taking into account personal as well as cultural preferences and special requirements of the particular user group providing the learning demonstrations. We evaluate our approach in a user study and show that the directions generated by our policy sound similar to human-given directions and substantially more natural than directions provided by commercial web services.",,,,"Department of Computer Science, University of Freiburg, 79110 Freiburg, Germany",,Author1,Author2,a,a,a,a,a,a,Personal Assistants,,,,a,a,2014 IEEE International Conference on Robotics and Automation (ICRA),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6907334,,S. Oßwald; H. Kretzschmar; W. Burgard; C. Stachniss,,3,,10.1109/ICRA.2014.6907334,IEEE Xplore,1,,20140929,3308,,Computers;Context;Cultural differences;Entropy;Learning (artificial intelligence);Measurement;Web services,control engineering computing;learning (artificial intelligence);mobile robots;path planning,computer systems;cultural preferences;human demonstrations;human-given directions;human-written directions;inverse reinforcement learning;learning demonstrations;personal preferences;robots;route descriptions;route directions;route segment,,1050-4729;10504729,,May 31 2014-June 7 2014,,,,,,,,,,,,IEEE,25,,Electronic:978-1-4799-3685-4; POD:978-1-4799-3686-1; USB:978-1-4799-3684-7,,3303,IEEE Conferences,,,,,2014,2014,0,3,1,Author3
158,1,1,1,1,Agent-based assistance in ambient assisted living through reinforcement learning and semantic technologies: (Short paper),"For impaired people, the conduction of certain daily life activities is problematic due to motoric and cognitive handicaps. For that reason, assistive agents in ambient assisted environments provide services that aim at supporting elderly and impaired people. However, these agents act in complex stochastic and indeterministic environments where the concrete effects of a performed action are usually unknown at design time. Furthermore, they have to perform varying tasks according to the user’s context and needs, wherefore an agent has to be flexible and able to recognize required capabilities in a certain situation in order to provide adequate, unobtrusive assistance. Hence, an expressive representation framework is required that relates user-specific impairments to required agent capabilities. This work presents an approach which (a) describes and links user impairments and capabilities using the formal, model-theoretic semantics expressed in OWL2 DL ontologies, (b) computes optimal policies through Reinforcement Learning and propagates these in an agent network. The presented approach improves the collaborative, personalized and adequate assistance of assistive agents and tailors the agent-based services to the user’s missing capabilities. © 2017, Springer International Publishing AG.","For impaired people, the conduction of certain daily life activities is problematic due to motoric and cognitive handicaps. For that reason, assistive agents in ambient assisted environments provide services that aim at supporting elderly and impaired people. However, these agents act in complex stochastic and indeterministic environments where the concrete effects of a performed action are usually unknown at design time. Furthermore, they have to perform varying tasks according to the user’s context and needs, wherefore an agent has to be flexible and able to recognize required capabilities in a certain situation in order to provide adequate, unobtrusive assistance. Hence, an expressive representation framework is required that relates user-specific impairments to required agent capabilities. This work presents an approach which (a) describes and links user impairments and capabilities using the formal, model-theoretic semantics expressed in OWL2 DL ontologies, (b) computes optimal policies through Reinforcement Learning and propagates these in an agent network. The presented approach improves the collaborative, personalized and adequate assistance of assistive agents and tailors the agent-based services to the user’s missing capabilities. © 2017, Springer International Publishing AG.",,"FZI Forschungszentrum Informatik am KIT, Information Process Engineering, Haid-und-Neu-Str. 10-14, Karlsruhe, Germany; Institute for Computer Science, University of Applied Sciences Darmstadt, Schöfferstrasse 8B, Darmstadt, Germany",,,,Author2,Author3,a,a,u,a,a,a,Personal Assistants,a,,I think we should accept and see in the eligibility check whether the RL is used specifically for personalization,u,a,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032656608&doi=10.1007%2f978-3-319-69459-7_12&partnerID=40&md5=34e55c071586274d84d0079823608e8d,"Merkle N., Zander S.","Merkle, N., FZI Forschungszentrum Informatik am KIT, Information Process Engineering, Haid-und-Neu-Str. 10-14, Karlsruhe, Germany; Zander, S., Institute for Computer Science, University of Applied Sciences Darmstadt, Schöfferstrasse 8B, Darmstadt, Germany",,,10.1007/978-3-319-69459-7_12,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85032656608,,,,,188,180,,,,,Scopus,,,,Conference Paper,,,10574 LNCS,,,2017,0,4,0,Author4
159,1,1,1,1,Personalizing a Service Robot by Learning Human Habits from Behavioral Footprints,"For a domestic personal robot, personalized services are as important as predesigned tasks, because the robot needs to adjust the home state based on the operator's habits. An operator's habits are composed of cues, behaviors, and rewards. This article introduces behavioral footprints to describe the operator's behaviors in a house, and applies the inverse reinforcement learning technique to extract the operator's habits, represented by a reward function. We implemented the proposed approach with a mobile robot on indoor temperature adjustment, and compared this approach with a baseline method that recorded all the cues and behaviors of the operator. The result shows that the proposed approach allows the robot to reveal the operator's habits accurately and adjust the environment state accordingly. © 2015 THE AUTHORS. Published by Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company","For a domestic personal robot, personalized services are as important as predesigned tasks, because the robot needs to adjust the home state based on the operator's habits. An operator's habits are composed of cues, behaviors, and rewards. This article introduces behavioral footprints to describe the operator's behaviors in a house, and applies the inverse reinforcement learning technique to extract the operator's habits, represented by a reward function. We implemented the proposed approach with a mobile robot on indoor temperature adjustment, and compared this approach with a baseline method that recorded all the cues and behaviors of the operator. The result shows that the proposed approach allows the robot to reveal the operator's habits accurately and adjust the environment state accordingly. © 2015 THE AUTHORS. Published by Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company",Open Access,"California Institute of Technology, Pasadena, CA, United States; The Chinese University of Hong Kong, Hong Kong, China",,,behavioral footprints; habit learning; personalized robot,Author4,Author1,a,a,a,a,a,a,Personal Assistants,Robotics (robot assistant).,,,a,a,Engineering,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988727903&doi=10.15302%2fJ-ENG-2015024&partnerID=40&md5=bd73d8218407187c4a2a349249f2a84c,"Li K., Meng M.Q.-H.","Li, K., California Institute of Technology, Pasadena, CA, United States; Meng, M.Q.-H., The Chinese University of Hong Kong, Hong Kong, China",,,10.15302/J-ENG-2015024,SCOPUS,1,,,,,,,,,,1,,,,2-s2.0-84988727903,,,,,84,79,,,,,Scopus,,,,Article,,,1,,2015,2015,0,4,0,Author1
160,0,1,0,0,Inducing effective pedagogical strategies using learning context features,"Effective pedagogical strategies are important for e-learning environments. While it is assumed that an effective learning environment should craft and adapt its actions to the user's needs, it is often not clear how to do so. In this paper, we used a Natural Language Tutoring System named Cordillera and applied Reinforcement Learning (RL) to induce pedagogical strategies directly from pre-existing human user interaction corpora. 50 features were explored to model the learning context. Of these features, domain-oriented and system performance features were the most influential while user performance and background features were rarely selected. The induced pedagogical strategies were then evaluated on real users and results were compared with pre-existing human user interaction corpora. Overall, our results show that RL is a feasible approach to induce effective, adaptive pedagogical strategies by using a relatively small training corpus. Moreover, we believe that our approach can be used to develop other adaptive and personalized learning environments. © 2010 Springer-Verlag.","Effective pedagogical strategies are important for e-learning environments. While it is assumed that an effective learning environment should craft and adapt its actions to the user's needs, it is often not clear how to do so. In this paper, we used a Natural Language Tutoring System named Cordillera and applied Reinforcement Learning (RL) to induce pedagogical strategies directly from pre-existing human user interaction corpora. 50 features were explored to model the learning context. Of these features, domain-oriented and system performance features were the most influential while user performance and background features were rarely selected. The induced pedagogical strategies were then evaluated on real users and results were compared with pre-existing human user interaction corpora. Overall, our results show that RL is a feasible approach to induce effective, adaptive pedagogical strategies by using a relatively small training corpus. Moreover, we believe that our approach can be used to develop other adaptive and personalized learning environments. © 2010 Springer-Verlag.",,"Machine Learning Department, Carnegie Mellon University, PA 15213, United States; School of Computing and Informatics, Arizona State University, AZ 85287, United States; Department of Computer Science, University of Pittsburgh, PA 15260, United States; Department of Biomedical Informatics, University of Pittsburgh, Pittsburgh, PA 15260, United States",,,,Author2,Author3,a,a,a,a,a,a,Interfaces / HCI,d,,domain should be intelligent tutors I believe,a,a,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954602915&doi=10.1007%2f978-3-642-13470-8_15&partnerID=40&md5=7383a8c4df03d5465e9cffbeffaa8641,"Chi M., Vanlehn K., Litman D., Jordan P.","Chi, M., Machine Learning Department, Carnegie Mellon University, PA 15213, United States; Vanlehn, K., School of Computing and Informatics, Arizona State University, AZ 85287, United States; Litman, D., Department of Computer Science, University of Pittsburgh, PA 15260, United States; Jordan, P., Department of Biomedical Informatics, University of Pittsburgh, Pittsburgh, PA 15260, United States",10,,10.1007/978-3-642-13470-8_15,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-77954602915,,,,,158,147,,,,,Scopus,,,,Conference Paper,,,6075 LNCS,,2010,2010,3,1,0,Author2
161,0,0,1,0,Learning social relations for culture aware interaction,"Each person has their private physical and/or psychological area where they do not want to share with others during social interactions. This area gives them comfort about interactions and its size usually depends on various factors such as culture, personal traits, and acquaintanceship. This issue may also arise in case of human-robot interaction, especially when the robot is required to generate a socially competent interaction strategy toward people they are interacting with. Here, we propose a new robot exploration strategy to socially interact with people by considering the social relationship between the robot and each person. To that end, two definitions of interaction area are made: (1) Acceptable area allowed to be shared with other people and robots, and (2) Private area where a human does not want to be interfered by others. Based on these definitions, the robot can optimize the path to maximize the frequency/degree of visiting the acceptable area of each person and to minimize the frequency/degree of trespassing into the private area of them at the same time in an iterative way. In this paper, the social force model (SFM) of each person, based on the potential field concept, is designed by a fuzzy inference system and its parameter is optimized by the reinforcement learning model during interactions. We have shown that the proposed model can generate a suitable SFM of each person, which was quite similar to a ground truth model, allowing to plan a path to simultaneously optimize the two factors of interaction area, respectively.","Each person has their private physical and/or psychological area where they do not want to share with others during social interactions. This area gives them comfort about interactions and its size usually depends on various factors such as culture, personal traits, and acquaintanceship. This issue may also arise in case of human-robot interaction, especially when the robot is required to generate a socially competent interaction strategy toward people they are interacting with. Here, we propose a new robot exploration strategy to socially interact with people by considering the social relationship between the robot and each person. To that end, two definitions of interaction area are made: (1) Acceptable area allowed to be shared with other people and robots, and (2) Private area where a human does not want to be interfered by others. Based on these definitions, the robot can optimize the path to maximize the frequency/degree of visiting the acceptable area of each person and to minimize the frequency/degree of trespassing into the private area of them at the same time in an iterative way. In this paper, the social force model (SFM) of each person, based on the potential field concept, is designed by a fuzzy inference system and its parameter is optimized by the reinforcement learning model during interactions. We have shown that the proposed model can generate a suitable SFM of each person, which was quite similar to a ground truth model, allowing to plan a path to simultaneously optimize the two factors of interaction area, respectively.",,,,"School of Information Science, Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Nomi, Ishikawa 923-1292, Japan",,Author3,Author4,a,a,a,a,a,a,Interfaces / HCI,d,,Not robotics?,a,a,2017 14th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7992879,,P. Patompak; S. Jeong; I. Nilkhamhang; N. Y. Chong,,,,10.1109/URAI.2017.7992879,IEEE Xplore,1,,20170727,31,,Adaptation models;Force;Human-robot interaction;Iron;Mathematical model;Navigation;Robots,fuzzy reasoning;human-robot interaction;learning (artificial intelligence);social aspects of automation,SFM;culture aware interaction;fuzzy inference system;human-robot interaction;potential field concept;reinforcement learning model;robot exploration strategy;social force model;social interactions;social relations;socially competent interaction strategy,,,,June 28 2017-July 1 2017,,,,,,,,,,,,IEEE,,,Electronic:978-1-5090-3056-9; POD:978-1-5090-3057-6,,26,IEEE Conferences,,,,,2017,2017,3,1,0,Author3
162,2,2,0,2,New Statistical Learning Methods for Estimating Optimal Dynamic Treatment Regimes,"Dynamic treatment regimes (DTRs) are sequential decision rules for individual patients that can adapt over time to an evolving illness. The goal is to accommodate heterogeneity among patients and find the DTR which will produce the best long-term outcome if implemented. We introduce two new statistical learning methods for estimating the optimal DTR, termed backward outcome weighted learning (BOWL), and simultaneous outcome weighted learning (SOWL). These approaches convert individualized treatment selection into an either sequential or simultaneous classification problem, and can thus be applied by modifying existing machine learning techniques. The proposed methods are based on directly maximizing over all DTRs a nonparametric estimator of the expected long-term outcome; this is fundamentally different than regression-based methods, for example, Q-learning, which indirectly attempt such maximization and rely heavily on the correctness of postulated regression models.  We prove that the resulting rules are consistent, and provide finite sample bounds for the errors using the estimated rules. Simulation results suggest the proposed methods produce superior DTRs compared with Q-learning especially in small samples. We illustrate the methods using data from a clinical trial for smoking cessation. Supplementary materials for this article are available online. © 2015 American Statistical Association.","Dynamic treatment regimes (DTRs) are sequential decision rules for individual patients that can adapt over time to an evolving illness. The goal is to accommodate heterogeneity among patients and find the DTR which will produce the best long-term outcome if implemented. We introduce two new statistical learning methods for estimating the optimal DTR, termed backward outcome weighted learning (BOWL), and simultaneous outcome weighted learning (SOWL). These approaches convert individualized treatment selection into an either sequential or simultaneous classification problem, and can thus be applied by modifying existing machine learning techniques. The proposed methods are based on directly maximizing over all DTRs a nonparametric estimator of the expected long-term outcome; this is fundamentally different than regression-based methods, for example, Q-learning, which indirectly attempt such maximization and rely heavily on the correctness of postulated regression models.  We prove that the resulting rules are consistent, and provide finite sample bounds for the errors using the estimated rules. Simulation results suggest the proposed methods produce superior DTRs compared with Q-learning especially in small samples. We illustrate the methods using data from a clinical trial for smoking cessation. Supplementary materials for this article are available online. © 2015 American Statistical Association.",,"Department of Biostatistics and Medical Informatics, University of Wisconsin-MadisonWI, United States; Department of Biostatistics, University of North Carolina at Chapel HillNC, United States; Department of Statistics, North Carolina State UniversityNC, United States; Department of Statistics and Operations Research, University of North Carolina at Chapel Hill, Chapel Hill, NC, United States",,,Classification; Personalized medicine; Q-learning; Reinforcement learning; Risk bound; Support vector machine,Author4,Author1,a,a,a,a,a,a,Healthcare,,,,a,a,Journal of the American Statistical Association,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936797778&doi=10.1080%2f01621459.2014.937488&partnerID=40&md5=4be5602bc2c4dc8e467b01449fb353a8,"Zhao Y.-Q., Zeng D., Laber E.B., Kosorok M.R.","Zhao, Y.-Q., Department of Biostatistics and Medical Informatics, University of Wisconsin-MadisonWI, United States; Zeng, D., Department of Biostatistics, University of North Carolina at Chapel HillNC, United States; Laber, E.B., Department of Statistics, North Carolina State UniversityNC, United States; Kosorok, M.R., Department of Statistics and Operations Research, University of North Carolina at Chapel Hill, Chapel Hill, NC, United States",27,,10.1080/01621459.2014.937488,SCOPUS,1,,,,,,,,,,510,,,,2-s2.0-84936797778,,,,,598,583,,,,,Scopus,,,,Article,,,110,,2015,2015,1,0,3,Author2
163,0,0,1,0,An Effective Approach to Continuous User Authentication for Touch Screen Smart Devices,"Due to the rapid increase in the use of personal smart devices, more sensitive data is stored and viewed on these smart devices. This trend makes it easier for attackers to access confidential data by physically compromising (including stealing) these smart devices. Currently, most personal smart devices employ one of the one-time user authentication schemes, such as four-to-six digits, fingerprint or pattern-based schemes. These authentication schemes are often not good enough for securing personal smart devices because the attackers can easily extract all the confidential data from the smart device by breaking such schemes, or by keeping the authenticated session open on a physically compromised smart device. In addition, existing re-authentication or continuous authentication techniques for protecting personal smart devices use centralized architecture and require servers at a centralized location to train and update the learning model used for continuous authentication, which impose additional communication overhead. In this paper, an approach is presented to generating and updating the authentication model on the user's smart device with user's gestures, instead of a centralized server. There are two major advantages in this approach. One is that this approach continuously learns and authenticates finger gestures of the user in the background without requiring the user to provide specific gesture inputs. The other major advantage is to have better authentication accuracy by treating uninterrupted user finger gestures over a short time interval as a single gesture for continuous user authentication.","Due to the rapid increase in the use of personal smart devices, more sensitive data is stored and viewed on these smart devices. This trend makes it easier for attackers to access confidential data by physically compromising (including stealing) these smart devices. Currently, most personal smart devices employ one of the one-time user authentication schemes, such as four-to-six digits, fingerprint or pattern-based schemes. These authentication schemes are often not good enough for securing personal smart devices because the attackers can easily extract all the confidential data from the smart device by breaking such schemes, or by keeping the authenticated session open on a physically compromised smart device. In addition, existing re-authentication or continuous authentication techniques for protecting personal smart devices use centralized architecture and require servers at a centralized location to train and update the learning model used for continuous authentication, which impose additional communication overhead. In this paper, an approach is presented to generating and updating the authentication model on the user's smart device with user's gestures, instead of a centralized server. There are two major advantages in this approach. One is that this approach continuously learns and authenticates finger gestures of the user in the background without requiring the user to provide specific gesture inputs. The other major advantage is to have better authentication accuracy by treating uninterrupted user finger gestures over a short time interval as a single gesture for continuous user authentication.",,,,"Inf. Assurance Center, Sch. of Comput., Inf., & Decision Syst. Eng., Arizona State Univ., Tempe, AZ, USA",Touch-screen smart devices;adaptive continuous user authentication;and user re-authentication;reinforcement learning,Author1,Author2,u,a,a,a,a,a,"Other (please in ""remark"")",,,,u,a,"2015 IEEE International Conference on Software Quality, Reliability and Security",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272936,,A. B. Buduru; S. S. Yau,,2,,10.1109/QRS.2015.40,IEEE Xplore,1,,20150924,226,,Accuracy;Authentication;Context;Fingers;Object recognition;Performance evaluation,authorisation;computer crime;gesture recognition;message authentication;smart phones;touch sensitive screens,attackers;authentication accuracy;authentication model;centralized server;confidential data access;continuous user authentication;finger gestures authentication;personal smart devices;sensitive data;touch screen smart devices;uninterrupted user finger gestures,,,,3-5 Aug. 2015,,,,,,,,,,,,IEEE,19,,Electronic:978-1-4673-7989-2; POD:978-1-4673-7990-8,,219,IEEE Conferences,,,,,,2015,3,1,0,Author3
164,0,0,2,0,Personalized web-document filtering using reinforcement learning,"Document filtering is increasingly deployed in Web environments to reduce information overload of users. We formulate online information filtering as a reinforcement learning problem, i.e., TD(0). The goal is to learn user profiles that best represent information needs and thus maximize the expected value of user relevance feedback. A method is then presented that acquires reinforcement signals automatically by estimating user's implicit feedback from direct observations of browsing behaviors. This ""learning by observation"" approach is contrasted with conventional relevance feedback methods which require explicit user feedbacks. Field tests have been performed that involved 10 users reading a total of 18,750 HTML documents during 45 days. Compared to the existing document filtering techniques, the proposed learning method showed superior performance in information quality and adaptation speed to user preferences in online filtering.","Document filtering is increasingly deployed in Web environments to reduce information overload of users. We formulate online information filtering as a reinforcement learning problem, i.e., TD(0). The goal is to learn user profiles that best represent information needs and thus maximize the expected value of user relevance feedback. A method is then presented that acquires reinforcement signals automatically by estimating user's implicit feedback from direct observations of browsing behaviors. This ""learning by observation"" approach is contrasted with conventional relevance feedback methods which require explicit user feedbacks. Field tests have been performed that involved 10 users reading a total of 18,750 HTML documents during 45 days. Compared to the existing document filtering techniques, the proposed learning method showed superior performance in information quality and adaptation speed to user preferences in online filtering.",,"Biointelligence Lab., Sch. of Comp. Sci. and Engineering, Seoul National University, Seoul, South Korea; Biointelligence Lab., Sch. of Comp. Sci. and Engineering, Seoul National University, Seoul 151-742, South Korea",,,,Author3,Author4,a,a,a,a,a,a,"Other (please in ""remark"")",a,domain: information retrieval,,a,a,Applied Artificial Intelligence,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0242322792&doi=10.1080%2f088395101750363993&partnerID=40&md5=068c1f95f84f5da8db38e9ff190d8805,"Zhang B.-T., Seo Y.-W.","Zhang, B.-T., Biointelligence Lab., Sch. of Comp. Sci. and Engineering, Seoul National University, Seoul, South Korea, Biointelligence Lab., Sch. of Comp. Sci. and Engineering, Seoul National University, Seoul 151-742, South Korea; Seo, Y.-W., Biointelligence Lab., Sch. of Comp. Sci. and Engineering, Seoul National University, Seoul, South Korea",41,,10.1080/088395101750363993,SCOPUS,1,,,,,,,,,,7,,,,2-s2.0-0242322792,,,,,685,665,,,,,Scopus,,,,Article,,,15,,2001,2001,3,0,1,Author3
165,1,0,2,1,A hybrid web recommender system based on Q-learning,"Different efforts have been made to address the problem of information overload on the Internet. Recommender systems aim at directing users through this information space, toward the resources that best meet their needs and interests. Web Content Recommendation has been an active application area for Information Filtering, Web Mining and Machine Learning research. Recent studies show that combining the conceptual and usage information can improve the quality of web recommendations. In this paper we exploit this idea to enhance a reinforcement learning framework, primarily devised for web recommendations based on web usage data. A hybrid web recommendation method is proposed by making use of the conceptual relationships among web resources to derive a novel model of the problem, enriched with semantic knowledge about the usage behavior. With our hybrid model for the web page recommendation problem we show the apt and flexibility of the reinforcement learning framework in the web recommendation domain, and demonstrate how it can be extended in order to incorporate various sources of information. We evaluate our method under different settings and show how this method can improve the overall quality of web recommendations. Copyright 2008 ACM.","Different efforts have been made to address the problem of information overload on the Internet. Recommender systems aim at directing users through this information space, toward the resources that best meet their needs and interests. Web Content Recommendation has been an active application area for Information Filtering, Web Mining and Machine Learning research. Recent studies show that combining the conceptual and usage information can improve the quality of web recommendations. In this paper we exploit this idea to enhance a reinforcement learning framework, primarily devised for web recommendations based on web usage data. A hybrid web recommendation method is proposed by making use of the conceptual relationships among web resources to derive a novel model of the problem, enriched with semantic knowledge about the usage behavior. With our hybrid model for the web page recommendation problem we show the apt and flexibility of the reinforcement learning framework in the web recommendation domain, and demonstrate how it can be extended in order to incorporate various sources of information. We evaluate our method under different settings and show how this method can improve the overall quality of web recommendations. Copyright 2008 ACM.",,"Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran",,,Machine learning; Personalization; Recommender systems; Reinforcement learning; Web mining,Author2,Author3,a,a,a,a,a,a,Recommender Systems,a,,,a,a,Proceedings of the ACM Symposium on Applied Computing,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56749160807&doi=10.1145%2f1363686.1363954&partnerID=40&md5=b3d254c9ed6f9435078120241cad1399,"Taghipour N., Kardan A.","Taghipour, N., Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran; Kardan, A., Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran",11,,10.1145/1363686.1363954,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-56749160807,,,,,1168,1164,,,,,Scopus,,,,Conference Paper,,,,,2008,2008,1,2,1,Author3
166,2,2,0,2,Personalizing mobile fitness apps using reinforcement learning,"Despite the vast number of mobile fitness applications (apps) and their potential advantages in promoting physical activity, many existing apps lack behavior-change features and are not able to maintain behavior change motivation. This paper describes a novel fitness app called CalFit, which implements important behavior-change features like dynamic goal setting and self-monitoring. CalFit uses a reinforcement learning algorithm to generate personalized daily step goals that are challenging but attainable. We conducted the Mobile Student Activity Reinforcement (mSTAR) study with 13 college students to evaluate the efficacy of the CalFit app. The control group (receiving goals of 10,000 steps/day) had a decrease in daily step count of 1,520 (SD ± 740) between baseline and 10-weeks, compared to an increase of 700 (SD ± 830) in the intervention group (receiving personalized step goals). The difference in daily steps between the two groups was 2,220, with a statistically significant p = 0:039. © 2018 Copyright for the individual papers remains with the authors.","Despite the vast number of mobile fitness applications (apps) and their potential advantages in promoting physical activity, many existing apps lack behavior-change features and are not able to maintain behavior change motivation. This paper describes a novel fitness app called CalFit, which implements important behavior-change features like dynamic goal setting and self-monitoring. CalFit uses a reinforcement learning algorithm to generate personalized daily step goals that are challenging but attainable. We conducted the Mobile Student Activity Reinforcement (mSTAR) study with 13 college students to evaluate the efficacy of the CalFit app. The control group (receiving goals of 10,000 steps/day) had a decrease in daily step count of 1,520 (SD ± 740) between baseline and 10-weeks, compared to an increase of 700 (SD ± 830) in the intervention group (receiving personalized step goals). The difference in daily steps between the two groups was 2,220, with a statistically significant p = 0:039. © 2018 Copyright for the individual papers remains with the authors.",,"Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Department of Physiological, Nursing Institute for Health and Aging, School of Nursing, University of California, San Francisco, CA, United States; Department of Physiological Nursing, School of Nursing, University of California, San Francisco, CA, United States",,,Fitness app; Goal setting; Interface design; Mobile app; Personalization; Physical activity,Author3,Author4,a,a,a,a,a,a,Healthcare,a,"This is not so much about 'healthcare' rather than 'health in general
",We can broaden the categories to health and wellbeing?,a,a,CEUR Workshop Proceedings,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044520942&partnerID=40&md5=fa48eaf14863091847e69eecc5d02618,"Zhou M., Mintz Y., Fukuoka Y., Goldberg K., Flowers E., Kaminsky P., Castillejo A., Aswani A.","Zhou, M., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Mintz, Y., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Fukuoka, Y., Department of Physiological, Nursing Institute for Health and Aging, School of Nursing, University of California, San Francisco, CA, United States; Goldberg, K., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Flowers, E., Department of Physiological Nursing, School of Nursing, University of California, San Francisco, CA, United States; Kaminsky, P., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Castillejo, A., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Aswani, A., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States",,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-85044520942,,,,,,,,,,,Scopus,,,,Conference Paper,,,2068,,2018,2018,1,0,3,Author2
167,1,0,2,1,Interactive narrative personalization with deep reinforcement learning,"Data-driven techniques for interactive narrative generation are the subject of growing interest. Reinforcement learning (RL) offers significant potential for devising data-driven interactive narrative generators that tailor players' story experiences by inducing policies from player interaction logs. A key open question in RL-based interactive narrative generation is how to model complex player interaction patterns to learn effective policies. In this paper we present a deep RL-based interactive narrative generation framework that leverages synthetic data produced by a bipartite simulated player model. Specifically, the framework involves training a set of Q-networks to control adaptable narrative event sequences with long short-term memory network-based simulated players. We investigate the deep RL framework's performance with an educational interactive narrative, Crystal Island. Results suggest that the deep RL-based narrative generation framework yields effective personalized interactive narratives.","Data-driven techniques for interactive narrative generation are the subject of growing interest. Reinforcement learning (RL) offers significant potential for devising data-driven interactive narrative generators that tailor players' story experiences by inducing policies from player interaction logs. A key open question in RL-based interactive narrative generation is how to model complex player interaction patterns to learn effective policies. In this paper we present a deep RL-based interactive narrative generation framework that leverages synthetic data produced by a bipartite simulated player model. Specifically, the framework involves training a set of Q-networks to control adaptable narrative event sequences with long short-term memory network-based simulated players. We investigate the deep RL framework's performance with an educational interactive narrative, Crystal Island. Results suggest that the deep RL-based narrative generation framework yields effective personalized interactive narratives.",,"Department of Computer Science, North Carolina State University, Raleigh, NC, United States",,,,Author4,Author1,a,a,a,a,a,a,Games,,,,a,a,IJCAI International Joint Conference on Artificial Intelligence,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031928990&partnerID=40&md5=147d78aa4f5d60f8ded6123ba8b73b09,"Wang P., Rowe J., Min W., Mott B., Lester J.","Wang, P., Department of Computer Science, North Carolina State University, Raleigh, NC, United States; Rowe, J., Department of Computer Science, North Carolina State University, Raleigh, NC, United States; Min, W., Department of Computer Science, North Carolina State University, Raleigh, NC, United States; Mott, B., Department of Computer Science, North Carolina State University, Raleigh, NC, United States; Lester, J., Department of Computer Science, North Carolina State University, Raleigh, NC, United States",,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-85031928990,,,,,3858,3852,,,,,Scopus,,,,Conference Paper,,,,,2017,2017,1,2,1,Author3
168,1,0,2,1,Exploration in interactive personalized music recommendation: A reinforcement learning approach,"Current music recommender systems typically act in a greedymanner by recommending songs with the highest user ratings. Greedy recommendation, however, is suboptimal over the long term: it does not actively gather information on user preferences and fails to recommend novel songs that are potentially interesting. A successful recommender system must balance the needs to explore user preferences and to exploit this information for recommendation. This article presents a new approach to music recommendation by formulating this exploration-exploitation trade-off as a reinforcement learning task. To learn user preferences, it uses Bayesian model that accounts for both audio content and the novelty of recommendations. A piecewise-linear approximation to the model and a variational inference algorithm help to speed up Bayesian inference. One additional benefit of our approach is a single unified model for both music recommendation and playlist generation. We demonstrate the strong potential of the proposed approach with simulation results and a user study. © 2014 ACM.","Current music recommender systems typically act in a greedymanner by recommending songs with the highest user ratings. Greedy recommendation, however, is suboptimal over the long term: it does not actively gather information on user preferences and fails to recommend novel songs that are potentially interesting. A successful recommender system must balance the needs to explore user preferences and to exploit this information for recommendation. This article presents a new approach to music recommendation by formulating this exploration-exploitation trade-off as a reinforcement learning task. To learn user preferences, it uses Bayesian model that accounts for both audio content and the novelty of recommendations. A piecewise-linear approximation to the model and a variational inference algorithm help to speed up Bayesian inference. One additional benefit of our approach is a single unified model for both music recommendation and playlist generation. We demonstrate the strong potential of the proposed approach with simulation results and a user study. © 2014 ACM.",,"Department of Computer Science, National University of Singapore, Singapore 117417, Singapore; Computing Science Department, Institute of High Performance Computing, A STAR, Singapore 138632, Singapore",7,,Application; Machine learning; Model; Music; Recommender systems,Author2,Author3,a,a,a,a,a,a,Recommender Systems,a,,,a,a,"ACM Transactions on Multimedia Computing, Communications and Applications",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906856108&doi=10.1145%2f2623372&partnerID=40&md5=da52cbcf39aaff70ff8fd51e7294ca75,"Wang Y., Wang X., Wang Y., Hsu D.","Wang, Y., Department of Computer Science, National University of Singapore, Singapore 117417, Singapore; Wang, X., Department of Computer Science, National University of Singapore, Singapore 117417, Singapore; Wang, Y., Computing Science Department, Institute of High Performance Computing, A STAR, Singapore 138632, Singapore; Hsu, D., Department of Computer Science, National University of Singapore, Singapore 117417, Singapore",15,,10.1145/2623372,SCOPUS,1,,,,,,,,,,1,,,,2-s2.0-84906856108,,,,,,,,,,,Scopus,,,,Article,,,11,,2014,2014,1,2,1,Author3
169,1,0,2,1,Dynamic personalization in conversational recommender systems,"Conversational recommender systems are E-Commerce applications which interactively assist online users to acquire their interaction goals during their sessions. In our previous work, we have proposed and validated a methodology for conversational systems which autonomously learns the particular web page to display to the user, at each step of the session. We employed reinforcement learning to learn an optimal strategy, i.e., one that is personalized for a real user population. In this paper, we extend our methodology by allowing it to autonomously learn and update the optimal strategy dynamically (at run-time), and individually for each user. This learning occurs perpetually after every session, as long as the user continues her interaction with the system. We evaluate our approach in an off-line simulation with four simulated users, as well as in an online evaluation with thirteen real users. The results show that an optimal strategy is learnt and updated for each real and simulated user. For each simulated user, the optimal behavior is reasonably adapted to this user’s characteristics, but converges after several hundred sessions. For each real user, the optimal behavior converges only in several sessions. It provides assistance only in certain situations, allowing many users to buy several products together in shorter time and with more page-views and lesser number of query executions. We prove that our approach is novel and show how its current limitations can catered. © Springer-Verlag Berlin Heidelberg 2013.","Conversational recommender systems are E-Commerce applications which interactively assist online users to acquire their interaction goals during their sessions. In our previous work, we have proposed and validated a methodology for conversational systems which autonomously learns the particular web page to display to the user, at each step of the session. We employed reinforcement learning to learn an optimal strategy, i.e., one that is personalized for a real user population. In this paper, we extend our methodology by allowing it to autonomously learn and update the optimal strategy dynamically (at run-time), and individually for each user. This learning occurs perpetually after every session, as long as the user continues her interaction with the system. We evaluate our approach in an off-line simulation with four simulated users, as well as in an online evaluation with thirteen real users. The results show that an optimal strategy is learnt and updated for each real and simulated user. For each simulated user, the optimal behavior is reasonably adapted to this user’s characteristics, but converges after several hundred sessions. For each real user, the optimal behavior converges only in several sessions. It provides assistance only in certain situations, allowing many users to buy several products together in shorter time and with more page-views and lesser number of query executions. We prove that our approach is novel and show how its current limitations can catered. © Springer-Verlag Berlin Heidelberg 2013.",,"Department of Computer Science, National University of Computer and Emerging Sciences (NUCES), Shah Lateef Town, National Highway, Karachi, Pakistan; Sukkur Institute of Business Administration, Airport Road, Sukkur, Sindh, Pakistan; ECTRL Solutions SRL, Via Solteri 38, Trento, Italy",,,Conversational recommender systems; Dynamic personalization; Individual user; Off-line simulation; On-line experiment; Optimal strategy; Real users; Reinforcement learning,Author4,Author1,a,a,a,a,a,a,Recommender Systems,,,,a,a,Information Systems and e-Business Management,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953638450&doi=10.1007%2fs10257-013-0222-3&partnerID=40&md5=4692e5b4f1f7c8a9be659d012a702aee,"Mahmood T., Mujtaba G., Venturini A.","Mahmood, T., Department of Computer Science, National University of Computer and Emerging Sciences (NUCES), Shah Lateef Town, National Highway, Karachi, Pakistan; Mujtaba, G., Sukkur Institute of Business Administration, Airport Road, Sukkur, Sindh, Pakistan; Venturini, A., ECTRL Solutions SRL, Via Solteri 38, Trento, Italy",9,,10.1007/s10257-013-0222-3,SCOPUS,1,,,,,,,,,,2,,,,2-s2.0-84953638450,,,,,238,213,,,,,Scopus,,,,Article,,,12,,2014,2014,1,2,1,Author3
170,2,0,2,2,Online context-aware recommendation with time varying multi-armed bandit,"Contextual multi-armed bandit problems have gained increasing popularity and attention in recent years due to their capability of leveraging contextual information to deliver online personalized recommendation services (e.g., online advertising and news article selection). To predict the reward of each arm given a particular context, existing relevant research studies for contextual multi-armed bandit problems often assume the existence of a fixed yet unknown reward mapping function. However, this assumption rarely holds in practice, since real-world problems often involve underlying processes that are dynamically evolving over time. In this paper, we study the time varying contextual multi-armed problem where the reward mapping function changes over time. In particular, we propose a dynamical context drift model based on particle learning. In the proposed model, the drift on the reward mapping function is explicitly modeled as a set of random walk particles, where good fitted particles are selected to learn the mapping dynamically. Taking advantage of the fully adaptive inference strategy of particle learning, our model is able to effectively capture the context change and learn the latent parameters. In addition, those learnt parameters can be naturally integrated into existing multi-arm selection strategies such as LinUCB and Thompson sampling. Empirical studies on two real-world applications, including online personalized advertising and news recommendation, demonstrate the effectiveness of our proposed approach. The experimental results also show that our algorithm can dynamically track the changing reward over time and consequently improve the click-through rate. © 2016 ACM.","Contextual multi-armed bandit problems have gained increasing popularity and attention in recent years due to their capability of leveraging contextual information to deliver online personalized recommendation services (e.g., online advertising and news article selection). To predict the reward of each arm given a particular context, existing relevant research studies for contextual multi-armed bandit problems often assume the existence of a fixed yet unknown reward mapping function. However, this assumption rarely holds in practice, since real-world problems often involve underlying processes that are dynamically evolving over time. In this paper, we study the time varying contextual multi-armed problem where the reward mapping function changes over time. In particular, we propose a dynamical context drift model based on particle learning. In the proposed model, the drift on the reward mapping function is explicitly modeled as a set of random walk particles, where good fitted particles are selected to learn the mapping dynamically. Taking advantage of the fully adaptive inference strategy of particle learning, our model is able to effectively capture the context change and learn the latent parameters. In addition, those learnt parameters can be naturally integrated into existing multi-arm selection strategies such as LinUCB and Thompson sampling. Empirical studies on two real-world applications, including online personalized advertising and news recommendation, demonstrate the effectiveness of our proposed approach. The experimental results also show that our algorithm can dynamically track the changing reward over time and consequently improve the click-through rate. © 2016 ACM.",,"School of Computing and Information Science, Florida International University, Miami, United States",,,Particle learning; Personalization; Probability matching; Recommender system; Time varying contextual bandit,Author2,Author3,a,a,a,a,a,a,Recommender Systems,a,,,a,a,Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985020774&doi=10.1145%2f2939672.2939878&partnerID=40&md5=26e3553e4a237cf5525077e959b4d46e,"Zeng C., Wang Q., Mokhtari S., Li T.","Zeng, C., School of Computing and Information Science, Florida International University, Miami, United States; Wang, Q., School of Computing and Information Science, Florida International University, Miami, United States; Mokhtari, S., School of Computing and Information Science, Florida International University, Miami, United States; Li, T., School of Computing and Information Science, Florida International University, Miami, United States",8,,10.1145/2939672.2939878,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84985020774,,,,,2034,2025,,,,,Scopus,,,,Conference Paper,,,13-17-August-2016,,2016,2016,1,0,3,Author4
171,2,0,1,1,Multi-task learning for contextual bandits,"Contextual bandits are a form of multi-armed bandit in which the agent has access to predictive side information (known as the context) for each arm at each time step, and have been used to model personalized news recommendation, ad placement, and other applications. In this work, we propose a multi-task learning framework for contextual bandit problems. Like multi-task learning in the batch setting, the goal is to leverage similarities in contexts for different arms so as to improve the agent's ability to predict rewards from contexts. We propose an upper confidence bound-based multi-task learning algorithm for contextual bandits, establish a corresponding regret bound, and interpret this bound to quantify the advantages of learning in the presence of high task (arm) similarity. We also describe an effective scheme for estimating task similarity from data, and demonstrate our algorithm's performance on several data sets. © 2017 Neural information processing systems foundation. All rights reserved.","Contextual bandits are a form of multi-armed bandit in which the agent has access to predictive side information (known as the context) for each arm at each time step, and have been used to model personalized news recommendation, ad placement, and other applications. In this work, we propose a multi-task learning framework for contextual bandit problems. Like multi-task learning in the batch setting, the goal is to leverage similarities in contexts for different arms so as to improve the agent's ability to predict rewards from contexts. We propose an upper confidence bound-based multi-task learning algorithm for contextual bandits, establish a corresponding regret bound, and interpret this bound to quantify the advantages of learning in the presence of high task (arm) similarity. We also describe an effective scheme for estimating task similarity from data, and demonstrate our algorithm's performance on several data sets. © 2017 Neural information processing systems foundation. All rights reserved.",,"Department of EECS, University of Michigan, Ann Arbor, Ann Arbor, MI, United States; Microsoft Research, Cambridge, United Kingdom",,,,Author3,Author4,a,a,u,a,a,a,Recommender Systems,,"The application is not stated in the abstract, however there are benchmarks on 'several datasets'. These could include personalization ones","True, more fundamental work it seems.",u,a,Advances in Neural Information Processing Systems,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047009825&partnerID=40&md5=70d38299b0bb67817c5702fd0efabb71,"Deshmukh A.A., Dogan U., Scott C.","Deshmukh, A.A., Department of EECS, University of Michigan, Ann Arbor, Ann Arbor, MI, United States; Dogan, U., Microsoft Research, Cambridge, United Kingdom; Scott, C., Department of EECS, University of Michigan, Ann Arbor, Ann Arbor, MI, United States",,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-85047009825,,,,,4857,4849,,,,,Scopus,,,,Conference Paper,,,2017-December,,,2017,1,2,1,Author1
172,1,0,1,1,Hierarchical exploration for accelerating contextual bandits,"Contextual bandit learning is an increasingly popular approach to optimizing recommender systems via user feedback, but can be slow to converge in practice due to the need for exploring a large feature space. In this paper, we propose a coarse-to-fine hierarchical approach for encoding prior knowledge that drastically reduces the amount of exploration required. Intuitively, user preferences can be reasonably embedded in a coarse low-dimensional feature space that can be explored efficiently, requiring exploration in the high-dimensional space only as necessary. We introduce a bandit algorithm that explores within this coarse-to-fine spectrum, and prove performance guarantees that depend on how well the coarse space captures the user's preferences. We demonstrate substantial improvement over conventional bandit algorithms through extensive simulation as well as a live user study in the setting of personalized news recommendation. Copyright 2012 by the author(s)/owner(s).","Contextual bandit learning is an increasingly popular approach to optimizing recommender systems via user feedback, but can be slow to converge in practice due to the need for exploring a large feature space. In this paper, we propose a coarse-to-fine hierarchical approach for encoding prior knowledge that drastically reduces the amount of exploration required. Intuitively, user preferences can be reasonably embedded in a coarse low-dimensional feature space that can be explored efficiently, requiring exploration in the high-dimensional space only as necessary. We introduce a bandit algorithm that explores within this coarse-to-fine spectrum, and prove performance guarantees that depend on how well the coarse space captures the user's preferences. We demonstrate substantial improvement over conventional bandit algorithms through extensive simulation as well as a live user study in the setting of personalized news recommendation. Copyright 2012 by the author(s)/owner(s).",,"iLab, H. John Heinz III College, Carnegie Mellon University, Pittsburgh, PA 15213, United States; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States",,,,Author4,Author1,a,a,a,a,a,a,"Other (please in ""remark"")",,"Alg. that tailors based on user prefs, but more fundamental, not applied to case.",,a,a,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867112205&partnerID=40&md5=4dc1f9081d737b65dfff6e12543f0bba,"Yue Y., Hong S.A., Guestrin C.","Yue, Y., iLab, H. John Heinz III College, Carnegie Mellon University, Pittsburgh, PA 15213, United States; Hong, S.A., School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States; Guestrin, C., School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States",8,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-84867112205,,,,,1902,1895,,,,,Scopus,,,,Conference Paper,,,2,,2012,2012,1,3,0,Author1
173,1,1,1,1,Evolution of context-aware user profiles,"Context-awareness and adaptation are key issues in mobile and ubiquitous computing. Applications on mobile devices use context information to adapt themselves to changing environments. User profiles play an important role in these systems as they serve as an individualization filter in a wide range of possible context adaptation parameters. In this paper we propose a modeling approach for the evolution of context-aware user profiles. A motivating scenario, the intelligent selection of a suitable medical expert in an emergency situation, shows the need for context-aware matching of user profiles. This is achieved by a similarity matching algorithm and reinforcement learning.","Context-awareness and adaptation are key issues in mobile and ubiquitous computing. Applications on mobile devices use context information to adapt themselves to changing environments. User profiles play an important role in these systems as they serve as an individualization filter in a wide range of possible context adaptation parameters. In this paper we propose a modeling approach for the evolution of context-aware user profiles. A motivating scenario, the intelligent selection of a suitable medical expert in an emergency situation, shows the need for context-aware matching of user profiles. This is achieved by a similarity matching algorithm and reinforcement learning.",,,,"Condat AG, Berlin, Germany",,Author1,Author2,a,a,a,a,a,a,"Other (please in ""remark"")",,Software/systems and network engineering,,a,a,2009 International Conference on Ultra Modern Telecommunications & Workshops,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5345395,,J. Thomsen; Y. Vanrompay; Y. Berbers,,1,,10.1109/ICUMT.2009.5345395,IEEE Xplore,1,,20091204,6,,Application software;Computer science;Context modeling;Filters;Intelligent networks;Mobile computing;Multiple signal classification;Personal digital assistants;Runtime;Ubiquitous computing,learning (artificial intelligence);medical computing;mobile computing,adaptation;context-aware user profiles;context-awareness;medical expert;mobile computing;reinforcement learning;similarity matching algorithm;ubiquitous computing,,2157-0221;21570221,,12-14 Oct. 2009,,,,,,,,,,,,IEEE,21,,POD:978-1-4244-3942-3,,1,IEEE Conferences,,,,,2009,2009,0,4,0,Author2
174,2,1,0,1,Using Contextual Learning to Improve Diagnostic Accuracy: Application in Breast Cancer Screening,"Clinicians need to routinely make management decisions about patients who are at risk for a disease such as breast cancer. This paper presents a novel clinical decision support tool that is capable of helping physicians make diagnostic decisions. We apply this support system to improve the specificity of breast cancer screening and diagnosis. The system utilizes clinical context (e.g., demographics, medical history) to minimize the false positive rates while avoiding false negatives. An online contextual learning algorithm is used to update the diagnostic strategy presented to the physicians over time. We analytically evaluate the diagnostic performance loss of the proposed algorithm, in which the true patient distribution is not known and needs to be learned, as compared with the optimal strategy where all information is assumed known, and prove that the false positive rate of the proposed learning algorithm asymptotically converges to the optimum. In addition, our algorithm also has the important merit that it can provide individualized confidence estimates about the accuracy of the diagnosis recommendation. Moreover, the relevancy of contextual features is assessed, enabling the approach to identify specific contextual features that provide the most value of information in reducing diagnostic errors. Experiments were conducted using patient data collected at a large academic medical center. Our proposed approach outperforms the current clinical practice by 36% in terms of false positive rate given a 2% false negative rate.","Clinicians need to routinely make management decisions about patients who are at risk for a disease such as breast cancer. This paper presents a novel clinical decision support tool that is capable of helping physicians make diagnostic decisions. We apply this support system to improve the specificity of breast cancer screening and diagnosis. The system utilizes clinical context (e.g., demographics, medical history) to minimize the false positive rates while avoiding false negatives. An online contextual learning algorithm is used to update the diagnostic strategy presented to the physicians over time. We analytically evaluate the diagnostic performance loss of the proposed algorithm, in which the true patient distribution is not known and needs to be learned, as compared with the optimal strategy where all information is assumed known, and prove that the false positive rate of the proposed learning algorithm asymptotically converges to the optimum. In addition, our algorithm also has the important merit that it can provide individualized confidence estimates about the accuracy of the diagnosis recommendation. Moreover, the relevancy of contextual features is assessed, enabling the approach to identify specific contextual features that provide the most value of information in reducing diagnostic errors. Experiments were conducted using patient data collected at a large academic medical center. Our proposed approach outperforms the current clinical practice by 36% in terms of false positive rate given a 2% false negative rate.",,,,"Department of Electrical Engineering, University of California, Los Angeles, Los Angeles, CA, USA",Breast cancer;computer-aided diagnosis system;contextual learning;multiarmed bandit (MAB);online learning,Author1,Author2,a,a,a,a,a,a,Healthcare,,,,a,a,IEEE Journal of Biomedical and Health Informatics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7064753,,L. Song; W. Hsu; J. Xu; M. van der Schaar,,2,,10.1109/JBHI.2015.2414934,IEEE Xplore,1,,20170520,914,10.13039/100000181 - U.S. Air Force Office of Scientific Research; ,Biopsy;Breast cancer;Clustering algorithms;Context;Imaging,cancer;learning (artificial intelligence);medical diagnostic computing;patient diagnosis,breast cancer screening;clinical decision support tool;contextual features;diagnostic accuracy;disease;false positive rates;online contextual learning algorithm,,2168-2194;21682194,3,May 2016,,,,,"Aged;Algorithms;Breast Neoplasms;Diagnosis, Computer-Assisted;Early Detection of Cancer;Electronic Health Records;Female;Humans;Mammography;Middle Aged;Signal Processing, Computer-Assisted",20150320,,,,,,IEEE,42,,,,902,IEEE Journals & Magazines,,,20,,2016,2016,1,2,1,Author1
175,0,1,0,0,Discovering virtual interest groups across chat rooms,"Chat has becoming an increasingly popular communication tool in our everyday life. When the number of related concurrent chat rooms gets large, tracking them 24x7 becomes very difficult. To address this research problem, we have developed VIGIR (Virtual Interest Group & Information Recommender), a tool for automatic chat room monitoring. The tool builds adaptive interest models for chat users, which are used to provide a number of personalized services including finding virtual interest groups (VIGs) for chat users. Dynamic identification of the VIG addresses the distributed user collaboration challenge, which is acute problem especially in military operations. VIGIR extends our prior work in user interest modeling into the domain of real-time text-based communications. We have evaluated the effectiveness of VIGIR in two studies. The first is a user-centred evaluation where we have achieved a precision at 60% and recall at 80% for VIG identification. In the second study using military chat data, we have demonstrated an average precision of 45% to 50%. In addition, we have shown that the precision for predicting VIG increases over time as more data become available.","Chat has becoming an increasingly popular communication tool in our everyday life. When the number of related concurrent chat rooms gets large, tracking them 24x7 becomes very difficult. To address this research problem, we have developed VIGIR (Virtual Interest Group & Information Recommender), a tool for automatic chat room monitoring. The tool builds adaptive interest models for chat users, which are used to provide a number of personalized services including finding virtual interest groups (VIGs) for chat users. Dynamic identification of the VIG addresses the distributed user collaboration challenge, which is acute problem especially in military operations. VIGIR extends our prior work in user interest modeling into the domain of real-time text-based communications. We have evaluated the effectiveness of VIGIR in two studies. The first is a user-centred evaluation where we have achieved a precision at 60% and recall at 80% for VIG identification. In the second study using military chat data, we have demonstrated an average precision of 45% to 50%. In addition, we have shown that the precision for predicting VIG increases over time as more data become available.",,"SAIC, Inc., 1710 SAIC Drive, McLean, VA 22102, United States",,,Chat; IRC; Machine learning; Reinforcement learning; User modeling; Virtual interest group; XMPP,Author2,Author3,u,a,a,a,a,a,"Other (please in ""remark"")",d,Chat bot,"I think we can accept on the algo criterion based on RL being one of the keywords.

I would argue the domain should be recommender systems, since VIGIR is used to recommend chat channels ('interest groups').
",u,a,KMIS 2012 - Proceedings of the International Conference on Knowledge Management and Information Sharing,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881594185&partnerID=40&md5=c6e60adc01232386122bd4ff27e82e5f,"Li H., Lau J., Alonso R.","Li, H., SAIC, Inc., 1710 SAIC Drive, McLean, VA 22102, United States; Lau, J., SAIC, Inc., 1710 SAIC Drive, McLean, VA 22102, United States; Alonso, R., SAIC, Inc., 1710 SAIC Drive, McLean, VA 22102, United States",1,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-84881594185,,,,,157,152,,,,,Scopus,,,,Conference Paper,,,,,,2012,3,1,0,Author2
176,0,0,0,0,Multi-agents and learning: Implications for Webusage mining,"Characterization of user activities is an important issue in the design and maintenance of websites. Server weblog files have abundant information about the user's current interests. This information can be mined and analyzed therefore the administrators may be able to guide the users in their browsing activity so they may obtain relevant information in a shorter span of time to obtain user satisfaction. Web-based technology facilitates the creation of personally meaningful and socially useful knowledge through supportive interactions, communication and collaboration among educators, learners and information. This paper suggests a new methodology based on learning techniques for a Web-based Multiagent-based application to discover the hidden patterns in the user's visited links. It presents a new approach that involves unsupervised, reinforcement learning, and cooperation between agents. It is utilized to discover patterns that represent the user's profiles in a sample website into specific categories of materials using significance percentages. These profiles are used to make recommendations of interesting links and categories to the user. The experimental results of the approach showed successful user pattern recognition, and cooperative learning among agents to obtain user profiles. It indicates that combining different learning algorithms is capable of improving user satisfaction indicated by the percentage of precision, recall, the progressive category weight and F1-measure. © 2015.Production and hosting by Elsevier B.V.","Characterization of user activities is an important issue in the design and maintenance of websites. Server weblog files have abundant information about the user's current interests. This information can be mined and analyzed therefore the administrators may be able to guide the users in their browsing activity so they may obtain relevant information in a shorter span of time to obtain user satisfaction. Web-based technology facilitates the creation of personally meaningful and socially useful knowledge through supportive interactions, communication and collaboration among educators, learners and information. This paper suggests a new methodology based on learning techniques for a Web-based Multiagent-based application to discover the hidden patterns in the user's visited links. It presents a new approach that involves unsupervised, reinforcement learning, and cooperation between agents. It is utilized to discover patterns that represent the user's profiles in a sample website into specific categories of materials using significance percentages. These profiles are used to make recommendations of interesting links and categories to the user. The experimental results of the approach showed successful user pattern recognition, and cooperative learning among agents to obtain user profiles. It indicates that combining different learning algorithms is capable of improving user satisfaction indicated by the percentage of precision, recall, the progressive category weight and F1-measure. © 2015.Production and hosting by Elsevier B.V.",Open Access,"Computer Science, Mathematics Department, Faculty of Science, AinShams University, Cairo, Egypt; Pure Math. and Computer Science, Menoufia University, Menoufia, Egypt",,,Cooperative learning; Personalized web search; Recommendation system; Reinforcement learning; Unsupervised learning,Author3,Author4,a,a,a,a,a,a,"Other (please in ""remark"")",a,domain: information retrieval,,a,a,Journal of Advanced Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959451312&doi=10.1016%2fj.jare.2015.06.005&partnerID=40&md5=df411efcfc997150cd8dc04bbd48d678,"Lotfy H.M.S., Khamis S.M.S., Aboghazalah M.M.","Lotfy, H.M.S., Computer Science, Mathematics Department, Faculty of Science, AinShams University, Cairo, Egypt; Khamis, S.M.S., Computer Science, Mathematics Department, Faculty of Science, AinShams University, Cairo, Egypt; Aboghazalah, M.M., Pure Math. and Computer Science, Menoufia University, Menoufia, Egypt",1,,10.1016/j.jare.2015.06.005,SCOPUS,1,,,,,,,,,,2,,,,2-s2.0-84959451312,,,,,295,285,,,,,Scopus,,,,Article,,,7,,2016,2016,4,0,0,Author3
177,0,0,0,0,Business process outsourcing enhanced by fuzzy linguistic consensus model,"Business process outsourcing represents a strategic option to obtain the overall improvement of performance in business process management context. It consists in externalizing whole sub-processes (e.g., production, logistics, human resources) of a value chain. Last decade, the concept of value chain moved toward the more flexible concept of value net that implies the assembly of several value chains tailored to specifics, objectives, markets, etc. Thus, the composition of a value chain within a value net environments can be understood as the modeling of a macro business process in which sub-processes can be outsourced. Such composition activity foresees crucial decision-making moments that need to be sustained by a group of decision-makers owning several and heterogeneous competences in order to select the most suitable external providers to which delegate specific sub-processes. This work proposes a framework to enhance business process outsourcing by introducing group decision-making support that relies on a fuzzy linguistic consensus model. In addition, the framework implements algorithms to learn and assign different weights to decision-makers considering the context and time at which they participate in the group decision making. The framework is applied to an Italian footwear company by describing a numerical example. © 2017 Elsevier B.V.","Business process outsourcing represents a strategic option to obtain the overall improvement of performance in business process management context. It consists in externalizing whole sub-processes (e.g., production, logistics, human resources) of a value chain. Last decade, the concept of value chain moved toward the more flexible concept of value net that implies the assembly of several value chains tailored to specifics, objectives, markets, etc. Thus, the composition of a value chain within a value net environments can be understood as the modeling of a macro business process in which sub-processes can be outsourced. Such composition activity foresees crucial decision-making moments that need to be sustained by a group of decision-makers owning several and heterogeneous competences in order to select the most suitable external providers to which delegate specific sub-processes. This work proposes a framework to enhance business process outsourcing by introducing group decision-making support that relies on a fuzzy linguistic consensus model. In addition, the framework implements algorithms to learn and assign different weights to decision-makers considering the context and time at which they participate in the group decision making. The framework is applied to an Italian footwear company by describing a numerical example. © 2017 Elsevier B.V.",,"Dipartimento di Scienze Aziendali – Management & Innovation Systems, University of Salerno, Fisciano (SA), Italy; Department of Computer Science and Artificial Intelligence, University of Granada, Granada, Spain; Department of Electrical and Computer Engineering, Faculty of Engineering, King Abdulaziz University, Jeddah, Saudi Arabia",,,Business processes outsourcing; Context awareness; Fuzzy linguistic consensus model; Reinforcement learning; Value net,Author4,Author1,a,a,a,a,a,a,Personal Assistants,,"Decision making, doubt a bit about personalization, but seems to be about learning from individuals as well.",,a,a,Applied Soft Computing Journal,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039847146&doi=10.1016%2fj.asoc.2017.12.020&partnerID=40&md5=978d181b7df8a4116a9a4b237f0e7148,"Ciasullo M.V., Fenza G., Loia V., Orciuoli F., Troisi O., Herrera-Viedma E.","Ciasullo, M.V., Dipartimento di Scienze Aziendali – Management & Innovation Systems, University of Salerno, Fisciano (SA), Italy; Fenza, G., Dipartimento di Scienze Aziendali – Management & Innovation Systems, University of Salerno, Fisciano (SA), Italy; Loia, V., Dipartimento di Scienze Aziendali – Management & Innovation Systems, University of Salerno, Fisciano (SA), Italy; Orciuoli, F., Dipartimento di Scienze Aziendali – Management & Innovation Systems, University of Salerno, Fisciano (SA), Italy; Troisi, O., Dipartimento di Scienze Aziendali – Management & Innovation Systems, University of Salerno, Fisciano (SA), Italy; Herrera-Viedma, E., Department of Computer Science and Artificial Intelligence, University of Granada, Granada, Spain, Department of Electrical and Computer Engineering, Faculty of Engineering, King Abdulaziz University, Jeddah, Saudi Arabia",,,10.1016/j.asoc.2017.12.020,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85039847146,,,,,444,436,,,,,Scopus,,,,Article,,,64,,2018,2018,4,0,0,Author4
178,1,1,0,1,The use of reinforcement learning algorithms to meet the challenges of an artificial pancreas.,"Blood glucose control, for example, in diabetes mellitus or severe illness, requires strict adherence to a protocol of food, insulin administration and exercise personalized to each patient. An artificial pancreas for automated treatment could boost quality of glucose control and patients' independence. The components required for an artificial pancreas are: i) continuous glucose monitoring (CGM), ii) smart controllers and iii) insulin pumps delivering the optimal amount of insulin. In recent years, medical devices for CGM and insulin administration have undergone rapid progression and are now commercially available. Yet, clinically available devices still require regular patients' or caregivers' attention as they operate in open-loop control with frequent user intervention. Dosage-calculating algorithms are currently being studied in intensive care patients [1] , for short overnight control to supplement conventional insulin delivery [2] , and for short periods where patients rest and follow a prescribed food regime [3] . Fully automated algorithms that can respond to the varying activity levels seen in outpatients, with unpredictable and unreported food intake, and which provide the necessary personalized control for individuals is currently beyond the state-of-the-art. Here, we review and discuss reinforcement learning algorithms, controlling insulin in a closed-loop to provide individual insulin dosing regimens that are reactive to the immediate needs of the patient.","Blood glucose control, for example, in diabetes mellitus or severe illness, requires strict adherence to a protocol of food, insulin administration and exercise personalized to each patient. An artificial pancreas for automated treatment could boost quality of glucose control and patients' independence. The components required for an artificial pancreas are: i) continuous glucose monitoring (CGM), ii) smart controllers and iii) insulin pumps delivering the optimal amount of insulin. In recent years, medical devices for CGM and insulin administration have undergone rapid progression and are now commercially available. Yet, clinically available devices still require regular patients' or caregivers' attention as they operate in open-loop control with frequent user intervention. Dosage-calculating algorithms are currently being studied in intensive care patients [1] , for short overnight control to supplement conventional insulin delivery [2] , and for short periods where patients rest and follow a prescribed food regime [3] . Fully automated algorithms that can respond to the varying activity levels seen in outpatients, with unpredictable and unreported food intake, and which provide the necessary personalized control for individuals is currently beyond the state-of-the-art. Here, we review and discuss reinforcement learning algorithms, controlling insulin in a closed-loop to provide individual insulin dosing regimens that are reactive to the immediate needs of the patient.",,"Fresenius Kabi Deutschland GmbH, Else-Kröner-Strasse 1, 61352 Bad Homburg, Germany.",,,,Author2,Author3,a,a,a,a,a,a,Healthcare,a,,,a,a,Expert review of medical devices,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901870499&doi=10.1586%2f17434440.2013.827515&partnerID=40&md5=5e57f84d94e8248985013780a9d9b267,"Bothe M.K., Dickens L., Reichel K., Tellmann A., Ellger B., Westphal M., Faisal A.A.","Bothe, M.K., Fresenius Kabi Deutschland GmbH, Else-Kröner-Strasse 1, 61352 Bad Homburg, Germany.; Dickens, L.; Reichel, K.; Tellmann, A.; Ellger, B.; Westphal, M.; Faisal, A.A.",9,,10.1586/17434440.2013.827515,SCOPUS,1,,,,,,,,,,5,,,,2-s2.0-84901870499,,,,,673,661,,,,,Scopus,,,,Review,,,10,,2013,2013,1,3,0,Author4
179,0,0,2,0,Consistent Goal-Directed User Model for Realisitc Man-Machine Task-Oriented Spoken Dialogue Simulation,"Because of the great variability of factors to take into account, designing a spoken dialogue system is still a tailoring task. Rapid design and reusability of previous work is made very difficult. For these reasons, the application of machine learning methods to dialogue strategy optimization has become a leading subject of researches this last decade. Yet, techniques such as reinforcement learning are very demanding in training data while obtaining a substantial amount of data in the particular case of spoken dialogues is time-consuming and therefore expansive. In order to expand existing data sets, dialogue simulation techniques are becoming a standard solution. In this paper we describe a user modeling technique for realistic simulation of man-machine goal-directed spoken dialogues. This model, based on a stochastic description of man-machine communication, unlike previously proposed models, is consistent along the interaction according to its history and a predefined user goal","Because of the great variability of factors to take into account, designing a spoken dialogue system is still a tailoring task. Rapid design and reusability of previous work is made very difficult. For these reasons, the application of machine learning methods to dialogue strategy optimization has become a leading subject of researches this last decade. Yet, techniques such as reinforcement learning are very demanding in training data while obtaining a substantial amount of data in the particular case of spoken dialogues is time-consuming and therefore expansive. In order to expand existing data sets, dialogue simulation techniques are becoming a standard solution. In this paper we describe a user modeling technique for realistic simulation of man-machine goal-directed spoken dialogues. This model, based on a stochastic description of man-machine communication, unlike previously proposed models, is consistent along the interaction according to its history and a predefined user goal",,,,"&#201;cole Sup&#233;rieure d'&#201;lectricit&#233; - SUPELEC, Metz Campus - STS Team, 2 rue &#201;douard Belin - F-57070 Metz - FRANCE. email: olivier.pietquin@supelec.fr",,Author4,Author1,a,a,u,a,a,a,"Other (please in ""remark"")",,Dialogue system. In doubt about personalization. Does model specific user behavior.,,u,a,2006 IEEE International Conference on Multimedia and Expo,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4036627,,O. Pietquin,,4,,10.1109/ICME.2006.262563,IEEE Xplore,1,,20061226,428,,Acoustic noise;Automatic speech recognition;History;Learning systems;Man machine systems;Optimization methods;Sociotechnical systems;Speech processing;Stochastic processes;Training data,interactive systems;learning (artificial intelligence);man-machine systems;optimisation;speech processing;stochastic processes;user modelling,machine learning method;optimization;realistic man-machine simulation;spoken dialogue system;stochastic description;task-oriented dialogue simulation technique;user modeling technique,,1945-7871;19457871,,9-12 July 2006,,,,,,,,,,,,IEEE,14,,CD-ROM:1-4244-0367-7; POD:1-4244-0366-7,,425,IEEE Conferences,,,,,,2006,3,0,1,Author3
180,2,2,0,2,Encouraging Physical Activity in Patients With Diabetes: Intervention Using a Reinforcement Learning System,"BACKGROUND: Regular physical activity is known to be beneficial for people with type 2 diabetes. Nevertheless, most of the people who have diabetes lead a sedentary lifestyle. Smartphones create new possibilities for helping people to adhere to their physical activity goals through continuous monitoring and communication, coupled with personalized feedback.OBJECTIVE: The aim of this study was to help type 2 diabetes patients increase the level of their physical activity.METHODS: We provided 27 sedentary type 2 diabetes patients with a smartphone-based pedometer and a personal plan for physical activity. Patients were sent short message service messages to encourage physical activity between once a day and once per week. Messages were personalized through a Reinforcement Learning algorithm so as to improve each participant's compliance with the activity regimen. The algorithm was compared with a static policy for sending messages and weekly reminders.RESULTS: Our results show that participants who received messages generated by the learning algorithm increased the amount of activity and pace of walking, whereas the control group patients did not. Patients assigned to the learning algorithm group experienced a superior reduction in blood glucose levels (glycated hemoglobin [HbA1c]) compared with control policies, and longer participation caused greater reductions in blood glucose levels. The learning algorithm improved gradually in predicting which messages would lead participants to exercise.CONCLUSIONS: Mobile phone apps coupled with a learning algorithm can improve adherence to exercise in diabetic patients. This algorithm can be used in large populations of diabetic patients to improve health and glycemic control. Our results can be expanded to other areas where computer-led health coaching of humans may have a positive impact. Summary of a part of this manuscript has been previously published as a letter in Diabetes Care, 2016.","BACKGROUND: Regular physical activity is known to be beneficial for people with type 2 diabetes. Nevertheless, most of the people who have diabetes lead a sedentary lifestyle. Smartphones create new possibilities for helping people to adhere to their physical activity goals through continuous monitoring and communication, coupled with personalized feedback.OBJECTIVE: The aim of this study was to help type 2 diabetes patients increase the level of their physical activity.METHODS: We provided 27 sedentary type 2 diabetes patients with a smartphone-based pedometer and a personal plan for physical activity. Patients were sent short message service messages to encourage physical activity between once a day and once per week. Messages were personalized through a Reinforcement Learning algorithm so as to improve each participant's compliance with the activity regimen. The algorithm was compared with a static policy for sending messages and weekly reminders.RESULTS: Our results show that participants who received messages generated by the learning algorithm increased the amount of activity and pace of walking, whereas the control group patients did not. Patients assigned to the learning algorithm group experienced a superior reduction in blood glucose levels (glycated hemoglobin [HbA1c]) compared with control policies, and longer participation caused greater reductions in blood glucose levels. The learning algorithm improved gradually in predicting which messages would lead participants to exercise.CONCLUSIONS: Mobile phone apps coupled with a learning algorithm can improve adherence to exercise in diabetic patients. This algorithm can be used in large populations of diabetic patients to improve health and glycemic control. Our results can be expanded to other areas where computer-led health coaching of humans may have a positive impact. Summary of a part of this manuscript has been previously published as a letter in Diabetes Care, 2016.",,"Microsoft Research, Herzeliya, Israel; Technion - Israel Institute of Technology, Faculty of Medicine, Haifa, Israel; Technion - Israel Institute of Technology, Faculty of Electrical Engineering, Haifa, Israel; Technion - Israel Institute of Technology, Faculty of Industrial Engineering, Haifa, Israel; Rambam Healthcare Campus, Institute of Endocrinology, Haifa, Israel",,,diabetes type 2; physical activity; reinforcement learning,Author4,Author1,a,a,a,a,a,a,Healthcare,,,,a,a,Journal of medical Internet research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042765009&doi=10.2196%2fjmir.7994&partnerID=40&md5=d20112fde48afda622fd704f8b2ae508,"Yom-Tov E., Feraru G., Kozdoba M., Mannor S., Tennenholtz M., Hochberg I.","Yom-Tov, E., Microsoft Research, Herzeliya, Israel; Feraru, G., Technion - Israel Institute of Technology, Faculty of Medicine, Haifa, Israel; Kozdoba, M., Technion - Israel Institute of Technology, Faculty of Electrical Engineering, Haifa, Israel; Mannor, S., Technion - Israel Institute of Technology, Faculty of Electrical Engineering, Haifa, Israel; Tennenholtz, M., Technion - Israel Institute of Technology, Faculty of Industrial Engineering, Haifa, Israel; Hochberg, I., Rambam Healthcare Campus, Institute of Endocrinology, Haifa, Israel",,,10.2196/jmir.7994,SCOPUS,1,,,,,,,,,,10,,,,2-s2.0-85042765009,,,,,,e338,,,,,Scopus,,,,Article,,,19,,2017,2017,1,0,3,Author2
181,2,2,0,2,Intelligent real-time therapy: Harnessing the power of machine learning to optimise the delivery of momentary cognitivebehavioural interventions,"Background Experience sampling methodology (ESM) [Csikszentmihalyi, M. & Larson, R. (1987). Validity and reliability of the experience-sampling method. Journal of Nervous and Mental Disease, 175(9), 526536] has been used to elucidate the cognitivebehavioural mechanisms underlying the development and maintenance of complex mental disorders as well as mechanisms involved in resilience from such states. We present an argument for the development of intelligent real-time therapy (iRTT). Machine learning and reinforcement learning specifically may be used to optimise the delivery of interventions by observing and altering the timing of real-time therapies based on ongoing ESM measures.Aims The aims of the present article are to outline the principles of iRTT and to consider how it would be applied to complex problems such as suicide prevention.Methods Relevant literature was identified through use of PychInfo.Results iRTT may provide an important and ecologically valid adjunct to traditional CBT, providing a means of balancing population-based data with individual data, thus addressing the ""knowledgepractice gap"" [Tarrier, N. (2010b). The cognitive and behavioral treatment of PTSD, what is known and what is known to be unknown: How not to fall into the practice gap. Clinical Psychology: Science and Practice, 17(2), 134143] and facilitating the delivery of interventions in situ, thereby addressing the ""therapyreal-world gap"".Conclusions iRTT may provide a platform for the development of individualised and multifaceted momentary intervention strategies that are ecologically valid and aimed at attenuating pathological pathways to complex mental health problems and amplifying pathways associated with resilience. © 2012 Informa UK, Ltd.","Background Experience sampling methodology (ESM) [Csikszentmihalyi, M. & Larson, R. (1987). Validity and reliability of the experience-sampling method. Journal of Nervous and Mental Disease, 175(9), 526536] has been used to elucidate the cognitivebehavioural mechanisms underlying the development and maintenance of complex mental disorders as well as mechanisms involved in resilience from such states. We present an argument for the development of intelligent real-time therapy (iRTT). Machine learning and reinforcement learning specifically may be used to optimise the delivery of interventions by observing and altering the timing of real-time therapies based on ongoing ESM measures.Aims The aims of the present article are to outline the principles of iRTT and to consider how it would be applied to complex problems such as suicide prevention.Methods Relevant literature was identified through use of PychInfo.Results iRTT may provide an important and ecologically valid adjunct to traditional CBT, providing a means of balancing population-based data with individual data, thus addressing the ""knowledgepractice gap"" [Tarrier, N. (2010b). The cognitive and behavioral treatment of PTSD, what is known and what is known to be unknown: How not to fall into the practice gap. Clinical Psychology: Science and Practice, 17(2), 134143] and facilitating the delivery of interventions in situ, thereby addressing the ""therapyreal-world gap"".Conclusions iRTT may provide a platform for the development of individualised and multifaceted momentary intervention strategies that are ecologically valid and aimed at attenuating pathological pathways to complex mental health problems and amplifying pathways associated with resilience. © 2012 Informa UK, Ltd.",,"Greater Manchester West Mental Health NHS Foundation Trust, Department of Psychology, Prestwich Hospital, Prestwich M25 3BL, United Kingdom; School of Psychological Sciences, University of Manchester, Manchester, United Kingdom; School of Community Based Medicine, University of Manchester, Manchester, United Kingdom; Department of Psychology, Institute of Psychiatry, King's College London, London, United Kingdom",,,Cognitive behaviour therapy (CBT); Ecological momentary intervention (EMI); Experience sampling methodology (ESM); Intelligent real-time therapy (iRTT); Smartphone; Suicide prevention,Author1,Author2,a,a,a,a,a,a,Healthcare,,,,a,a,Journal of Mental Health,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864214034&doi=10.3109%2f09638237.2011.638001&partnerID=40&md5=a00355d44c25f890ea793fe20c824beb,"Kelly J., Gooding P., Pratt D., Ainsworth J., Welford M., Tarrier N.","Kelly, J., Greater Manchester West Mental Health NHS Foundation Trust, Department of Psychology, Prestwich Hospital, Prestwich M25 3BL, United Kingdom, School of Psychological Sciences, University of Manchester, Manchester, United Kingdom; Gooding, P., School of Psychological Sciences, University of Manchester, Manchester, United Kingdom; Pratt, D., School of Psychological Sciences, University of Manchester, Manchester, United Kingdom; Ainsworth, J., School of Community Based Medicine, University of Manchester, Manchester, United Kingdom; Welford, M., Greater Manchester West Mental Health NHS Foundation Trust, Department of Psychology, Prestwich Hospital, Prestwich M25 3BL, United Kingdom; Tarrier, N., School of Psychological Sciences, University of Manchester, Manchester, United Kingdom, Department of Psychology, Institute of Psychiatry, King's College London, London, United Kingdom",22,,10.3109/09638237.2011.638001,SCOPUS,1,,,,,,,,,,4,,,,2-s2.0-84864214034,,,,,414,404,,,,,Scopus,,,,Article,,,21,,2012,2012,1,0,3,Author2
182,2,0,2,2,Quantifying uncertainty in batch personalized sequential decision making,"As the amount of data collected from individuals increases, there are more opportunities to use it to offer personalized experiences (e.g., using electronic health records to offer personalized treatments). We advocate applying techniques from batch reinforcement learning to predict the range of effectiveness that policies might have for individuals. We identify three sources of uncer-tainty and present a method that addresses all of them. It handles the uncertainty caused by population mismatch by modeling the data as a latent mixture of different subpopulations of individuals, it explicitly quantifies data sparsity by accounting for the limited data available about the underlying models, and incorporates intrinsic stochasticity to yield estimated percentile ranges of the effectiveness of a policy for a particular new individual. Using this approach, we highlight some interesting variability in policy effectiveness amongst HIV patients given a prior patient treatment dataset. Our approach highlights the potential benefit of taking into account individual variability and data limitations when performing batch policy evaluation for new individuals. © Copyright 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","As the amount of data collected from individuals increases, there are more opportunities to use it to offer personalized experiences (e.g., using electronic health records to offer personalized treatments). We advocate applying techniques from batch reinforcement learning to predict the range of effectiveness that policies might have for individuals. We identify three sources of uncer-tainty and present a method that addresses all of them. It handles the uncertainty caused by population mismatch by modeling the data as a latent mixture of different subpopulations of individuals, it explicitly quantifies data sparsity by accounting for the limited data available about the underlying models, and incorporates intrinsic stochasticity to yield estimated percentile ranges of the effectiveness of a policy for a particular new individual. Using this approach, we highlight some interesting variability in policy effectiveness amongst HIV patients given a prior patient treatment dataset. Our approach highlights the potential benefit of taking into account individual variability and data limitations when performing batch policy evaluation for new individuals. © Copyright 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Department of Computer Science, Rutgers University, United States; Department of Computer Science, Carnegie Mellon University, United States; Department of Computer Science, Brown University, United States",,,,Author2,Author3,a,a,a,a,a,a,Healthcare,a,,,a,a,AAAI Workshop - Technical Report,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974800780&partnerID=40&md5=436733a7134fcc8b7ea47a3703f08edc,"Marivate V., Chemali J., Brunskill E., Littmanf M.","Marivate, V., Department of Computer Science, Rutgers University, United States; Chemali, J., Department of Computer Science, Carnegie Mellon University, United States; Brunskill, E., Department of Computer Science, Carnegie Mellon University, United States; Littmanf, M., Department of Computer Science, Brown University, United States",1,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-84974800780,,,,,30,26,,,,,Scopus,,,,Conference Paper,,,WS-14-08,,2014,2014,1,0,3,Author4
183,0,0,0,0,Path planning with user route preference - A reward surface approximation approach using orthogonal Legendre polynomials,"As self driving cars become more ubiquitous, users would look for natural ways of informing the car AI about their personal choice of routes. This choice is not always dictated by straightforward logic such as shortest distance or shortest time, and can be influenced by hidden factors, such as comfort and familiarity. This paper presents a path learning algorithm for such applications, where from limited positive demonstrations, an autonomous agent learns the user's path preference and honors that choice in its route planning, but has the capability to adopt alternate routes, if the original choice(s) become impractical. The learning problem is modeled as a Markov decision process. The states (way-points) and actions (to move from one way-point to another) are pre-defined according to the existing network of paths between the origin and destination and the user's demonstration is assumed to be a sample of the preferred path. The underlying reward function which captures the essence of the demonstration is computed using an inverse reinforcement learning algorithm and from that the entire path mirroring the expert's demonstration is extracted. To alleviate the problem of state space explosion when dealing with a large state space, the reward function is approximated using a set of orthogonal polynomial basis functions with a fixed number of coefficients regardless of the size of the state space. A six fold reduction in total learning time is achieved compared to using simple basis functions, that has dimensionality equal to the number of distinct states.","As self driving cars become more ubiquitous, users would look for natural ways of informing the car AI about their personal choice of routes. This choice is not always dictated by straightforward logic such as shortest distance or shortest time, and can be influenced by hidden factors, such as comfort and familiarity. This paper presents a path learning algorithm for such applications, where from limited positive demonstrations, an autonomous agent learns the user's path preference and honors that choice in its route planning, but has the capability to adopt alternate routes, if the original choice(s) become impractical. The learning problem is modeled as a Markov decision process. The states (way-points) and actions (to move from one way-point to another) are pre-defined according to the existing network of paths between the origin and destination and the user's demonstration is assumed to be a sample of the preferred path. The underlying reward function which captures the essence of the demonstration is computed using an inverse reinforcement learning algorithm and from that the entire path mirroring the expert's demonstration is extracted. To alleviate the problem of state space explosion when dealing with a large state space, the reward function is approximated using a set of orthogonal polynomial basis functions with a fixed number of coefficients regardless of the size of the state space. A six fold reduction in total learning time is achieved compared to using simple basis functions, that has dimensionality equal to the number of distinct states.",,,,"Department of Mechanical Aerospace and Biomedical Engineering, University of Tennessee, Knoxville, 37996 USA",,Author4,Author1,a,a,a,a,a,a,Personal Assistants,,Personalized navigation system.,,a,a,2016 IEEE International Conference on Automation Science and Engineering (CASE),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7743527,,A. R. Srinivasan; S. Chakraborty,,,,10.1109/COASE.2016.7743527,IEEE Xplore,1,,20161117,1105,,Automobiles;Databases;Learning (artificial intelligence);Markov processes;Path planning;Planning;Real-time systems,Markov processes;learning (artificial intelligence);mobile robots;path planning;state-space methods,Markov decision process;autonomous agent learns;dimensionality;inverse reinforcement learning algorithm;large state space;orthogonal Legendre polynomials;orthogonal polynomial basis functions;path planning;reward function;reward surface approximation;route planning;self driving cars;six fold reduction;state space explosion;user path preference;user route preference,,,,21-25 Aug. 2016,,,,,,,,,,,,IEEE,,,Electronic:978-1-5090-2409-4; POD:978-1-5090-2410-0; USB:978-1-5090-2408-7,,1100,IEEE Conferences,,,,,2016,2016,4,0,0,Author1
184,2,2,0,2,Personalized tuning of a reinforcement learning control algorithm for glucose regulation,"Artificial pancreas is in the forefront of research towards the automatic insulin infusion for patients with type 1 diabetes. Due to the high inter- and intra-variability of the diabetic population, the need for personalized approaches has been raised. This study presents an adaptive, patient-specific control strategy for glucose regulation based on reinforcement learning and more specifically on the Actor-Critic (AC) learning approach. The control algorithm provides daily updates of the basal rate and insulin-to-carbohydrate (IC) ratio in order to optimize glucose regulation. A method for the automatic and personalized initialization of the control algorithm is designed based on the estimation of the transfer entropy (TE) between insulin and glucose signals. The algorithm has been evaluated in silico in adults, adolescents and children for 10 days. Three scenarios of initialization to i) zero values, ii) random values and iii) TE-based values have been comparatively assessed. The results have shown that when the TE-based initialization is used, the algorithm achieves faster learning with 98%, 90% and 73% in the A+B zones of the Control Variability Grid Analysis for adults, adolescents and children respectively after five days compared to 95%, 78%, 41% for random initialization and 93%, 88%, 41% for zero initial values. Furthermore, in the case of children, the daily Low Blood Glucose Index reduces much faster when the TE-based tuning is applied. The results imply that automatic and personalized tuning based on TE reduces the learning period and improves the overall performance of the AC algorithm.","Artificial pancreas is in the forefront of research towards the automatic insulin infusion for patients with type 1 diabetes. Due to the high inter- and intra-variability of the diabetic population, the need for personalized approaches has been raised. This study presents an adaptive, patient-specific control strategy for glucose regulation based on reinforcement learning and more specifically on the Actor-Critic (AC) learning approach. The control algorithm provides daily updates of the basal rate and insulin-to-carbohydrate (IC) ratio in order to optimize glucose regulation. A method for the automatic and personalized initialization of the control algorithm is designed based on the estimation of the transfer entropy (TE) between insulin and glucose signals. The algorithm has been evaluated in silico in adults, adolescents and children for 10 days. Three scenarios of initialization to i) zero values, ii) random values and iii) TE-based values have been comparatively assessed. The results have shown that when the TE-based initialization is used, the algorithm achieves faster learning with 98%, 90% and 73% in the A+B zones of the Control Variability Grid Analysis for adults, adolescents and children respectively after five days compared to 95%, 78%, 41% for random initialization and 93%, 88%, 41% for zero initial values. Furthermore, in the case of children, the daily Low Blood Glucose Index reduces much faster when the TE-based tuning is applied. The results imply that automatic and personalized tuning based on TE reduces the learning period and improves the overall performance of the AC algorithm.",,,,"Diabetes Technology Research Group, ARTORG Center for Biomedical Engineering Research, University of Bern, 3010, Switzerland",,Author1,Author2,a,a,a,a,a,a,Healthcare,,,,a,a,2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6610293,,E. Daskalaki; P. Diem; S. G. Mougiakakou,,3,,10.1109/EMBC.2013.6610293,IEEE Xplore,1,,20130926,3490,,Algorithm design and analysis;Diabetes;Insulin;Integrated circuits;Learning (artificial intelligence);Sugar;Vectors,blood;entropy;learning (artificial intelligence);medical computing;paediatrics;sugar,Actor-Critic learning approach;TE-based initialization;adolescents;artificial pancreas;automatic insulin infusion;basal rate;control variability grid analysis;glucose regulation;glucose signals;insulin signals;insulin-to-carbohydrate ratio;low-blood glucose index;patient-specific control strategy;personalized tuning;reinforcement learning control algorithm;transfer entropy;type 1 diabetes,,1094-687X;1094687X,,3-7 July 2013,,,,,"Adolescent;Adult;Algorithms;Blood Glucose;Child;Humans;Insulin;Pancreas, Artificial;Precision Medicine;Signal Processing, Computer-Assisted",,,,,,,IEEE,27,,Electronic:978-1-4577-0216-7; POD:978-1-4577-0215-0; USB:978-1-4577-0214-3,,3487,IEEE Conferences,,,,,2013,2013,1,0,3,Author2
185,1,0,2,1,Research on user's interest of web personalized information service,"An important problem in Web Personalized Information Service is to describe, measure, acquire and update a user's interest information. Aiming at the problem, this paper puts forward a new approach: Firstly, a general model for the user's interest information is constructed; Secondly, the learning agent initializes the interestingness of the user's interest topic according to his background information; Lastly, the learning agent updates the interestingness dynamically, based on his browsing actions or behaviors, and on his feedbacks by adopting Q-Learning algorithm of Reinforcement Learning. The above approach has four advantages: 1)the user need not describe and edit his profile; 2)the interestingness of the user's interest topic can be computed and quantified; 3)the agent can track the user's interest navigation and expansion dynamically and adapt to it; 4)the approach can be implemented easily.","An important problem in Web Personalized Information Service is to describe, measure, acquire and update a user's interest information. Aiming at the problem, this paper puts forward a new approach: Firstly, a general model for the user's interest information is constructed; Secondly, the learning agent initializes the interestingness of the user's interest topic according to his background information; Lastly, the learning agent updates the interestingness dynamically, based on his browsing actions or behaviors, and on his feedbacks by adopting Q-Learning algorithm of Reinforcement Learning. The above approach has four advantages: 1)the user need not describe and edit his profile; 2)the interestingness of the user's interest topic can be computed and quantified; 3)the agent can track the user's interest navigation and expansion dynamically and adapt to it; 4)the approach can be implemented easily.",,"Department of Compute Science and Engineering, Shanghai Jiaotong University, Shanghai 200030, China; School of Information Engineering, East China Jiaotong University, Nanchang 330013, China; School of Computer Information Engineering, Jiangxi Normal University, Nanchang 330022, China",,,Interestingness; Personalized information service; Q-learning; User interest model; Web,Author2,Author3,a,a,a,a,a,a,Recommender Systems,a,,,a,a,Journal of Computational Information Systems,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748151446&partnerID=40&md5=a62c93d05f8749b3abd44d7597fa92b5,"Zhong M., Wang M., Lu R.","Zhong, M., Department of Compute Science and Engineering, Shanghai Jiaotong University, Shanghai 200030, China, School of Information Engineering, East China Jiaotong University, Nanchang 330013, China; Wang, M., School of Computer Information Engineering, Jiangxi Normal University, Nanchang 330022, China; Lu, R., Department of Compute Science and Engineering, Shanghai Jiaotong University, Shanghai 200030, China",,,,SCOPUS,0,,,,,,,,,,4,,,,2-s2.0-33748151446,,,,,965,959,,,,,Scopus,,,,Article,,,1,,2005,2005,1,2,1,Author3
186,2,1,0,1,Model-free machine learning in biomedicine: Feasibility study in type 1 diabetes,"Although reinforcement learning (RL) is suitable for highly uncertain systems, the applicability of this class of algorithms to medical treatment may be limited by the patient variability which dictates individualised tuning for their usually multiple algorithmic parameters. This study explores the feasibility of RL in the framework of artificial pancreas development for type 1 diabetes (T1D). In this approach, an Actor-Critic (AC) learning algorithm is designed and developed for the optimisation of insulin infusion for personalised glucose regulation. AC optimises the daily basal insulin rate and insulin:carbohydrate ratio for each patient, on the basis of his/her measured glucose profile. Automatic, personalised tuning of AC is based on the estimation of information transfer (IT) from insulin to glucose signals. Insulinto-glucose IT is linked to patient-specific characteristics related to total daily insulin needs and insulin sensitivity (SI). The AC algorithm is evaluated using an FDA-accepted T1D simulator on a large patient database under a complex meal protocol, meal uncertainty and diurnal SI variation. The results showed that 95.66% of time was spent in normoglycaemia in the presence of meal uncertainty and 93.02% when meal uncertainty and SI variation were simultaneously considered. The time spent in hypoglycaemia was 0.27% in both cases. The novel tuning method reduced the risk of severe hypoglycaemia, especially in patients with low SI. © 2016 Daskalaki et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.","Although reinforcement learning (RL) is suitable for highly uncertain systems, the applicability of this class of algorithms to medical treatment may be limited by the patient variability which dictates individualised tuning for their usually multiple algorithmic parameters. This study explores the feasibility of RL in the framework of artificial pancreas development for type 1 diabetes (T1D). In this approach, an Actor-Critic (AC) learning algorithm is designed and developed for the optimisation of insulin infusion for personalised glucose regulation. AC optimises the daily basal insulin rate and insulin:carbohydrate ratio for each patient, on the basis of his/her measured glucose profile. Automatic, personalised tuning of AC is based on the estimation of information transfer (IT) from insulin to glucose signals. Insulinto-glucose IT is linked to patient-specific characteristics related to total daily insulin needs and insulin sensitivity (SI). The AC algorithm is evaluated using an FDA-accepted T1D simulator on a large patient database under a complex meal protocol, meal uncertainty and diurnal SI variation. The results showed that 95.66% of time was spent in normoglycaemia in the presence of meal uncertainty and 93.02% when meal uncertainty and SI variation were simultaneously considered. The time spent in hypoglycaemia was 0.27% in both cases. The novel tuning method reduced the risk of severe hypoglycaemia, especially in patients with low SI. © 2016 Daskalaki et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",Open Access,"Diabetes Technology Research Group, ARTORG Center for Biomedical Engineering Research, University of Bern, Murtenstrasse 50, Bern, Switzerland; Division of Endocrinology, Diabetes and Clinical Nutrition, Bern University Hospital Inselspital, Bern, Switzerland", e0158722,,,Author1,Author2,a,a,a,a,a,a,Healthcare,,,,a,a,PLoS ONE,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979704088&doi=10.1371%2fjournal.pone.0158722&partnerID=40&md5=ef957c6dffd5fa2b20f63df7b81047f6,"Daskalaki E., Diem P., Mougiakakou S.G.","Daskalaki, E., Diabetes Technology Research Group, ARTORG Center for Biomedical Engineering Research, University of Bern, Murtenstrasse 50, Bern, Switzerland; Diem, P., Division of Endocrinology, Diabetes and Clinical Nutrition, Bern University Hospital Inselspital, Bern, Switzerland; Mougiakakou, S.G., Diabetes Technology Research Group, ARTORG Center for Biomedical Engineering Research, University of Bern, Murtenstrasse 50, Bern, Switzerland, Division of Endocrinology, Diabetes and Clinical Nutrition, Bern University Hospital Inselspital, Bern, Switzerland",,,10.1371/journal.pone.0158722,SCOPUS,1,,,,,,,,,,7,,,,2-s2.0-84979704088,,,,,,,,,,,Scopus,,,,Article,,,11,,2016,2016,1,2,1,Author1
187,1,0,0,0,Studies on Drivers' Driving Styles Based on Inverse Reinforcement Learning,"Although advanced driver assistance systems (ADAS) have been widely introduced in automotive industry to enhance driving safety and comfort, and to reduce drivers' driving burden, they do not in general reflect different drivers' driving styles or customized with individual personalities. This can be important to comfort and enjoyable driving experience, and to improved market acceptance. However, it is challenging to understand and further identify drivers' driving styles due to large number and great variations of driving population. Previous research has mainly adopted physical approaches in modeling drivers' driving behavior, which however are often very much limited, if not impossible, in capturing human drivers' driving characteristics. This paper proposes a reinforcement learning based approach, in which the driving styles are formulated through drivers' learning processes from interaction with surrounding environment. Based on the reinforcement learning theory, driving action can be treated as maximizing a reward function. Instead of calibrating the unknown reward function to satisfy driver's desired response, we try to recover it from the human driving data, utilizing maximum likelihood inverse reinforcement learning (MLIRL). An IRL-based longitudinal driving assistance system is also proposed in this paper. Firstly, large amount of real world driving data is collected from a test vehicle, and the data is split into two sets for training and for testing purposes respectively. Then, the longitudinal acceleration is modeled as a Boltzmann distribution in human driving activity. The reward function is denoted as a linear combination of some kernelized basis functions. The driving style parameter vector is estimated using MLIRL based on the training set. Finally, a learning-based longitudinal driving assistance algorithm is developed and evaluated on the testing set. The results demonstrate that the proposed method can satisfactorily reflect human drivers' driving behavior. © 2018 SAE International; General Motors LLC.","Although advanced driver assistance systems (ADAS) have been widely introduced in automotive industry to enhance driving safety and comfort, and to reduce drivers' driving burden, they do not in general reflect different drivers' driving styles or customized with individual personalities. This can be important to comfort and enjoyable driving experience, and to improved market acceptance. However, it is challenging to understand and further identify drivers' driving styles due to large number and great variations of driving population. Previous research has mainly adopted physical approaches in modeling drivers' driving behavior, which however are often very much limited, if not impossible, in capturing human drivers' driving characteristics. This paper proposes a reinforcement learning based approach, in which the driving styles are formulated through drivers' learning processes from interaction with surrounding environment. Based on the reinforcement learning theory, driving action can be treated as maximizing a reward function. Instead of calibrating the unknown reward function to satisfy driver's desired response, we try to recover it from the human driving data, utilizing maximum likelihood inverse reinforcement learning (MLIRL). An IRL-based longitudinal driving assistance system is also proposed in this paper. Firstly, large amount of real world driving data is collected from a test vehicle, and the data is split into two sets for training and for testing purposes respectively. Then, the longitudinal acceleration is modeled as a Boltzmann distribution in human driving activity. The reward function is denoted as a linear combination of some kernelized basis functions. The driving style parameter vector is estimated using MLIRL based on the training set. Finally, a learning-based longitudinal driving assistance algorithm is developed and evaluated on the testing set. The results demonstrate that the proposed method can satisfactorily reflect human drivers' driving behavior. © 2018 SAE International; General Motors LLC.",,"Jilin University, China; General Motors LLC, China",,,,Author2,Author3,a,a,a,a,a,a,Personal Assistants,a,,,a,a,SAE Technical Papers,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045526693&doi=10.4271%2f2018-01-0612&partnerID=40&md5=b739715ea81417e25a5fbccb4160734d,"Jiang Y., Deng W., Wang J., Zhu B.","Jiang, Y., Jilin University, China; Deng, W., Jilin University, China; Wang, J., General Motors LLC, China; Zhu, B., Jilin University, China",,,10.4271/2018-01-0612,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-85045526693,,,,,,,,,,,Scopus,,,,Conference Paper,,,2018-April,,2018,2018,3,1,0,Author1
188,1,0,0,0,A personalized and integrative comparison-shopping engine and its applications,"Agents are the catalysts for commerce on the Web today. For example, comparison-shopping agents mediate the interactions between consumers and suppliers in order to yield markets that are more efficient. However, today's shopping agents are price-dominated, unreflective of the nature of supplier/consumer differentiation or the changing course of differentiation over time. This paper aims to tackle this dilemma and advances shopping agents into a stage where both kinds of differentiation are taken into account for enhanced understanding of the realities. We call them personalized and integrative shopping agents. These agents can leverage the interactive power of the Web for a more accurate understanding of consumer's preferences. This paper then presents a comparison-shopping engine that can be easily instantiated to become personalized and integrative shopping agents. This engine comprises of a product/merchant information collector, a consumer behavior extractor, a user profile manager, and an on-line learning personalized ranking module. We have built this engine and instantiated a comparison-shopping system for collecting preliminary evaluation results. The results show that this system is quite promising in overcoming the reality challenges of comparison shopping. In order to strengthen the contributions of this engine, we also gave a fielded application of this engine for personalized travel information discovery and explained the great potentials of this engine for a variety of comparison-shopping tasks. © 2002 Elsevier Science B.V. All rights reserved.","Agents are the catalysts for commerce on the Web today. For example, comparison-shopping agents mediate the interactions between consumers and suppliers in order to yield markets that are more efficient. However, today's shopping agents are price-dominated, unreflective of the nature of supplier/consumer differentiation or the changing course of differentiation over time. This paper aims to tackle this dilemma and advances shopping agents into a stage where both kinds of differentiation are taken into account for enhanced understanding of the realities. We call them personalized and integrative shopping agents. These agents can leverage the interactive power of the Web for a more accurate understanding of consumer's preferences. This paper then presents a comparison-shopping engine that can be easily instantiated to become personalized and integrative shopping agents. This engine comprises of a product/merchant information collector, a consumer behavior extractor, a user profile manager, and an on-line learning personalized ranking module. We have built this engine and instantiated a comparison-shopping system for collecting preliminary evaluation results. The results show that this system is quite promising in overcoming the reality challenges of comparison shopping. In order to strengthen the contributions of this engine, we also gave a fielded application of this engine for personalized travel information discovery and explained the great potentials of this engine for a variety of comparison-shopping tasks. © 2002 Elsevier Science B.V. All rights reserved.",,"Information Management Department, Fu-Jen University, 242 Taipei, Taiwan",,,(Multi-) agent systems; Comparison shopping; Consumer valuation models; Neural networks; Reinforcement learning,Author4,Author1,a,a,a,a,a,a,Personal Assistants,,,,a,a,Decision Support Systems,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037209477&doi=10.1016%2fS0167-9236%2802%2900077-5&partnerID=40&md5=37491eee2acf272198efc1cf74b290e9,Yuan S.-T.,"Yuan, S.-T., Information Management Department, Fu-Jen University, 242 Taipei, Taiwan",38,,10.1016/S0167-9236(02)00077-5,SCOPUS,1,,,,,,,,,,2,,,,2-s2.0-0037209477,,,,,156,139,,,,,Scopus,,,,Conference Paper,,,34,,2003,2003,3,1,0,Author1
189,0,1,2,1,Towards adaptive dialogue systems for assistive living environments,"Adaptive Dialogue Systems can be seen as smart interfaces that typically use natural language (spoken or written) as a means of communication. They are being used in many applications, such as customer service, in-car interfaces, even in rehabilitation, and therefore it is essential that these systems are robust, scalable and quickly adaptable in order to cope with changing user or system needs or environmental conditions. Making Dialogue Systems adaptive means overcoming several challenges, such as scalability or lack of training data. Achieving adaptation online has thus been an even greater challenge. We propose to build such a system, that will operate in an Assistive Living Environment and provide its services as a coach to patients that need to perform rehabilitative exercises. We are currently in the process of developing it, using Robot Operating System on a robotic platform.","Adaptive Dialogue Systems can be seen as smart interfaces that typically use natural language (spoken or written) as a means of communication. They are being used in many applications, such as customer service, in-car interfaces, even in rehabilitation, and therefore it is essential that these systems are robust, scalable and quickly adaptable in order to cope with changing user or system needs or environmental conditions. Making Dialogue Systems adaptive means overcoming several challenges, such as scalability or lack of training data. Achieving adaptation online has thus been an even greater challenge. We propose to build such a system, that will operate in an Assistive Living Environment and provide its services as a coach to patients that need to perform rehabilitative exercises. We are currently in the process of developing it, using Robot Operating System on a robotic platform.",,"Deptartment of Computer Science and Engineering, University of Texas, Arlington, United States; Institute of Informatics and Telecommunications, National Center for Scientific Research Demokritos, Greece",,,Adaptive dialogue systems; Dialogue management; Personalization; Reinforcement learning,Author2,Author3,u,a,u,a,a,a,"Other (please in ""remark"")",a,Robotics,"I think we should accept on the basis of having Reinforcement Learning as one of the keywords. This is a strong indicator that a RL algo is in fact used (even though it isn't clear from the abstract. The abstract mentions adaptation of a robot for assisted living. This could very well fit our definition of personalization, if the adaptation is to the individual rather than the home environment.",u,a,"International Conference on Intelligent User Interfaces, Proceedings IUI",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875830088&doi=10.1145%2f2451176.2451185&partnerID=40&md5=53894c815dba91d26bada3c5954a70c6,"Papangelis A., Karkaletsis V., Huang H.","Papangelis, A., Deptartment of Computer Science and Engineering, University of Texas, Arlington, United States; Karkaletsis, V., Institute of Informatics and Telecommunications, National Center for Scientific Research Demokritos, Greece; Huang, H., Deptartment of Computer Science and Engineering, University of Texas, Arlington, United States",1,,10.1145/2451176.2451185,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84875830088,,,,,32,29,,,,,Scopus,,,,Conference Paper,,,,,,2013,1,2,1,Author3
190,1,1,0,1,Reinforcement-Learning-Based Personalization of Head-Related Transfer Functions,"In order to perceive spatial locations of virtual sounds using stereo headphones, individual head-related transfer functions (HRTFs) are required for each listener. However, accurate HRTF measurement is usually difficult. While previous studies have proposed methods of HRTF personalization without HRTF measurement, localization errors often remain and further modifications are challenging. This research proposes a method that uses reinforcement learning and listener evaluation to obtain an accurate individual HRTF without measurement. The authors conducted a proof-of-concept simulation with an experiment involving human subjects. In the simulation, it was confirmed that the proposed method could acquire individual HRTFs close to the measured dummy-head HRTF. A learning experiment in one direction used the proposed method without individual HRTFs. The results showed improved horizontal-plane localization for the learned HRTF as compared to the dummy-head HRTF. These experiments collectively demonstrate the possibility of the proposed reinforcement-learning-based personalization method for individual HRTFs that enables listeners to experience accurate virtual sound environments.","Accurately perceiving spatial locations of virtual sounds using stereo earphones or headphones requires individual head-related transfer functions (HRTFs) for each listener. However, accurate HRTF measurement is usually difficult. While previous studies have …",,,,,,Author3,Author4,a,a,a,a,a,a,"Other (please in ""remark"")",domain: audio engineering,,,a,a,Journal of the Audio Engineering Society,,http://www.aes.org/e-lib/browse.cfm?elib=19563,,,0,,,Google Scholar,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2018,2018,1,3,0,Author1
191,2,1,0,1,Controlling blood glucose variability under uncertainty using reinforcement learning and Gaussian processes,"Abstract Automated control of blood glucose (BG) concentration with a fully automated artificial pancreas will certainly improve the quality of life for insulin-dependent patients. Closed-loop insulin delivery is challenging due to inter- and intra-patient variability, errors in glucose sensors and delays in insulin absorption. Responding to the varying activity levels seen in outpatients, with unpredictable and unreported food intake, and providing the necessary personalized control for individuals is a challenging task for existing control algorithms. A novel approach for controlling glycemic variability using simulation-based learning is presented. A policy iteration algorithm that combines reinforcement learning with Gaussian process approximation is proposed. To account for multiple sources of uncertainty, a control policy is learned off-line using an Ito's stochastic model of the glucose-insulin dynamics. For safety and performance, only relevant data are sampled through Bayesian active learning. Results obtained demonstrate that a generic policy is both safe and efficient for controlling subject-specific variability due to a patient's lifestyle and its distinctive metabolic response. © 2015 Elsevier B.V.","Abstract Automated control of blood glucose (BG) concentration with a fully automated artificial pancreas will certainly improve the quality of life for insulin-dependent patients. Closed-loop insulin delivery is challenging due to inter- and intra-patient variability, errors in glucose sensors and delays in insulin absorption. Responding to the varying activity levels seen in outpatients, with unpredictable and unreported food intake, and providing the necessary personalized control for individuals is a challenging task for existing control algorithms. A novel approach for controlling glycemic variability using simulation-based learning is presented. A policy iteration algorithm that combines reinforcement learning with Gaussian process approximation is proposed. To account for multiple sources of uncertainty, a control policy is learned off-line using an Ito's stochastic model of the glucose-insulin dynamics. For safety and performance, only relevant data are sampled through Bayesian active learning. Results obtained demonstrate that a generic policy is both safe and efficient for controlling subject-specific variability due to a patient's lifestyle and its distinctive metabolic response. © 2015 Elsevier B.V.",,"INTELYMEC - Centro de Investigaciones en Física e Ingeniería del Centro - CIFICEN - CONICET, Facultad de Ingeniería, Universidad Nacional del Centro de la Provincia de Buenos Aires - UNCPBA, Av. del Valle 5737, Olavarría, Argentina; INGAR (CONICET - UTN), Avellaneda 3657, Santa Fe, Argentina",3038,,Artificial pancreas; Diabetes; Gaussian processes; Policy iteration; Reinforcement learning; Stochastic optimal control,Author1,Author2,a,a,a,a,a,a,Healthcare,,,,a,a,Applied Soft Computing Journal,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937399738&doi=10.1016%2fj.asoc.2015.06.041&partnerID=40&md5=44631a8b252ce7b708616dfd8199006e,"De Paula M., Ávila L.O., Martínez E.C.","De Paula, M., INTELYMEC - Centro de Investigaciones en Física e Ingeniería del Centro - CIFICEN - CONICET, Facultad de Ingeniería, Universidad Nacional del Centro de la Provincia de Buenos Aires - UNCPBA, Av. del Valle 5737, Olavarría, Argentina; Ávila, L.O., INGAR (CONICET - UTN), Avellaneda 3657, Santa Fe, Argentina; Martínez, E.C., INGAR (CONICET - UTN), Avellaneda 3657, Santa Fe, Argentina",3,,10.1016/j.asoc.2015.06.041,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84937399738,,,,,332,310,,,,,Scopus,,,,Article,,,35,,2015,2015,1,2,1,Author1
192,2,0,1,1,Reinforcement learning utilizes proxemics: An avatar learns to manipulate the position of people in immersive virtual reality,"A reinforcement learning (RL) method was used to train a virtual character to move participants to a specified location. The virtual environment depicted an alleyway displayed through a wide field-of-view head-tracked stereo head-mounted display. Based on proxemics theory, we predicted that when the character approached within a personal or intimate distance to the participants, they would be inclined to move backwards out of the way. We carried out a between-groups experiment with 30 female participants, with 10 assigned arbitrarily to each of the following three groups: In the Intimate condition the character could approach within 0.38m and in the Social condition no nearer than 1.2m. In the Random condition the actions of the virtual character were chosen randomly from among the same set as in the RL method, and the virtual character could approach within 0.38m. The experiment continued in each case until the participant either reached the target or 7 minutes had elapsed. The distributions of the times taken to reach the target showed significant differences between the three groups, with 9 out of 10 in the Intimate condition reaching the target significantly faster than the 6 out of 10 who reached the target in the Social condition. Only 1 out of 10 in the Random condition reached the target. The experiment is an example of applied presence theory: we rely on the many findings that people tend to respond realistically in immersive virtual environments, and use this to get people to achieve a task of which they had been unaware. This method opens up the door for many such applications where the virtual environment adapts to the responses of the human participants with the aim of achieving particular goals. © 2012 ACM 1544-3558/2012/03- ART3 ©10.00.","A reinforcement learning (RL) method was used to train a virtual character to move participants to a specified location. The virtual environment depicted an alleyway displayed through a wide field-of-view head-tracked stereo head-mounted display. Based on proxemics theory, we predicted that when the character approached within a personal or intimate distance to the participants, they would be inclined to move backwards out of the way. We carried out a between-groups experiment with 30 female participants, with 10 assigned arbitrarily to each of the following three groups: In the Intimate condition the character could approach within 0.38m and in the Social condition no nearer than 1.2m. In the Random condition the actions of the virtual character were chosen randomly from among the same set as in the RL method, and the virtual character could approach within 0.38m. The experiment continued in each case until the participant either reached the target or 7 minutes had elapsed. The distributions of the times taken to reach the target showed significant differences between the three groups, with 9 out of 10 in the Intimate condition reaching the target significantly faster than the 6 out of 10 who reached the target in the Social condition. Only 1 out of 10 in the Random condition reached the target. The experiment is an example of applied presence theory: we rely on the many findings that people tend to respond realistically in immersive virtual environments, and use this to get people to achieve a task of which they had been unaware. This method opens up the door for many such applications where the virtual environment adapts to the responses of the human participants with the aim of achieving particular goals. © 2012 ACM 1544-3558/2012/03- ART3 ©10.00.",,"University College London, United Kingdom; Facultat de Psicologia, Campus de Mundet - Edifici Teatre, Universitat de Barcelona, Passeig de la Vall d'Hebron 171, 08035 Barcelona, Spain; Facultat de Psicologia, Campus de Mundet - Edifici Teatre, ICREA-Universitat de Barcelona, Passeig de la Vall d'Hebron 171, 08035 Barcelona, Spain",3,,Experimentation; Human Factors,Author1,Author2,a,a,u,a,a,a,"Other (please in ""remark"")",,,,u,a,ACM Transactions on Applied Perception,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859474974&doi=10.1145%2f2134203.2134206&partnerID=40&md5=83e08cb9da9427c493cfb59a2296e9f2,"Kastanis I., Slater M.","Kastanis, I., Facultat de Psicologia, Campus de Mundet - Edifici Teatre, Universitat de Barcelona, Passeig de la Vall d'Hebron 171, 08035 Barcelona, Spain; Slater, M., University College London, United Kingdom, Facultat de Psicologia, Campus de Mundet - Edifici Teatre, ICREA-Universitat de Barcelona, Passeig de la Vall d'Hebron 171, 08035 Barcelona, Spain",5,,10.1145/2134203.2134206,SCOPUS,1,,,,,,,,,,1,,,,2-s2.0-84859474974,,,,,,,,,,,Scopus,,,,Article,,,9,,,2012,1,2,1,Author1
193,2,1,1,1,A Reinforcement Learning Approach to Emotion-based Automatic Playlist Generation,"A novel trend emerged in music exploration is to organize and search songs according to their emotions. However, research on automatic playlist generation (APG) primarily focuses on metadata and audio similarity. Mainstream solutions view APG as a static problem. This paper argues that the APG problem is better modeled as a continuous optimization problem, and proposes an adaptive preference model for personalized APG based on emotions. The main idea is to collect a user's behavior in music playing, e.g., rating, skipping and replaying, as immediate feedback in learning the user's preferences for music emotion within a playlist. Reinforcement learning is adopted to learn the user's current preferences, which are used to generate personalized playlists. Learning parameters are tuned by simulation of two hypothetical users. A two-month user study is conducted to evaluate the APG solutions. The results show that the proposed approach reduces the Miss Ratio by 10% in comparison with the baseline approach.","A novel trend emerged in music exploration is to organize and search songs according to their emotions. However, research on automatic playlist generation (APG) primarily focuses on metadata and audio similarity. Mainstream solutions view APG as a static problem. This paper argues that the APG problem is better modeled as a continuous optimization problem, and proposes an adaptive preference model for personalized APG based on emotions. The main idea is to collect a user's behavior in music playing, e.g., rating, skipping and replaying, as immediate feedback in learning the user's preferences for music emotion within a playlist. Reinforcement learning is adopted to learn the user's current preferences, which are used to generate personalized playlists. Learning parameters are tuned by simulation of two hypothetical users. A two-month user study is conducted to evaluate the APG solutions. The results show that the proposed approach reduces the Miss Ratio by 10% in comparison with the baseline approach.",,,,"Dept. of CSIE, Nat. Taiwan Univ., Taipei, Taiwan",automatic playlist generation;reinforcement learning;song emotion,Author3,Author4,a,a,a,a,a,a,Recommender Systems,,,,a,a,2010 International Conference on Technologies and Applications of Artificial Intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5695433,,C. Y. Chi; R. T. H. Tsai; J. Y. Lai; J. Y. j. Hsu,,1,,10.1109/TAAI.2010.21,IEEE Xplore,1,,20110120,65,,,learning (artificial intelligence);music;optimisation;user interfaces,adaptive preference model;continuous optimization problem;emotion-based automatic playlist generation;music emotion;reinforcement learning;user preference,,2376-6816;23766816,,18-20 Nov. 2010,,,,,,,,,,,,IEEE,20,,Electronic:978-0-7695-4253-9; POD:978-1-4244-8668-7,,60,IEEE Conferences,,,,,2010,2010,0,3,1,Author1
194,2,1,0,1,An Actor-Critic based controller for glucose regulation in type 1 diabetes,"A novel adaptive approach for glucose control in individuals with type 1 diabetes under sensor-augmented pump therapy is proposed. The controller, is based on Actor-Critic (AC) learning and is inspired by the principles of reinforcement learning and optimal control theory. The main characteristics of the proposed controller are (i) simultaneous adjustment of both the insulin basal rate and the bolus dose, (ii) initialization based on clinical procedures, and (iii) real-time personalization. The effectiveness of the proposed algorithm in terms of glycemic control has been investigated in silico in adults, adolescents and children under open-loop and closed-loop approaches, using announced meals with uncertainties in the order of ±25% in the estimation of carbohydrates.The results show that glucose regulation is efficient in all three groups of patients, even with uncertainties in the level of carbohydrates in the meal. The percentages in the A. +. B zones of the Control Variability Grid Analysis (CVGA) were 100% for adults, and 93% for both adolescents and children.The AC based controller seems to be a promising approach for the automatic adjustment of insulin infusion in order to improve glycemic control. After optimization of the algorithm, the controller will be tested in a clinical trial. © 2012 Elsevier Ireland Ltd.","A novel adaptive approach for glucose control in individuals with type 1 diabetes under sensor-augmented pump therapy is proposed. The controller, is based on Actor-Critic (AC) learning and is inspired by the principles of reinforcement learning and optimal control theory. The main characteristics of the proposed controller are (i) simultaneous adjustment of both the insulin basal rate and the bolus dose, (ii) initialization based on clinical procedures, and (iii) real-time personalization. The effectiveness of the proposed algorithm in terms of glycemic control has been investigated in silico in adults, adolescents and children under open-loop and closed-loop approaches, using announced meals with uncertainties in the order of ±25% in the estimation of carbohydrates.The results show that glucose regulation is efficient in all three groups of patients, even with uncertainties in the level of carbohydrates in the meal. The percentages in the A. +. B zones of the Control Variability Grid Analysis (CVGA) were 100% for adults, and 93% for both adolescents and children.The AC based controller seems to be a promising approach for the automatic adjustment of insulin infusion in order to improve glycemic control. After optimization of the algorithm, the controller will be tested in a clinical trial. © 2012 Elsevier Ireland Ltd.",,"ARTORG Center for Biomedical Engineering Research, Diabetes Technology Research Group, University of Bern, Murtenstrasse 50, 3010 Bern, Switzerland; Division of Endocrinology, Diabetes and Clinical Nutrition, University Hospital, Inselspital, University of Bern, 3010 Bern, Switzerland",,,Actor-Critic; Closed-loop control; Glucose control; Reinforcement learning,Author4,Author1,a,a,a,a,a,a,Healthcare,,,,a,a,Computer Methods and Programs in Biomedicine,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873458229&doi=10.1016%2fj.cmpb.2012.03.002&partnerID=40&md5=9fd626af8395921d3a16e711438dc11d,"Daskalaki E., Diem P., Mougiakakou S.G.","Daskalaki, E., ARTORG Center for Biomedical Engineering Research, Diabetes Technology Research Group, University of Bern, Murtenstrasse 50, 3010 Bern, Switzerland; Diem, P., Division of Endocrinology, Diabetes and Clinical Nutrition, University Hospital, Inselspital, University of Bern, 3010 Bern, Switzerland; Mougiakakou, S.G., ARTORG Center for Biomedical Engineering Research, Diabetes Technology Research Group, University of Bern, Murtenstrasse 50, 3010 Bern, Switzerland",6,,10.1016/j.cmpb.2012.03.002,SCOPUS,1,,,,,,,,,,2,,,,2-s2.0-84873458229,,,,,125,116,,,,,Scopus,,,,Article,,,109,,2013,2013,1,2,1,Author1
195,2,0,2,2,A learning multi-agent system for personalized information filtering,"A multi-agent hybrid learning approach to the problem of personalized information filtering is proposed in this paper. There are four agents in the multi-agent model. The problem is modeled as Monte Carlo reinforcement learning. Our proposed algorithm is modified Monte Carlo method combined with features of unsupervised suffix tree clustering and supervised backpropagation network. We argue that this proposed approach could precisely capture the user's interest without repeatedly asking for his/her explicit rates and converge to the user's interest quickly. A conclusion is drawn that our approach is efficient, precise and converges more quickly compared with existing approaches. A prototype system is being developed.","A multi-agent hybrid learning approach to the problem of personalized information filtering is proposed in this paper. There are four agents in the multi-agent model. The problem is modeled as Monte Carlo reinforcement learning. Our proposed algorithm is modified Monte Carlo method combined with features of unsupervised suffix tree clustering and supervised backpropagation network. We argue that this proposed approach could precisely capture the user's interest without repeatedly asking for his/her explicit rates and converge to the user's interest quickly. A conclusion is drawn that our approach is efficient, precise and converges more quickly compared with existing approaches. A prototype system is being developed.",,,,"Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore, Singapore",,Author2,Author3,a,a,a,a,a,a,Recommender Systems,a,,,a,a,"Fourth International Conference on Information, Communications and Signal Processing, 2003 and the Fourth Pacific Rim Conference on Multimedia. Proceedings of the 2003 Joint",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1292790,,Junhua Chen; Zhonghua Yang,,1,,10.1109/ICICS.2003.1292790,IEEE Xplore,1,,20040504,1868 vol.3,,Backpropagation algorithms;Clustering algorithms;Contacts;Information filtering;Information filters;Information retrieval;Monte Carlo methods;Multiagent systems;Search engines;Supervised learning,Internet;Monte Carlo methods;backpropagation;information filters;multi-agent systems,Monte Carlo reinforcement learning;backpropagation network;learning multiagent system;personalized information filtering;suffix tree clustering,,,,15-18 Dec. 2003,,,,,,,,,,,,IEEE,9,,POD:0-7803-8185-8,,1864,IEEE Conferences,,,3,,2003,2003,1,0,3,Author4
196,1,0,2,1,Increasing Retrieval Quality in Conversational Recommenders,"A major task of research in conversational recommender systems is personalization. Critiquing is a common and powerful form of feedback, where a user can express her feature preferences by applying a series of directional critiques over the recommendations instead of providing specific preference values. Incremental Critiquing (IC) is a conversational recommender system that uses critiquing as a feedback to efficiently personalize products. The expectation is that in each cycle the system retrieves the products that best satisfy the user's soft product preferences from a minimal information input. In this paper, we present a novel technique that increases retrieval quality based on a combination of compatibility and similarity scores. Under the hypothesis that a user learns during the recommendation process, we propose two novel exponential Reinforcement Learning (RL) approaches for compatibility that take into account both the instant at which the user makes a critique and the number of satisfied critiques. Moreover, we consider that the impact of features on the similarity differs according to the preferences manifested by the user. We propose a Global Weighting (GW) approach that uses a common weight for nearest cases in order to focus on groups of relevant products. We show that our methodology significantly improves recommendation efficiency in four data sets of different sizes in terms of session length in comparison with state-of-the-art approaches. Moreover, our recommender shows higher robustness against noisy user data when compared to classical approaches.","A major task of research in conversational recommender systems is personalization. Critiquing is a common and powerful form of feedback, where a user can express her feature preferences by applying a series of directional critiques over the recommendations instead of providing specific preference values. Incremental Critiquing (IC) is a conversational recommender system that uses critiquing as a feedback to efficiently personalize products. The expectation is that in each cycle the system retrieves the products that best satisfy the user's soft product preferences from a minimal information input. In this paper, we present a novel technique that increases retrieval quality based on a combination of compatibility and similarity scores. Under the hypothesis that a user learns during the recommendation process, we propose two novel exponential Reinforcement Learning (RL) approaches for compatibility that take into account both the instant at which the user makes a critique and the number of satisfied critiques. Moreover, we consider that the impact of features on the similarity differs according to the preferences manifested by the user. We propose a Global Weighting (GW) approach that uses a common weight for nearest cases in order to focus on groups of relevant products. We show that our methodology significantly improves recommendation efficiency in four data sets of different sizes in terms of session length in comparison with state-of-the-art approaches. Moreover, our recommender shows higher robustness against noisy user data when compared to classical approaches.",,,,"Universitat de Barcelona, Barcelona",Conversational recommender systems;case-based reasoning;critiquing elicitation;personalization.,Author4,Author1,a,a,a,a,a,a,Recommender Systems,,,,a,a,IEEE Transactions on Knowledge and Data Engineering,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5871618,,M. S. Llorente; S. E. Guerrero,,3,,10.1109/TKDE.2011.116,IEEE Xplore,1,,20120817,1888,,Cognition;Current measurement;Learning systems;Monte Carlo methods;Recommender systems;Space exploration,,,,1041-4347;10414347,10,Oct. 2012,,,,,,20110609,,,,,,IEEE,38,,,,1876,IEEE Journals & Magazines,,,24,,2012,2012,1,2,1,Author3
197,2,1,0,1,Dynamic treatment regimes,"A dynamic treatment regime consists of a sequence of decision rules, one per stage of intervention, that dictate how to individualize treatments to patients, based on evolving treatment and covariate history. These regimes are particularly useful for managing chronic disorDers and fit well into the larger paradigm of personalized medicine. They provide one way to operationalize a clinical decision support system. Statistics plays a key role in the construction of evidence-based dynamic treatment regimes-informing the best study design as well as efficient estimation and valid inference. Owing to the many novel methodological challenges this area offers, it has been growing in popularity among statisticians in recent years. In this article, we review the key developments in this exciting field of research. In particular, we discuss the sequential multiple assignment randomized trial designs, estimation techniques like Q-learning and marginal structural models, and several inference techniques designed to address the associated nonstandard asymptotics. We reference software whenever available. We also outline some important future directions. © 2014 by Annual Reviews.","A dynamic treatment regime consists of a sequence of decision rules, one per stage of intervention, that dictate how to individualize treatments to patients, based on evolving treatment and covariate history. These regimes are particularly useful for managing chronic disorDers and fit well into the larger paradigm of personalized medicine. They provide one way to operationalize a clinical decision support system. Statistics plays a key role in the construction of evidence-based dynamic treatment regimes-informing the best study design as well as efficient estimation and valid inference. Owing to the many novel methodological challenges this area offers, it has been growing in popularity among statisticians in recent years. In this article, we review the key developments in this exciting field of research. In particular, we discuss the sequential multiple assignment randomized trial designs, estimation techniques like Q-learning and marginal structural models, and several inference techniques designed to address the associated nonstandard asymptotics. We reference software whenever available. We also outline some important future directions. © 2014 by Annual Reviews.",,"Duke-NUS Graduate Medical School, National University of Singapore, Singapore, Singapore; Department of Statistics and Institute for Social Research, University of Michigan, Ann Arbor, MI 48109, United States",,,Dynamic treatment regime; Nonregularity; Q-learning; Reinforcement learning; Sequential randomization,Author1,Author2,a,a,a,a,a,a,Healthcare,,,,a,a,Annual Review of Statistics and Its Application,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906077445&doi=10.1146%2fannurev-statistics-022513-115553&partnerID=40&md5=ce47ee4c06d6e2fbd95907910f5528cc,"Chakraborty B., Murphy S.A.","Chakraborty, B., Duke-NUS Graduate Medical School, National University of Singapore, Singapore, Singapore; Murphy, S.A., Department of Statistics and Institute for Social Research, University of Michigan, Ann Arbor, MI 48109, United States",24,,10.1146/annurev-statistics-022513-115553,SCOPUS,1,,,,,,,,,,,,,,2-s2.0-84906077445,,,,,464,447,,,,,Scopus,,,,Article,,,1,,2014,2014,1,2,1,Author1
198,1,0,2,1,Dynamic Information Retrieval Modeling,"<p> Big data and human-computer information retrieval (HCIR) are changing IR. They capture the dynamic changes in the data and dynamic interactions of users with IR systems. A dynamic system is one which changes or adapts over time or a sequence of events. Many modern IR systems and data exhibit these characteristics which are largely ignored by conventional techniques. What is missing is an ability for the model to change over time and be responsive to stimulus. Documents, relevance, users and tasks all exhibit dynamic behavior that is captured in data sets typically collected over long time spans and models need to respond to these changes. Additionally, the size of modern datasets enforces limits on the amount of learning a system can achieve. Further to this, advances in IR interface, personalization and ad display demand models that can react to users in real time and in an intelligent, contextual way. </p> <p>In this book we provide a comprehensive and up to-date introduction to Dynamic Information Retrieval Modeling, the statistical modeling of IR systems that can adapt to change. We define <i>dynamics</i>, what it means within the context of IR and highlight examples of problems where dynamics play an important role. We cover techniques ranging from classic relevance feedback to the latest applications of partially observable Markov decision processes (POMDPs) and a handful of useful algorithms and tools for solving IR problems incorporating dynamics. </p> <p>The theoretical component is based around the Markov Decision Process (MDP), a mathematical framework taken from the field of Artificial Intelligence (AI) that enables us to construct models that change according to sequential inputs. We define the framework and the algorithms commonly used to optimize over it and generalize it to the case where the inputs aren't reliable. We explore the topic of reinforcement learning more broadly and introduce nother tool known as a Multi-Armed Bandit which is useful for cases where exploring model parameters is beneficial. Following this we introduce theories and algorithms which can be used to incorporate dynamics into an IR model before presenting an array of state-of-the-art research that already does, such as in the areas of session search and online advertising. </p> <p>Change is at the heart of modern Information Retrieval systems and this book will help equip the reader with the tools and knowledge needed to understand <i>Dynamic Information Retrieval Modeling</i>. </p>","<p> Big data and human-computer information retrieval (HCIR) are changing IR. They capture the dynamic changes in the data and dynamic interactions of users with IR systems. A dynamic system is one which changes or adapts over time or a sequence of events. Many modern IR systems and data exhibit these characteristics which are largely ignored by conventional techniques. What is missing is an ability for the model to change over time and be responsive to stimulus. Documents, relevance, users and tasks all exhibit dynamic behavior that is captured in data sets typically collected over long time spans and models need to respond to these changes. Additionally, the size of modern datasets enforces limits on the amount of learning a system can achieve. Further to this, advances in IR interface, personalization and ad display demand models that can react to users in real time and in an intelligent, contextual way. </p> <p>In this book we provide a comprehensive and up to-date introduction to Dynamic Information Retrieval Modeling, the statistical modeling of IR systems that can adapt to change. We define <i>dynamics</i>, what it means within the context of IR and highlight examples of problems where dynamics play an important role. We cover techniques ranging from classic relevance feedback to the latest applications of partially observable Markov decision processes (POMDPs) and a handful of useful algorithms and tools for solving IR problems incorporating dynamics. </p> <p>The theoretical component is based around the Markov Decision Process (MDP), a mathematical framework taken from the field of Artificial Intelligence (AI) that enables us to construct models that change according to sequential inputs. We define the framework and the algorithms commonly used to optimize over it and generalize it to the case where the inputs aren't reliable. We explore the topic of reinforcement learning more broadly and introduce nother tool known as a Multi-Armed Bandit which is useful for cases where exploring model parameters is beneficial. Following this we introduce theories and algorithms which can be used to incorporate dynamics into an IR model before presenting an array of state-of-the-art research that already does, such as in the areas of session search and online advertising. </p> <p>Change is at the heart of modern Information Retrieval systems and this book will help equip the reader with the tools and knowledge needed to understand <i>Dynamic Information Retrieval Modeling</i>. </p>",,,,,Markov decision process;dynamic information retrieval;information retrieval;information retrieval evaluation;information retrieval models;recommender systems;reinforcement learning,Author1,Author2,a,a,u,a,a,a,Interfaces / HCI,,,,u,a,Dynamic Information Retrieval Modeling,https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=7503460.pdf&bkn=7503459&pdfType=book,,G. H. Yang; M. Sloan; J. Wang,,,2016,10.2200/S00718ED1V01Y201605ICR049,IEEE Xplore,1,,20160706,,,,,,,,,,,,,,,,,,,,,Morgan & Claypool,,,97816270552,,,Morgan and Claypool eBooks,,,,,,2016,1,2,1,Author3
199,0,1,0,0,Adapting difficulty levels in personalized robot-child tutoring interactions,".Social roliots can be used to tutor children in one-on-one interactions. Because students have different learning needs, they consequenUy require complex, non-scripted teaching behaviors that adapt to the learning needs of each child. As a result of this, robot tutors are more effective given a means of adaptively customizing the pace and content of a student's curriculum. In this paper we propose a reinforcement learning-based approach that affords such capabilities to a tutoring robot, with the goals of fostering measurable learning gains and sustained engagement. We outline an architecture in which the robot uses reinforcement learning to adapt the difficulty of its exercises. Further, we describe a proposed study capable of evaluating the effectiveness of our Intelligent Tutoring System. © Copyright 2014, Association for the Advancement of Artificial Intelligence (www.aaia.org). All rights reserved.",".Social roliots can be used to tutor children in one-on-one interactions. Because students have different learning needs, they consequenUy require complex, non-scripted teaching behaviors that adapt to the learning needs of each child. As a result of this, robot tutors are more effective given a means of adaptively customizing the pace and content of a student's curriculum. In this paper we propose a reinforcement learning-based approach that affords such capabilities to a tutoring robot, with the goals of fostering measurable learning gains and sustained engagement. We outline an architecture in which the robot uses reinforcement learning to adapt the difficulty of its exercises. Further, we describe a proposed study capable of evaluating the effectiveness of our Intelligent Tutoring System. © Copyright 2014, Association for the Advancement of Artificial Intelligence (www.aaia.org). All rights reserved.",,"Yale University, 51 Prospect St., New Haven, CT, United States",,,,Author3,Author4,a,a,a,a,a,a,Intelligent Tutors,a,,,a,a,AAAI Workshop - Technical Report,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974815499&partnerID=40&md5=75adeaedcf63bcb3f1c3d765aa90e4d7,"Ramacliandran A., Scassellati B.","Ramacliandran, A., Yale University, 51 Prospect St., New Haven, CT, United States; Scassellati, B., Yale University, 51 Prospect St., New Haven, CT, United States",3,,,SCOPUS,0,,,,,,,,,,,,,,2-s2.0-84974815499,,,,,59,56,,,,,Scopus,,,,Conference Paper,,,WS-14-07,,2014,2014,3,1,0,Author2
200,0,1,0,0,Synergies Between Evolutionary Computation and Multiagent Reinforcement Learning: The Benefits of Exchanging Solutions,"In many real-world situations in which resources are scarce, aligning the optimum of the system with the optimum of agents can be conflicting. For instance, in traffic assignment, the system's and the agents' welfare may not be aligned. In order to deal with this, in this paper a new approach is proposed, based on a synergy between: (i) a global optimization process in which the traffic authority employs metaheuristics, and (ii) reinforcement learning processes that run at each individual driver agent. Both the agents and the system authority exchange solutions that are incorporated by the other party in order to come up with an assignment of routes.","In many real-world situations in which resources are scarce, aligning the optimum of the system with the optimum of agents can be conflicting. For instance, in traffic assignment, the system's and the agents' welfare may not be aligned. In order to deal with this, in this paper a new approach is proposed, based on a synergy between: (i) a global optimization process in which the traffic authority employs metaheuristics, and (ii) reinforcement learning processes that run at each individual driver agent. Both the agents and the system authority exchange solutions that are incorporated by the other party in order to come up with an assignment of routes.",,,,,"evolutionary computation, multiagent systems, reinforcement learning",Author1,Author2,a,a,a,a,a,a,"Other (please in ""remark"")",,Simulation,,a,a,Proceedings of the Genetic and Evolutionary Computation Conference Companion,,,Ana L. C. Bazzan,,,,10.1145/3067695.3075970,ACM-DL,1,,,202,,,,,978-1-4503-4939-0,,,,,,3075970,,,,,,,201--202,,ACM,,,,,201,article,,,,,2017,2017,3,1,0,Author2
201,1,0,1,1,A Contextual Bandits Framework for Personalized Learning Action Selection.,"Recent developments in machine learning have the potential
to revolutionize education by providing an optimized, personalized
learning experience for each student. We study the
problem of selecting the best personalized learning action that
each student should take next given their learning history;
possible actions could include reading a textbook section,
watching a lecture video, interacting with a simulation or lab,
solving a practice question, and so on. We first estimate each
student’s knowledge profile from their binary-valued graded
responses to questions in their previous assessments using
the SPARFA framework. We then employ these knowledge
profiles as contexts in the contextual (multi-armed) bandits
framework to learn a policy that selects the personalized
learning actions that maximize each student’s immediate
success, i.e., their performance on their next assessment.
We develop two algorithms for personalized learning action
selection. While one is mainly of theoretical interest, we
experimentally validate the other using a real-world educational
dataset. Our experimental results demonstrate that
our approach achieves superior or comparable performance
as compared to existing algorithms in terms of maximizing
the students’ immediate success.",,,,,,,Author2,Author3,a,a,a,a,a,a,Intelligent Tutors,a,,,a,a,,http://www.educationaldatamining.org/EDM2016/proceedings/paper_18.pdf,https://dblp.org/rec/conf/edm/LanB16,"Andrew S. Lan, Richard G. Baraniuk",,,,,DBLP,0,,,429,,,,,,,,,,,534724,,,,,,,,,,,,,,424,Conference and Workshop Papers,,,,,2016,2016,1,3,0,Author4
202,0,0,0,0,TacTex'13: A Champion Adaptive Power Trading Agent,"Sustainable energy systems of the future could no longer rely on the current paradigm that energy supply follows demand. Since many of the renewable energy resources do not produce power on demand, there is a need for new market structures that motivate sustainable behaviors by participants. The Power Trading Agent Competition(Power TAC) is a new annual competition that focuses on the design and operation of future retail power markets, specifically in smart grid environments with renewable energy production, smart metering, and autonomous agents acting on behalf of customers and retailers. It uses a rich, open-source simulation platform that is based on real-world data and state-of-the-art customer models. Its purpose is to help researchers understand the dynamics of customer and retailer decision-making, as well as the robustness of proposed market designs. This research contributes to the former, by introducing TacTex'13, the champion agent from the inaugural competition in 2013. TacTex'13 learns and adapts to the environment in which it operates, by heavily relying on reinforcement-learning and prediction methods. We formalize the complex decision-making problem that TacTex'13 faces, and approximate its solution in TacTex'13's constituent components. We examine the success of the complete agent through analysis of competition results.","Sustainable energy systems of the future could no longer rely on the current paradigm that energy supply follows demand. Since many of the renewable energy resources do not produce power on demand, there is a need for new market structures that motivate sustainable behaviors by participants. The Power Trading Agent Competition(Power TAC) is a new annual competition that focuses on the design and operation of future retail power markets, specifically in smart grid environments with renewable energy production, smart metering, and autonomous agents acting on behalf of customers and retailers. It uses a rich, open-source simulation platform that is based on real-world data and state-of-the-art customer models. Its purpose is to help researchers understand the dynamics of customer and retailer decision-making, as well as the robustness of proposed market designs. This research contributes to the former, by introducing TacTex'13, the champion agent from the inaugural competition in 2013. TacTex'13 learns and adapts to the environment in which it operates, by heavily relying on reinforcement-learning and prediction methods. We formalize the complex decision-making problem that TacTex'13 faces, and approximate its solution in TacTex'13's constituent components. We examine the success of the complete agent through analysis of competition results.",,,,,"energy trading, machine learning, smart-grid",Author3,Author4,a,a,u,a,a,a,"Other (please in ""remark"")",a,domain: energy management,There is not real hint that this is personalized I think.,u,a,Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems,,,Daniel  Urieli and Peter  Stone,,,,,ACM-DL,0,,,1448,,,,,978-1-4503-2738-1,,,,,,2617516,,,,,,,1447--1448,,International Foundation for Autonomous Agents and Multiagent Systems,,,,,1447,article,,,,,,2014,4,0,0,Author2
203,1,0,0,0,An Efficient Bandit Algorithm for Realtime Multivariate Optimization,"Optimization is commonly employed to determine the content of web pages, such as to maximize conversions on landing pages or click-through rates on search engine result pages. Often the layout of these pages can be decoupled into several separate decisions. For example, the composition of a landing page may involve deciding which image to show, which wording to use, what color background to display, etc. Thus, optimization is a combinatorial problem over an exponentially large decision space. Randomized experiments do not scale well to this setting, and therefore, in practice, one is typically limited to optimizing a single aspect of a web page at a time. This represents a missed opportunity in both the speed of experimentation and the exploitation of possible interactions between layout decisions.","Optimization is commonly employed to determine the content of web pages, such as to maximize conversions on landing pages or click-through rates on search engine result pages. Often the layout of these pages can be decoupled into several separate decisions. For example, the composition of a landing page may involve deciding which image to show, which wording to use, what color background to display, etc. Thus, optimization is a combinatorial problem over an exponentially large decision space. Randomized experiments do not scale well to this setting, and therefore, in practice, one is typically limited to optimizing a single aspect of a web page at a time. This represents a missed opportunity in both the speed of experimentation and the exploitation of possible interactions between layout decisions.",,,,,"a/b testing, hill-climbing, multi-armed bandit, multivariate optimization",Author1,Author2,a,a,a,a,a,a,"Other (please in ""remark"")",,,,a,a,Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,,,Daniel N. Hill and Houssam  Nassif and Yi  Liu and Anand  Iyer and S.V.N.  Vishwanathan,,,,10.1145/3097983.3098184,ACM-DL,1,,,1821,,,,,978-1-4503-4887-4,,,,,,3098184,,,,,,,1813--1821,,ACM,,,,,1813,article,,,,,2017,2017,3,1,0,Author1
204,1,1,0,1,Learning User Preferences by Observing User-Items Interactions in an IoT Augmented Space,"Recommender systems generate recommendations by analysing which items the user consumes or likes. Moreover, in many scenarios, e.g., when a user is visiting an exhibition or a city, users are faced with a sequence of decisions, and the recommender should therefore suggest, at each decision step, a set of viable recommendations (attractions). In these scenarios the order and the context of the past user choices is a valuable source of data, and the recommender has to effectively exploit this information for understanding the user preferences in order to recommend compelling items.

For addressing these scenarios, this paper proposes a novel preference learning model that takes into account the sequential nature of item consumption. The model is based on Inverse Reinforcement Learning, which enables to exploit observations of users' behaviours, when they are making decisions and taking actions, i.e., choosing the items to consume. The results of a proof of concept experiment show that the proposed model can effectively capture the user preferences, the rationale of users decision making process when consuming items in a sequential manner, and can replicate the observed user behaviours.",,,,,,"cultural heritage, implicit feedback, internet of things, inverse reinforcement learning, museum, recommder systems, reinforcement learning, user modelling",Author2,Author3,a,a,a,a,a,a,Recommender Systems,a,,,a,a,"Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization",,,David  Massimo and Mehdi  Elahi and Francesco  Ricci,,,,10.1145/3099023.3099070,ACM-DL,1,,,40,,,,,978-1-4503-5067-9,,,,,,3099070,,,,,,,35--40,,ACM,,,,,35,article,,,,,2017,2017,1,3,0,Author1
205,1,0,1,1,SciNet: A System for Browsing Scientific Literature Through Keyword Manipulation,"Techniques for both exploratory and known item search tend to direct only to more specific subtopics or individual documents, as opposed to allowing directing the exploration of the information space. We present SciNet, an interactive information retrieval system that combines Reinforcement Learning techniques along with a novel user interface design to allow active engagement of users in directing the search. Users can directly manipulate document features (keywords) to indicate their interests and Reinforcement Learning is used to model the user by allowing the system to trade off between exploration and exploitation. This gives users the opportunity to more effectively direct their search.","Techniques for both exploratory and known item search tend to direct only to more specific subtopics or individual documents, as opposed to allowing directing the exploration of the information space. We present SciNet, an interactive information retrieval system that combines Reinforcement Learning techniques along with a novel user interface design to allow active engagement of users in directing the search. Users can directly manipulate document features (keywords) to indicate their interests and Reinforcement Learning is used to model the user by allowing the system to trade off between exploration and exploitation. This gives users the opportunity to more effectively direct their search.",,,,,"adaptive interfaces, datamining and machine learning, information retrieval, recommender/filtering systems",Author1,Author2,a,a,a,a,a,a,Interfaces / HCI,,,,a,a,Proceedings of the Companion Publication of the 2013 International Conference on Intelligent User Interfaces Companion,,,Dorota  G&#322;owacka and Tuukka  Ruotsalo and Ksenia  Konyushkova and Kumaripaba  Athukorala and Samuel  Kaski and Giulio  Jacucci,,,,10.1145/2451176.2451199,ACM-DL,1,,,62,,,,,978-1-4503-1966-9,,,,,,2451199,,,,,,,61--62,,ACM,,,,,61,article,,,,,2013,2013,1,3,0,Author4
206,0,0,1,0,Directing Exploratory Search: Reinforcement Learning from User Interactions with Keywords,"Techniques for both exploratory and known item search tend to direct only to more specific subtopics or individual documents, as opposed to allowing directing the exploration of the information space. We present an interactive information retrieval system that combines Reinforcement Learning techniques along with a novel user interface design to allow active engagement of users in directing the search. Users can directly manipulate document features (keywords) to indicate their interests and Reinforcement Learning is used to model the user by allowing the system to trade off between exploration and exploitation. This gives users the opportunity to more effectively direct their search nearer, further and following a direction. A task-based user study conducted with 20 participants comparing our system to a traditional query-based baseline indicates that our system significantly improves the effectiveness of information retrieval by providing access to more relevant and novel information without having to spend more time acquiring the information.",,,,,,"adaptive interfaces, data mining, information filtering, machine learning, recommender systems",Author2,Author3,a,a,a,a,a,a,Interfaces / HCI,a,,,a,a,Proceedings of the 2013 International Conference on Intelligent User Interfaces,,,Dorota  Glowacka and Tuukka  Ruotsalo and Ksenia  Konuyshkova and kumaripaba  Athukorala and Samuel  Kaski and Giulio  Jacucci,,,,10.1145/2449396.2449413,ACM-DL,1,,,128,,,,,978-1-4503-1965-2,,,,,,2449413,,,,,,,117--128,,ACM,,,,,117,article,,,,,2013,2013,3,1,0,Author3
207,1,0,0,0,Artificial Intelligence in XPRIZE DeepQ Tricorder,"The DeepQ tricorder device developed by HTC from 2013 to 2016 was entered in the Qualcomm Tricorder XPRIZE competition and awarded the second prize in April 2017. This paper presents DeepQ»s three modules powered by artificial intelligence: symptom checker, optical sense, and vital sense. We depict both their initial design and ongoing enhancements.",,,,,,"artificial intelligence, deepq, medical iot, xprize tricorder",Author2,Author3,u,a,u,a,a,a,"Other (please in ""remark"")",d,Domain seems to be healthcare,,u,a,Proceedings of the 2Nd International Workshop on Multimedia for Personal Health and Health Care,,,Edward Y. Chang and Meng-Hsi  Wu and Kai-Fu Tang  Tang and Hao-Cheng  Kao and Chun-Nan  Chou,,,,10.1145/3132635.3132637,ACM-DL,1,,,18,,,,,978-1-4503-5504-9,,,,,,3132637,,,,,,,11--18,,ACM,,,,,11,article,,,,,,2017,3,1,0,Author1
208,1,1,1,1,DJ-MC: A Reinforcement-Learning Agent for Music Playlist Recommendation,"In recent years, there has been growing 
focus on the study of automated recommender systems. Music 
recommendation systems serve as a prominent domain for such works, both 
from an academic and a commercial perspective. A fundamental aspect of 
music perception is that music is experienced in temporal context and in
 sequence. In this work we present DJ-MC, a novel reinforcement-learning
 framework for music recommendation that does not recommend songs 
individually but rather song sequences, or playlists, based on a model 
of preferences for both songs and song transitions. The model is learned
 online and is uniquely adapted for each listener. To reduce exploration
 time, DJ-MC exploits user feedback to initialize a model, which it 
subsequently updates by reinforcement. We evaluate our framework with 
human participants using both real song and playlist data. Our results 
indicate that DJ-MC's ability to recommend sequences of songs provides a
 significant improvement over more straightforward approaches, which do 
not take transitions into account.
","In recent years, there has been growing 
focus on the study of automated recommender systems. Music 
recommendation systems serve as a prominent domain for such works, both 
from an academic and a commercial perspective. A fundamental aspect of 
music perception is that music is experienced in temporal context and in
 sequence. In this work we present DJ-MC, a novel reinforcement-learning
 framework for music recommendation that does not recommend songs 
individually but rather song sequences, or playlists, based on a model 
of preferences for both songs and song transitions. The model is learned
 online and is uniquely adapted for each listener. To reduce exploration
 time, DJ-MC exploits user feedback to initialize a model, which it 
subsequently updates by reinforcement. We evaluate our framework with 
human participants using both real song and playlist data. Our results 
indicate that DJ-MC's ability to recommend sequences of songs provides a
 significant improvement over more straightforward approaches, which do 
not take transitions into account.
",,,,,,Author3,Author4,a,a,a,a,a,a,Recommender Systems,a,,,a,a,Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,,,Elad  Liebman and Maytal  Saar-Tsechansky and Peter  Stone,,,,,ACM-DL,0,,,599,,,,,978-1-4503-3413-6,,,,,,2772954,,,,,,,591--599,,International Foundation for Autonomous Agents and Multiagent Systems,,,,,591,article,,,,,2015,2015,0,4,0,Author4
209,0,0,0,0,Clustering Household Preferences in Local Electricity Markets,"The current hierarchical, fossil-fuel based energy system is shifting towards a sustainable system based on distributed renewable generation. Simultaneously, energy end consumers become increasingly important as active prosumers. Local electricity markets (LEMs), on which prosumers and consumers can trade electricity locally, enable sustainable, distributed local electricity balances with an active involvement of the end customers. However, trading needs to be automated, and specified to the household's specific preferences in terms of price and electricity source. We show how intelligent agent strategies can fulfill both objectives. To this end, we conduct a multi-agent simulation of a LEM between 100 households and a community storage in a merit order LEM. LEM agents maximize their individual utility via automated Erev-Roth reinforcement learning. The learning strategies take into account the households' individual electricity preferences. To this end, agent preferences are grouped into truly greens, price-sensitive greens, and non adopters. The evaluation of the strategies is based on the agents' revenues, costs and electricity source mix. It shows that reinforcement learning can represent household preferences on LEMs.","The current hierarchical, fossil-fuel based energy system is shifting towards a sustainable system based on distributed renewable generation. Simultaneously, energy end consumers become increasingly important as active prosumers. Local electricity markets (LEMs), on which prosumers and consumers can trade electricity locally, enable sustainable, distributed local electricity balances with an active involvement of the end customers. However, trading needs to be automated, and specified to the household's specific preferences in terms of price and electricity source. We show how intelligent agent strategies can fulfill both objectives. To this end, we conduct a multi-agent simulation of a LEM between 100 households and a community storage in a merit order LEM. LEM agents maximize their individual utility via automated Erev-Roth reinforcement learning. The learning strategies take into account the households' individual electricity preferences. To this end, agent preferences are grouped into truly greens, price-sensitive greens, and non adopters. The evaluation of the strategies is based on the agents' revenues, costs and electricity source mix. It shows that reinforcement learning can represent household preferences on LEMs.",,,,,"Local electricity market, household agents, intelligent agents, reinforcement learning",Author3,Author4,a,a,a,a,a,a,"Other (please in ""remark"")",a,domain: electricity management,,a,a,Proceedings of the Ninth International Conference on Future Energy Systems,,,Esther  Mengelkamp and Christof  Weinhardt,,,,10.1145/3208903.3214348,ACM-DL,1,,,543,,,,,978-1-4503-5767-8,,,,,,3214348,,,,,,,538--543,,ACM,,,,,538,article,,,,,2018,2018,4,0,0,Author3
210,0,0,0,0,Intelligent Agent Strategies for Residential Customers in Local Electricity Markets,10.1145/3208903.3208907,,,,,,"Local electricity market, intelligent agents, reinforcement learning",Author4,Author1,a,a,a,a,a,a,"Other (please in ""remark"")",,Power management.,,a,a,Proceedings of the Ninth International Conference on Future Energy Systems,,,Esther  Mengelkamp and Johannes  G&#228;rttner and Christof  Weinhardt,,,,10.1145/3208903.3208907,ACM-DL,1,,,107,,,,,978-1-4503-5767-8,,,,,,3208907,,,,,,,97--107,,ACM,,,,,97,article,,,,,2018,2018,4,0,0,Author4
211,0,0,0,0,Learning User Preferences for Wireless Services Provisioning,"The problem of interest is how to dynamically allocate wireless access services in a competitive market which implements a take-it-or-leave-it allocation mechanism. In this paper we focus on the subproblem of preference elicitation, given a mechanism. The user, due to a number of cognitive and technical reasons, is assumed to be initially uninformed over their preferences in the wireless domain. The solution we have developed is a closed-loop user-agent system that assists the user in application, task and context dependent service provisioning by adaptively and interactively learning to select the best wireless data service. The agent learns an incrementally revealed user preference model given explicit or implicit feedback on its decisions by the user. We model this closed-loop system as a Markov Decision Process, where the agent actions are rewarded by the user, and show how a reinforcement learning algorithm can be used to learn a model of the userýs preferences on-line in the given allocation mechanism. We evaluate the performance and value of the agent in a series of preliminary empirical user studies.",,,,,,,Author2,Author3,a,a,a,a,a,a,Recommender Systems,a,,,a,a,Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 1,,,G.  Lee and S.  Bauer and P.  Faratin and J.  Wroclawski,,,,10.1109/AAMAS.2004.161,ACM-DL,1,,,487,,,,,1-58113-864-4,,,,,,1018782,,,,,,,480--487,,IEEE Computer Society,,,,,480,article,,,,,2004,2004,4,0,0,Author1
212,0,0,0,0,Demonstrating Cognitive Packet Network Resilience to Worm Attacks,"The need for network stability and reliability has led to the growth of autonomic networks that can provide more stable and more reliable communications via on-line measurement, learning and adaptation. A promising architecture is the Cognitive Packet Network (CPN) that rapidly adapts to varying network conditions and user requirements using QoS driven reinforcement learning algorithms that drive the routing control. Contrary to conventional mechanisms, the users rather than the nodes, control the routing by specifying their desired QoS requirements (QoS Goals), such as Minimum Delay, Maximum Bandwidth, Minimum Cost, etc., and the network then routes each user's traffic individually based on their specific needs and on a ""glocal"" view. In CPN the user has the ability to explore the network for its own needs, and evaluate its own impact on the network as a whole and vice-versa, and then take appropriate decisions. CPN routing has been evaluated extensively under normal operating conditions and has proven to be very adaptive to network changes such as congestion. Here we show how CPN can respond and survive to catastrophic node failures caused by the spread of network worms. This survival is based on two complementary approaches that are run concurrently: one the one hand, each user attempts to concurrently and adaptively avoid paths which are infected, and secondly patching algorithms are continuously run to repair the network. Experiments show that this approach assures the stability of network communications throughout the course of an attack.
",,,,,,"cognitive packet network, network worms, reliability, routing protocols, self-aware networks",Author3,Author4,a,a,a,a,a,a,"Other (please in ""remark"")",,"It is unclear to what extent personalization is applied in scheduling and whether this is a contribution or the background of this paper

domain: packet switching",Seems that users can provide preferences right?,a,a,Proceedings of the 17th ACM Conference on Computer and Communications Security,,,Georgia  Sakellari and Erol  Gelenbe,,,,10.1145/1866307.1866380,ACM-DL,1,,,638,,,,,978-1-4503-0245-6,,,,,,1866380,,,,,,,636--638,,ACM,,,,,636,article,,,,,2010,2010,4,0,0,Author2
213,1,0,2,1,Ad Recommendation Systems for Life-Time Value Optimization,"The main objective in the ad recommendation problem is to find a strategy that, for each visitor of the website, selects the ad that has the highest probability of being clicked. This strategy could be computed using supervised learning or contextual bandit algorithms, which treat two visits of the same user as two separate independent visitors, and thus, optimize greedily for a single step into the future. Another approach would be to use reinforcement learning (RL) methods, which differentiate between two visits of the same user and two different visitors, and thus, optimizes for multiple steps into the future or the life-time value (LTV) of a customer. While greedy methods have been well-studied, the LTV approach is still in its infancy, mainly due to two fundamental challenges: how to compute a good LTV strategy and how to evaluate a solution using historical data to ensure its “safety” before deployment. In this paper, we tackle both of these challenges by proposing to use a family of off-policy evaluation techniques with statistical guarantees about the performance of a new strategy. We apply these methods to a real ad recommendation problem, both for evaluating the final performance and for optimizing the parameters of the RL algorithm. Our results show that our LTV optimization algorithm equipped with these off-policy evaluation techniques outperforms the greedy approaches. They also give fundamental insights on the difference between the click through rate (CTR) and LTV metrics for performance evaluation in the ad recommendation problem.","The main objective in the ad recommendation problem is to find a strategy that, for each visitor of the website, selects the ad that has the highest probability of being clicked. This strategy could be computed using supervised learning or contextual bandit algorithms, which treat two visits of the same user as two separate independent visitors, and thus, optimize greedily for a single step into the future. Another approach would be to use reinforcement learning (RL) methods, which differentiate between two visits of the same user and two different visitors, and thus, optimizes for multiple steps into the future or the life-time value (LTV) of a customer. While greedy methods have been well-studied, the LTV approach is still in its infancy, mainly due to two fundamental challenges: how to compute a good LTV strategy and how to evaluate a solution using historical data to ensure its “safety” before deployment. In this paper, we tackle both of these challenges by proposing to use a family of off-policy evaluation techniques with statistical guarantees about the performance of a new strategy. We apply these methods to a real ad recommendation problem, both for evaluating the final performance and for optimizing the parameters of the RL algorithm. Our results show that our LTV optimization algorithm equipped with these off-policy evaluation techniques outperforms the greedy approaches. They also give fundamental insights on the difference between the click through rate (CTR) and LTV metrics for performance evaluation in the ad recommendation problem.",,,,,"ad recommendation, off-policy evaluation, reinforcement learning",Author1,Author2,a,a,u,a,a,a,Recommender Systems,,,,u,a,Proceedings of the 24th International Conference on World Wide Web,,,Georgios  Theocharous and Philip S. Thomas and Mohammad  Ghavamzadeh,,,,10.1145/2740908.2741998,ACM-DL,1,,,1310,,,,,978-1-4503-3473-0,,,,,,2741998,,,,,,,1305--1310,,ACM,,,,,1305,article,,,,,,2015,1,2,1,Author3
214,2,0,2,2,DRN: A Deep Reinforcement Learning Framework for News Recommendation,10.1145/3178876.3185994,,,,,,"deep Q-Learning, news recommendation, reinforcement learning",Author4,Author1,a,a,a,a,a,a,Recommender Systems,,,,a,a,Proceedings of the 2018 World Wide Web Conference,,,Guanjie  Zheng and Fuzheng  Zhang and Zihan  Zheng and Yang  Xiang and Nicholas Jing  Yuan and Xing  Xie and Zhenhui  Li,,,,10.1145/3178876.3185994,ACM-DL,1,,,176,,,,,978-1-4503-5639-8,,,,,,3185994,,,,,,,167--176,,International World Wide Web Conferences Steering Committee,,,,,167,article,,,,,2018,2018,1,0,3,Author1
215,1,1,1,1,Automatic Computer Game Balancing: A Reinforcement Learning Approach,"Designing agents whose behavior challenges human players adequately is a key issue in computer games development. This work presents a novel technique, based on reinforcement learning (RL), to automatically control the game level, adapting it to the human player skills in order to guarantee a good game balance. RL has commonly been used in competitive environments, in which the agent must perform as well as possible to beat its opponent. The innovative use of RL proposed here makes use of a challenge function, which estimates the current player's level, as well as changes on the action selection mechanism of the RL framework. The technique is applied to a fighting game, Knock'em, to provide empirical validation of the approach.",,,,,,"adaptive agents, game balancing, reinforcement learning",Author2,Author3,a,a,a,a,a,a,Games,a,,,a,a,Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multiagent Systems,,,Gustavo  Andrade and Geber  Ramalho and Hugo  Santana and Vincent  Corruble,,,,10.1145/1082473.1082648,ACM-DL,1,,,1112,,,,,1-59593-093-0,,,,,,1082648,,,,,,,1111--1112,,ACM,,,,,1111,article,,,,,2005,2005,0,4,0,Author1
216,1,0,2,1,Real-Time Robot Personality Adaptation Based on Reinforcement Learning and Social Signals,10.1145/3029798.3038381,,,,,,"adaptation, dialog, introversion/extraversion, personality, reinforcement learning, social robotics, social signals",Author4,Author1,a,a,a,a,a,a,"Other (please in ""remark"")",,Robot,,a,a,Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction,,,Hannes  Ritschel and Elisabeth  Andr&#233;,,,,10.1145/3029798.3038381,ACM-DL,1,,,266,,,,,978-1-4503-4885-0,,,,,,3038381,,,,,,,265--266,,ACM,,,,,265,article,,,,,2017,2017,1,2,1,Author3
217,0,1,0,0,Dynamic Task Allocation Within an Open Service-oriented MAS Architecture,"A MAS architecture consisting of service centers is proposed. Within each service center, a mediator coordinates service delivery by allocating individual tasks to corresponding task specialist agents depending on their prior performance while anticipating performance of newly entering agents. By basing mediator behavior on a novel multicriteria-driven (including quality of service, deadline, reputation, cost, and user preferences) reinforcement learning algorithm, integrating the exploitation of acquired knowledge with optimal, undirected, continual exploration, adaptability to changes in agent availability and performance is ensured. The reported experiments indicate the algorithm behaves as expected and outperforms two standard approaches.",,,,,,"agent architectures, architecture, mediation, multiagent systems, reinforcement learning, task allocation",Author2,Author3,a,a,u,a,a,a,"Other (please in ""remark"")",d,,Task Allocation,u,a,Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems,,,Ivan J. Jureta and Stephane  Faulkner and Youssef  Achbany and Marco  Saerens,,,,10.1145/1329125.1329375,ACM-DL,1,,,209,,,,,978-81-904262-7-5,,,,,,1329375,,,,,,,206:1--206:3,,ACM,,,,,206,article,,,206,,,2007,3,1,0,Author2
218,0,0,1,0,Optimal Testing for Crowd Workers,"Requesters on crowdsourcing platforms, such as Amazon Mechanical Turk, routinely insert gold questions to verify that a worker is diligent and is providing high-quality answers. However, there is no clear understanding of when and how many gold questions to insert. Typically, requesters mix a flat 10-30% of gold questions into the task stream of every worker. This static policy is arbitrary and wastes valuable budget --- the exact percentage is often chosen with little experimentation, and, more importantly, it does not adapt to individual workers, the current mixture of spamming vs. diligent workers, or the number of tasks workers perform before quitting.

We formulate the problem of balancing between (1) testing workers to determine their accuracy and (2) actually getting work performed as a partially-observable Markov decision process (POMDP) and apply reinforcement learning to dynamically calculate the best policy. Evaluations on both synthetic data and with real Mechanical Turk workers show that our agent learns adaptive testing policies that produce up to 111% more reward than the non-adaptive policies used by most requesters. Furthermore, our method is fully automated, easy to apply, and runs mostly out of the box.
",,,,,,"crowdsourcing, reinforcement learning",Author3,Author4,a,a,a,a,a,a,"Other (please in ""remark"")",a,domain: quality control/data collection,Crowd sourcing? Or is that too specific?,a,a,Proceedings of the 2016 International Conference on Autonomous Agents &#38; Multiagent Systems,,,Jonathan  Bragg and   Mausam and Daniel S. Weld,,,,,ACM-DL,0,,,974,,,,,978-1-4503-4239-1,,,,,,2937066,,,,,,,966--974,,International Foundation for Autonomous Agents and Multiagent Systems,,,,,966,article,,,,,2016,2016,3,1,0,Author3
219,1,0,1,1,Learning to Collaborate: Multi-Scenario Ranking via Multi-Agent Reinforcement Learning,"Ranking is a fundamental and widely studied problem in scenarios such as search, advertising, and recommendation. However, joint optimization for multi-scenario ranking, which aims to improve the overall performance of several ranking strategies in different scenarios, is rather untouched. Separately optimizing each individual strategy has two limitations. The first one is lack of collaboration between scenarios meaning that each strategy maximizes its own objective but ignores the goals of other strategies, leading to a sub-optimal overall performance. The second limitation is the inability of modeling the correlation between scenarios meaning that independent optimization in one scenario only uses its own user data but ignores the context in other scenarios. In this paper, we formulate multi-scenario ranking as a fully cooperative, partially observable, multi-agent sequential decision problem. We propose a novel model named Multi-Agent Recurrent Deterministic Policy Gradient (MA-RDPG) which has a communication component for passing messages, several private actors (agents) for making actions for ranking, and a centralized critic for evaluating the overall performance of the co-working actors. Each scenario is treated as an agent (actor). Agents collaborate with each other by sharing a global action-value function (the critic) and passing messages that encodes historical information across scenarios. The model is evaluated with online settings on a large E-commerce platform. Results show that the proposed model exhibits significant improvements against baselines in terms of the overall performance.
",,,,,,"joint optimization, learning to rank, multi-agent learning, reinforcement learning",Author3,Author4,a,a,u,a,a,a,Recommender Systems,,This domain seems to fit best out of the domains currently selected,,u,a,Proceedings of the 2018 World Wide Web Conference,,,Jun  Feng and Heng  Li and Minlie  Huang and Shichen  Liu and Wenwu  Ou and Zhirong  Wang and Xiaoyan  Zhu,,,,10.1145/3178876.3186165,ACM-DL,1,,,1948,,,,,978-1-4503-5639-8,,,,,,3186165,,,,,,,1939--1948,,International World Wide Web Conferences Steering Committee,,,,,1939,article,,,,,,2018,1,3,0,Author1
220,0,0,2,0,Personalizing a Dialogue System With Transfer Reinforcement Learning.,"It is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient. Personalized dialogue systems trained on a small dataset can overfit and make it difficult to adapt to different user needs. One way to solve this problem is to consider a collection of multiple users' data as a source domain and an individual user's data as a target domain, and to perform a transfer learning from the source to the target domain. By following this idea, we propose ""PETAL""(PErsonalized Task-oriented diALogue), a transfer-learning framework based on POMDP to learn a personalized dialogue system. The system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user. This framework can avoid the negative transfer problem by considering differences between source and target users. The policy in the personalized POMDP can learn to choose different actions appropriately for different users. Experimental results on a real-world coffee-shopping data and simulation data show that our personalized dialogue system can choose different optimal actions for different users, and thus effectively improve the dialogue quality under the personalized setting.","It is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient. Personalized dialogue systems trained on a small dataset can overfit and make it difficult to adapt to different user needs. One way to solve this problem is to consider a collection of multiple users' data as a source domain and an individual user's data as a target domain, and to perform a transfer learning from the source to the target domain. By following this idea, we propose ""PETAL""(PErsonalized Task-oriented diALogue), a transfer-learning framework based on POMDP to learn a personalized dialogue system. The system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user. This framework can avoid the negative transfer problem by considering differences between source and target users. The policy in the personalized POMDP can learn to choose different actions appropriately for different users. Experimental results on a real-world coffee-shopping data and simulation data show that our personalized dialogue system can choose different optimal actions for different users, and thus effectively improve the dialogue quality under the personalized setting.",,,,,,Author1,Author2,a,a,a,a,a,a,Personal Assistants,,,,a,a,,https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16104,https://dblp.org/rec/conf/aaai/MoZLLY18,"Kaixiang Mo, Yu Zhang, Shuangyin Li, Jiajun Li, Qiang Yang ",,,,,DBLP,0,,,,,,,,,,,,,,53863,,,,,,,,,,,,,,,Conference and Workshop Papers,,,,,2018,2018,3,0,1,Author3
221,0,1,0,0,Motivated Reinforcement Learning for Non-player Characters in Persistent Computer Game Worlds,10.1145/1178823.1178828,,,,,,"computer games, motivation, persistent virtual worlds, reinforcement learning",Author4,Author1,a,a,u,a,a,a,Games,,"Adapts to new worlds created by the user, would we then consider this personalization?",,u,a,Proceedings of the 2006 ACM SIGCHI International Conference on Advances in Computer Entertainment Technology,,,Kathryn  Merrick and Mary Lou Maher,,,,10.1145/1178823.1178828,ACM-DL,1,,,,,,,,1-59593-380-8,,,,,,1178828,,,,,,,,,ACM,,,,,3,article,,,3,,,2006,3,1,0,Author2
222,1,0,2,1,Modelling User Behaviors with Evolving Users and Catalogs of Evolving Items,10.1145/3099023.3102251,,,,,,"evolving items, evolving users, online learning, recommender systems, reinforcement learning, sequental decision making",Author4,Author1,a,a,a,a,a,a,Recommender Systems,,,,a,a,"Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization",,,Leonardo  Cella,,,,10.1145/3099023.3102251,ACM-DL,1,,,116,,,,,978-1-4503-5067-9,,,,,,3102251,,,,,,,115--116,,ACM,,,,,115,article,,,,,2017,2017,1,2,1,Author3
223,2,0,0,0,Adaptive Cognitive Orthotics: Combining Reinforcement Learning and Constraint-based Temporal Reasoning,10.1145/1015330.1015411,,,,,,,Author4,Author1,a,a,a,a,a,a,Healthcare,,,,a,a,Proceedings of the Twenty-first International Conference on Machine Learning,,,Matthew  Rudary and Satinder  Singh and Martha E. Pollack,,,,10.1145/1015330.1015411,ACM-DL,1,,,,,,,,1-58113-838-5,,,,,,1015411,,,,,,,91--,,ACM,,,,,91,article,,,,,2004,2004,3,0,1,Author1
224,2,0,1,1,Fair Algorithms for Machine Learning,10.1145/3033274.3084096,,,,,,keynote,Author4,Author1,a,a,u,a,a,a,,,In doubt about personalization. Personal factors are taken into account.,,u,a,Proceedings of the 2017 ACM Conference on Economics and Computation,,,Michael  Kearns,,,,10.1145/3033274.3084096,ACM-DL,1,,,1,,,,,978-1-4503-4527-9,,,,,,3084096,,,,,,,1--1,,ACM,,,,,1,article,,,,,,2017,1,2,1,Author1
225,2,0,1,1,Intelligent Adapted e-Learning System Based on Deep Reinforcement Learning,10.1145/3167486.3167574,,,,,,"Decision Support System, Deep Neural network, E-learning, Learning Management System (LMS), Personalized learning, Reinforcement Learning",Author4,Author1,a,a,a,a,a,a,Intelligent Tutors,,,,a,a,Proceedings of the 2Nd International Conference on Computing and Wireless Communication Systems,,,Mohammed  El Fouki and Noura  Aknin and K. Ed El. Kadiri,,,,10.1145/3167486.3167574,ACM-DL,1,,,91,,,,,978-1-4503-5306-9,,,,,,3167574,,,,,,,85:1--85:6,,ACM,,,,,85,article,,,85,,2017,2017,1,2,1,Author1
226,2,0,0,0,Cross Channel Optimized Marketing by Reinforcement Learning,10.1145/1014052.1016912,,,,,,"CRM, cost sensitive learning, customer life time value, reinforcement learning, targeted marketing",Author4,Author1,a,a,a,a,a,a,"Other (please in ""remark"")",,Marketing. Personalization not 100% sure.,,a,a,Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,,,Naoki  Abe and Naval  Verma and Chid  Apte and Robert  Schroko,,,,10.1145/1014052.1016912,ACM-DL,1,,,772,,,,,1-58113-888-1,,,,,,1016912,,,,,,,767--772,,ACM,,,,,767,article,,,,,2004,2004,3,0,1,Author1
227,0,0,1,0,A Learning Interface Agent for Scheduling Meetings,"This paper describes a Learning Interface Agent for a meeting scheduling application. The agent employs Machine Learning techniques to customize itself to the user’s personal scheduling rules and preferences by ob- serving the user’s actions and receiving direct user- feedback. Our approach provides the user with sophis- ticated control over the gradual delegation of schedul- ing tasks to the agent, as a trust relationship is built. We report upon an experiment in which a collection of such assistants became gradually more helpful to their users through the use of memory-based and reinforce- ment learning. The experimental data reported upon demonstrate that the learning approach to building in- telligent interface agents is a very promising one which has several advantages over more standard approaches. ",,,,,,"interface agents, learning interface agents, machine learning, personal assistants, software agents",Author3,Author4,a,a,a,a,a,a,Personal Assistants,a,,,a,a,Proceedings of the 1st International Conference on Intelligent User Interfaces,,,Robyn  Kozierok and Pattie  Maes,,,,10.1145/169891.169908,ACM-DL,1,,,88,,,,,0-89791-556-9,,,,,,169908,,,,,,,81--88,,ACM,,,,,81,article,,,,,1993,1993,3,1,0,Author3
228,0,1,0,0,Developing a Healthcare Robot with Personalized Behaviors and Social Skills for the Elderly,"My PhD research aims to develop a general framework for a behavior 
control architecture that will provide customized interaction between 
the robot and an elderly individual suffering from mild cognitive 
impairment (MCI). This framework will enable the robot to learn from 
past events and to adapt its behavior to the specific needs of the 
person that it interacts with. For this purpose, models will be created 
for both verbal and non-verbal communication. The user profile (e.g., 
personality, cognitive disability level, emotional internal states, 
preferences) will provide the input based on which the robot will adapt 
its behavior.",,,,,,"adaptation, episodic memory, human-robot interaction, learning",Author2,Author3,u,a,a,a,a,a,Personal Assistants,a,,Let's include and check for the use of RL algo during eligiblity check,u,a,The Eleventh ACM/IEEE International Conference on Human Robot Interaction,,,Roxana Madalina Agrigoroaie and Adriana  Tapus,,,,,ACM-DL,0,,,590,,,,,978-1-4673-8370-7,,,,,,2906995,,,,,,,589--590,,IEEE Press,,,,,589,article,,,,,,2016,3,1,0,Author2
229,1,1,1,1,Using Personality Models As Prior Knowledge to Accelerate Learning About Stress-Coping Preferences: (Demonstration),"The management of (dis)stress is an important factor for a long and healthy life. Yet, stress affects people differently and everyone manages stress in different ways. In this paper we introduce PeSA, the Personality-enabled Stress Assistant, an agent-based application that accounts for this individualism. PeSA merges several agent techniques: Reinforcement learning is used to learn about preferences of the users, prior knowledge and knowledge transfer is applied to accelerate the learning process, agent mirroring helps to enable communication and offline functionalities. Based on these mechanisms, PeSA guides through stressful phases by proposing coping strategies that are tailored to the personality of each individual user. Users can assess these advices and thus provide a reward or punishment signal that helps PeSA to improve its suggestions.",,,,,,"human-agent teamwork, human-behaviour models, reinforcement learning",Author3,Author4,a,a,a,a,a,a,Personal Assistants,a,,,a,a,Proceedings of the 2016 International Conference on Autonomous Agents &#38; Multiagent Systems,,,Sebastian  Ahrndt and Marco  L&#252;tzenberger and Stephen M. Prochnow,,,,,ACM-DL,0,,,1487,,,,,978-1-4503-4239-1,,,,,,2937221,,,,,,,1485--1487,,International Foundation for Autonomous Agents and Multiagent Systems,,,,,1485,article,,,,,2016,2016,0,4,0,Author2
230,2,1,1,1,"Reinforcement Learning: The Sooner the Better, or the Later the Better?","Reinforcement Learning (RL) is one of the best machine learning approaches for decision making in interactive environments. RL focuses on inducing effective decision making policies with the goal of maximizing the agent's cumulative reward. In this study, we investigated the impact of both immediate and delayed reward functions on RL-induced policies and empirically evaluated the effectiveness of induced policies within an Intelligent Tutoring System called Deep Thought. Moreover, we divided students into Fast and Slow learners based on their incoming competence as measured by their average response time on the initial tutorial level. Our results show that there was a significant interaction effect between the induced policies and the students' incoming competence. More specifically, Fast learners are less sensitive to learning environments in that they can learn equally well regardless of the pedagogical strategies employed by the tutor, but Slow learners benefit significantly more from effective pedagogical strategies than from ineffective ones. In fact, with effective pedagogical strategies the slow learners learned as much as their faster peers, but with ineffective pedagogical strategies the former learned significantly less than the latter.",,,,,,"delayed reward, immediate reward, pedagogical strategy, problem solving, reinforcement learning, worked example",Author3,Author4,a,a,a,a,a,a,Intelligent Tutors,a,This work seems to only fit our definition of personalization when we view two categories of learners ('fast' and 'slow') sufficiently `personalized'.,"Indeed, I guess it's personalization over groups, which should be fine.",a,a,Proceedings of the 2016 Conference on User Modeling Adaptation and Personalization,,,Shitian  Shen and Min  Chi,,,,10.1145/2930238.2930247,ACM-DL,1,,,44,,,,,978-1-4503-4368-8,,,,,,2930247,,,,,,,37--44,,ACM,,,,,37,article,,,,,2016,2016,0,3,1,Author1
231,0,0,0,0,Designing Intelligent Sales-agent for Online Selling,10.1145/1089551.1089605,,,,,,"abstract argumentation framework, negotiation, persuasion, reinforcement learning, sales-agent",Author4,Author1,a,a,a,a,a,a,Personal Assistants,,E-commerce sales agent,,a,a,Proceedings of the 7th International Conference on Electronic Commerce,,,Shiu-li  Huang and Fu-ren  Lin,,,,10.1145/1089551.1089605,ACM-DL,1,,,286,,,,,1-59593-112-0,,,,,,1089605,,,,,,,279--286,,ACM,,,,,279,article,,,,,2005,2005,4,0,0,Author3
232,0,0,0,0,An Agent-based Model of Regional Food Supply Chain Disintermediation,"Regional  food  hubs  provide  logistics services  for  small  and  mid-sized food  producers,  giving  them  the  ability to reach larger markets and customers than they can reach on their own. However, once food hub managers  have  helped  to  establish  connections  between  producers  and  new  customers,  they  often  find  themselves cut out of the regional food supply chain when the farmers decide to sell their products directly to the customers, thereby avoiding the food hub’s service fees. Widespread disintermediation can eventually lead to food hub failure, which can disrupt the entire regional food system.  This paper describes an agent-based model that incorporates reinforcement learning to study disintermediation behavior in a regional food supply network in Iowa. The model is designed to serve as a decision support tool for food hub managers, allowing them to simulate the effects of various supply chain management strategies on producer decision making and long-term system success. ",,,,,,"agent-based modeling, regional food hubs, reinforcement learning, supply chain management",Author2,Author3,a,a,u,a,a,a,Personal Assistants,d,,Seems to be a system that helps food hub (distribution centers) avoid disintermediation (the phenomenon of locals buying from farmers directly). This seems not to pass our definition of personalization,u,a,Proceedings of the Agent-Directed Simulation Symposium,,,Teri J. Craven and Caroline C. Krejci,,,,,ACM-DL,0,,,18,,,,,,,,,,,3106086,,,,,,,8:1--8:10,,Society for Computer Simulation International,,,,,8,article,,,8,,,2017,4,0,0,Author4
233,0,1,0,0,Developing Learning from Demonstration Techniques for Individuals with Physical Disabilities,"Learning from demonstration research often assumes that the demonstrator
 can quickly give feedback or demonstrations. Individuals with severe 
motor disabilities are often slow and prone to human errors in 
demonstrations while teaching. Our work develops tools to allow persons 
with severe motor disabilities, who stand to benefit most from assistive
 robots, to train these systems. To accommodate slower feedback, we will
 develop a movie-reel style learning from demonstration interface. To 
handle human error, we will use dimensionality reduction to develop new 
reinforcement learning techniques.",,,,,,"human-robot interaction, learning from demonstration",Author2,Author3,a,a,a,a,a,a,Personal Assistants,a,,,a,a,Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction Extended Abstracts,,,William  Curran,,,,10.1145/2701973.2702710,ACM-DL,1,,,234,,,,,978-1-4503-3318-4,,,,,,2702710,,,,,,,233--234,,ACM,,,,,233,article,,,,,2015,2015,3,1,0,Author2
234,1,1,0,1,LEAP: Learning to Prescribe Effective and Safe Treatment Combinations for Multimorbidity,10.1145/3097983.3098109,,,,,,"multi-instance multilabel learning, multimorbidity, treatment recommendation",Author4,Author1,a,a,u,a,a,a,Healthcare,,Personalization does not become clear from the abstract.,,u,a,Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,,,Yutao  Zhang and Robert  Chen and Jie  Tang and Walter F. Stewart and Jimeng  Sun,,,,10.1145/3097983.3098109,ACM-DL,1,,,1324,,,,,978-1-4503-4887-4,,,,,,3098109,,,,,,,1315--1324,,ACM,,,,,1315,article,,,,,,2017,1,3,0,Author2
235,2,1,1,1,"Statistical Methods for Dynamic Treatment Regimes: Reinforcement Learning, Causal Inference, and Personalized Medicine. Vol. 76","Statistical Methods for Dynamic Treatment Regimes shares state of the art of statistical methods developed to address questions of estimation and inference for dynamic treatment regimes, a branch of personalized medicine. This volume demonstrates these methods with their conceptual underpinnings and illustration through analysis of real and simulated data. These methods are immediately applicable to the practice of personalized medicine, which is a medical paradigm that emphasizes the systematic use of individual patient information to optimize patient health care. This is the first single source to provide an overview of methodology and results gathered from journals, proceedings, and technical reports with the goal of orienting researchers to the field. The first chapter establishes context for the statistical reader in the landscape of personalized medicine. Readers need only have familiarity with elementary calculus, linear algebra, and basic large-sample theory to use this text. Throughout the text, authors direct readers to available code or packages in different statistical languages to facilitate implementation. In cases where code does not already exist, the authors provide analytic approaches in sufficient detail that any researcher with knowledge of statistical programming could implement the methods from scratch. This will be an important volume for a wide range of researchers, including statisticians, epidemiologists, medical researchers, and machine learning researchers interested in medical applications. Advanced graduate students in statistics and biostatistics will also find material in Statistical Methods for Dynamic Treatment Regimes to be a critical part of their studies.",,,,,,,Author4,Author1,a,a,a,a,u,a,,,"This is a book in the series 'statistical methods for biology and health'. In general these books are not peer reviewed, although the proposals usually are. ",,u,a,,,,,,2,,,Google Scholar,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2013,0,3,1,Author1
