database,Authors,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,DOI,Link,Affiliations,Authors with affiliations,Abstract,Author Keywords,Document Type,Access Type,Source,EID
SCOPUS,"Pendharkar P.C., Cusatis P.",Trading financial indices with reinforcement learning agents,2018,Expert Systems with Applications,103,,,1,13,,,10.1016/j.eswa.2018.02.032,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043385087&doi=10.1016%2fj.eswa.2018.02.032&partnerID=40&md5=2fd6c8ceb9934f691c18f8f4a04b1f8f,"School of Business Administration, Pennsylvania State University at Harrisburg, Middletown, 777 West Harrisburg PikePA, United States","Pendharkar, P.C., School of Business Administration, Pennsylvania State University at Harrisburg, Middletown, 777 West Harrisburg PikePA, United States; Cusatis, P., School of Business Administration, Pennsylvania State University at Harrisburg, Middletown, 777 West Harrisburg PikePA, United States","Intelligent agents are often used in professional portfolio management. The use of intelligent agents in personal retirement portfolio management is not investigated in the past. In this research, we consider a two-asset personal retirement portfolio and propose several reinforcement learning agents for trading portfolio assets. In particular, we design an on-policy SARSA (λ) and an off-policy Q(λ) discrete state and discrete action agents that maximize either portfolio returns or differential Sharpe ratios. Additionally, we design a temporal-difference learning, TD(λ), agent that uses a linear valuation function in discrete state and continuous action settings. Using two different two-asset portfolios, the first asset being the S&P 500 Index and the second asset being either a broad bond market index or a 10-year U.S. Treasury note (T-note), we test the performance of different agents on different holdout (test) samples. The results of our experiments indicate that the high-learning frequency (i.e., adaptive learning) TD(λ) agent consistently beats both the single asset stock and bond cumulative returns by a significant margin. © 2018 Elsevier Ltd",Markov decision process; Multi-agent systems; Portfolio management; Reinforcement learning,Article,,Scopus,2-s2.0-85043385087
SCOPUS,"Yang M., Tu W., Qu Q., Zhao Z., Chen X., Zhu J.",Personalized response generation by Dual-learning based domain adaptation,2018,Neural Networks,103,,,72,82,,,10.1016/j.neunet.2018.03.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045705590&doi=10.1016%2fj.neunet.2018.03.009&partnerID=40&md5=62c0e7468a872e17eeb031613314833a,"Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; School of Information Management and Engineering, Shanghai University of Finance and Economics, Shanghai, China; School of Computing Science, Zhejiang University, Hangzhou, China; College of Computer Science and Software, Shenzhen University, Shenzhen, China; School of Computer Science, South China Normal University, Guangzhou, China","Yang, M., Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Tu, W., School of Information Management and Engineering, Shanghai University of Finance and Economics, Shanghai, China; Qu, Q., Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Zhao, Z., School of Computing Science, Zhejiang University, Hangzhou, China; Chen, X., College of Computer Science and Software, Shenzhen University, Shenzhen, China; Zhu, J., School of Computer Science, South China Normal University, Guangzhou, China","Open-domain conversation is one of the most challenging artificial intelligence problems, which involves language understanding, reasoning, and the utilization of common sense knowledge. The goal of this paper is to further improve the response generation, using personalization criteria. We propose a novel method called PRGDDA (Personalized Response Generation by Dual-learning based Domain Adaptation) which is a personalized response generation model based on theories of domain adaptation and dual learning. During the training procedure, PRGDDA first learns the human responding style from large general data (without user-specific information), and then fine-tunes the model on a small size of personalized data to generate personalized conversations with a dual learning mechanism. We conduct experiments to verify the effectiveness of the proposed model on two real-world datasets in both English and Chinese. Experimental results show that our model can generate better personalized responses for different users. © 2018 Elsevier Ltd",Deep reinforcement learning; Domain adaptation; Dual learning; Personalized response generation,Article,,Scopus,2-s2.0-85045705590
SCOPUS,"Kim Y.K., Ullah S., Kwon K., Jang Y., Lee J., Hong C.S.",Reinforcement learning based data self-destruction scheme for secured data management,2018,Symmetry,10,5,136,,,,,10.3390/sym10050136,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047266273&doi=10.3390%2fsym10050136&partnerID=40&md5=a2c437f3cefa1531010a989debbacb75,"Department of Computer Science and Engineering, Kyung Hee University, Yongin-si, South Korea; Databank Systems, Daegu, South Korea","Kim, Y.K., Department of Computer Science and Engineering, Kyung Hee University, Yongin-si, South Korea; Ullah, S., Department of Computer Science and Engineering, Kyung Hee University, Yongin-si, South Korea; Kwon, K., Databank Systems, Daegu, South Korea; Jang, Y., Databank Systems, Daegu, South Korea; Lee, J., Databank Systems, Daegu, South Korea; Hong, C.S., Department of Computer Science and Engineering, Kyung Hee University, Yongin-si, South Korea","As technologies and services that leverage cloud computing have evolved, the number of businesses and individuals who use them are increasing rapidly. In the course of using cloud services, as users store and use data that include personal information, research on privacy protection models to protect sensitive information in the cloud environment is becoming more important. As a solution to this problem, a self-destructing scheme has been proposed that prevents the decryption of encrypted user data after a certain period of time using a Distributed Hash Table (DHT) network. However, the existing self-destructing scheme does not mention how to set the number of key shares and the threshold value considering the environment of the dynamic DHT network. This paper proposes a method to set the parameters to generate the key shares needed for the self-destructing scheme considering the availability and security of data. The proposed method defines state, action, and reward of the reinforcement learning model based on the similarity of the graph, and applies the self-destructing scheme process by updating the parameter based on the reinforcement learning model. Through the proposed technique, key sharing parameters can be set in consideration of data availability and security in dynamic DHT network environments. © 2018 by the authors.",DHT network; Privacy protection; Reinforcement learning; Self-destructing scheme,Article,,Scopus,2-s2.0-85047266273
SCOPUS,"Rivas-Blanco I., López-Casado C., Pérez-Del-Pulgar C.J., García-Vacas F., Fraile J.C., Muñoz V.F.",Smart Cable-Driven Camera Robotic Assistant,2018,IEEE Transactions on Human-Machine Systems,48,2,,183,196,,,10.1109/THMS.2017.2767286,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044112146&doi=10.1109%2fTHMS.2017.2767286&partnerID=40&md5=85a18779eea5386bbb2f7ed0bd559205,"Department of System Engineering and Automation, University of Malaga, Andalucía Tech, Málaga, Spain; Department of Mechanical Engineering and Fluid Mechanics, University of Malaga, Andalucía Tech, Málaga, Spain; Department of System Engineering and Automation, University of Valladolid, Valladolid, Spain","Rivas-Blanco, I., Department of System Engineering and Automation, University of Malaga, Andalucía Tech, Málaga, Spain; López-Casado, C., Department of System Engineering and Automation, University of Malaga, Andalucía Tech, Málaga, Spain; Pérez-Del-Pulgar, C.J., Department of System Engineering and Automation, University of Malaga, Andalucía Tech, Málaga, Spain; García-Vacas, F., Department of Mechanical Engineering and Fluid Mechanics, University of Malaga, Andalucía Tech, Málaga, Spain; Fraile, J.C., Department of System Engineering and Automation, University of Valladolid, Valladolid, Spain; Muñoz, V.F., Department of System Engineering and Automation, University of Malaga, Andalucía Tech, Málaga, Spain","This paper describes the mechanical design and a cognition system for a novel concept-of-camera robotic assistant. The system combines the advantages of intra-abdominal devices and autonomous camera navigation. The robotic assistant is composed of a magnetic intra-abdominal camera robot with two internal cable-driven degrees of freedom and an external robot that handles an external magnet. The intelligence of the robot is implemented in a cognitive architecture based on a long-term memory that stores the robot's knowledge and learning capabilities to improve the robot's behavior. The navigation strategy combines a reactive control based on instrument tracking and a proactive control based on predefined behaviors, depending on the actual state of the task. The robot's learning capabilities include a semantic customization, to adapt the camera's behavior to the surgeon's preferences, and reinforcement learning, to improve the camera navigation strategy. This paper details both the hardware implementation of the system and the software implementation in a robotic operating system architecture. The cognition system and the performance of the cable-driven mechanism have been validated with a set of in vitro experiments. Moreover, the camera robot has been evaluated through an in-vivo experiment in a pig. © 2017 IEEE.",Cognitive robotics; mechatronics; medical robotics; robot control; robot motion,Article,,Scopus,2-s2.0-85044112146
SCOPUS,"Zhang Y., Jiang C., Wang J., Han Z., Yuan J., Cao J.",Green Wi-Fi Implementation and Management in Dense Autonomous Environments for Smart Cities,2018,IEEE Transactions on Industrial Informatics,14,4,,1552,1563,,,10.1109/TII.2017.2785820,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039799466&doi=10.1109%2fTII.2017.2785820&partnerID=40&md5=a3e91952410df5430ce58cda71d81e74,"Department of Electrical Engineering, Tsinghua University, Beijing, China; Tsinghua Space Center, Tsinghua University, Beijing, China; University of Houston, Houston, TX, United States; Department of Computer Science and Engineering, Kyung Hee University, Seoul, South Korea; Department of Computing, Hong Kong Polytechnic University, Hung Hom, Hong Kong","Zhang, Y., Department of Electrical Engineering, Tsinghua University, Beijing, China; Jiang, C., Tsinghua Space Center, Tsinghua University, Beijing, China; Wang, J., Department of Electrical Engineering, Tsinghua University, Beijing, China; Han, Z., University of Houston, Houston, TX, United States, Department of Computer Science and Engineering, Kyung Hee University, Seoul, South Korea; Yuan, J., Department of Electrical Engineering, Tsinghua University, Beijing, China; Cao, J., Department of Computing, Hong Kong Polytechnic University, Hung Hom, Hong Kong","Advanced informatics technologies facilitate the construction of green smart cities, especially the Wi-Fi implementation and management, for rapidly increasing personal Wi-Fi devices in autonomous environments residing in nonoverlapped channels often result in low energy efficiency and severe cochannel interference. In this paper, a green Wi-Fi management framework is constructed in order to reduce the overall energy consumption through turning off a portion of access points (APs) and aggregating their users to the other active APs. A Tabu-search-assisted active AP selection algorithm is proposed to minimize the power consumption with a seamless wireless converge. For the active APs, based on our defined metric airtime cost that is integrated by the in-range interference and the hidden terminal interference, a reinforcement-learning-aided AP self-management algorithm is proposed to dynamically adjust APs' channels in the partially overlapped channel space. Extensive simulations and field experiments demonstrate that the power consumption can be reduced by about 65%, and the airtime cost of APs can be reduced by 50% compared with the typical least congestion channel search algorithm. © 2005-2012 IEEE.",Energy efficiency; green Wi-Fi; partially overlapped channels (POCs); self-management,Article,,Scopus,2-s2.0-85039799466
SCOPUS,"Shah A., Ganesan R., Jajodia S., Cam H.",Dynamic optimization of the level of operational effectiveness of a CSOC under adverse conditions,2018,ACM Transactions on Intelligent Systems and Technology,9,5,51,,,,,10.1145/3173457,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047116204&doi=10.1145%2f3173457&partnerID=40&md5=a83cea6dd84e9af765943a9f8f535d02,"Center for Secure Information Systems, George Mason University, 4400 University Dr., Fairfax, VA, United States; Army Research Laboratory, 2800 Powder Mill Road, Adelphi, MD, United States","Shah, A., Center for Secure Information Systems, George Mason University, 4400 University Dr., Fairfax, VA, United States; Ganesan, R., Center for Secure Information Systems, George Mason University, 4400 University Dr., Fairfax, VA, United States; Jajodia, S., Center for Secure Information Systems, George Mason University, 4400 University Dr., Fairfax, VA, United States; Cam, H., Army Research Laboratory, 2800 Powder Mill Road, Adelphi, MD, United States","The analysts at a cybersecurity operations center (CSOC) analyze the alerts that are generated by intrusion detection systems (IDSs). Under normal operating conditions, sufficient numbers of analysts are available to analyze the alert workload. For the purpose of this article, this means that the cybersecurity analysts in each shift can fully investigate each and every alert that is generated by the IDSs in a reasonable amount of time and perform their normal tasks in a shift. Normal tasks include analysis time, time to attend training programs, report writing time, personal break time, and time to update the signatures on new patterns in alerts as detected by the IDS. There are several disruptive factors that occur randomly and can adversely impact the normal operating condition of a CSOC, such as (1) higher alert generation rates from a few IDSs, (2) new alert patterns that decrease the throughput of the alert analysis process, and (3) analyst absenteeism. The impact of the preceding factors is that the alerts wait for a long duration before being analyzed, which impacts the level of operational effectiveness (LOE) of the CSOC. To return the CSOC to normal operating conditions, the manager of a CSOC can take several actions, such as increasing the alert analysis time spent by analysts in a shift by canceling a training program, spending some of his own time to assist the analysts in alert investigation, and calling upon the on-call analyst workforce to boost the service rate of alerts. However, additional resources are limited in quantity over a 14-day work cycle, and the CSOC manager must determine when and how much action to take in the face of uncertainty, which arises from both the intensity and the random occurrences of the disruptive factors. The preceding decision by the CSOC manager is nontrivial and is often made in an ad hoc manner using prior experiences. This work develops a reinforcement learning (RL) model for optimizing the LOE throughout the entire 14-day work cycle of a CSOC in the face of uncertainties due to disruptive events. Results indicate that the RL model is able to assist the CSOCmanager with a decision support tool to make better decisions than current practices in determining when and how much resource to allocate when the LOE of a CSOC deviates from the normal operating condition. © 2018 ACM.",Absenteeism in shift; Allocate resources; Analysts; Average time to analyze alerts; Cybersecurity; Level of operational effectiveness; Oncall analysts; Reinforcement learning; Resource allocation; Stochastic optimization,Article,,Scopus,2-s2.0-85047116204
SCOPUS,"Wu C., Parvate K., Kheterpal N., DIckstein L., Mehta A., Vinitsky E., Bayen A.M.",Framework for control and deep reinforcement learning in traffic,2018,"IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC",2018-March,,,1,8,,,10.1109/ITSC.2017.8317694,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046287181&doi=10.1109%2fITSC.2017.8317694&partnerID=40&md5=c7af52c1314b6153d8d33d15e9c7f463,"UC Berkeley, Electrical Engineering and Computer Science, United States; UCLA, Electrical Engineering, United States; UC Berkeley, Mechanical Engineering, United States; UC Berkeley, Institute for Transportation Studies, United States","Wu, C., UC Berkeley, Electrical Engineering and Computer Science, United States; Parvate, K., UC Berkeley, Electrical Engineering and Computer Science, United States; Kheterpal, N., UC Berkeley, Electrical Engineering and Computer Science, United States; DIckstein, L., UC Berkeley, Electrical Engineering and Computer Science, United States; Mehta, A., UCLA, Electrical Engineering, United States; Vinitsky, E., UC Berkeley, Mechanical Engineering, United States; Bayen, A.M., UC Berkeley, Electrical Engineering and Computer Science, United States, UC Berkeley, Institute for Transportation Studies, United States","Recent advances in deep reinforcement learning (RL) offer an opportunity to revisit complex traffic control problems at the level of vehicle dynamics, with the aim of learning locally optimal policies (with respect to the policy parameterization) for a variety of objectives such as matching a target velocity or minimizing fuel consumption. In this article, we present a framework called CISTAR (Customized Interface for SUMO, TraCI, and RLLab) that integrates the widely used traffic simulator SUMO with a standard deep reinforcement learning library RLLab. We create an interface allowing for easy customization of SUMO, allowing users to easily implement new controllers, heterogeneous experiments, and user-defined cost functions that depend on arbitrary state variables. We demonstrate the usage of CISTAR with several benchmark control and RL examples. © 2017 IEEE.",control; deep reinforcement learning; Simulation; vehicle dynamics,Conference Paper,,Scopus,2-s2.0-85046287181
SCOPUS,"Amravati A., Nasir S.B., Thangadurai S., Yoon I., Raychowdhury A.",A 55nm time-domain mixed-signal neuromorphic accelerator with stochastic synapses and embedded reinforcement learning for autonomous micro-robots,2018,Digest of Technical Papers - IEEE International Solid-State Circuits Conference,61,,,124,126,,1,10.1109/ISSCC.2018.8310215,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046496872&doi=10.1109%2fISSCC.2018.8310215&partnerID=40&md5=795df12235486a5aed039de4d8fe60f8,"Georgia Institute of Technology, Atlanta, GA, United States","Amravati, A., Georgia Institute of Technology, Atlanta, GA, United States; Nasir, S.B., Georgia Institute of Technology, Atlanta, GA, United States; Thangadurai, S., Georgia Institute of Technology, Atlanta, GA, United States; Yoon, I., Georgia Institute of Technology, Atlanta, GA, United States; Raychowdhury, A., Georgia Institute of Technology, Atlanta, GA, United States","Even as rapid advances are being made in the areas of deep neural networks (DNNs) and convolutional neural networks (CNNs) with most hardware demonstrations geared towards inference in vision-based platforms [1-5], we recognize that true autonomy in intelligent agents will only emerge when such bio-mimetic systems can perform continuous learning through interactions with the environment. Reinforcement learning (RL) presents one such computational paradigm inspired by behaviorist psychology, where autonomous agents take actions in an environment to maximize a notion of cumulative reward. This concept is deeply rooted in the human brain where dopamine mediated neurotransmitters (in the cortex, striatum and thalamus of the brain) have been shown to encourage reward-motivated behavior in all our social interactions (Fig. 7.4.1). In this paper, we present a 690μW (VCC=1.2V) neuromorphic accelerator fabricated in 55nm CMOS, which: (1) inherits unique properties of stochastic neural networks, (2) leverages recent advances in Q-learning as an implementation of RL, and (3) demonstrates energy-efficient time-domain mixed-signal (TD-MS) circuit architectures, to provide autonomy to a mobile, self-driving micro-robot at the edge of the cloud, with possible applications in disaster relief, reconnaissance and personal robotics. © 2018 IEEE.",,Conference Paper,,Scopus,2-s2.0-85046496872
SCOPUS,"Hemminghaus J., Kopp S.",Adaptive Behavior Generation for Child-Robot Interaction,2018,ACM/IEEE International Conference on Human-Robot Interaction,,,,295,296,,,10.1145/3173386.3176916,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045246276&doi=10.1145%2f3173386.3176916&partnerID=40&md5=5d60132774050efaff259e83b239d95d,"CITEC, Bielefeld University, Bielefeld, Germany","Hemminghaus, J., CITEC, Bielefeld University, Bielefeld, Germany; Kopp, S., CITEC, Bielefeld University, Bielefeld, Germany","Social robots are increasingly applied in assistive settings where they interact with human users to support them in their daily life. There, abilities for a robust and reliable social interaction are required, especially for robots that interact autonomously with humans. Apart from challenges regarding safety and trust, the complexity and difficulty of attaining mutual understanding, engagement or assistance in social interactions that comprise spoken languages and non-verbal behaviors need to be taken into account. In addition, different users or user groups have inter-individual differences with respect to their personal preferences, skills and limitations. This makes it more difficult to develop reliable and understandable robots that work well in different situations or for different users. © 2018 Authors.",Assistive Robotics; Child-Robot-Interaction; Multimodal Social Behavior; Reinforcement Learning,Conference Paper,,Scopus,2-s2.0-85045246276
SCOPUS,"Ciasullo M.V., Fenza G., Loia V., Orciuoli F., Troisi O., Herrera-Viedma E.",Business process outsourcing enhanced by fuzzy linguistic consensus model,2018,Applied Soft Computing Journal,64,,,436,444,,,10.1016/j.asoc.2017.12.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039847146&doi=10.1016%2fj.asoc.2017.12.020&partnerID=40&md5=978d181b7df8a4116a9a4b237f0e7148,"Dipartimento di Scienze Aziendali – Management & Innovation Systems, University of Salerno, Fisciano (SA), Italy; Department of Computer Science and Artificial Intelligence, University of Granada, Granada, Spain; Department of Electrical and Computer Engineering, Faculty of Engineering, King Abdulaziz University, Jeddah, Saudi Arabia","Ciasullo, M.V., Dipartimento di Scienze Aziendali – Management & Innovation Systems, University of Salerno, Fisciano (SA), Italy; Fenza, G., Dipartimento di Scienze Aziendali – Management & Innovation Systems, University of Salerno, Fisciano (SA), Italy; Loia, V., Dipartimento di Scienze Aziendali – Management & Innovation Systems, University of Salerno, Fisciano (SA), Italy; Orciuoli, F., Dipartimento di Scienze Aziendali – Management & Innovation Systems, University of Salerno, Fisciano (SA), Italy; Troisi, O., Dipartimento di Scienze Aziendali – Management & Innovation Systems, University of Salerno, Fisciano (SA), Italy; Herrera-Viedma, E., Department of Computer Science and Artificial Intelligence, University of Granada, Granada, Spain, Department of Electrical and Computer Engineering, Faculty of Engineering, King Abdulaziz University, Jeddah, Saudi Arabia","Business process outsourcing represents a strategic option to obtain the overall improvement of performance in business process management context. It consists in externalizing whole sub-processes (e.g., production, logistics, human resources) of a value chain. Last decade, the concept of value chain moved toward the more flexible concept of value net that implies the assembly of several value chains tailored to specifics, objectives, markets, etc. Thus, the composition of a value chain within a value net environments can be understood as the modeling of a macro business process in which sub-processes can be outsourced. Such composition activity foresees crucial decision-making moments that need to be sustained by a group of decision-makers owning several and heterogeneous competences in order to select the most suitable external providers to which delegate specific sub-processes. This work proposes a framework to enhance business process outsourcing by introducing group decision-making support that relies on a fuzzy linguistic consensus model. In addition, the framework implements algorithms to learn and assign different weights to decision-makers considering the context and time at which they participate in the group decision making. The framework is applied to an Italian footwear company by describing a numerical example. © 2017 Elsevier B.V.",Business processes outsourcing; Context awareness; Fuzzy linguistic consensus model; Reinforcement learning; Value net,Article,,Scopus,2-s2.0-85039847146
SCOPUS,"Pianese F., Danielsen P.J.",Augmenting practical cross-layer MAC schedulers via offline reinforcement learning,2018,"IEEE International Symposium on Personal, Indoor and Mobile Radio Communications, PIMRC",2017-October,,,1,6,,,10.1109/PIMRC.2017.8292409,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045281550&doi=10.1109%2fPIMRC.2017.8292409&partnerID=40&md5=267e87476de755f33b4843f292418c43,"Nokia Bell Labs, United States","Pianese, F., Nokia Bell Labs, United States; Danielsen, P.J., Nokia Bell Labs, United States","An automated offline design process for optimized cross-layer schedulers can produce augmented scheduling algorithms tailored to a target deployment scenario. We discuss the application of ODDS, a reinforcement learning technique we introduced, for augmenting LTE MAC scheduler algorithms of practical significance. ODDS observes the correlation between the value of a utility function and the cross-layer state parameters seen by an instrumented baseline scheduler, determining the best actions via an offline Monte Carlo exploration of the problem space. The result of the ODDS process is a compact definition of a scheduling policy that has been optimized for a target scenario and utility function. In this paper we instrument a production scheduler definition to evaluate the potential of augmented schedulers in practical use, and experiment with awareness to application traffic properties by using a multi-class utility function, yielding scheduling policies that behave differently depending on the features of individual traffic classes. © 2017 IEEE.",,Conference Paper,,Scopus,2-s2.0-85045281550
SCOPUS,"Pant V., Bhasin S., Jain S.",Self-Learning system for personalized E-Learning,2018,"2017 International Conference on Emerging Trends in Computing and Communication Technologies, ICETCCT 2017",2018-January,,,1,6,,,10.1109/ICETCCT.2017.8280344,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047102774&doi=10.1109%2fICETCCT.2017.8280344&partnerID=40&md5=b1709992e2a8d64608d7bc2cf9965ddd,"Dept. of Computer Science and Engineering, Graphic Era University, Dehradun, India","Pant, V., Dept. of Computer Science and Engineering, Graphic Era University, Dehradun, India; Bhasin, S., Dept. of Computer Science and Engineering, Graphic Era University, Dehradun, India; Jain, S., Dept. of Computer Science and Engineering, Graphic Era University, Dehradun, India","Knowledge is a basic requirement in the era of globalization. The act of acquiring knowledge is learning and with the advent of technology, learning has become easier. E-Learning is a popular learning method which uses the web or other technologies to promote learning. A very traditional learning method is discussed and some new methods with the integration of modern technology have been discussed. Although technologies like artificial intelligence, cloud computing, ontology, fuzzy logic etc. have been used in learning systems they are still less automated, not personalized and do not pose the capability to self-learn. A method is proposed to increase the automation and self-learning in the e-learning management systems. Further, the personalization of learning for the user so that every type of learner can learn from the type and level of the content the learner seems fit is the main aim. The proposed system is feasible, economical and scalable which makes it suitable for every level. © 2017 IEEE.",e-learning system; learning management system; Machine learning; reinforcement learning,Conference Paper,,Scopus,2-s2.0-85047102774
SCOPUS,"Wu G., Ding Y., Li Y., Luo J., Zhang F., Fu J.",Data-driven inverse learning of passenger preferences in urban public transits,2018,"2017 IEEE 56th Annual Conference on Decision and Control, CDC 2017",2018-January,,,5068,5073,,,10.1109/CDC.2017.8264410,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046145794&doi=10.1109%2fCDC.2017.8264410&partnerID=40&md5=7ee01929937ff224417b2a7f23ad844d,"Worcester Polytechnic Institute (WPI), 100 Institute Road, Worcester, MA, United States; Shenzhen Institutes of Advanced Technology, Shenzhen Guangdong, China","Wu, G., Worcester Polytechnic Institute (WPI), 100 Institute Road, Worcester, MA, United States; Ding, Y., Worcester Polytechnic Institute (WPI), 100 Institute Road, Worcester, MA, United States; Li, Y., Worcester Polytechnic Institute (WPI), 100 Institute Road, Worcester, MA, United States; Luo, J., Shenzhen Institutes of Advanced Technology, Shenzhen Guangdong, China; Zhang, F., Shenzhen Institutes of Advanced Technology, Shenzhen Guangdong, China; Fu, J., Worcester Polytechnic Institute (WPI), 100 Institute Road, Worcester, MA, United States","Urban public transit planning is crucial in reducing traffic congestion and enabling green transportation. However, there is no systematic way to integrate passengers' personal preferences in planning public transit routes and schedules so as to achieve high occupancy rates and efficiency gain of ride-sharing. In this paper, we take the first step tp exact passengers' preferences in planning from history public transit data. We propose a data-driven method to construct a Markov decision process model that characterizes the process of passengers making sequential public transit choices, in bus routes, subway lines, and transfer stops/stations. Using the model, we integrate softmax policy iteration into maximum entropy inverse reinforcement learning to infer the passenger's reward function from observed trajectory data. The inferred reward function will enable an urban planner to predict passengers' route planning decisions given some proposed transit plans, for example, opening a new bus route or subway line. Finally, we demonstrate the correctness and accuracy of our modeling and inference methods in a large-scale (three months) passenger-level public transit trajectory data from Shenzhen, China. Our method contributes to smart transportation design and human-centric urban planning. © 2017 IEEE.",,Conference Paper,,Scopus,2-s2.0-85046145794
SCOPUS,"Zhou M., Mintz Y., Fukuoka Y., Goldberg K., Flowers E., Kaminsky P., Castillejo A., Aswani A.",Personalizing mobile fitness apps using reinforcement learning,2018,CEUR Workshop Proceedings,2068,,,,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044520942&partnerID=40&md5=fa48eaf14863091847e69eecc5d02618,"Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Department of Physiological, Nursing Institute for Health and Aging, School of Nursing, University of California, San Francisco, CA, United States; Department of Physiological Nursing, School of Nursing, University of California, San Francisco, CA, United States","Zhou, M., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Mintz, Y., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Fukuoka, Y., Department of Physiological, Nursing Institute for Health and Aging, School of Nursing, University of California, San Francisco, CA, United States; Goldberg, K., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Flowers, E., Department of Physiological Nursing, School of Nursing, University of California, San Francisco, CA, United States; Kaminsky, P., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Castillejo, A., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States; Aswani, A., Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, United States","Despite the vast number of mobile fitness applications (apps) and their potential advantages in promoting physical activity, many existing apps lack behavior-change features and are not able to maintain behavior change motivation. This paper describes a novel fitness app called CalFit, which implements important behavior-change features like dynamic goal setting and self-monitoring. CalFit uses a reinforcement learning algorithm to generate personalized daily step goals that are challenging but attainable. We conducted the Mobile Student Activity Reinforcement (mSTAR) study with 13 college students to evaluate the efficacy of the CalFit app. The control group (receiving goals of 10,000 steps/day) had a decrease in daily step count of 1,520 (SD ± 740) between baseline and 10-weeks, compared to an increase of 700 (SD ± 830) in the intervention group (receiving personalized step goals). The difference in daily steps between the two groups was 2,220, with a statistically significant p = 0:039. © 2018 Copyright for the individual papers remains with the authors.",Fitness app; Goal setting; Interface design; Mobile app; Personalization; Physical activity,Conference Paper,,Scopus,2-s2.0-85044520942
SCOPUS,"Jiang Y., Deng W., Wang J., Zhu B.",Studies on Drivers' Driving Styles Based on Inverse Reinforcement Learning,2018,SAE Technical Papers,2018-April,,,,,,,10.4271/2018-01-0612,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045526693&doi=10.4271%2f2018-01-0612&partnerID=40&md5=b739715ea81417e25a5fbccb4160734d,"Jilin University, China; General Motors LLC, China","Jiang, Y., Jilin University, China; Deng, W., Jilin University, China; Wang, J., General Motors LLC, China; Zhu, B., Jilin University, China","Although advanced driver assistance systems (ADAS) have been widely introduced in automotive industry to enhance driving safety and comfort, and to reduce drivers' driving burden, they do not in general reflect different drivers' driving styles or customized with individual personalities. This can be important to comfort and enjoyable driving experience, and to improved market acceptance. However, it is challenging to understand and further identify drivers' driving styles due to large number and great variations of driving population. Previous research has mainly adopted physical approaches in modeling drivers' driving behavior, which however are often very much limited, if not impossible, in capturing human drivers' driving characteristics. This paper proposes a reinforcement learning based approach, in which the driving styles are formulated through drivers' learning processes from interaction with surrounding environment. Based on the reinforcement learning theory, driving action can be treated as maximizing a reward function. Instead of calibrating the unknown reward function to satisfy driver's desired response, we try to recover it from the human driving data, utilizing maximum likelihood inverse reinforcement learning (MLIRL). An IRL-based longitudinal driving assistance system is also proposed in this paper. Firstly, large amount of real world driving data is collected from a test vehicle, and the data is split into two sets for training and for testing purposes respectively. Then, the longitudinal acceleration is modeled as a Boltzmann distribution in human driving activity. The reward function is denoted as a linear combination of some kernelized basis functions. The driving style parameter vector is estimated using MLIRL based on the training set. Finally, a learning-based longitudinal driving assistance algorithm is developed and evaluated on the testing set. The results demonstrate that the proposed method can satisfactorily reflect human drivers' driving behavior. © 2018 SAE International; General Motors LLC.",,Conference Paper,,Scopus,2-s2.0-85045526693
SCOPUS,"Bhattacharjee D., Paul A., Kim J.H., Karthigaikumar P.",An immersive learning model using evolutionary learning,2018,Computers and Electrical Engineering,65,,,236,249,,1,10.1016/j.compeleceng.2017.08.023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032829128&doi=10.1016%2fj.compeleceng.2017.08.023&partnerID=40&md5=e276a65771f4540bb5bcd68a3cc6b0e1,"Department of Computer Science and Engineering, Kyungpook National University, Daegu, South Korea; Department of Electronics and Telecommunication Engineering, Karpagam College of Engineering, Coimbatore, TN, India","Bhattacharjee, D., Department of Computer Science and Engineering, Kyungpook National University, Daegu, South Korea; Paul, A., Department of Computer Science and Engineering, Kyungpook National University, Daegu, South Korea; Kim, J.H., Department of Computer Science and Engineering, Kyungpook National University, Daegu, South Korea; Karthigaikumar, P., Department of Electronics and Telecommunication Engineering, Karpagam College of Engineering, Coimbatore, TN, India","In this article, we have proposed an educational model using virtual reality on a mobile platform by personalizing the simulated environments as per user actions. We have also proposed an evolutionary learning algorithm based on which the user learning path is designed and the corresponding simulated learning environment is modified. The main objective of this study is to create a personalized learning path for each student as per their calibre and make the learning immersive and retainable using virtual reality. Our proposed model emulates the innate natural learning process in humans and uses that to customize the virtual simulations of the lessons by applying the evolutionary learning technique. A quasi-experimental study is conducted by taking different case studies to establish the effectiveness of our learning model. The results show that our learning model is immersive and gives long term retention while enhancing creativity through reinforced customization of the simulations. © 2017 Elsevier Ltd",Education; Evolutionary learning; Immersive learning; Immersive virtual reality; m-learning; Personalized learning; Reinforcement learning,Article,,Scopus,2-s2.0-85032829128
SCOPUS,"Shawky D., Badawi A.",A Reinforcement Learning-Based Adaptive Learning System,2018,Advances in Intelligent Systems and Computing,723,,,221,231,,,10.1007/978-3-319-74690-6_22,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041837617&doi=10.1007%2f978-3-319-74690-6_22&partnerID=40&md5=7086141d27c694a3b289e9ba955acc61,"Center for Learning Technologies, University of Science and Technology, Zewail City, Giza, Egypt","Shawky, D., Center for Learning Technologies, University of Science and Technology, Zewail City, Giza, Egypt; Badawi, A., Center for Learning Technologies, University of Science and Technology, Zewail City, Giza, Egypt","With the plethora of educational and e-learning systems and the great variation in students’ personal and social factors that affect their learning behaviors and outcomes, it has become mandatory for all educational systems to adapt to the variability of these factors for each student. Since there is a large number of factors that need to be taken into consideration, the task is very challenging. In this paper, we present an approach that adapts to the most influential factors in a way that varies from one learner to another, and in different learning settings, including individual and collaborative learning. The approach utilizes reinforcement learning for building an intelligent environment that, not only provides a method for suggesting suitable learning materials, but also provides a methodology for accounting for the continuously-changing students’ states and acceptance of technology. We evaluate our system through simulations. The obtained results are promising and show the feasibility of the proposed approach. © 2018, Springer International Publishing AG.",Adaptive learning; Computer-supported collaborative learning; Reinforcement Learning,Conference Paper,,Scopus,2-s2.0-85041837617
SCOPUS,"Triki S., Hanachi C.",A self-adaptive system for improving autonomy and public spaces accessibility for elderly,2018,"Smart Innovation, Systems and Technologies",74,,,53,66,,,10.1007/978-3-319-59394-4_6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020375682&doi=10.1007%2f978-3-319-59394-4_6&partnerID=40&md5=e06d5cd071339abef3f12d85ab89af53,"118 Route de Narbonne, Toulouse, France; 2 Rue du Doyen-Gabriel-Marty, Toulouse, France","Triki, S., 118 Route de Narbonne, Toulouse, France; Hanachi, C., 2 Rue du Doyen-Gabriel-Marty, Toulouse, France","Nowadays, there is an increasing need to provide a safe and independent living for cognitively deficient population. Notably, we have to improve seniors’ autonomy and their public spaces accessibility. Giving these observations, the aim of this paper is to provide a personalized adaptive assisting system for elderly. More precisely, this paper presents the specification and implementation of a self-organizing multi-agent system able to abstract the different distributed components involved in user’s environment. This system is able to detect different possible situations that a user could face in his daily outdoors activities and propose accordingly appropriate actions. This system not only learns user’s habits from its perceptions but also improves its recommendations thanks to feedbacks provided by stakeholders (family, doctors …) following a reinforcement learning reasoning. Finally, we present our system evaluation specially its learning capabilities through different scenarios that have been generated automatically. © Springer International Publishing AG 2018.",AMAS theory; Assisted living system; Multi-agent system; Reinforcement learning,Conference Paper,,Scopus,2-s2.0-85020375682
SCOPUS,"Lee C.S., Cho S.B.",Learning classifier systems for adaptive learning of intrusion detection system,2018,Advances in Intelligent Systems and Computing,649,,,557,566,,,10.1007/978-3-319-67180-2_54,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028680808&doi=10.1007%2f978-3-319-67180-2_54&partnerID=40&md5=c0423f92b3d8090bc33934e9e4184701,"Department of Computer Science, Yonsei University, 50Yonsei-Ro, Seodaemun-Gu, Seoul, South Korea","Lee, C.S., Department of Computer Science, Yonsei University, 50Yonsei-Ro, Seodaemun-Gu, Seoul, South Korea; Cho, S.B., Department of Computer Science, Yonsei University, 50Yonsei-Ro, Seodaemun-Gu, Seoul, South Korea","Relational databases contain information that must be protected such as personal information, the problem of intrusion detection of relational database is considered important. Also, the pattern of attacks evolves and it is difficult to grasp by rule-based method or general machine learning, so adaptive learning is needed. Learning classifier systems are system that combines supervised learning, reinforcement learning and evolutionary computation. It creates and updates classifiers according to data input. Learning classifier systems can learn adaptive because they generate and evaluate classifiers in real time. In this paper, we apply accuracy based learning classifier systems to relational database and confirm that adaptive learning is possible. Also, we confirmed their practical usability that they close to the best accuracy, though were not the best. © 2018, Springer International Publishing AG.",Anomaly detection; Database; Learning classifier systems; SQL query,Conference Paper,,Scopus,2-s2.0-85028680808
SCOPUS,"Curran W., Pocius R., Smart W.D.",Neural networks for incremental dimensionality reduced reinforcement learning,2017,IEEE International Conference on Intelligent Robots and Systems,2017-September,,8205962,1559,1565,,,10.1109/IROS.2017.8205962,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041961677&doi=10.1109%2fIROS.2017.8205962&partnerID=40&md5=ee1da89d48dbe32e42c707c9bac096c3,"Oregon State University, Corvallis, United States","Curran, W., Oregon State University, Corvallis, United States; Pocius, R., Oregon State University, Corvallis, United States; Smart, W.D., Oregon State University, Corvallis, United States","State-of-the-art personal robots must perform complex manipulation tasks to be viable in assistive scenarios. However, many of these robots, like the PR2, use manipulators with high degrees-of-freedom. The complexity of these robots lead to large dimensional state spaces, which are difficult to fully explore. Our previous work introduced the IDRRL algorithm, which compresses the learning space by transforming a high-dimensional learning space onto a lower-dimensional manifold while preserving expressivity. In this work we formally prove that IDRRL maintains PAC-MDP guarantees. We then improve upon our previous formulation of IDRRL by introducing cascading autoencoders (CAE) for dimensionality reduction, producing the new algorithm IDRRL-CAE. We demonstrate the improvement of this extension over our previous formulation, IDRRL-PCA, in the Mountain Car and Swimmers domains. © 2017 IEEE.",,Conference Paper,,Scopus,2-s2.0-85041961677
SCOPUS,"Tseng H.-H., Luo Y., Cui S., Chien J.-T., Ten Haken R.K., Naqa I.E.",Deep reinforcement learning for automated radiation adaptation in lung cancer:,2017,Medical Physics,44,12,,6690,6705,,2,10.1002/mp.12625,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037829216&doi=10.1002%2fmp.12625&partnerID=40&md5=64aadbb97a2f33ad182430a659f43b97,"Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States; Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan","Tseng, H.-H., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States; Luo, Y., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States; Cui, S., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States; Chien, J.-T., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States, Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan; Ten Haken, R.K., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States; Naqa, I.E., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States","Purpose: To investigate deep reinforcement learning (DRL) based on historical treatment plans for developing automated radiation adaptation protocols for nonsmall cell lung cancer (NSCLC) patients that aim to maximize tumor local control at reduced rates of radiation pneumonitis grade 2 (RP2). Methods: In a retrospective population of 114 NSCLC patients who received radiotherapy, a three-component neural networks framework was developed for deep reinforcement learning (DRL) of dose fractionation adaptation. Large-scale patient characteristics included clinical, genetic, and imaging radiomics features in addition to tumor and lung dosimetric variables. First, a generative adversarial network (GAN) was employed to learn patient population characteristics necessary for DRL training from a relatively limited sample size. Second, a radiotherapy artificial environment (RAE) was reconstructed by a deep neural network (DNN) utilizing both original and synthetic data (by GAN) to estimate the transition probabilities for adaptation of personalized radiotherapy patients' treatment courses. Third, a deep Q-network (DQN) was applied to the RAE for choosing the optimal dose in a response-adapted treatment setting. This multicomponent reinforcement learning approach was benchmarked against real clinical decisions that were applied in an adaptive dose escalation clinical protocol. In which, 34 patients were treated based on avid PET signal in the tumor and constrained by a 17.2% normal tissue complication probability (NTCP) limit for RP2. The uncomplicated cure probability (P+) was used as a baseline reward function in the DRL. Results: Taking our adaptive dose escalation protocol as a blueprint for the proposed DRL (GAN + RAE + DQN) architecture, we obtained an automated dose adaptation estimate for use at ∼2/3 of the way into the radiotherapy treatment course. By letting the DQN component freely control the estimated adaptive dose per fraction (ranging from 1-5 Gy), the DRL automatically favored dose escalation/de-escalation between 1.5 and 3.8 Gy, a range similar to that used in the clinical protocol. The same DQN yielded two patterns of dose escalation for the 34 test patients, but with different reward variants. First, using the baseline P+ reward function, individual adaptive fraction doses of the DQN had similar tendencies to the clinical data with an RMSE = 0.76 Gy; but adaptations suggested by the DQN were generally lower in magnitude (less aggressive). Second, by adjusting the P+ reward function with higher emphasis on mitigating local failure, better matching of doses between the DQN and the clinical protocol was achieved with an RMSE = 0.5 Gy. Moreover, the decisions selected by the DQN seemed to have better concordance with patients eventual outcomes. In comparison, the traditional temporal difference (TD) algorithm for reinforcement learning yielded an RMSE = 3.3 Gy due to numerical instabilities and lack of sufficient learning. Conclusion: We demonstrated that automated dose adaptation by DRL is a feasible and a promising approach for achieving similar results to those chosen by clinicians. The process may require customization of the reward function if individual cases were to be considered. However, development of this framework into a fully credible autonomous system for clinical decision support would require further validation on larger multi-institutional datasets. © 2017 American Association of Physicists in Medicine.",adaptive radiotherapy; deep learning; lung cancer; reinforcement learning,Article,,Scopus,2-s2.0-85037829216
SCOPUS,"Horzyk A., Starzyk J.A., Graham J.",Integration of Semantic and Episodic Memories,2017,IEEE Transactions on Neural Networks and Learning Systems,28,12,8008846,3084,3095,,2,10.1109/TNNLS.2017.2728203,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028959432&doi=10.1109%2fTNNLS.2017.2728203&partnerID=40&md5=674c65eec0a85fe1bd2531d4a8704e97,"Department of Automatics and Biomedical Engineering, AGH University of Science and Technology, Krakow, Poland; School of Electrical Engineering and Computer Science, Stocker Center, Ohio University, Athens, OH, United States; University of Information Technology and Management, Rzeszow, Poland; Riverside Research, Dayton, OH, United States","Horzyk, A., Department of Automatics and Biomedical Engineering, AGH University of Science and Technology, Krakow, Poland; Starzyk, J.A., School of Electrical Engineering and Computer Science, Stocker Center, Ohio University, Athens, OH, United States, University of Information Technology and Management, Rzeszow, Poland; Graham, J., Riverside Research, Dayton, OH, United States","This paper describes the integration of semantic and episodic memory (EM) models and the benefits of such integration. Semantic memory (SM) is used as a foundation of knowledge and concept learning, and is needed for the operation of any cognitive system. EM retains personal experiences stored based on their significance - it is supported by the SM, and in return, it supports SM operations. Integrated declarative memories are critical for cognitive system development, yet very little research has been done to develop their computational models. We considered structural self-organization of both semantic and episodic memories with a symbolic representation of input events. Sequences of events are stored in EM and are used to build associations in SM. We demonstrated that integration of semantic and episodic memories improves the native operation of both types of memories. Experimental results are presented to illustrate how the two memories complement each other by improving recognition, prediction, and context-based generalization of individual memories. © 2012 IEEE.",Cognitive system; episodic memory (EM); event significance; motivated and reinforcement learning; semantic memory (SM),Article,,Scopus,2-s2.0-85028959432
SCOPUS,"Chasparis G.C., Rossbory M.",Efficient Dynamic Pinning of Parallelized Applications by Distributed Reinforcement Learning,2017,International Journal of Parallel Programming,,,,1,15,,,10.1007/s10766-017-0541-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035118833&doi=10.1007%2fs10766-017-0541-y&partnerID=40&md5=37f483cb6f54fd0fcddba092af0a2eec,"Software Competence Center Hagenberg GmbH, Softwarepark 21, Hagenberg, Austria","Chasparis, G.C., Software Competence Center Hagenberg GmbH, Softwarepark 21, Hagenberg, Austria; Rossbory, M., Software Competence Center Hagenberg GmbH, Softwarepark 21, Hagenberg, Austria","This paper introduces a resource allocation framework specifically tailored for addressing the problem of dynamic placement (or pinning) of parallelized applications to processing units. Under the proposed setup each thread of the parallelized application constitutes an independent decision maker (or agent), which (based on its own prior performance measurements and its own prior CPU-affinities) decides on which processing unit to run next. Decisions are updated recursively for each thread by a resource manager/scheduler which runs in parallel to the application’s threads and periodically records their performances and assigns to them new CPU affinities. For updating the CPU-affinities, the scheduler uses a distributed reinforcement-learning algorithm, each branch of which is responsible for assigning a new placement strategy to each thread. The proposed framework is flexible enough to address alternative optimization criteria, such as maximum average processing speed and minimum speed variance among threads. We demonstrate analytically that convergence to locally-optimal placements is achieved asymptotically. Finally, we validate these results through experiments in Linux platforms. © 2017 Springer Science+Business Media, LLC, part of Springer Nature",Dynamic pinning; Parallel applications; Reinforcement learning,Article in Press,,Scopus,2-s2.0-85035118833
SCOPUS,[No author name available],HAI 2017 - Proceedings of the 5th International Conference on Human Agent Interaction,2017,HAI 2017 - Proceedings of the 5th International Conference on Human Agent Interaction,,,,,,535,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034806063&partnerID=40&md5=565d4f0d978b019b4194b3d7d52961b9,,,The proceedings contain 77 papers. The topics discussed include: investigating design implications towards a social robot as a memory trainer; proposal of a parameterized atmosphere generation model in a virtual classroom; indirect control of user's e-learning motivation by controlling activity ratio of multiple agents; using rapid prototyping to explore design implications for a pill-dispensing social agent; autonomous self-explanation of behavior for interactive reinforcement learning agents; active perception based on energy minimization in multimodal human-robot interaction; comparison of behaviour-based architectures for a collaborative package delivery task; adaptive behavior generation for conversational robot in human-robot negotiation environment; the impact of personalisation on human-robot interaction in learning scenarios; prediction of next-utterance timing using head movement in multi-party meetings; designing emotionally expressive robots: a comparative study on the perception of communication modalities; trust lengthens decision time on unexpected recommendations in human-agent interaction; investigation of approach to others for modeling of physical interaction by communication needs; effect of an agent's contingent responses on maintaining an intentional stance; collaborative robots learning spatial language for picking and placing objects on a tables; and impact of spontaneous human inputs during gesture based interaction on a real-world manufacturing scenario.,,Conference Review,,Scopus,2-s2.0-85034806063
SCOPUS,"Yom-Tov E., Feraru G., Kozdoba M., Mannor S., Tennenholtz M., Hochberg I.",Encouraging Physical Activity in Patients With Diabetes: Intervention Using a Reinforcement Learning System,2017,Journal of medical Internet research,19,10,,e338,,,,10.2196/jmir.7994,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042765009&doi=10.2196%2fjmir.7994&partnerID=40&md5=d20112fde48afda622fd704f8b2ae508,"Microsoft Research, Herzeliya, Israel; Technion - Israel Institute of Technology, Faculty of Medicine, Haifa, Israel; Technion - Israel Institute of Technology, Faculty of Electrical Engineering, Haifa, Israel; Technion - Israel Institute of Technology, Faculty of Industrial Engineering, Haifa, Israel; Rambam Healthcare Campus, Institute of Endocrinology, Haifa, Israel","Yom-Tov, E., Microsoft Research, Herzeliya, Israel; Feraru, G., Technion - Israel Institute of Technology, Faculty of Medicine, Haifa, Israel; Kozdoba, M., Technion - Israel Institute of Technology, Faculty of Electrical Engineering, Haifa, Israel; Mannor, S., Technion - Israel Institute of Technology, Faculty of Electrical Engineering, Haifa, Israel; Tennenholtz, M., Technion - Israel Institute of Technology, Faculty of Industrial Engineering, Haifa, Israel; Hochberg, I., Rambam Healthcare Campus, Institute of Endocrinology, Haifa, Israel","BACKGROUND: Regular physical activity is known to be beneficial for people with type 2 diabetes. Nevertheless, most of the people who have diabetes lead a sedentary lifestyle. Smartphones create new possibilities for helping people to adhere to their physical activity goals through continuous monitoring and communication, coupled with personalized feedback.OBJECTIVE: The aim of this study was to help type 2 diabetes patients increase the level of their physical activity.METHODS: We provided 27 sedentary type 2 diabetes patients with a smartphone-based pedometer and a personal plan for physical activity. Patients were sent short message service messages to encourage physical activity between once a day and once per week. Messages were personalized through a Reinforcement Learning algorithm so as to improve each participant's compliance with the activity regimen. The algorithm was compared with a static policy for sending messages and weekly reminders.RESULTS: Our results show that participants who received messages generated by the learning algorithm increased the amount of activity and pace of walking, whereas the control group patients did not. Patients assigned to the learning algorithm group experienced a superior reduction in blood glucose levels (glycated hemoglobin [HbA1c]) compared with control policies, and longer participation caused greater reductions in blood glucose levels. The learning algorithm improved gradually in predicting which messages would lead participants to exercise.CONCLUSIONS: Mobile phone apps coupled with a learning algorithm can improve adherence to exercise in diabetic patients. This algorithm can be used in large populations of diabetic patients to improve health and glycemic control. Our results can be expanded to other areas where computer-led health coaching of humans may have a positive impact. Summary of a part of this manuscript has been previously published as a letter in Diabetes Care, 2016.",diabetes type 2; physical activity; reinforcement learning,Article,,Scopus,2-s2.0-85042765009
SCOPUS,"Shao S., Luk W.",Customised pearlmutter propagation: A hardware architecture for trust region policy optimisation,2017,"2017 27th International Conference on Field Programmable Logic and Applications, FPL 2017",,,8056789,,,,,10.23919/FPL.2017.8056789,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034441518&doi=10.23919%2fFPL.2017.8056789&partnerID=40&md5=b8efcfd4d78398b6af5efd6be7f4f85f,"Department of Computing, Imperial College London, United Kingdom","Shao, S., Department of Computing, Imperial College London, United Kingdom; Luk, W., Department of Computing, Imperial College London, United Kingdom","Reinforcement Learning (RL) is an area of machine learning in which an agent interacts with the environment by making sequential decisions. The agent receives reward from the environment to find an optimal policy that maximises the reward. Trust Region Policy Optimisation (TRPO) is a recent policy optimisation algorithm that achieves superior results in various RL benchmarks, but is computationally expensive. This paper proposes Customised Pearlmutter Propagation (CPP), a novel hardware architecture that accelerates TRPO on FPGA. We use the Pearlmutter Algorithm to address the key computational bottleneck of TRPO in a hardware efficient manner, avoiding symbolic differentiation with change of variables. Experimental evaluation using robotic locomotion benchmarks demonstrates that the proposed CPP architecture implemented on Stratix-V FPGA can achieve up to 20 times speed-up against 6-threaded Keras deep learning library with Theano backend running on a Core i7-5930K CPU. © 2017 Ghent University.",,Conference Paper,,Scopus,2-s2.0-85034441518
SCOPUS,"Li Z., Kiseleva J., De Rijke M., Grotov A.",Towards learning reward functions from user interactions,2017,ICTIR 2017 - Proceedings of the 2017 ACM SIGIR International Conference on the Theory of Information Retrieval,,,,289,292,,,10.1145/3121050.3121098,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033215376&doi=10.1145%2f3121050.3121098&partnerID=40&md5=ddbe414389c116035f0fe3ceee854a01,"University of Amsterdam, Amsterdam, Netherlands; UserSat.com, University of Amsterdam, Amsterdam, Netherlands","Li, Z., University of Amsterdam, Amsterdam, Netherlands; Kiseleva, J., UserSat.com, University of Amsterdam, Amsterdam, Netherlands; De Rijke, M., University of Amsterdam, Amsterdam, Netherlands; Grotov, A., University of Amsterdam, Amsterdam, Netherlands","In the physical world, people have dynamic preferences, e.g., the same situation can lead to satisfaction for some humans and to frustration for others. Personalization is called for. The same observation holds for online behavior with interactive systems. It is natural to represent the behavior of users who are engaging with interactive systems such as a search engine or a recommender system, as a sequence of actions where each next action depends on the current situation and the user reward of taking a particular action. By and large, current online evaluation metrics for interactive systems such as search engines or recommender systems, are static and do not reflect differences in user behavior. They rarely capture or model the reward experienced by a user while interacting with an interactive system.We argue that knowing a user's reward function is essential for an interactive system as both for learning and evaluation. We propose to learn users' reward functions directly from observed interaction traces. In particular, we present how users' reward functions can be uncovered directly using inverse reinforcement learning techniques. We also show how to incorporate user features into the learning process. Our main contribution is a novel and dynamic approach to restore a user's reward function. We present an analytic approach to this problem and complement it with initial experiments using the interaction logs of a cultural heritage institution that demonstrate the feasibility of the approach by uncovering different reward functions for different user groups. © 2017 Copyright held by the owner/author(s).",Interactive systems; Inverse reinforcement learning; Online evaluation,Conference Paper,,Scopus,2-s2.0-85033215376
SCOPUS,"Pravin Renold A., Chandrakala S.","Erratum to: MRL-SCSO: Multi-agent Reinforcement Learning-Based Self-Configuration and Self-Optimization Protocol for Unattended Wireless Sensor Networks (Wireless Personal Communications, (2017), 96, 4, (5061-5079), 10.1007/s11277-016-3729-3)",2017,Wireless Personal Communications,96,4,,5081,,,,10.1007/s11277-016-3832-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992349001&doi=10.1007%2fs11277-016-3832-5&partnerID=40&md5=d0b9e0407f8fd1fbae0768b468384dea,"Mobile Ad-hoc and Sensor Network Lab, TIFAC-CORE in Pervasive Computing Technologies, Velammal Engineering College, Chennai, India; Department of Computer Science and Engineering, Sri Krishna College of Engineering and Technology, Coimbatore, India","Pravin Renold, A., Mobile Ad-hoc and Sensor Network Lab, TIFAC-CORE in Pervasive Computing Technologies, Velammal Engineering College, Chennai, India; Chandrakala, S., Department of Computer Science and Engineering, Sri Krishna College of Engineering and Technology, Coimbatore, India","In the original publication, the variable Er in Eq. (5) should read Er and the three levels mentioned in the text between Eqs. (8) and (9) should have read ‘α, β, and Γ’ instead of ‘α, β, and γ.’. © 2017, Springer Science+Business Media New York.",,Erratum,,Scopus,2-s2.0-84992349001
SCOPUS,"Cushman F., Kumar V., Railton P.",Moral learning: Psychological and philosophical perspectives,2017,Cognition,167,,,1,10,,,10.1016/j.cognition.2017.06.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020828418&doi=10.1016%2fj.cognition.2017.06.008&partnerID=40&md5=443f3960e177478132939cdf99032dd8,"Department of Psychology, Harvard University, United States; Department of Philosophy, Boston University, United States; Department of Philosophy, University of Michigan, United States","Cushman, F., Department of Psychology, Harvard University, United States; Kumar, V., Department of Philosophy, Boston University, United States; Railton, P., Department of Philosophy, University of Michigan, United States","The past 15 years occasioned an extraordinary blossoming of research into the cognitive and affective mechanisms that support moral judgment and behavior. This growth in our understanding of moral mechanisms overshadowed a crucial and complementary question, however: How are they learned? As this special issue of the journal Cognition attests, a new crop of research into moral learning has now firmly taken root. This new literature draws on recent advances in formal methods developed in other domains, such as Bayesian inference, reinforcement learning and other machine learning techniques. Meanwhile, it also demonstrates how learning and deciding in a social domain—and especially in the moral domain—sometimes involves specialized cognitive systems. We review the contributions to this special issue and situate them within the broader contemporary literature. Our review focuses on how we learn moral values and moral rules, how we learn about personal moral character and relationships, and the philosophical implications of these emerging models. © 2017 Elsevier B.V.",,Editorial,,Scopus,2-s2.0-85020828418
SCOPUS,"Koban L., Jepma M., Geuter S., Wager T.D.","What's in a word? How instructions, suggestions, and social information change pain and emotion",2017,Neuroscience and Biobehavioral Reviews,81,,,29,42,,7,10.1016/j.neubiorev.2017.02.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016506177&doi=10.1016%2fj.neubiorev.2017.02.014&partnerID=40&md5=f9cbdf9bc0d54ccf146f59f39b4cf521,"Institute of Cognitive Science, University of Colorado Boulder, United States; Department of Psychology and Neuroscience, University of Colorado Boulder, United States; Cognitive Psychology Unit, Institute of Psychology, Leiden University, Netherlands; Leiden Institute for Brain and Cognition, Leiden University, Netherlands","Koban, L., Institute of Cognitive Science, University of Colorado Boulder, United States, Department of Psychology and Neuroscience, University of Colorado Boulder, United States; Jepma, M., Cognitive Psychology Unit, Institute of Psychology, Leiden University, Netherlands, Leiden Institute for Brain and Cognition, Leiden University, Netherlands; Geuter, S., Institute of Cognitive Science, University of Colorado Boulder, United States, Department of Psychology and Neuroscience, University of Colorado Boulder, United States; Wager, T.D., Institute of Cognitive Science, University of Colorado Boulder, United States, Department of Psychology and Neuroscience, University of Colorado Boulder, United States","Instructions, suggestions, and other types of social information can have powerful effects on pain and emotion. Prominent examples include observational learning, social influence, placebo, and hypnosis. These different phenomena and their underlying brain mechanisms have been studied in partially separate literatures, which we discuss, compare, and integrate in this review. Converging findings from these literatures suggest that (1) instructions and social information affect brain systems associated with the generation of pain and emotion, and with reinforcement learning, and that (2) these changes are mediated by alterations in prefrontal systems responsible for top-down control and the generation of affective meaning. We argue that changes in expectation and appraisal, a process of assessing personal meaning and implications for wellbeing, are two potential key mediators of the effects of instructions and social information on affective experience. Finally, we propose a tentative model of how prefrontal regions, especially dorsolateral and ventromedial prefrontal cortex may regulate affective processing based on instructions and socially transmitted expectations more broadly. © 2017 Elsevier Ltd",Appraisal; dlPFC; Emotion regulation; Expectation; Fear; fMRI; Hypnosis; Observational learning; Placebo; Social conformity; Social influence; vmPFC,Review,,Scopus,2-s2.0-85016506177
SCOPUS,"Lim J., Son H., Lee D., Lee D.",An MARL-Based Distributed Learning Scheme for Capturing User Preferences in a Smart Environment,2017,"Proceedings - 2017 IEEE 14th International Conference on Services Computing, SCC 2017",,,8034977,132,139,,1,10.1109/SCC.2017.24,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032375138&doi=10.1109%2fSCC.2017.24&partnerID=40&md5=a03b653a6746fb44b78181ad6aee5872,"School of Computing, KAIST, Daejeon, South Korea","Lim, J., School of Computing, KAIST, Daejeon, South Korea; Son, H., School of Computing, KAIST, Daejeon, South Korea; Lee, D., School of Computing, KAIST, Daejeon, South Korea; Lee, D., School of Computing, KAIST, Daejeon, South Korea","Providing a personalized service to a user in a smart environment has been one of the key goals in the area of pervasive computing. The proliferation of individually developed smart devices in the name of Internet of Things opens up a possibility of providing personalized services to a user in an autonomous and distributed manner. As a user's task often involves services supported by multiple devices, capturing a device-specific service preference is not enough to maximize a user's comfort. In this paper, we propose a distributed learning scheme for capturing multiple device service preferences in a smart environment. We exploit multi-Agent reinforcement learning (MARL) method where each smart device acts as a reinforcement learning agent to incrementally and cooperatively capture a user specific preference of a task. Experiments confirm that smart devices with the proposed scheme are able to capture multiple device service preferences from a small number of interactions with a user and an environment. Also, the proposed transfer learning method improves learning performance for a new task. © 2017 IEEE.",Context aware; Distributed learning; Personalization; Smart device; User preference,Conference Paper,,Scopus,2-s2.0-85032375138
SCOPUS,"Liu Y., Logan B., Liu N., Xu Z., Tang J., Wang Y.",Deep Reinforcement Learning for Dynamic Treatment Regimes on Medical Registry Data,2017,"Proceedings - 2017 IEEE International Conference on Healthcare Informatics, ICHI 2017",,,8031178,380,385,,1,10.1109/ICHI.2017.45,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032350439&doi=10.1109%2fICHI.2017.45&partnerID=40&md5=ddb30b458a20668f1f130c70ee00c65e,"Division of Biostatistics, Medical College of Wisconsin, Milwaukee, WI, United States; Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY, United States","Liu, Y., Division of Biostatistics, Medical College of Wisconsin, Milwaukee, WI, United States; Logan, B., Division of Biostatistics, Medical College of Wisconsin, Milwaukee, WI, United States; Liu, N., Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY, United States; Xu, Z., Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY, United States; Tang, J., Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY, United States; Wang, Y., Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY, United States","In this paper, we propose the first deep reinforce-ment learning framework to estimate the optimal Dynamic Treat-ment Regimes from observational medical data. This framework is more flexible and adaptive for high dimensional action and state spaces than existing reinforcement learning methods to model real life complexity in heterogeneous disease progression and treatment choices, with the goal to provide doctor and patients the data-driven personalized decision recommendations. The proposed deep reinforcement learning framework contains a supervised learning step to predict the most possible expert actions; and a deep reinforcement learning step to estimate the long term value function of Dynamic Treatment Regimes. We motivated and implemented the proposed framework on a data set from the Center for International Bone Marrow Transplant Research (CIBMTR) registry database, focusing on the sequence of prevention and treatments for acute and chronic graft versus host disease. We showed results of the initial implementation that demonstrates promising accuracy in predicting human expert decisions and initial implementation for the reinforcement learning step. © 2017 IEEE.",,Conference Paper,,Scopus,2-s2.0-85032350439
SCOPUS,"Zhao Y., Wang S., Zou Y., Ng J., Ng T.",Automatically Learning User Preferences for Personalized Service Composition,2017,"Proceedings - 2017 IEEE 24th International Conference on Web Services, ICWS 2017",,,8029835,776,783,,,10.1109/ICWS.2017.93,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032335863&doi=10.1109%2fICWS.2017.93&partnerID=40&md5=e98f88a57685c4a2b5bbde7f54d9f1bf,"Queen's University, Kingston, ON, Canada; IBM CAS Research, Markham, Canada","Zhao, Y., Queen's University, Kingston, ON, Canada; Wang, S., Queen's University, Kingston, ON, Canada; Zou, Y., Queen's University, Kingston, ON, Canada; Ng, J., IBM CAS Research, Markham, Canada; Ng, T., IBM CAS Research, Markham, Canada","With the rapid growth of the web services technologies, users often leverage various web services to perform their daily activities, such as on-line shopping. Due to the massive amount of web services available, a user faces numerous choices to meet their personal preferences when selecting the desired services from the web services with the similar functionality. Therefore, it becomes tedious and cumbersome tasks for users to discover and compose services. To reduce user's cognitive burden, it is critical to support automated service composition and make efficient recommendation of personalized services to achieve user's overall goals. However, existing approaches only offer users with limited options designed for the interest of a group of users without considering individual users' interests. To allow users to compose personalized services without much manual specification, we propose a machine learning approach that applies a learning-to-rank algorithm, RankBoost, to automatically learn user preferences and the prioritization of the preferences from users' historical data. Moreover, our approach uses the multi-objective reinforcement learning (MORL) algorithm to make trade-offs among user preferences and recommends a collection of services to achieve the highest objective. We conduct an empirical study to evaluate our approach by collecting the historical data from 12 subjects. The results demonstrate that our approach outperforms the two well-established baseline approaches by 100%-200% in terms of precision on recommending services. © 2017 IEEE.",,Conference Paper,,Scopus,2-s2.0-85032335863
SCOPUS,"Gašić M., Mrkšić N., Rojas-Barahona L.M., Su P.-H., Ultes S., Vandyke D., Wen T.-H., Young S.",Dialogue manager domain adaptation using Gaussian process reinforcement learning,2017,Computer Speech and Language,45,,,552,569,,,10.1016/j.csl.2016.09.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007550419&doi=10.1016%2fj.csl.2016.09.003&partnerID=40&md5=d21df44714e6a813bc0ce40cbb80b74f,"University of Cambridge, Trumpington Street, Cambridge, United Kingdom","Gašić, M., University of Cambridge, Trumpington Street, Cambridge, United Kingdom; Mrkšić, N., University of Cambridge, Trumpington Street, Cambridge, United Kingdom; Rojas-Barahona, L.M., University of Cambridge, Trumpington Street, Cambridge, United Kingdom; Su, P.-H., University of Cambridge, Trumpington Street, Cambridge, United Kingdom; Ultes, S., University of Cambridge, Trumpington Street, Cambridge, United Kingdom; Vandyke, D., University of Cambridge, Trumpington Street, Cambridge, United Kingdom; Wen, T.-H., University of Cambridge, Trumpington Street, Cambridge, United Kingdom; Young, S., University of Cambridge, Trumpington Street, Cambridge, United Kingdom","Spoken dialogue systems allow humans to interact with machines using natural speech. As such, they have many benefits. By using speech as the primary communication medium, a computer interface can facilitate swift, human-like acquisition of information. In recent years, speech interfaces have become ever more popular, as is evident from the rise of personal assistants such as Siri, Google Now, Cortana and Amazon Alexa. Recently, data-driven machine learning methods have been applied to dialogue modelling and the results achieved for limited-domain applications are comparable to or out-perform traditional approaches. Methods based on Gaussian processes are particularly effective as they enable good models to be estimated from limited training data. Furthermore, they provide an explicit estimate of the uncertainty which is particularly useful for reinforcement learning. This article explores the additional steps that are necessary to extend these methods to model multiple dialogue domains. We show that Gaussian process reinforcement learning is an elegant framework that naturally supports a range of methods, including prior knowledge, Bayesian committee machines and multi-agent learning, for facilitating extensible and adaptable dialogue systems. © 2016 The Authors",Dialogue systems; Gaussian process; Reinforcement learning,Article,Open Access,Scopus,2-s2.0-85007550419
SCOPUS,"Wicker J., Kramer S.",The best privacy defense is a good privacy offense: obfuscating a search engine user’s profile,2017,Data Mining and Knowledge Discovery,31,5,,1419,1443,,,10.1007/s10618-017-0524-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025085865&doi=10.1007%2fs10618-017-0524-z&partnerID=40&md5=247afda6050dd89a6a7d18d84d0dc5ee,"Institute of Computer Science, Johannes Gutenberg University Mainz, Staudingerweg 9, Mainz, Germany","Wicker, J., Institute of Computer Science, Johannes Gutenberg University Mainz, Staudingerweg 9, Mainz, Germany; Kramer, S., Institute of Computer Science, Johannes Gutenberg University Mainz, Staudingerweg 9, Mainz, Germany","User privacy on the internet is an important and unsolved problem. So far, no sufficient and comprehensive solution has been proposed that helps a user to protect his or her privacy while using the internet. Data are collected and assembled by numerous service providers. Solutions so far focused on the side of the service providers to store encrypted or transformed data that can be still used for analysis. This has a major flaw, as it relies on the service providers to do this. The user has no chance of actively protecting his or her privacy. In this work, we suggest a new approach, empowering the user to take advantage of the same tool the other side has, namely data mining to produce data which obfuscates the user’s profile. We apply this approach to search engine queries and use feedback of the search engines in terms of personalized advertisements in an algorithm similar to reinforcement learning to generate new queries potentially confusing the search engine. We evaluated the approach using a real-world data set. While evaluation is hard, we achieve results that indicate that it is possible to influence the user’s profile that the search engine generates. This shows that it is feasible to defend a user’s privacy from a new and more practical perspective. © 2017, The Author(s).",Personalized ads; Privacy; Reinforcement learning; Search engines; Web mining,Article,,Scopus,2-s2.0-85025085865
SCOPUS,"Yang M., Zhao Z., Zhao W., Chen X., Zhu J., Zhou L., Cao Z.",Personalized response generation via domain adaptation,2017,SIGIR 2017 - Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,,,,1021,1024,,,10.1145/3077136.3080706,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029357642&doi=10.1145%2f3077136.3080706&partnerID=40&md5=7dbab8245c15ee222478932988b1f01b,"Tencent AI Lab, United States; Zhejiang University, China; Tencent, China; Normal University, China; IIE, Chinese Academy of Sciences, China; South China Normal University, China","Yang, M., Tencent AI Lab, United States; Zhao, Z., Zhejiang University, China; Zhao, W., Tencent, China; Chen, X., Normal University, China; Zhu, J., IIE, Chinese Academy of Sciences, China; Zhou, L., South China Normal University, China; Cao, Z., Tencent AI Lab, United States","In this paper, we propose a novel personalized response generation model via domain adaptation (PRG-DM). First, we learn the human responding style from large general data (without user-specific information). Second, we fine tune the model on a small size of personalized data to generate personalized responses with a dual learning mechanism. Moreover, we propose three new rewards to characterize good conversations that are personalized, informative and grammatical. We employ the policy gradient method to generate highly rewarded responses. Experimental results show that our model can generate better personalized responses for different users. © 2017 Copyright held by the owner/author(s).",Domain adaptation; Reinforcement learning; Response generation,Conference Paper,,Scopus,2-s2.0-85029357642
SCOPUS,"Chen X., Zhai Y., Lu C., Gong J., Wang G.",A learning model for personalized adaptive cruise control,2017,"IEEE Intelligent Vehicles Symposium, Proceedings",,,7995748,379,384,,,10.1109/IVS.2017.7995748,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028054739&doi=10.1109%2fIVS.2017.7995748&partnerID=40&md5=b26dc5f442e29c27aa9eb5b653598e32,"Intelligent Vehicle Research Center, Beijing Institute of Technology, Beijing, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China","Chen, X., Intelligent Vehicle Research Center, Beijing Institute of Technology, Beijing, China; Zhai, Y., Intelligent Vehicle Research Center, Beijing Institute of Technology, Beijing, China; Lu, C., Intelligent Vehicle Research Center, Beijing Institute of Technology, Beijing, China; Gong, J., Intelligent Vehicle Research Center, Beijing Institute of Technology, Beijing, China; Wang, G., Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China","This paper develops a learning model for personalized adaptive cruise control that can learn from human demonstration online and mimic a human driver's driving strategies in the dynamic traffic environment. Under the framework of the proposed model, reinforcement learning is used to capture the human-desired driving strategy, and the proportion-integration-differentiation controller is adopted to convert the learning strategy to low-level control commands. The performance of the learning model is tested in the simulation environment built in a driving simulator using PreScan. Experimental results show that the learning model can duplicate human driving strategies with acceptable errors. Moreover, compared with the traditional adaptive cruise control, the proposed model can provide better driving comfort and smoothness in the dynamic situation. © 2017 IEEE.",,Conference Paper,,Scopus,2-s2.0-85028054739
SCOPUS,"Patompak P., Jeong S., Nilkhamhang I., Chong N.Y.",Learning social relations for culture aware interaction,2017,"2017 14th International Conference on Ubiquitous Robots and Ambient Intelligence, URAI 2017",,,7992879,26,31,,,10.1109/URAI.2017.7992879,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034241305&doi=10.1109%2fURAI.2017.7992879&partnerID=40&md5=ad8b619b874c09954225a50c37cbdff2,"School of Information Science, Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Nomi, Ishikawa, Japan; School of Information Computer and Communication Technology, Sirindhorn International Institute of Technology, Thammasat University, 99 Khlong Luang, Pathum Thani, Thailand","Patompak, P., School of Information Science, Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Nomi, Ishikawa, Japan; Jeong, S., School of Information Science, Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Nomi, Ishikawa, Japan; Nilkhamhang, I., School of Information Computer and Communication Technology, Sirindhorn International Institute of Technology, Thammasat University, 99 Khlong Luang, Pathum Thani, Thailand; Chong, N.Y., School of Information Science, Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Nomi, Ishikawa, Japan","Each person has their private physical and/or psychological area where they do not want to share with others during social interactions. This area gives them comfort about interactions and its size usually depends on various factors such as culture, personal traits, and acquaintanceship. This issue may also arise in case of human-robot interaction, especially when the robot is required to generate a socially competent interaction strategy toward people they are interacting with. Here, we propose a new robot exploration strategy to socially interact with people by considering the social relationship between the robot and each person. To that end, two definitions of interaction area are made: (1) Acceptable area allowed to be shared with other people and robots, and (2) Private area where a human does not want to be interfered by others. Based on these definitions, the robot can optimize the path to maximize the frequency/degree of visiting the acceptable area of each person and to minimize the frequency/degree of trespassing into the private area of them at the same time in an iterative way. In this paper, the social force model (SFM) of each person, based on the potential field concept, is designed by a fuzzy inference system and its parameter is optimized by the reinforcement learning model during interactions. We have shown that the proposed model can generate a suitable SFM of each person, which was quite similar to a ground truth model, allowing to plan a path to simultaneously optimize the two factors of interaction area, respectively. © 2017 IEEE.",,Conference Paper,,Scopus,2-s2.0-85034241305
SCOPUS,"Kuzmanovic B., Rigoux L.",Valence-dependent belief updating: Computational validation,2017,Frontiers in Psychology,8,JUN,1087,,,,1,10.3389/fpsyg.2017.01087,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021326063&doi=10.3389%2ffpsyg.2017.01087&partnerID=40&md5=3fe7f419a8d8399d3369b3a1f11610a2,"Translational Neurocircuitry Group, Max Planck Institute for Metabolism Research, Cologne, Germany; Translational Neuromodeling Unit, Institute for Biomedical Engineering, University of Zurich and ETH Zurich, Zurich, Switzerland","Kuzmanovic, B., Translational Neurocircuitry Group, Max Planck Institute for Metabolism Research, Cologne, Germany; Rigoux, L., Translational Neurocircuitry Group, Max Planck Institute for Metabolism Research, Cologne, Germany, Translational Neuromodeling Unit, Institute for Biomedical Engineering, University of Zurich and ETH Zurich, Zurich, Switzerland","People tend to update beliefs about their future outcomes in a valence-dependent way: they are likely to incorporate good news and to neglect bad news. However, belief formation is a complex process which depends not only on motivational factors such as the desire for favorable conclusions, but also on multiple cognitive variables such as prior beliefs, knowledge about personal vulnerabilities and resources, and the size of the probabilities and estimation errors. Thus, we applied computational modeling in order to test for valence-induced biases in updating while formally controlling for relevant cognitive factors. We compared biased and unbiased Bayesian models of belief updating, and specified alternative models based on reinforcement learning. The experiment consisted of 80 trials with 80 different adverse future life events. In each trial, participants estimated the base rate of one of these events and estimated their own risk of experiencing the event before and after being confronted with the actual base rate. Belief updates corresponded to the difference between the two self-risk estimates. Valence-dependent updating was assessed by comparing trials with good news (better-than-expected base rates) with trials with bad news (worse-than-expected base rates). After receiving bad relative to good news, participants' updates were smaller and deviated more strongly from rational Bayesian predictions, indicating a valence-induced bias. Model comparison revealed that the biased (i.e., optimistic) Bayesian model of belief updating better accounted for data than the unbiased (i.e., rational) Bayesian model, confirming that the valence of the new information influenced the amount of updating. Moreover, alternative computational modeling based on reinforcement learning demonstrated higher learning rates for good than for bad news, as well as a moderating role of personal knowledge. Finally, in this specific experimental context, the approach based on reinforcement learning was superior to the Bayesian approach. The computational validation of valence-dependent belief updating represents a novel support for a genuine optimism bias in human belief formation. Moreover, the precise control of relevant cognitive variables justifies the conclusion that the motivation to adopt the most favorable self-referential conclusions biases human judgments. © 2017 Kuzmanovic and Rigoux.","Bayesian theorem; Belief updating; Computational modeling; Desirability; Motivation, probability; Optimism bias; Risk judgments",Article,,Scopus,2-s2.0-85021326063
SCOPUS,"Bornstein A.M., Norman K.A.",Reinstated episodic context guides sampling-based decisions for reward,2017,Nature Neuroscience,20,7,,997,1003,,5,10.1038/nn.4573,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021392099&doi=10.1038%2fnn.4573&partnerID=40&md5=cbb4c23087bc185fa012c8825f2baf87,"Neuroscience Institute, Princeton University, Princeton, NJ, United States; Department of Psychology, Princeton University, Princeton, NJ, United States","Bornstein, A.M., Neuroscience Institute, Princeton University, Princeton, NJ, United States; Norman, K.A., Neuroscience Institute, Princeton University, Princeton, NJ, United States, Department of Psychology, Princeton University, Princeton, NJ, United States","How does experience inform decisions? In episodic sampling, decisions are guided by a few episodic memories of past choices. This process can yield choice patterns similar to model-free reinforcement learning; however, samples can vary from trial to trial, causing decisions to vary. Here we show that context retrieved during episodic sampling can cause choice behavior to deviate sharply from the predictions of reinforcement learning. Specifically, we show that, when a given memory is sampled, choices (in the present) are influenced by the properties of other decisions made in the same context as the sampled event. This effect is mediated by fMRI measures of context retrieval on each trial, suggesting a mechanism whereby cues trigger retrieval of context, which then triggers retrieval of other decisions from that context. This result establishes a new avenue by which experience can guide choice and, as such, has broad implications for the study of decisions. © 2017 Nature America, Inc., part of Springer Nature. All rights reserved.",,Article,,Scopus,2-s2.0-85021392099
SCOPUS,"Chasparis G.C., Natschläger T.",Supervisory output prediction for bilinear systems by reinforcement learning,2017,IET Control Theory and Applications,11,10,,1514,1521,,,10.1049/iet-cta.2016.1400,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020475446&doi=10.1049%2fiet-cta.2016.1400&partnerID=40&md5=1e360ea4f2acfdac8762b69687fb1685,"Software Competence Center Hagenberg GmbH, Softwarepark 21, Hagenberg, Austria","Chasparis, G.C., Software Competence Center Hagenberg GmbH, Softwarepark 21, Hagenberg, Austria; Natschläger, T., Software Competence Center Hagenberg GmbH, Softwarepark 21, Hagenberg, Austria","Online output prediction is an indispensable part of any model predictive control implementation. For several application scenarios, operating conditions may change quite often, while designing the data collection process may not be an option. To this end, this study introduces a supervisory output prediction scheme, tailored specifically for input-output stable bilinear systems, that intends on automating the process of selecting the most appropriate prediction model during runtime. The selection process is based upon a reinforcement-learning scheme, where prediction models are selected according to their prior prediction performance. An additional selection process is concerned with appropriately partitioning the control-inputs' domain also to allow for switched-system approximations of the original bilinear dynamics. The authors show analytically that the proposed scheme converges (in probability) to the best model and partition. They also demonstrate these properties through simulations of temperature prediction in residential buildings. © The Institution of Engineering and Technology 2017.",,Article,,Scopus,2-s2.0-85020475446
SCOPUS,"Tsiakas K., Papakostas M., Papakostas M., Bell M., Mihalcea R., Wang S., Burzo M., Makedon F.",An interactive multisensing framework for personalized human robot collaboration and assistive training using reinforcement learning,2017,ACM International Conference Proceeding Series,Part F128530,,,423,427,,,10.1145/3056540.3076191,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025128294&doi=10.1145%2f3056540.3076191&partnerID=40&md5=c676d60143fb0a043e53e429754aa90b,"HERACLEIA Human-Centered Computing Laboratory, CSE Dept., University of Texas, Arlington, United States; Industrial Engineering Dept., University of Texas, Arlington, United States; Dept. of Psychiatry, Yale School of Medicine, United States; CSE Dept., University of Michigan, United States; Mechanical Engineering Dept., University of Michigan-Flint, United States","Tsiakas, K., HERACLEIA Human-Centered Computing Laboratory, CSE Dept., University of Texas, Arlington, United States; Papakostas, M., HERACLEIA Human-Centered Computing Laboratory, CSE Dept., University of Texas, Arlington, United States; Papakostas, M., HERACLEIA Human-Centered Computing Laboratory, CSE Dept., University of Texas, Arlington, United States; Bell, M., Dept. of Psychiatry, Yale School of Medicine, United States; Mihalcea, R., CSE Dept., University of Michigan, United States; Wang, S., Industrial Engineering Dept., University of Texas, Arlington, United States; Burzo, M., Mechanical Engineering Dept., University of Michigan-Flint, United States; Makedon, F., HERACLEIA Human-Centered Computing Laboratory, CSE Dept., University of Texas, Arlington, United States","There is a recent trend of research and applications of Cyber- Physical Systems (CPS) in manufacturing to enhance humanrobot collaboration and production. In this paper, we propose a CPS framework for personalized Human-Robot Collaboration and Training to promote safe human-robot collaboration in manufacturing environments. We propose a human-centric CPS approach that focuses on multimodal human behavior monitoring and assessment, to promote human worker safety and enable human training in Human- Robot Collaboration tasks. We present the architecture of our proposed system, our experimental testbed and our proposed methods for multimodal physiological sensing, human state monitoring and interactive robot adaptation, to enable personalized interaction. © 2017 ACM.",Cyber Physical Systems; Human Robot Collaboration; Intelligent Manufacturing; Vocational Assessment and Training,Conference Paper,,Scopus,2-s2.0-85025128294
SCOPUS,"Manickam I., Lan A.S., Baraniuk R.G.",Contextual multi-armed bandit algorithms for personalized learning action selection,2017,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",,,7953377,6344,6348,,1,10.1109/ICASSP.2017.7953377,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023754279&doi=10.1109%2fICASSP.2017.7953377&partnerID=40&md5=a08f9f6b9d744276c74a60ed3e66a164,"Rice University, United States","Manickam, I., Rice University, United States; Lan, A.S., Rice University, United States; Baraniuk, R.G., Rice University, United States","Optimizing the selection of learning resources and practice questions to address each individual student's needs has the potential to improve students' learning efficiency. In this paper, we study the problem of selecting a personalized learning action for each student (e.g. watching a lecture video, working on a practice question, etc.), based on their prior performance, in order to maximize their learning outcome. We formulate this problem using the contextual multi-armed bandits framework, where students' prior concept knowledge states (estimated from their responses to questions in previous assessments) correspond to contexts, the personalized learning actions correspond to arms, and their performance on future assessments correspond to rewards. We propose three new Bayesian policies to select personalized learning actions for students that each exhibits advantages over prior work, and experimentally validate them using real-world datasets. © 2017 IEEE.",contextual bandits; personalized learning,Conference Paper,,Scopus,2-s2.0-85023754279
SCOPUS,Sekhavat Y.A.,MPRL: Multiple-Periodic Reinforcement Learning for difficulty adjustment in rehabilitation games,2017,"2017 IEEE 5th International Conference on Serious Games and Applications for Health, SeGAH 2017",,,7939260,,,,,10.1109/SeGAH.2017.7939260,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021753365&doi=10.1109%2fSeGAH.2017.7939260&partnerID=40&md5=1a32299aab57acfa9bf6686973c5c572,"Faculty of Multimedia, Tabriz Islamic Art University, Tabriz, Iran","Sekhavat, Y.A., Faculty of Multimedia, Tabriz Islamic Art University, Tabriz, Iran","Generally, the difficulty level of a therapeutic game is regulated manually by a therapist. However, home-based rehabilitation games require a technique for automatic difficulty adjustment. This paper proposes a personalized difficulty adjustment technique for a rehabilitation game that automatically regulates difficulty settings based on a patient's skills in real-time. To this end, ideas from reinforcement learning are used to dynamically adjust the difficulty of a game. We show that difficulty adjustment is a multiple-objective problem, in which some objectives might be evaluated at different periods. To address this problem, we propose and use Multiple-Periodic Reinforcement Learning (MPRL) that makes it possible to evaluate different objectives of difficulty adjustment in separate periods. The results of experiments show that MPRL outperforms traditional Multiple-Objective Reinforcement Learning (MORL) in terms of user satisfaction parameters as well as improving the movement skills of patients. © 2017 IEEE.",,Conference Paper,,Scopus,2-s2.0-85021753365
SCOPUS,"Ferretti S., Mirri S., Prandi C., Salomoni P.",On personalizing Web content through reinforcement learning,2017,Universal Access in the Information Society,16,2,,395,410,,1,10.1007/s10209-016-0463-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960114893&doi=10.1007%2fs10209-016-0463-2&partnerID=40&md5=5261867eed4eafe7188ba50052e7410c,"Department of Computer Science and Engineering, Università di Bologna, Via Mura Anteo Zamboni 7, Bologna, Italy","Ferretti, S., Department of Computer Science and Engineering, Università di Bologna, Via Mura Anteo Zamboni 7, Bologna, Italy; Mirri, S., Department of Computer Science and Engineering, Università di Bologna, Via Mura Anteo Zamboni 7, Bologna, Italy; Prandi, C., Department of Computer Science and Engineering, Università di Bologna, Via Mura Anteo Zamboni 7, Bologna, Italy; Salomoni, P., Department of Computer Science and Engineering, Università di Bologna, Via Mura Anteo Zamboni 7, Bologna, Italy","Nowadays, a large use of personalization techniques is used to adapt Web content to users’ habits, mainly with the aim of offering appropriate products and services. This paper presents a system that uses personalization and adaptation techniques, in order to transcode or modify contents (e.g., adapt text fonts) so as to meet preferences and needs of elderly users and users with disabilities. Such an adaptation can have a positive effect, in particular for users with some reading-related disabilities (i.e., people with dyslexia, users with low vision, users with color blindness, elderly people.). To avoid issues arising from applying transformations to the whole content, the proposed system uses Web intelligence to perform automatic adaptations on single elements composing a Web page. The transformation is applied on the basis of a reinforcement learning algorithm which manages a user profile. The system is evaluated through simulations and a real assessment, where elderly users where asked to use the system prototype for a time period and to perform some specific tasks. Results of the qualitative evaluation confirm the feasibility of the proposal, showing its validity and the users’ appreciation. © 2016, Springer-Verlag Berlin Heidelberg.",Content adaptation; Legibility; Reinforcement learning; User profiling; Web personalization,Article,,Scopus,2-s2.0-84960114893
SCOPUS,"Zhu R., Zhao Y.-Q., Chen G., Ma S., Zhao H.",Greedy outcome weighted tree learning of optimal personalized treatment rules,2017,Biometrics,73,2,,391,400,,1,10.1111/biom.12593,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994875692&doi=10.1111%2fbiom.12593&partnerID=40&md5=f7fb0d98e7a8421f1d12ffbc09a05fee,"University of Illinois at Urbana-Champaign, Champaign, IL, United States; Fred Hutchinson Cancer Research Center, Seattle, WA, United States; Vanderbilt University, Nashville, TN, United States; Yale University, New Haven, CT, United States","Zhu, R., University of Illinois at Urbana-Champaign, Champaign, IL, United States; Zhao, Y.-Q., Fred Hutchinson Cancer Research Center, Seattle, WA, United States; Chen, G., Vanderbilt University, Nashville, TN, United States; Ma, S., Yale University, New Haven, CT, United States; Zhao, H., Yale University, New Haven, CT, United States","We propose a subgroup identification approach for inferring optimal and interpretable personalized treatment rules with high-dimensional covariates. Our approach is based on a two-step greedy tree algorithm to pursue signals in a high-dimensional space. In the first step, we transform the treatment selection problem into a weighted classification problem that can utilize tree-based methods. In the second step, we adopt a newly proposed tree-based method, known as reinforcement learning trees, to detect features involved in the optimal treatment rules and to construct binary splitting rules. The method is further extended to right censored survival data by using the accelerated failure time model and introducing double weighting to the classification trees. The performance of the proposed method is demonstrated via simulation studies, as well as analyses of the Cancer Cell Line Encyclopedia (CCLE) data and the Tamoxifen breast cancer data. © 2016, The International Biometric Society",High-dimensional data; Optimal treatment rules; Personalized medicine; Reinforcement learning trees; Survival analysis; Tree-based method,Article,,Scopus,2-s2.0-84994875692
SCOPUS,"Zald D.H., Treadway M.T.","Reward Processing, Neuroeconomics, and Psychopathology",2017,Annual Review of Clinical Psychology,13,,,471,495,,2,10.1146/annurev-clinpsy-032816-044957,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019122756&doi=10.1146%2fannurev-clinpsy-032816-044957&partnerID=40&md5=a8b5422deb2aa507c9408e66bf128fa2,"Department of Psychology, Department of Psychiatry, Vanderbilt University, Nashville, TN, United States; Department of Psychology, Emory University, Atlanta, GA, United States","Zald, D.H., Department of Psychology, Department of Psychiatry, Vanderbilt University, Nashville, TN, United States; Treadway, M.T., Department of Psychology, Emory University, Atlanta, GA, United States","Abnormal reward processing is a prominent transdiagnostic feature of psychopathology. The present review provides a framework for considering the different aspects of reward processing and their assessment, and highlights recent insights from the field of neuroeconomics that may aid in understanding these processes. Although altered reward processing in psychopathology has often been treated as a general hypo- or hyperresponsivity to reward, increasing data indicate that a comprehensive understanding of reward dysfunction requires characterization within more specific reward-processing domains, including subjective valuation, discounting, hedonics, reward anticipation and facilitation, and reinforcement learning. As such, more nuanced models of the nature of these abnormalities are needed. We describe several processing abnormalities capable of producing the types of selective alterations in reward-related behavior observed in different forms of psychopathology, including (mal)adaptive scaling and anchoring, dysfunctional weighting of reward and cost variables, competition between valuation systems, and reward prediction error signaling. © 2017 by Annual Reviews. All rights reserved.",Addiction; Anhedonia; Computational psychiatry; Dopamine; Major depression; Prediction error,Article,,Scopus,2-s2.0-85019122756
SCOPUS,Pereira G.C.,Genomics and artificial intelligence working together in drug discovery and repositioning: The advent of adaptive pharmacogenomics in glioblastoma and chronic arterial inflammation therapies,2017,Biotechnology and Production of Anti-Cancer Compounds,,,,253,281,,,10.1007/978-3-319-53880-8_11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034400377&doi=10.1007%2f978-3-319-53880-8_11&partnerID=40&md5=fde62df000191d3078169ff2d8da4bd5,"Biotechnology, Icelandic Institute for Intelligent Machines, Reykjavik, Iceland; Department of Computer Sciences, Polytechnic Institute, University Autonoma of Madrid, Madrid, Spain; Department of Bioengineering, Imperial College London, London, United Kingdom","Pereira, G.C., Biotechnology, Icelandic Institute for Intelligent Machines, Reykjavik, Iceland, Department of Computer Sciences, Polytechnic Institute, University Autonoma of Madrid, Madrid, Spain, Department of Bioengineering, Imperial College London, London, United Kingdom","The field of pharmacogenomics investigates how genomics may modulate pathological trends using information on both genotype and phenotype, with the aim of designing personalised healthcare. Homoeostasis is partially regulated through the expression of core protein groups whose functionality is determined at gene level and modulated by environmental factors. Harmful changes in physiology may promote several dis-functionalities. In prior work gene expression was used as a biomarker to assess both pathological propensity and disease progression. A growing body of pharmacogenomics research has developed new compounds, on one hand, and on the other, it has proposed novel therapeutic applications for the existing ones. Over the past decades, collective efforts have significantly increased the number of omics information available. However, efficient and deterministic in silico mechanisms that efficiently analyse and detect trends on the basis of often unknown and limited physiological information responding to challenging clinical questions are still lacking. In this context, computational automation via artificial intelligence methodologies has proven to be accurate, robust to noise, cost efficient, and dynamic dealing with massive databases and forecasting on the basis of the available information. Moreover, this set of computational techniques, based on well-established mathematical models, provide efficient ways of determining trends based on both a priori knowledge and dynamically acquired information, working successfully on incomplete datasets. Therefore, in this chapter we assess developmental similarities between two major causes of worldwide death: glioblastoma and chronic arterial inflammation; and discuss the potential applicability of two artificial intelligence approaches for drug discovery and repositioning. According to the World Health Organization (WHO) a glioblastoma multiform is the most malignant glial-type tumour (graded level IV in the WHO scale); and inflammatory diseases affecting the cardiovascular network are the cause of high mortality. As suggested, these two pathologies have several developmental similarities and share common genetic variants. Therefore, we additionally seek to discuss the main promoters presented in the current literature, aiming at benefiting from their similarities in drug discovery and repositioning, via automatic artificial intelligence pattern recognition, forecasting, and computational design. © Springer International Publishing AG 2017.",Adaptive pharmacogenomics; Artificial intelligence; Chronic arterial inflammation; Cytokines; Deep neural networks; Drug discovery; Drug repositioning; Genetic fingerprints; Genomics; Glioblastoma; Inflammatory signalling cascades; Reinforcement learning; Transcription factors,Book Chapter,,Scopus,2-s2.0-85034400377
SCOPUS,"Jaywilliams J., Rafferty A.N., Maldonado S., Ang A., Tingley D., Kim J.",MOOClets: A framework for dynamic experimentation and personalization,2017,L@S 2017 - Proceedings of the 4th (2017) ACM Conference on Learning at Scale,,,,287,290,,,10.1145/3051457.3054006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018407094&doi=10.1145%2f3051457.3054006&partnerID=40&md5=e7315bb93f2d39de7391ea6a6ad5617a,"Harvard University, Cambridge, MA, United States; Carleton College, Northfield, MN, United States; San Jose State University, San Jose, CA, United States; KAIST, Daejeon, South Korea","Jaywilliams, J., Harvard University, Cambridge, MA, United States; Rafferty, A.N., Carleton College, Northfield, MN, United States; Maldonado, S., San Jose State University, San Jose, CA, United States; Ang, A., Harvard University, Cambridge, MA, United States; Tingley, D., Harvard University, Cambridge, MA, United States; Kim, J., KAIST, Daejeon, South Korea","Randomized experiments in online educational environments are ubiquitous as a scientific method for investigating learning and motivation, but they rarely improve educational resources and produce practical benefits for learners. We suggest that tools for experimentally comparing resources are designed primarily through the lens of experiments as a scientific methodology, and therefore miss a tremendous opportunity for online experiments to serve as engines for dynamic improvement and personalization. We present the MOOClet requirements specification to guide the implementation of software tools for experiments to ensure that whenever alternative versions of a resource can be experimentally compared (by randomly assigning versions), the resource can also be dynamically improved (by changing which versions are presented), and personalized (by presenting different versions to different people). The MOOClet specification was used to implement DEXPER, a proof-of-concept web service backend that enables dynamic experimentation and personalization of resources embedded in frontend educational platforms. We describe three use cases of MOOClets for dynamic experimentation and personalization of motivational emails, explanations, and problems. © 2017 ACM.",A/B experiment; Adaptive learning; Dynamic experimentation; MOOCLet; Multi-Armed bandit; Personalization; Reinforcement learning; Statistical machine learning,Conference Paper,,Scopus,2-s2.0-85018407094
SCOPUS,"Delli Priscoli F., Di Giorgio A., Lisi F., Monaco S., Pietrabissa A., Celsi L.R., Suraci V.",Multi-agent quality of experience control,2017,"International Journal of Control, Automation and Systems",15,2,,892,904,,5,10.1007/s12555-015-0465-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009932576&doi=10.1007%2fs12555-015-0465-5&partnerID=40&md5=17ab2471409262bc448290a2494ab7e4,"Department of Computer, Control and Management Engineering “Antonio Ruberti”, University of Rome “La Sapienza”, via Ariosto 25, Rome, Italy; eCampus University, via Isimbardi 10, Novedrate, Italy","Delli Priscoli, F., Department of Computer, Control and Management Engineering “Antonio Ruberti”, University of Rome “La Sapienza”, via Ariosto 25, Rome, Italy; Di Giorgio, A., Department of Computer, Control and Management Engineering “Antonio Ruberti”, University of Rome “La Sapienza”, via Ariosto 25, Rome, Italy; Lisi, F., Department of Computer, Control and Management Engineering “Antonio Ruberti”, University of Rome “La Sapienza”, via Ariosto 25, Rome, Italy; Monaco, S., Department of Computer, Control and Management Engineering “Antonio Ruberti”, University of Rome “La Sapienza”, via Ariosto 25, Rome, Italy; Pietrabissa, A., Department of Computer, Control and Management Engineering “Antonio Ruberti”, University of Rome “La Sapienza”, via Ariosto 25, Rome, Italy; Celsi, L.R., Department of Computer, Control and Management Engineering “Antonio Ruberti”, University of Rome “La Sapienza”, via Ariosto 25, Rome, Italy; Suraci, V., eCampus University, via Isimbardi 10, Novedrate, Italy","In the framework of the Future Internet, the aim of the Quality of Experience (QoE) Control functionalities is to track the personalized desired QoE level of the applications. The paper proposes to perform such a task by dynamically selecting the most appropriate Classes of Service (among the ones supported by the network), this selection being driven by a novel heuristic Multi-Agent Reinforcement Learning (MARL) algorithm. The paper shows that such an approach offers the opportunity to cope with some practical implementation problems: in particular, it allows to face the so-called “curse of dimensionality” of MARL algorithms, thus achieving satisfactory performance results even in the presence of several hundreds of Agents. © 2017, Institute of Control, Robotics and Systems and The Korean Institute of Electrical Engineers and Springer-Verlag Berlin Heidelberg.",Future internet; multi-agent reinforcement learning; quality of experience; quality of service,Article,,Scopus,2-s2.0-85009932576
SCOPUS,"Ansari Y., Manti M., Falotico E., Mollard Y., Cianchetti M., Laschi C.",Towards the development of a soft manipulator as an assistive robot for personal care of elderly people,2017,International Journal of Advanced Robotic Systems,14,2,,,,,,10.1177/1729881416687132,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018369528&doi=10.1177%2f1729881416687132&partnerID=40&md5=e3024e90f697abb148e6c74993638654,"BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy; Flowers Lab., Inria, France","Ansari, Y., BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy; Manti, M., BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy; Falotico, E., BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy; Mollard, Y., Flowers Lab., Inria, France; Cianchetti, M., BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy; Laschi, C., BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy","Manipulators based on soft robotic technologies exhibit compliance and dexterity which ensures safe human-robot interaction. This article is a novel attempt at exploiting these desirable properties to develop a manipulator for an assistive application, in particular, a shower arm to assist the elderly in the bathing task. The overall vision for the soft manipulator is to concatenate three modules in a serial manner such that (i) the proximal segment is made up of cable-based actuation to compensate for gravitational effects and (ii) the central and distal segments are made up of hybrid actuation to autonomously reach delicate body parts to perform the main tasks related to bathing. The role of the latter modules is crucial to the application of the system in the bathing task; however, it is a nontrivial challenge to develop a robust and controllable hybrid actuated system with advanced manipulation capabilities and hence, the focus of this article. We first introduce our design and experimentally characterize its functionalities, which include elongation, shortening, omnidirectional bending. Next, we propose a control concept capable of solving the inverse kinetics problem using multiagent reinforcement learning to exploit these functionalities despite high dimensionality and redundancy. We demonstrate the effectiveness of the design and control of this module by demonstrating an open-loop task space control where it successfully moves through an asymmetric 3-D trajectory sampled at 12 points with an average reaching accuracy of 0.79 cm ± 0.18 cm. Our quantitative experimental results present a promising step toward the development of the soft manipulator eventually contributing to the advancement of soft robotics. © The Author(s) 2017.",Assistive robotics; Machine learning; Robot control; Soft robotics,Article,,Scopus,2-s2.0-85018369528
SCOPUS,"Ekeinhor-Komi T., Bouraoui J.-L., Laroche R., Lefevre F.",Towards a virtual personal assistant based on a user-defined portfolio of multi-domain vocal applications,2017,"2016 IEEE Workshop on Spoken Language Technology, SLT 2016 - Proceedings",,,7846252,106,113,,,10.1109/SLT.2016.7846252,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015983061&doi=10.1109%2fSLT.2016.7846252&partnerID=40&md5=ab7fb18e3d198e54b7f524374c42cf94,"Orange Labs, France; CERI-LIA, Université d'Avignon, France","Ekeinhor-Komi, T., Orange Labs, France, CERI-LIA, Université d'Avignon, France; Bouraoui, J.-L., Orange Labs, France; Laroche, R., Orange Labs, France; Lefevre, F., CERI-LIA, Université d'Avignon, France","This paper proposes a novel approach to defining and simulating a new generation of virtual personal assistants as multi-application multi-domain distributed dialogue systems. The first contribution is the assistant architecture, composed of independent third-party applications handled by a Dispatcher. In this view, applications are black-boxes responding with a self-scored answer to user requests. Next, the Dispatcher distributes the current request to the most relevant application, based on these scores and the context (history of interaction etc.), and conveys its answer to the user. To address variations in the user-defined portfolio of applications, the second contribution, a stochastic model automates the online optimisation of the Dispatcher's behaviour. To evaluate the learnability of the Dispatcher's policy, several parametrisations of the user and application simulators are enabled, in such a way that they cover variations of realistic situations. Results confirm in all considered configurations of interest, that reinforcement learning can learn adapted strategies. © 2016 IEEE.",Dialogue strategy; Multi-application spoken dialogue systems; Multi-domain; Reinforcement learning,Conference Paper,,Scopus,2-s2.0-85015983061
SCOPUS,[No author name available],"2016 IEEE 3rd World Forum on Internet of Things, WF-IoT 2016",2017,"2016 IEEE 3rd World Forum on Internet of Things, WF-IoT 2016",,,,,,763,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015198866&partnerID=40&md5=da4c7228d1145d4a631a15e30e67a98f,,,"The proceedings contain 126 papers. The topics discussed include: a framework of scalable QoE modeling for application explosion in the internet of things; an IoT system to estimate personal thermal comfort; dynamic semantic interoperability of control in IoT-based systems: need for adaptive middleware; distributed live data search architecture for resource discovery on internet of things; event model to facilitate data sharing among services; human-motion based transmission power control in wireless body area networks; personalized ambience: an integration of learning model and intelligent lighting control; easing IoT application development through DataTweet framework; improving fast velocity and large volume data processing in IoT/M2M platforms; SRUP: the secure remote update protocol; iQAS: an integration platform for QoI assessment as a service for smart cities; understanding user privacy in Internet of things environments; a novel IoT access architecture for vehicle monitoring system; adapting sampling interval of sensor networks using on-line reinforcement learning; ASSIST: an agent-based SIoT simulator; parking-stall vacancy indicator system, based on deep convolutional neural networks; and recommendations for securing Internet of Things devices using commodity hardware.",,Conference Review,,Scopus,2-s2.0-85015198866
SCOPUS,"Pomprapa A., Leonhardt S., Misgeld B.J.E.",Optimal learning control of oxygen saturation using a policy iteration algorithm and a proof-of-concept in an interconnecting three-tank system,2017,Control Engineering Practice,59,,,194,203,,,10.1016/j.conengprac.2016.07.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995900104&doi=10.1016%2fj.conengprac.2016.07.014&partnerID=40&md5=8b494bd2a9f229c0bc0edde6e57e0fde,"Philips Chair for Medical Information Technology, RWTH Aachen University, Aachen, Germany","Pomprapa, A., Philips Chair for Medical Information Technology, RWTH Aachen University, Aachen, Germany; Leonhardt, S., Philips Chair for Medical Information Technology, RWTH Aachen University, Aachen, Germany; Misgeld, B.J.E., Philips Chair for Medical Information Technology, RWTH Aachen University, Aachen, Germany","In this work, “policy iteration algorithm” (PIA) is applied for controlling arterial oxygen saturation that does not require mathematical models of the plant. This technique is based on nonlinear optimal control to solve the Hamilton–Jacobi–Bellman equation. The controller is synthesized using a state feedback configuration based on an unidentified model of complex pathophysiology of pulmonary system in order to control gas exchange in ventilated patients, as under some circumstances (like emergency situations), there may not be a proper and individualized model for designing and tuning controllers available in time. The simulation results demonstrate the optimal control of oxygenation based on the proposed PIA by iteratively evaluating the Hamiltonian cost functions and synthesizing the control actions until achieving the converged optimal criteria. Furthermore, as a practical example, we examined the performance of this control strategy using an interconnecting three-tank system as a real nonlinear system. © 2016 Elsevier Ltd",Biomedical control system; Closed-loop ventilation; Control of oxygen saturation; Optimal control; Policy iteration algorithm; Reinforcement learning,Article,,Scopus,2-s2.0-84995900104
SCOPUS,Aijaz A.,Hap−SliceR: A Radio Resource Slicing Framework for 5G Networks With Haptic Communications,2017,IEEE Systems Journal,,,,,,,,10.1109/JSYST.2017.2647970,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010644351&doi=10.1109%2fJSYST.2017.2647970&partnerID=40&md5=2fe11834749f0b9b7ecdb3f9373535fe,,"Aijaz, A.","It is expected that the emerging 5G networks will not only support diverse use cases, but also enable unprecedented applications such as haptic communications. Therefore, network slicing will provide the required design flexibility. Radio resource slicing would be an indispensable component of any network slicing solution. This paper proposes Hap−SliceR, which is a novel radio resource slicing framework for 5G networks with haptic communications. First, Hap−SliceR derives a network-wide radio resource slicing strategy for 5G networks. The optimal slicing strategy, which is based on a reinforcement learning approach, allocates radio resources to different slices while accounting for the dynamics and utility requirements of different slices. Second, Hap−SliceR provides customization of radio resources for haptic communications over 5G networks. The radio resource allocation requirements of haptic communications have been translated into a unique radio resource allocation problem. A low-complexity heuristic algorithm has been developed for resource allocation. Finally, a comprehensive performance evaluation of Hap−SliceR has been conducted based on a recently proposed 5G air-interface design.",,Article in Press,,Scopus,2-s2.0-85010644351
SCOPUS,"Tavakol M., Brefeld U.",A Unified Contextual Bandit Framework for Long- and Short-Term Recommendations,2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10535 LNAI,,,269,284,,,10.1007/978-3-319-71246-8_17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040222425&doi=10.1007%2f978-3-319-71246-8_17&partnerID=40&md5=e4d3e682219cd10a53886c2234a9f5d5,"Leuphana Universität Lüneburg, Lüneburg, Germany; Technische Universität Darmstadt, Darmstadt, Germany","Tavakol, M., Leuphana Universität Lüneburg, Lüneburg, Germany, Technische Universität Darmstadt, Darmstadt, Germany; Brefeld, U., Leuphana Universität Lüneburg, Lüneburg, Germany","We present a unified contextual bandit framework for recommendation problems that is able to capture long- and short-term interests of users. The model is devised in dual space and the derivation is consequentially carried out using Fenchel-Legrende conjugates and thus leverages to a wide range of tasks and settings. We detail two instantiations for regression and classification scenarios and obtain well-known algorithms for these special cases. The resulting general and unified framework allows for quickly adapting contextual bandits to different applications at-hand. The empirical study demonstrates that the proposed long- and short-term framework outperforms both, short-term and long-term models on data. Moreover, a tweak of the combined model proves beneficial in cold start problems. © 2017, Springer International Publishing AG.",Contextual bandits; Dual optimization; Personalization; Recommendation,Conference Paper,,Scopus,2-s2.0-85040222425
SCOPUS,"Deshmukh A.A., Dogan U., Scott C.",Multi-task learning for contextual bandits,2017,Advances in Neural Information Processing Systems,2017-December,,,4849,4857,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047009825&partnerID=40&md5=70d38299b0bb67817c5702fd0efabb71,"Department of EECS, University of Michigan, Ann Arbor, Ann Arbor, MI, United States; Microsoft Research, Cambridge, United Kingdom","Deshmukh, A.A., Department of EECS, University of Michigan, Ann Arbor, Ann Arbor, MI, United States; Dogan, U., Microsoft Research, Cambridge, United Kingdom; Scott, C., Department of EECS, University of Michigan, Ann Arbor, Ann Arbor, MI, United States","Contextual bandits are a form of multi-armed bandit in which the agent has access to predictive side information (known as the context) for each arm at each time step, and have been used to model personalized news recommendation, ad placement, and other applications. In this work, we propose a multi-task learning framework for contextual bandit problems. Like multi-task learning in the batch setting, the goal is to leverage similarities in contexts for different arms so as to improve the agent's ability to predict rewards from contexts. We propose an upper confidence bound-based multi-task learning algorithm for contextual bandits, establish a corresponding regret bound, and interpret this bound to quantify the advantages of learning in the presence of high task (arm) similarity. We also describe an effective scheme for estimating task similarity from data, and demonstrate our algorithm's performance on several data sets. © 2017 Neural information processing systems foundation. All rights reserved.",,Conference Paper,,Scopus,2-s2.0-85047009825
SCOPUS,"Wang P., Rowe J., Min W., Mott B., Lester J.",Interactive narrative personalization with deep reinforcement learning,2017,IJCAI International Joint Conference on Artificial Intelligence,,,,3852,3858,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031928990&partnerID=40&md5=147d78aa4f5d60f8ded6123ba8b73b09,"Department of Computer Science, North Carolina State University, Raleigh, NC, United States","Wang, P., Department of Computer Science, North Carolina State University, Raleigh, NC, United States; Rowe, J., Department of Computer Science, North Carolina State University, Raleigh, NC, United States; Min, W., Department of Computer Science, North Carolina State University, Raleigh, NC, United States; Mott, B., Department of Computer Science, North Carolina State University, Raleigh, NC, United States; Lester, J., Department of Computer Science, North Carolina State University, Raleigh, NC, United States","Data-driven techniques for interactive narrative generation are the subject of growing interest. Reinforcement learning (RL) offers significant potential for devising data-driven interactive narrative generators that tailor players' story experiences by inducing policies from player interaction logs. A key open question in RL-based interactive narrative generation is how to model complex player interaction patterns to learn effective policies. In this paper we present a deep RL-based interactive narrative generation framework that leverages synthetic data produced by a bipartite simulated player model. Specifically, the framework involves training a set of Q-networks to control adaptable narrative event sequences with long short-term memory network-based simulated players. We investigate the deep RL framework's performance with an educational interactive narrative, Crystal Island. Results suggest that the deep RL-based narrative generation framework yields effective personalized interactive narratives.",,Conference Paper,,Scopus,2-s2.0-85031928990
SCOPUS,"Dheenadayalan K., Srinivasaraghavan G., Muralidhara V.N.",Policy gradient reinforcement learning for I/O reordering on storage servers,2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10634 LNCS,,,849,859,,,10.1007/978-3-319-70087-8_87,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035108108&doi=10.1007%2f978-3-319-70087-8_87&partnerID=40&md5=f15b7012441253d0502deabd14aefabf,"International Institute of Information Technology, Bangalore, Bengaluru, India","Dheenadayalan, K., International Institute of Information Technology, Bangalore, Bengaluru, India; Srinivasaraghavan, G., International Institute of Information Technology, Bangalore, Bengaluru, India; Muralidhara, V.N., International Institute of Information Technology, Bangalore, Bengaluru, India","Deep customization of storage architectures to the applications they support is often undesirable — nature of application data is dynamic, applications are replaced far more often than storage systems are and usage patterns change dynamically with time. A continuously learning software intervention that dynamically adapts to the changing workload pattern would be the easiest way to bridge this ‘gap’. As borne out by our experiments, the overhead induced by such software interventions turns out to be negligible for large-scale storage systems. Reinforcement Learning offers a way to dynamically learn from a continuous data stream and take appropriate actions towards optimizing a future goal. We adapt policy gradient reinforcement learning to learn a policy that minimizes I/O wait time that in turn maximizes I/O throughput. A set of discrete actions consisting of switches between scheduling schemes is considered to dynamically re-order client-specific I/O operations. Results reveal that I/O reordering policy learned using reinforcement learning results in significant improvement in the overall I/O throughput. © Springer International Publishing AG 2017.","Filer, I/O reordering; Overload; Policy gradient; Throughput",Conference Paper,,Scopus,2-s2.0-85035108108
SCOPUS,"Baniya A., Herrmann S., Qiao Q., Lu H.",Adaptive interventions treatment modelling and regimen optimization using Sequential Multiple Assignment Randomized Trials (SMART) and Q-learning,2017,67th Annual Conference and Expo of the Institute of Industrial Engineers 2017,,,,1187,1192,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031025743&partnerID=40&md5=80f6536d12d6f4028e4e66ec08b8128d,"Department of Electrical Engineering and Computer Science, United States; Sanford Health Research SD, United States; Department of Construction and Operations Management, South Dakota State University, United States","Baniya, A., Department of Electrical Engineering and Computer Science, United States, Department of Construction and Operations Management, South Dakota State University, United States; Herrmann, S., Sanford Health Research SD, United States; Qiao, Q., Department of Electrical Engineering and Computer Science, United States; Lu, H., Department of Construction and Operations Management, South Dakota State University, United States","Nowadays, pharmacological practices are focused on a single best treatment to treat a disease which sounds impractical as the same treatment may not work the same way for every patient. Thus, there is a need of shift towards more patient-centric rather than disease-centric approach, in which personal characteristics of a patient or biomarkers are used to determine the tailored optimal treatment. The ""one size fits all"" concept is contradicted by research area of personalized medicine. The Sequential Multiple Assignment Randomized Trial (SMART) is a multi-stage trials to inform the development of dynamic treatment regimens (DTR's). In SMART, a subject is randomized through different stages of treatment where each stage corresponds to a treatment decision. These types of adaptive interventions are individualized and are repeatedly adjusted across time based on patient's individual clinical characteristics and ongoing performance. The reinforcement learning (Q-learning), a computational algorithm for optimization of treatment regimens to maximize desired clinical outcome is used in optimizing the sequence of treatments. This statistical model contains regression analysis for function approximation of data from clinical trials. The model will predict a series of regimens across time, depending on the biomarkers of a new participant for optimizing the weight management decision rules.",Dynamic treatment regimens (DTR); Personalized medicine; Q-learning algorithm; Regression analysis; Sequential Multiple Assignment Randomized Trial (SMART),Conference Paper,,Scopus,2-s2.0-85031025743
SCOPUS,"Ratcliffe D.S., Devlin S., Kruschwit U., Citi Z.",Clyde: A deep reinforcement learning DOOM playing agent,2017,AAAI Workshop - Technical Report,WS-17-01 - WS-17-15,,,983,990,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044754102&partnerID=40&md5=c4dd4a346b27c0a2bdfdef7601a4fc4e,"University of Essex, Colchester, United Kingdom; University of York, York, United Kingdom","Ratcliffe, D.S., University of Essex, Colchester, United Kingdom; Devlin, S., University of York, York, United Kingdom; Kruschwit, U., University of Essex, Colchester, United Kingdom; Citi, Z., University of Essex, Colchester, United Kingdom","In this paper we present the use of deep reinforcement learning techniques in the context of playing partially observable multi-agent 3D games. These techniques have traditionally been applied to fully observable 2D environments, or navigation tasks in 3D environments. We show the performance of Clyde in comparison to other competitors within the context of the ViZDOOM competition that saw 9 bots compete against each other in DOOM death matches. Clyde managed to achieve 3rd place in the ViZDOOM competition held at the IEEE Conference on Computational Intelligence and Games 2016. Clyde performed very well considering its relative simplicity and the fact that we deliberately avoided a high level of customisation to keep the algorithm generic. © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,Conference Paper,,Scopus,2-s2.0-85044754102
SCOPUS,"Tripolitakis E., Chalkiadakis G.","Probabilistic topic modeling, reinforcement learning, and crowdsourcing for personalized recommendations",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10207 LNAI,,,157,171,,,10.1007/978-3-319-59294-7_14,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022214474&doi=10.1007%2f978-3-319-59294-7_14&partnerID=40&md5=e9c8b09a9368f99a2d652b2b8d3a9e8e,"School of Electrical and Computer Engineering, Technical University of Crete, Chania, Greece","Tripolitakis, E., School of Electrical and Computer Engineering, Technical University of Crete, Chania, Greece; Chalkiadakis, G., School of Electrical and Computer Engineering, Technical University of Crete, Chania, Greece","We put forward an innovative use of probabilistic topic modeling (PTM) intertwined with reinforcement learning (RL), to provide personalized recommendations. Specifically, we model items under recommendation as mixtures of latent topics following a distribution with Dirichlet priors; this can be achieved via the exploitation of crowd-sourced information for each item. Similarly, we model the user herself as an “evolving” document represented by its respective mixture of latent topics. The user’s topic distribution is appropriately updated each time she consumes an item. Recommendations are subsequently based on the divergence between the topic distributions of the user and available items. However, to tackle the exploration versus exploitation dilemma, we apply RL to vary the user’s topic distribution update rate. Our method is immune to the notorious “cold start” problem, and it can effectively cope with changing user preferences. Moreover, it is shown to be competitive against state-of-the-art algorithms, outperforming them in terms of sequential performance. © Springer International Publishing AG 2017.",Applications of reinforcement learning; Crowdsourcing; Graphical models; Recommender systems,Conference Paper,,Scopus,2-s2.0-85022214474
SCOPUS,Narvekar S.,Curriculum learning in reinforcement learning,2017,IJCAI International Joint Conference on Artificial Intelligence,,,,5195,5196,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031919399&partnerID=40&md5=bc1c2a4f7029255194de43ecc74eeb0d,"Department of Computer Science, University of Texas at Austin, United States","Narvekar, S., Department of Computer Science, University of Texas at Austin, United States","Transfer learning in reinforcement learning is an area of research that seeks to speed up or improve learning of a complex target task, by leveraging knowledge from one or more source tasks. This thesis will extend the concept of transfer learning to curriculum learning, where the goal is to design a sequence of source tasks for an agent to train on, such that final performance or learning speed is improved. We discuss completed work on this topic, including methods for semi-automatically generating source tasks tailored to an agent and the characteristics of a target domain, and automatically sequencing such tasks into a curriculum. Finally, we also present ideas for future work.",,Conference Paper,,Scopus,2-s2.0-85031919399
SCOPUS,[No author name available],CEUR Workshop Proceedings,2017,CEUR Workshop Proceedings,1905,,,,,50,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029217369&partnerID=40&md5=bb2a10c7da665839c225a51dd84a3964,,,"The proceedings contain 23 papers. The topics discussed include: intent-aware diversification using item-based subprofiles; explainable entity-based recommendations with knowledge graphs; WMRB: learning to rank in a scalable batch training approach; explanation chains: recommendations by explanation; multi cross domain recommendation using item embedding and canonical correlation analysis; the importance of song context in music playlists; alpenglow: open source recommender framework with time-aware learning and evaluation; towards a recommender system for undergraduate research; recommender systems in the Internet of Talking Things (IoTT); SemRevRec: a recommender system based on user reviews and linked data; a recommender system for personalized exploration of majors, minors, and concentrations; recommender systems for banking and financial services; can readability enhance recommendations on community question answering sites?; recommender popularity controls: an observational study; towards effective exploration/exploitation in sequential music recommendation; music emotion recognition via end-to-end multimodal neural networks; an explanatory matrix factorization with user comments data; the demographics of cool: popularity and recommender performance for different groups of users; kernalized collaborative contextual bandits; and pyRecLab: a software library for quick prototyping of recommender systems.",,Conference Review,,Scopus,2-s2.0-85029217369
SCOPUS,"Prasad N., Cheng L.-F., Chivers C., Draugelis M., Engelhardt B.E.",A reinforcement learning approach to weaning of mechanical ventilation in intensive care units,2017,"Uncertainty in Artificial Intelligence - Proceedings of the 33rd Conference, UAI 2017",,,,,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031120880&partnerID=40&md5=e33338ea1b60efc1a559dcfffb00b4f9,"Computer Science, Princeton University, United States; Electrical Engineering, Princeton University, United States; Penn Medicine, United States","Prasad, N., Computer Science, Princeton University, United States; Cheng, L.-F., Electrical Engineering, Princeton University, United States; Chivers, C., Penn Medicine, United States; Draugelis, M., Penn Medicine, United States; Engelhardt, B.E., Computer Science, Princeton University, United States","The management of invasive mechanical ventilation, and the regulation of sedation and analgesia during ventilation, constitutes a major part of the care of patients admitted to intensive care units. Both prolonged dependence on mechanical ventilation and premature extubation are associated with increased risk of complications and higher hospital costs, but clinical opinion on the best protocol for weaning patients off of a ventilator varies. This work aims to develop a decision support tool that uses available patient information to predict time-to-extubation readiness and to recommend a personalized regime of sedation dosage and ventilator support. To this end, we use off-policy reinforcement learning algorithms to determine the best action at a given patient state from sub-optimal historical ICU data. We compare treatment policies from fitted Qiteration with extremely randomized trees and with feedforward neural networks, and demonstrate that the policies learnt show promise in recommending weaning protocols with improved outcomes, in terms of minimizing rates of reintubation and regulating physiological stability.",,Conference Paper,,Scopus,2-s2.0-85031120880
SCOPUS,"Šošic A., KhudaBukhsh W.R., Zoubir A.M., Koeppl H.",Inverse reinforcement learning in swarm systems,2017,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",3,,,1413,1420,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040758666&partnerID=40&md5=5c6d8eb1951628959f8c41370a525801,"Department of Electrical Engineering and Information Technology, Technische Universität, Darmstadt, Germany","Šošic, A., Department of Electrical Engineering and Information Technology, Technische Universität, Darmstadt, Germany; KhudaBukhsh, W.R., Department of Electrical Engineering and Information Technology, Technische Universität, Darmstadt, Germany; Zoubir, A.M., Department of Electrical Engineering and Information Technology, Technische Universität, Darmstadt, Germany; Koeppl, H., Department of Electrical Engineering and Information Technology, Technische Universität, Darmstadt, Germany","Inverse reinforcement learning (IRL) has become a useful tool for learning behavioral models from demonstration data. However, IRL remains mostly unexplored for multi-agent systems. In this paper, we show how the principle of IRL can be extended to homogeneous large-scale problems, inspired by the collective swarming behavior of natural systems. In particular, we make the following contributions to the field: 1) We introduce the swarMDP framework, a subclass of decentralized partially observable Markov decision processes endowed with a swarm characterization. 2) Exploiting the inherent homogeneity of this framework, we reduce the resulting multi-agent IRL problem to a single-agent one by proving that the agent-specific value functions in this tnodel coincide. 3) To solve the corresponding control problem, we propose a novel heterogeneous learning scheme that is particularly tailored to the swarm setting. Results on two example systems demonstrate that our framework is able to produce meaningful local reward models from which we can replicate the observed global system dynamics. © Copyright 2017, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",Inverse reinforcement learning; Multi-agent systems; Swarms,Conference Paper,,Scopus,2-s2.0-85040758666
SCOPUS,"Merkle N., Zander S.",Agent-based assistance in ambient assisted living through reinforcement learning and semantic technologies: (Short paper),2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10574 LNCS,,,180,188,,,10.1007/978-3-319-69459-7_12,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032656608&doi=10.1007%2f978-3-319-69459-7_12&partnerID=40&md5=34e55c071586274d84d0079823608e8d,"FZI Forschungszentrum Informatik am KIT, Information Process Engineering, Haid-und-Neu-Str. 10-14, Karlsruhe, Germany; Institute for Computer Science, University of Applied Sciences Darmstadt, Schöfferstrasse 8B, Darmstadt, Germany","Merkle, N., FZI Forschungszentrum Informatik am KIT, Information Process Engineering, Haid-und-Neu-Str. 10-14, Karlsruhe, Germany; Zander, S., Institute for Computer Science, University of Applied Sciences Darmstadt, Schöfferstrasse 8B, Darmstadt, Germany","For impaired people, the conduction of certain daily life activities is problematic due to motoric and cognitive handicaps. For that reason, assistive agents in ambient assisted environments provide services that aim at supporting elderly and impaired people. However, these agents act in complex stochastic and indeterministic environments where the concrete effects of a performed action are usually unknown at design time. Furthermore, they have to perform varying tasks according to the user’s context and needs, wherefore an agent has to be flexible and able to recognize required capabilities in a certain situation in order to provide adequate, unobtrusive assistance. Hence, an expressive representation framework is required that relates user-specific impairments to required agent capabilities. This work presents an approach which (a) describes and links user impairments and capabilities using the formal, model-theoretic semantics expressed in OWL2 DL ontologies, (b) computes optimal policies through Reinforcement Learning and propagates these in an agent network. The presented approach improves the collaborative, personalized and adequate assistance of assistive agents and tailors the agent-based services to the user’s missing capabilities. © 2017, Springer International Publishing AG.",,Conference Paper,,Scopus,2-s2.0-85032656608
SCOPUS,[No author name available],"13th International Conference on Machine Learning and Data Mining in Pattern Recognition, MLDM 2017",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10358 LNAI,,,1,450,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025169323&partnerID=40&md5=d5a93704d0ac1e7a2c6f6c6ffef18cb4,,,The proceedings contain 31 papers. The special focus in this conference is on Machine Learning and Data Mining in Pattern Recognition. The topics include: An information retrieval approach for finding dependent subspaces of multiple views; predicting target events in industrial domains; importance of recommendation policy space in addressing click sparsity in personalized advertisement display; global flow and temporal-shape descriptors for human action recognition from 3D reconstruction data; reverse engineering gene regulatory networks using sampling and boosting techniques; detecting large concept extensions for conceptual analysis; qualitative and descriptive topic extraction from movie reviews using LDA; detecting relative anomaly; optimization for large-scale machine learning with distributed features and observations; a scalable and noise-resistant closed contiguous sequential patterns mining algorithm; sparse dynamic time warping; over-fitting in model selection with Gaussian process regression; machine learning-as-a-service and its application to medical informatics; anomaly detection from kepler satellite time-series data; prediction of insurance claim severity loss using regression models; a spectral clustering method for large-scale geostatistical datasets; vulnerability of deep reinforcement learning to policy induction attacks; mobile robot localization via machine learning; an analysis of the application of simplified silhouette to the evaluation of k-means clustering validity; summarization-guided greedy optimization of machine learning model; clustering aided support vector machines; mining player ranking dynamics in team sports and personalized visualization based upon wavelet transform for interactive software customization.,,Conference Review,,Scopus,2-s2.0-85025169323
SCOPUS,"Narvekar S., Sinapov J., Stone P.",Autonomous task sequencing for customized curriculum design in reinforcement learning,2017,IJCAI International Joint Conference on Artificial Intelligence,,,,2536,2542,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031926206&partnerID=40&md5=999d71abd37b44b4bd9f668a6b94f8b8,"Department of Computer Science, University of Texas, Austin, United States","Narvekar, S., Department of Computer Science, University of Texas, Austin, United States; Sinapov, J., Department of Computer Science, University of Texas, Austin, United States; Stone, P., Department of Computer Science, University of Texas, Austin, United States","Transfer learning is a method where an agent reuses knowledge learned in a source task to improve learning on a target task. Recent work has shown that transfer learning can be extended to the idea of curriculum learning, where the agent incrementally accumulates knowledge over a sequence of tasks (i.e. a curriculum). In most existing work, such curricula have been constructed manually. Furthermore, they are fixed ahead of time, and do not adapt to the progress or abilities of the agent. In this paper, we formulate the design of a curriculum as a Markov Decision Process, which directly models the accumulation of knowledge as an agent interacts with tasks, and propose a method that approximates an execution of an optimal policy in this MDP to produce an agent-specific curriculum. We use our approach to automatically sequence tasks for 3 agents with varying sensing and action capabilities in an experimental domain, and show that our method produces curricula customized for each agent that improve performance relative to learning from scratch or using a different agent's curriculum.",,Conference Paper,,Scopus,2-s2.0-85031926206
SCOPUS,"Bonini R.C., Da Silva F.L., Spina E., Costa A.H.R.",Using Options to Accelerate Learning of New Tasks According to Human Preferences,2017,AAAI Workshop - Technical Report,WS-17-01 - WS-17-15,,,643,650,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046100452&partnerID=40&md5=5301426c1524d266fb75276e43518ac7,"Escola Politdcnica da Universidade de São Paulo, São Paulo, Brazil","Bonini, R.C., Escola Politdcnica da Universidade de São Paulo, São Paulo, Brazil; Da Silva, F.L., Escola Politdcnica da Universidade de São Paulo, São Paulo, Brazil; Spina, E., Escola Politdcnica da Universidade de São Paulo, São Paulo, Brazil; Costa, A.H.R., Escola Politdcnica da Universidade de São Paulo, São Paulo, Brazil","Over the years, people need to incorporate a wider range of information and multiple objectives for their decision making. Nowadays, humans are dependent on computer systems to interpret and take profit from the huge amount of available data on the Internet. Hence, varied services, such as location- based systems, must combine a huge quantity of raw data to give the desired response to the user. However, as humans have different preferences, the optimal answer is different for each user profile, and few systems offer the service of solving tasks in a customized manner for each user. Reinforcement Learning (RL) has been used to autonomously train systems to solve (or assist on) decision-making tasks according to user preferences. However, the learning process is very slow and require many interactions with the environment. Therefore, we here propose to reuse knowledge from previous tasks to accelerate the learning process in a new task. Our proposal, called Multiobjective Options, accelerates learning while providing a customized solution according to the current user preferences. Our experiments in the Tourist World Domain show that our proposal learns faster and better than regular learning, and that the achieved solutions follow user preferences. © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,Conference Paper,,Scopus,2-s2.0-85046100452
SCOPUS,"Krakow E.F., Hemmer M., Wang T., Logan B., Arora M., Spellman S., Couriel D., Alousi A., Pidala J., Last M., Lachance S., Moodie E.E.M.",Tools for the Precision Medicine Era: How to Develop Highly Personalized Treatment Recommendations from Cohort and Registry Data Using Q-Learning,2017,American Journal of Epidemiology,186,2,,160,172,,1,10.1093/aje/kwx027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029668317&doi=10.1093%2faje%2fkwx027&partnerID=40&md5=1c867a9b4398186b0bd9c0698bd7be80,"Department of Epidemiology Biostatistics and Occupational Health, Faculty of Medicine, McGill University, 1020 Pine Avenue West, Montreal, QC, Canada; Division of Hematology and Oncology, Department of Medicine, Hopital MaisonneuveRosemont Research Center, University of Montreal, Montreal, QC, Canada; Center for International Blood and Marrow Transplant Research, Department of Medicine, Medical College of Wisconsin, Milwaukee, WI, United States; Division of Biostatistics, Institute for Health and Society, Medical College of Wisconsin, Milwaukee, WI, United States; Division of Hematology, Oncology, and Transplantation, Department of Medicine, University of Minnesota Medical Center, Minneapolis, MN, United States; Center for International Blood and Marrow Transplant Research, National Marrow Donor Program/Be the Match, Minneapolis, MN, United States; Division of Hematology/BMT, Department of Internal Medicine, University of Utah Health Care and Huntsman Cancer Center, University of Utah, Salt Lake City, UT, United States; Division of Cancer Medicine, Department of Stem Cell Transplantation, University of Texas M.D. Anderson Cancer Center, Houston, TX, United States; Department of Blood and Marrow Transplantation, Moffitt Cancer Center, Tampa, FL, United States; US Department of Defense, Fort Meade, MD, United States","Krakow, E.F., Department of Epidemiology Biostatistics and Occupational Health, Faculty of Medicine, McGill University, 1020 Pine Avenue West, Montreal, QC, Canada, Division of Hematology and Oncology, Department of Medicine, Hopital MaisonneuveRosemont Research Center, University of Montreal, Montreal, QC, Canada; Hemmer, M., Department of Epidemiology Biostatistics and Occupational Health, Faculty of Medicine, McGill University, 1020 Pine Avenue West, Montreal, QC, Canada; Wang, T., Center for International Blood and Marrow Transplant Research, Department of Medicine, Medical College of Wisconsin, Milwaukee, WI, United States, Division of Biostatistics, Institute for Health and Society, Medical College of Wisconsin, Milwaukee, WI, United States; Logan, B., Center for International Blood and Marrow Transplant Research, Department of Medicine, Medical College of Wisconsin, Milwaukee, WI, United States, Division of Biostatistics, Institute for Health and Society, Medical College of Wisconsin, Milwaukee, WI, United States; Arora, M., Division of Hematology, Oncology, and Transplantation, Department of Medicine, University of Minnesota Medical Center, Minneapolis, MN, United States; Spellman, S., Center for International Blood and Marrow Transplant Research, National Marrow Donor Program/Be the Match, Minneapolis, MN, United States; Couriel, D., Division of Hematology/BMT, Department of Internal Medicine, University of Utah Health Care and Huntsman Cancer Center, University of Utah, Salt Lake City, UT, United States; Alousi, A., Division of Cancer Medicine, Department of Stem Cell Transplantation, University of Texas M.D. Anderson Cancer Center, Houston, TX, United States; Pidala, J., Department of Blood and Marrow Transplantation, Moffitt Cancer Center, Tampa, FL, United States; Last, M., Center for International Blood and Marrow Transplant Research, Department of Medicine, Medical College of Wisconsin, Milwaukee, WI, United States, US Department of Defense, Fort Meade, MD, United States; Lachance, S., Division of Hematology and Oncology, Department of Medicine, Hopital MaisonneuveRosemont Research Center, University of Montreal, Montreal, QC, Canada; Moodie, E.E.M., Department of Epidemiology Biostatistics and Occupational Health, Faculty of Medicine, McGill University, 1020 Pine Avenue West, Montreal, QC, Canada","Q-Learning is a method of reinforcement learning that employs backwards stagewise estimation to identify sequences of actions that maximize some long-term reward. The method can be applied to sequential multipleassignment randomized trials to develop personalized adaptive treatment strategies (ATSs)-longitudinal practice guidelines highly tailored to time-varying attributes of individual patients. Sometimes, the basis for choosing which ATSs to include in a sequential multiple-assignment randomized trial (or randomized controlled trial) may be inadequate. Nonrandomized data sources may inform the initial design of ATSs, which could later be prospectively validated. In this paper, we illustrate challenges involved in using nonrandomized data for this purpose with a case study from the Center for International Blood and Marrow Transplant Research registry (1995-2007) aimed at 1) determining whether the sequence of therapeutic classes used in graft-versus-host disease prophylaxis and in refractory graft-versus-host disease is associated with improved survival and 2) identifying donor and patient factors with which to guide individualized immunosuppressant selections over time. We discuss how to communicate the potential benefit derived from following an ATS at the population and subgroup levels and how to evaluate its robustness to modeling assumptions. This worked example may serve as a model for developing ATSs from registries and cohorts in oncology and other fields requiring sequential treatment decisions. © 2017 The Author(s).",Adaptive treatment strategies; Dynamic treatment regimes; Graft-versus-host disease; Machine learning; Personalized medicine; Prediction; Q-learning; Registry data,Article,,Scopus,2-s2.0-85029668317
SCOPUS,"Dhingra B., Li L., Li X., Gao J., Chen Y.-N., Ahmed F., Deng L.",Towards end-to-end reinforcement learning of dialogue agents for information access,2017,"ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)",1,,,484,495,,3,10.18653/v1/P17-1045,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030087610&doi=10.18653%2fv1%2fP17-1045&partnerID=40&md5=c571cfd88d6caaedbd803bdfd2feb1df,"Carnegie Mellon University, Pittsburgh, PA, United States; Microsoft Research, Redmond, WA, United States; National, Taiwan University, Taipei, Taiwan","Dhingra, B., Carnegie Mellon University, Pittsburgh, PA, United States; Li, L., Microsoft Research, Redmond, WA, United States; Li, X., Microsoft Research, Redmond, WA, United States; Gao, J., Microsoft Research, Redmond, WA, United States; Chen, Y.-N., National, Taiwan University, Taipei, Taiwan; Ahmed, F., Microsoft Research, Redmond, WA, United States; Deng, L., Microsoft Research, Redmond, WA, United States","This paper proposes KB-InfoBot1 - a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. Such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge. Previous systems achieved this by issuing a symbolic query to the KB to retrieve entries based on their attributes. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced ""soft"" posterior distribution over the KB that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users. We also present a fully neural end-to-end agent, trained entirely from user feedback, and discuss its application towards personalized dialogue agents. © 2017 Association for Computational Linguistics.",,Conference Paper,,Scopus,2-s2.0-85030087610
SCOPUS,"Chen J., Wang C., Xiao L., He J., Li L., Deng L.",Q-LDA: Uncovering latent patterns in text-based sequential decision processes,2017,Advances in Neural Information Processing Systems,2017-December,,,4978,4987,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047005909&partnerID=40&md5=f243dfada6ff6acc567438af82c954c9,"Microsoft Research, Redmond, WA, United States; Google Inc., Kirkland, WA, United States; Citadel LLC, Seattle/Chicago, United States","Chen, J., Microsoft Research, Redmond, WA, United States; Wang, C., Google Inc., Kirkland, WA, United States; Xiao, L., Microsoft Research, Redmond, WA, United States; He, J., Citadel LLC, Seattle/Chicago, United States; Li, L., Google Inc., Kirkland, WA, United States; Deng, L., Citadel LLC, Seattle/Chicago, United States","In sequential decision making, it is often important and useful for end users to understand the underlying patterns or causes that lead to the corresponding decisions. However, typical deep reinforcement learning algorithms seldom provide such information due to their black-box nature. In this paper, we present a probabilistic model, Q-LDA, to uncover latent patterns in text-based sequential decision processes. The model can be understood as a variant of latent topic models that are tailored to maximize total rewards; we further draw an interesting connection between an approximate maximum-likelihood estimation of Q-LDA and the celebrated Q-learning algorithm. We demonstrate in the text-game domain that our proposed method not only provides a viable mechanism to uncover latent patterns in decision processes, but also obtains state-of-the-art rewards in these games. © 2017 Neural information processing systems foundation. All rights reserved.",,Conference Paper,,Scopus,2-s2.0-85047005909
SCOPUS,"Yu Z., Black A.W., Rudnicky A.I.",Learning conversational systems that interleave task and non-task content,2017,IJCAI International Joint Conference on Artificial Intelligence,,,,4214,4220,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031939839&partnerID=40&md5=35012a2acb00bcb4b13740192de24e3f,"Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA, United States","Yu, Z., Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA, United States; Black, A.W., Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA, United States; Rudnicky, A.I., Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA, United States","Task-oriented dialog systems have been applied in various tasks, such as automated personal assistants, customer service providers and tutors. These systems work well when users have clear and explicit intentions that are well-aligned to the systems' capabilities. However, they fail if users intentions are not explicit. To address this shortcoming, we propose a framework to interleave nontask content (i.e. everyday social conversation) into task conversations. When the task content fails, the system can still keep the user engaged with the non-task content. We trained a policy using reinforcement learning algorithms to promote long-turn conversation coherence and consistency, so that the system can have smooth transitions between task and non-task content. To test the effectiveness of the proposed framework, we developed a movie promotion dialog system. Experiments with human users indicate that a system that interleaves social and task content achieves a better task success rate and is also rated as more engaging compared to a pure task-oriented system.",,Conference Paper,,Scopus,2-s2.0-85031939839
SCOPUS,"Ribeiro R., Enembreck F., Guisi D.M., Casanova D., Teixeira M., De Souza F.A., Borges A.P.",An Advanced Software Tool to Simulate Service Restoration Problems: A case study on Power Distribution Systems,2017,Procedia Computer Science,108,,,675,684,,1,10.1016/j.procs.2017.05.248,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027365477&doi=10.1016%2fj.procs.2017.05.248&partnerID=40&md5=d2adc95a68ffa6a01f2a02e19646d737,"Federal University of Paraná, Curitiba, Parana, Brazil; Department of Informatics, Federal University of Technology of Paraná, Pato Branco, Brazil; Federal University of Technology of Paraná, Ponta Grossa, Brazil; Pontifical Catholic University of Paraná, Curitiba, Brazil","Ribeiro, R., Federal University of Paraná, Curitiba, Parana, Brazil; Enembreck, F., Pontifical Catholic University of Paraná, Curitiba, Brazil; Guisi, D.M., Department of Informatics, Federal University of Technology of Paraná, Pato Branco, Brazil; Casanova, D., Department of Informatics, Federal University of Technology of Paraná, Pato Branco, Brazil; Teixeira, M., Department of Informatics, Federal University of Technology of Paraná, Pato Branco, Brazil; De Souza, F.A., Federal University of Paraná, Curitiba, Parana, Brazil; Borges, A.P., Federal University of Technology of Paraná, Ponta Grossa, Brazil","This paper presents a software tool to simulate a practical problem in smart grid systems. A feature of the smart grid is a system self-recovery capability in the occurrence of anomalies, such as a recovery of a power distribution network after an occurrence of a fault. When this system has a capacity for self-recovery, it is called self-healing. The intersection among areas as computer science, telecommunication, automation and electrical engineering, has allowed power systems to gain new technologies. However, because it is a multi-area domain, self-recovery simulation tools in smart grids are often highly complex as well as presenting low fidelity by using approximation algorithms. The main contribution of this paper is a simulator with high fidelity and low complexity in terms of programming, usability and semantics. In this simulator, a computational intelligence technique and a derivative method for calculating the power flow were encapsulated. The result is a software tool with high abstraction and easy customization, aimed at a self-healing system for a reconfiguration of an electric power distribution network. © 2017 The Authors. Published by Elsevier B.V.",Reinforcement Learning; Restoration Problem; Self-Healing Smart Grid; Software Tool,Conference Paper,Open Access,Scopus,2-s2.0-85027365477
SCOPUS,"Kolomvatsos K., Hadjiefthymiades S.",Learning the engagement of query processors for intelligent analytics,2017,Applied Intelligence,46,1,,96,112,,,10.1007/s10489-016-0821-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979686223&doi=10.1007%2fs10489-016-0821-z&partnerID=40&md5=2f57169e41217e042590dc68523f42e8,"Department of Computer Science, University of Thessaly, Lamia, Greece; Department of Informatics and Telecommunications, University of Athens, Athens, Greece","Kolomvatsos, K., Department of Computer Science, University of Thessaly, Lamia, Greece; Hadjiefthymiades, S., Department of Informatics and Telecommunications, University of Athens, Athens, Greece","Current applications require the processing of huge amounts of data produced by applications or end users personal devices. In such settings, intelligent analytics on top of large scale data are the key research subject for future data driven decision making. Due to the huge amount of data, analytics should be based on an efficient technique for querying big data partitions. Each partition contains only a part of the data and a processor is dedicated to execute queries for the corresponding partition. A Query Controller (QC) is responsible for managing continuous queries and returning the final outcome to users / applications by using the underlying processors. In this paper, we propose a learning scheme to be adopted by the QC for allocating each query to the available processors. We adopt the Q-learning algorithm to calculate the reward that the QC obtains for every allocation between queries and processors. The outcome is an efficient model that derives the optimal allocation for the incoming queries. We provide mathematical formulations for solving the discussed problem and present our simulation results. Through a large number of simulations, we reveal the advantages of the proposed model and give numerical results while comparing our framework with a baseline model. © 2016, Springer Science+Business Media New York.",Intelligent analytics; Q-learning; Query streams; Reinforcement learning,Article,,Scopus,2-s2.0-84979686223
SCOPUS,[No author name available],"Confederated International Conference On the Move to Meaningful Internet Systems, OTM 2017 held in conjunction with Conferences on CoopIS, CandTC and ODBASE 2017",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10574 LNCS,,,1,764,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032682280&partnerID=40&md5=64f1a023d17fd6de503db528604bf594,,,The proceedings contain 80 papers. The special focus in this conference is on Move to Meaningful Internet Systems. The topics include: Agent-based assistance in ambient assisted living through reinforcement learning and semantic technologies: (Short paper); on the need for applications aware adaptive middleware in real-time RDF data analysis (short paper); learning probabilistic relational models using an ontology of transformation processes; oRDAIN: An ontology for trust management in the internet of things: (Short paper); aPOPSIS: A web-based platform for the analysis of structured dialogues; identifying opinion drivers on social media; representing fashion product data with Schema.org: Approach and use cases; semantic modeling and inference with episodic organization for managing personal digital traces: (Short paper); towards a JSON-based fast policy evaluation framework: (Short paper); linked open data for linguists: Publishing the hartmann von aue-portal in RDF; a survey of approaches to representing SPARQL variables in SQL queries; semantic OLAP patterns: Elements of reusable business analytics; an extensible ontology modeling approach using post coordinated expressions for semantic provenance in biomedical research; complete semantics to empower touristic service providers; distributed holistic clustering on linked data; ontologies for commitment-based smart contracts; a Framework for user-driven mapping discovery in rich spaces of heterogeneous data; ontologies and human users: A systematic analysis of the influence of ontology documentation on community agreement about type membership; dLUBM: A benchmark for distributed linked data knowledge base systems; gibbon: An availability evaluation framework for distributed databases; norwegian State of estate report as linked open data.,,Conference Review,,Scopus,2-s2.0-85032682280
SCOPUS,[No author name available],"Brazilian Symposium on Games and Digital Entertainment, SBGAMES",2016,"Brazilian Symposium on Games and Digital Entertainment, SBGAMES",,,,,,216,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010425265&partnerID=40&md5=a3ab42cda68be3660e2fe6f6bdf1555b,,,The proceedings contain 21 papers. The topics discussed include: an indoor navigation system for live-action virtual reality games; an adaptive methodology for facial expression transfer; dynamic game difficulty balancing in real time using evolutionary fuzzy cognitive maps; exploring parallel game architectures with tardiness policy; evaluation of game-based learning approaches through digital serious games in computer science higher education: a systematic mapping; MuSSE: a tool to extract meta-data from game sprite sheets using blob detection algorithm; GameVis: game data visualization for the web; attempting to discover infinite combos in fighting games using hidden Markov models; real-time procedural generation of personalized facade and interior appearances based on semantics; an authoring tool for location-based mobile games with augmented reality features; realistic rendering in 3D walkthroughs with high quality fast reflections; Grybot: a didactic guitar hero robot player on FPGA; sensor data fusion for full arm tracking using Myo armband and leap motion; simulating human behavior in fighting games using reinforcement learning and artificial neural networks; evolving finite-state machines controllers for the simulated car racing championship; and ATreVEE IN: using natural interaction in procedure simulator for training in the electricity sector.,,Conference Review,,Scopus,2-s2.0-85010425265
SCOPUS,"Neumann D., Mansi T., Itu L., Georgescu B., Kayvanpour E., Sedaghat-Hamedani F., Amr A., Haas J., Katus H., Meder B., Steidl S., Hornegger J., Comaniciu D.",A self-taught artificial agent for multi-physics computational model personalization,2016,Medical Image Analysis,34,,,52,64,,5,10.1016/j.media.2016.04.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964690557&doi=10.1016%2fj.media.2016.04.003&partnerID=40&md5=b1d426957966100eec637cc992aa1ff5,"Medical Imaging Technologies, Siemens Healthcare GmbH, Erlangen, Germany; Medical Imaging Technologies, Siemens Healthcare, Princeton, United States; Pattern Recognition Lab, FAU Erlangen-Nürnberg, Erlangen, Germany; Siemens Corporate Technology, Siemens SRL, Brasov, Romania; Transilvania University of Brasov, Brasov, Romania; Department of Internal Medicine III, University Hospital Heidelberg, Germany","Neumann, D., Medical Imaging Technologies, Siemens Healthcare GmbH, Erlangen, Germany, Pattern Recognition Lab, FAU Erlangen-Nürnberg, Erlangen, Germany; Mansi, T., Medical Imaging Technologies, Siemens Healthcare, Princeton, United States; Itu, L., Siemens Corporate Technology, Siemens SRL, Brasov, Romania, Transilvania University of Brasov, Brasov, Romania; Georgescu, B., Medical Imaging Technologies, Siemens Healthcare, Princeton, United States; Kayvanpour, E., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Sedaghat-Hamedani, F., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Amr, A., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Haas, J., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Katus, H., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Meder, B., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Steidl, S., Pattern Recognition Lab, FAU Erlangen-Nürnberg, Erlangen, Germany; Hornegger, J., Pattern Recognition Lab, FAU Erlangen-Nürnberg, Erlangen, Germany; Comaniciu, D., Medical Imaging Technologies, Siemens Healthcare, Princeton, United States","Personalization is the process of fitting a model to patient data, a critical step towards application of multi-physics computational models in clinical practice. Designing robust personalization algorithms is often a tedious, time-consuming, model- and data-specific process. We propose to use artificial intelligence concepts to learn this task, inspired by how human experts manually perform it. The problem is reformulated in terms of reinforcement learning. In an off-line phase, Vito, our self-taught artificial agent, learns a representative decision process model through exploration of the computational model: it learns how the model behaves under change of parameters. The agent then automatically learns an optimal strategy for on-line personalization. The algorithm is model-independent; applying it to a new model requires only adjusting few hyper-parameters of the agent and defining the observations to match. The full knowledge of the model itself is not required. Vito was tested in a synthetic scenario, showing that it could learn how to optimize cost functions generically. Then Vito was applied to the inverse problem of cardiac electrophysiology and the personalization of a whole-body circulation model. The obtained results suggested that Vito could achieve equivalent, if not better goodness of fit than standard methods, while being more robust (up to 11% higher success rates) and with faster (up to seven times) convergence rate. Our artificial intelligence approach could thus make personalization algorithms generalizable and self-adaptable to any patient and any model. © 2016",Artificial intelligence; Computational modeling; Model personalization; Reinforcement learning,Article,,Scopus,2-s2.0-84964690557
SCOPUS,"Peng B., Jiao Q., Kürner T.",Angle of arrival estimation in dynamic indoor THz channels with Bayesian filter and reinforcement learning,2016,European Signal Processing Conference,2016-November,,7760594,1975,1979,,1,10.1109/EUSIPCO.2016.7760594,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006058903&doi=10.1109%2fEUSIPCO.2016.7760594&partnerID=40&md5=dc72e06c6f7685dc17b3ad52ad3a3109,"Technische Universität Braunschweig, Schleinitzstraße 22, Braunschweig, Germany","Peng, B., Technische Universität Braunschweig, Schleinitzstraße 22, Braunschweig, Germany; Jiao, Q., Technische Universität Braunschweig, Schleinitzstraße 22, Braunschweig, Germany; Kürner, T., Technische Universität Braunschweig, Schleinitzstraße 22, Braunschweig, Germany","This paper presents a novel algorithm to estimate the Angle of Arrival (AoA) in a dynamic indoor Terahertz channel. In a realistic application, the user equipment is often moved by the user during the data transmission and the AoA must be estimated periodically, such that the adaptive directional antenna can be adjusted to realize a high antenna gain. The Bayesian filter is applied to exploit continuity and smoothness of the channel dynamics for the AoA estimation. Reinforcement learning is introduced to adapt the prior transition probabilities between system states, in order to fit the variation of application scenarios and personal habits. The algorithm is validated using the ray launching channel simulator and realistic human movement models. © 2016 IEEE.",Angle of arrival estimation; Bayesian filter; Dynamic channel; Reinforcement learning; Terahertz communication,Conference Paper,,Scopus,2-s2.0-85006058903
SCOPUS,"Jaradat S., Dokoohaki N., Matskin M., Ferrari E.",Trust and privacy correlations in social networks: A deep learning framework,2016,"Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2016",,,7752236,203,206,,1,10.1109/ASONAM.2016.7752236,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006765626&doi=10.1109%2fASONAM.2016.7752236&partnerID=40&md5=fb08bc9dfc0d971eb728bd1856cff9d7,"KTH, Royal Institute of Technology, Sweden; University of Insubria, Italy","Jaradat, S., KTH, Royal Institute of Technology, Sweden; Dokoohaki, N., KTH, Royal Institute of Technology, Sweden; Matskin, M., KTH, Royal Institute of Technology, Sweden; Ferrari, E., University of Insubria, Italy","Online Social Networks (OSNs) remain the focal point of Internet usage. Since the beginning, networking sites tried best to have right privacy mechanisms in place for users, enabling them to share the right content with the right audience. With all these efforts, privacy customizations remain hard for users across the sites. Existing research that address this problem mainly focus on semi-supervised strategies that introduce extra complexity by requiring the user to manually specify initial privacy preferences for their friends. In this work, we suggest an adaptive solution that can dynamically generate privacy labels for users in OSNs. To this end, we introduce a deep reinforcement learning framework that targets two key problems in OSNs like Facebook: the exposure of users' interactions through the network to less trusted direct friends, and the possibility of propagating user updates through direct friends' interactions to indirect friends. By implementing this framework, we aim at understanding how social trust and privacy could be correlated, specifically in a dynamic fashion. We report the ranked dependence between the generated privacy labels and the estimated user trust values, which indicate the ability of the framework to identify the highly trusted users and share with them higher percentages of data. © 2016 IEEE.",,Conference Paper,,Scopus,2-s2.0-85006765626
SCOPUS,"Srinivasan A.R., Chakraborty S.",Path planning with user route preference-A reward surface approximation approach using orthogonal Legendre polynomials,2016,IEEE International Conference on Automation Science and Engineering,2016-November,,7743527,1100,1105,,,10.1109/COASE.2016.7743527,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000983771&doi=10.1109%2fCOASE.2016.7743527&partnerID=40&md5=645b81682cd8447f905ebd9b7d45d417,"Department of Mechanical Aerospace and Biomedical Engineering, University of Tennessee, Knoxville, TN, United States","Srinivasan, A.R., Department of Mechanical Aerospace and Biomedical Engineering, University of Tennessee, Knoxville, TN, United States; Chakraborty, S., Department of Mechanical Aerospace and Biomedical Engineering, University of Tennessee, Knoxville, TN, United States","As self driving cars become more ubiquitous, users would look for natural ways of informing the car AI about their personal choice of routes. This choice is not always dictated by straightforward logic such as shortest distance or shortest time, and can be influenced by hidden factors, such as comfort and familiarity. This paper presents a path learning algorithm for such applications, where from limited positive demonstrations, an autonomous agent learns the user's path preference and honors that choice in its route planning, but has the capability to adopt alternate routes, if the original choice(s) become impractical. The learning problem is modeled as a Markov decision process. The states (way-points) and actions (to move from one way-point to another) are pre-defined according to the existing network of paths between the origin and destination and the user's demonstration is assumed to be a sample of the preferred path. The underlying reward function which captures the essence of the demonstration is computed using an inverse reinforcement learning algorithm and from that the entire path mirroring the expert's demonstration is extracted. To alleviate the problem of state space explosion when dealing with a large state space, the reward function is approximated using a set of orthogonal polynomial basis functions with a fixed number of coefficients regardless of the size of the state space. A six fold reduction in total learning time is achieved compared to using simple basis functions, that has dimensionality equal to the number of distinct states. © 2016 IEEE.",,Conference Paper,,Scopus,2-s2.0-85000983771
SCOPUS,[No author name available],HotNets 2016 - Proceedings of the 15th ACM Workshop on Hot Topics in Networks,2016,HotNets 2016 - Proceedings of the 15th ACM Workshop on Hot Topics in Networks,,,,,,214,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002202370&partnerID=40&md5=7cf293350616764078e33ce9d8b28351,,,"The proceedings contain 30 papers. The topics discussed include: helping the lone operator in the vast frontier; a case for personal virtual networks; towards comprehensive repositories of opinions; a knowledge-based systems approach to reason about networking; automatic synthesis of NF models by program analysis; canaries in the network; resource management with deep reinforcement learning; towards a redundancy-aware network stack for data centers; flat-tree: a convertible data center network architecture from clos to random graph; deadlocks in datacenter networks: why do they form, and how to avoid them; and switches are monitors too!: stateful property monitoring as a switch design criterion.",,Conference Review,,Scopus,2-s2.0-85002202370
SCOPUS,"Ferretti S., Mirri S., Prandi C., Salomoni P.",Automatic web content personalization through reinforcement learning,2016,Journal of Systems and Software,121,,,157,169,,9,10.1016/j.jss.2016.02.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975757906&doi=10.1016%2fj.jss.2016.02.008&partnerID=40&md5=8d7c3661bc0d939a042f1b25211981a7,"Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy","Ferretti, S., Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy; Mirri, S., Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy; Prandi, C., Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy; Salomoni, P., Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy","This paper deals with the automatic adaptation of Web contents. It is recognized that quite often users need some personalized adaptations to access Web contents. This is more evident when we focus on people with some accessibility needs. Based on the user profile, it is possible to transcode or modify contents (e.g., adapt text fonts) so as to meet the user preferences. The problem is that applying such a kind of transformations to the whole content might significantly alter Web pages that might become unreadable, hence making matters worse. We present a system that employs Web intelligence to perform automatic adaptations on single elements composing a Web page. A reinforcement learning algorithm is utilized to manage user profiles. We evaluate our system through simulation and a real assessment where elderly users where asked to use for a time period our system prototype. Results confirm the feasibility of the proposal. © 2016 Elsevier Inc.",Reinforcement learning; User profiling; Web personalization,Article,,Scopus,2-s2.0-84975757906
SCOPUS,"Hiraoka T., Neubig G., Sakti S., Toda T., Nakamura S.",Learning cooperative persuasive dialogue policies using framing,2016,Speech Communication,84,,,83,96,,3,10.1016/j.specom.2016.09.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989201964&doi=10.1016%2fj.specom.2016.09.002&partnerID=40&md5=6d48db88d86248c3e2ce1aa7891e78c2,"Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan; Nagoya University, Furo-cho, Chikusa-kuNagoya, Japan","Hiraoka, T., Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan; Neubig, G., Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan; Sakti, S., Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan; Toda, T., Nagoya University, Furo-cho, Chikusa-kuNagoya, Japan; Nakamura, S., Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan","In this paper, we propose a new framework of cooperative persuasive dialogue, where a dialogue system simultaneously attempts to achieve user satisfaction while persuading the user to take some action that achieves a pre-defined system goal. Within this framework, we describe a method for reinforcement learning of cooperative persuasive dialogue policies by defining a reward function that reflects both the system and user goal, and using framing, the use of emotionally charged statements common in persuasive dialogue between humans. In order to construct the various components necessary for reinforcement learning, we first describe a corpus of persuasive dialogues between human interlocutors, then propose a method to construct user simulators and reward functions specifically tailored to persuasive dialogue based on this corpus. Then, we implement a fully automatic text-based dialogue system for evaluating the learned policies. Using the implemented dialogue system, we evaluate the learned policy and the effect of framing through experiments both with a user simulator and with real users. The experimental evaluation indicates that the proposed method is effective for construction of cooperative persuasive dialogue systems. © 2016 Elsevier B.V.",Cooperative persuasive dialogue; Dialogue modeling; Dialogue system; Framing; Reinforcement learning,Article,,Scopus,2-s2.0-84989201964
SCOPUS,[No author name available],"Proceedings - 2016 3rd International Conference on Information Science and Control Engineering, ICISCE 2016",2016,"Proceedings - 2016 3rd International Conference on Information Science and Control Engineering, ICISCE 2016",,,,,,1492,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000916545&partnerID=40&md5=4c74c1038b0d5af31a3e3ad8de213d71,,,"The proceedings contain 301 papers. The topics discussed include: a cloud computing resource allocation model based on combinatorial double auction; a component business model-based approach for business architecture integration of C4ISR system; a framework of e-learning education clouds to efficiency and personalization; a framework of multi-index modeling for similarity-based remaining useful life estimation; a fused multi-feature based co-training approach for document clustering; a kind of efficient data archiving method for historical sensor data; a manycore processor based multilayer perceptron feedforward acceleration framework for embedded system; a method for cracking the password of WPA2-PSK based on SA and HMM; a new cuckoo search and its application of spread spectrum radar polly phase code design; a new image processing enabled approach for detection of scratch defects for wire-type objects; a subnetting mechanism with low cost deadlock-free design for irregular topologies in NoC-based Manycore processors; a uniform identity authentication method based on cookie ticket; access features analysis of things in the internet of things; acquisition of hovering by actual UAV using reinforcement learning; and computer-aided design, manufacturing and test of a ferrite toroidal planar transformer integrated in a printed circuit.",,Conference Review,,Scopus,2-s2.0-85000916545
SCOPUS,"Han M., Senellart P., Bressan S., Wu H.",Routing an autonomous taxi with reinforcement learning,2016,"International Conference on Information and Knowledge Management, Proceedings",24-28-October-2016,,,2421,2424,,1,10.1145/2983323.2983379,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996565915&doi=10.1145%2f2983323.2983379&partnerID=40&md5=68050ff7024ebcda3277136c6f8d0512,"Télécom ParisTech, Paris, France; IPAL, I2R, ASTAR, Singapore, Singapore; IPAL, NUS, Singapore, Singapore","Han, M., Télécom ParisTech, Paris, France, IPAL, I2R, ASTAR, Singapore, Singapore; Senellart, P., Télécom ParisTech, Paris, France, IPAL, NUS, Singapore, Singapore; Bressan, S., IPAL, NUS, Singapore, Singapore; Wu, H., IPAL, I2R, ASTAR, Singapore, Singapore","Singapore's vision of a Smart Nation encompasses the development of effective and efficient means of transportation. The government's target is to leverage new technologies to create services for a demand-driven intelligent transportation model including personal vehicles, public transport, and taxis. Singapore's government is strongly encouraging and supporting research and development of technologies for autonomous vehicles in general and autonomous taxis in particular. The design and implementation of intelligent routing algorithms is one of the keys to the deployment of autonomous taxis. In this paper we demonstrate that a reinforcement learning algorithm of the Q-learning family, based on a customized exploration and exploitation strategy, is able to learn optimal actions for the routing autonomous taxis in a real scenario at the scale of the city of Singapore with pick-up and drop-off events for a fleet of one thousand taxis. © 2016 Copyright held by the owner/author(s).",,Conference Paper,,Scopus,2-s2.0-84996565915
SCOPUS,"Xu J., Xing T., Van Der Schaar M.",Personalized Course Sequence Recommendations,2016,IEEE Transactions on Signal Processing,64,20,7524023,5340,5352,,6,10.1109/TSP.2016.2595495,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984987651&doi=10.1109%2fTSP.2016.2595495&partnerID=40&md5=3044563f1d29a3a7ab6c97d4301874bc,"Department of Electrical and Computer Engineering, University of Miami, Coral Gables, FL, United States; Department of Electrical Engineering, University of California, Los Angeles, CA, United States","Xu, J., Department of Electrical and Computer Engineering, University of Miami, Coral Gables, FL, United States; Xing, T., Department of Electrical Engineering, University of California, Los Angeles, CA, United States; Van Der Schaar, M., Department of Electrical Engineering, University of California, Los Angeles, CA, United States","Given the variability in student learning, it is becoming increasingly important to tailor courses as well as course sequences to student needs. This paper presents a systematic methodology for offering personalized course sequence recommendations to students. First, a forward-search backward-induction algorithm is developed that can optimally select course sequences to decrease the time required for a student to graduate. The algorithm accounts for prerequisite requirements (typically present in higher level education) and course availability. Second, using the tools of multiarmed bandits, an algorithm is developed that can optimally recommend a course sequence that both reduces the time to graduate while also increasing the overall GPA of the student. The algorithm dynamically learns how students with different contextual backgrounds perform for given course sequences and, then, recommends an optimal course sequence for new students. Using real-world student data from the UCLA Mechanical and Aerospace Engineering Department, we illustrate how the proposed algorithms outperform other methods that do not include student contextual information when making course sequence recommendations. ï¿½ 2016 IEEE.",contextual bandits; course sequence recommendation; dynamic programming; Personalized education,Article,,Scopus,2-s2.0-84984987651
SCOPUS,"Nemati S., Ghassemi M.M., Clifford G.D.",Optimal medication dosing from suboptimal clinical examples: A deep reinforcement learning approach,2016,"Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS",2016-October,,7591355,2978,2981,,7,10.1109/EMBC.2016.7591355,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009088494&doi=10.1109%2fEMBC.2016.7591355&partnerID=40&md5=920bf866b9bc894f7793ea30d6680525,"Dept. of Biomedical Informatics, Emory University, Atlanta, GA, United States; Dept. of Electrical Engineering and Computer Science, MIT, Cambrige, MA, United States; Dept. of Biomedical Engineering, Georgia Institute of Technology, Atlanta, GA, United States","Nemati, S., Dept. of Biomedical Informatics, Emory University, Atlanta, GA, United States; Ghassemi, M.M., Dept. of Electrical Engineering and Computer Science, MIT, Cambrige, MA, United States; Clifford, G.D., Dept. of Biomedical Informatics, Emory University, Atlanta, GA, United States, Dept. of Biomedical Engineering, Georgia Institute of Technology, Atlanta, GA, United States","Misdosing medications with sensitive therapeutic windows, such as heparin, can place patients at unnecessary risk, increase length of hospital stay, and lead to wasted hospital resources. In this work, we present a clinician-in-the-loop sequential decision making framework, which provides an individualized dosing policy adapted to each patient's evolving clinical phenotype. We employed retrospective data from the publicly available MIMIC II intensive care unit database, and developed a deep reinforcement learning algorithm that learns an optimal heparin dosing policy from sample dosing trails and their associated outcomes in large electronic medical records. Using separate training and testing datasets, our model was observed to be effective in proposing heparin doses that resulted in better expected outcomes than the clinical guidelines. Our results demonstrate that a sequential modeling approach, learned from retrospective data, could potentially be used at the bedside to derive individualized patient dosing policies. © 2016 IEEE.",,Conference Paper,,Scopus,2-s2.0-85009088494
SCOPUS,"Qin Z., Rishabh I., Carnahan J.",A scalable approach for periodical personalized recommendations,2016,RecSys 2016 - Proceedings of the 10th ACM Conference on Recommender Systems,,,,23,26,,,10.1145/2959100.2959139,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991205405&doi=10.1145%2f2959100.2959139&partnerID=40&md5=c2af9c4b44d2b019cd0dabd2f2537ed5,"Ticketmaster, Hollywood, CA, United States","Qin, Z., Ticketmaster, Hollywood, CA, United States; Rishabh, I., Ticketmaster, Hollywood, CA, United States; Carnahan, J., Ticketmaster, Hollywood, CA, United States","We develop a highly scalable and effective contextual bandit approach towards periodical personalized recommendations. The online bootstrapping-based technique provides a princi- pled way for UCB-type exploitation-exploration algorithms, while being able to handle arbitrary sized datasets, well suited to learn the ever evolving user preference drift from streaming data, and essentially parameter-free. We further introduce techniques to handle arbitrary sized feature spaces using feature hashing, leverage existing state-of-art machine learning via learning reduction, and increase cache hits by managing bootstrapped models in memory effectively. The resulted model trains on millions of examples and billions of features within minutes on a single personal computer. It shows persistent performance in both offline and online eval- uation. We observe around 10% click through rate (CTR) and conversion lift over a collaborative filtering approach in real-world A/B testing across more than 40 million users on the major Ticketmaster email recommendation product. © 2016 ACM.",Contextual Bandits; Learning Reductions; Online Learning; Personalization; Recommender Systems; Scalability,Conference Paper,,Scopus,2-s2.0-84991205405
SCOPUS,"Cheng Z., Zhao Q., Wang F., Jiang Y., Xia L., Ding J.",Satisfaction based Q-learning for integrated lighting and blind control,2016,Energy and Buildings,127,,,43,55,,7,10.1016/j.enbuild.2016.05.067,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971643867&doi=10.1016%2fj.enbuild.2016.05.067&partnerID=40&md5=57f6a8db85f9f0b6c01f3c0021a579e0,"Center for Intelligent and Networked System, Department of Automation and TNList, Tsinghua University, Beijing, China; Department of Building Science, Tsinghua University, Beijing, China; United Technologies Research Center (China) Ltd., Shanghai, China","Cheng, Z., Center for Intelligent and Networked System, Department of Automation and TNList, Tsinghua University, Beijing, China; Zhao, Q., Center for Intelligent and Networked System, Department of Automation and TNList, Tsinghua University, Beijing, China; Wang, F., Department of Building Science, Tsinghua University, Beijing, China; Jiang, Y., Department of Building Science, Tsinghua University, Beijing, China; Xia, L., Center for Intelligent and Networked System, Department of Automation and TNList, Tsinghua University, Beijing, China; Ding, J., United Technologies Research Center (China) Ltd., Shanghai, China","Various lighting and blind control methods have been presented to improve user comfort and reduce energy consumption simultaneously. However, there are opportunities to improve control performances by introducing more recent information and machine learning technologies which allow more comprehensive consideration of the balance between user comfort and system energy consumption. To be more specific, in terms of user comfort, unified set-point may not be desirable since different people may have different comfort preferences. In terms of energy consumption, the excessive cooling load of HVAC system should be considered in summer when utilizing solar incidence to reduce the lighting electricity consumption. The setting of the blind slat angle still has great room to improve instead of the cut-off angle. Moreover, users' demands are not fully met, so sometimes they still want to override the automated control. Thus, a closed-loop satisfaction based system is developed in this paper, specifically we introduce an improved reinforcement learning controller to obtain an optimal control strategy of blinds and lights. It could provide a personalized service via introducing subjects perceptions of surroundings gathered by a novel interface as the feedback signal. The proposed system was implemented on a practical test-bed in an energy-efficient building. Compared with the traditional control, it can provide a more acceptable and energy-efficient luminous environment. © 2016 Elsevier B.V.",Blinds; Day-light; Energy saving; Integrated control; Q-learning,Article,,Scopus,2-s2.0-84971643867
SCOPUS,"Zeng C., Wang Q., Mokhtari S., Li T.",Online context-aware recommendation with time varying multi-armed bandit,2016,Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,13-17-August-2016,,,2025,2034,,8,10.1145/2939672.2939878,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985020774&doi=10.1145%2f2939672.2939878&partnerID=40&md5=26e3553e4a237cf5525077e959b4d46e,"School of Computing and Information Science, Florida International University, Miami, United States","Zeng, C., School of Computing and Information Science, Florida International University, Miami, United States; Wang, Q., School of Computing and Information Science, Florida International University, Miami, United States; Mokhtari, S., School of Computing and Information Science, Florida International University, Miami, United States; Li, T., School of Computing and Information Science, Florida International University, Miami, United States","Contextual multi-armed bandit problems have gained increasing popularity and attention in recent years due to their capability of leveraging contextual information to deliver online personalized recommendation services (e.g., online advertising and news article selection). To predict the reward of each arm given a particular context, existing relevant research studies for contextual multi-armed bandit problems often assume the existence of a fixed yet unknown reward mapping function. However, this assumption rarely holds in practice, since real-world problems often involve underlying processes that are dynamically evolving over time. In this paper, we study the time varying contextual multi-armed problem where the reward mapping function changes over time. In particular, we propose a dynamical context drift model based on particle learning. In the proposed model, the drift on the reward mapping function is explicitly modeled as a set of random walk particles, where good fitted particles are selected to learn the mapping dynamically. Taking advantage of the fully adaptive inference strategy of particle learning, our model is able to effectively capture the context change and learn the latent parameters. In addition, those learnt parameters can be naturally integrated into existing multi-arm selection strategies such as LinUCB and Thompson sampling. Empirical studies on two real-world applications, including online personalized advertising and news recommendation, demonstrate the effectiveness of our proposed approach. The experimental results also show that our algorithm can dynamically track the changing reward over time and consequently improve the click-through rate. © 2016 ACM.",Particle learning; Personalization; Probability matching; Recommender system; Time varying contextual bandit,Conference Paper,,Scopus,2-s2.0-84985020774
SCOPUS,"Pröllochs N., Feuerriegel S., Neumann D.",Negation scope detection in sentiment analysis: Decision support for news-driven trading,2016,Decision Support Systems,88,,,67,75,,5,10.1016/j.dss.2016.05.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978808835&doi=10.1016%2fj.dss.2016.05.009&partnerID=40&md5=ee31c2cc1c8cee3ff33aa745c91d605f,"Chair for Information Systems Research, University of Freiburg, Platz der Alten Synagoge, Freiburg, Germany","Pröllochs, N., Chair for Information Systems Research, University of Freiburg, Platz der Alten Synagoge, Freiburg, Germany; Feuerriegel, S., Chair for Information Systems Research, University of Freiburg, Platz der Alten Synagoge, Freiburg, Germany; Neumann, D., Chair for Information Systems Research, University of Freiburg, Platz der Alten Synagoge, Freiburg, Germany","Decision support for financial news using natural language processing requires robust methods that process all sentences correctly, including those that are negated. To predict the corresponding negation scope, related literature commonly utilizes rule-based algorithms and generative probabilistic models. In contrast, we propose the use of a tailored reinforcement learning method, since it can conquer learning task of arbitrary length. We then perform a thorough comparison with a two-pronged evaluation. First, we compare the predictive performance using a manually-labeled dataset. Here, reinforcement learning outperforms common approaches from the related literature, leading to a balanced classification accuracy of up to 70.17%. Second, we examine how detecting negation scopes can improve the accuracy of sentiment analysis for financial news, leading to an improvement of up to 10.63% in the correlation between news sentiment and stock market returns. This reveals negation scope detection as a crucial leverage in decision support from sentiment. © 2016 Elsevier B.V.",Decision support; Financial news; Machine learning; Negation scope detection; Sentiment analysis,Article,,Scopus,2-s2.0-84978808835
SCOPUS,"Salahuddin M.A., Al-Fuqaha A., Guizani M.",Reinforcement learning for resource provisioning in the vehicular cloud,2016,IEEE Wireless Communications,23,4,7553036,128,135,,2,10.1109/MWC.2016.7553036,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986220271&doi=10.1109%2fMWC.2016.7553036&partnerID=40&md5=690b5aabe71a86c056ea38338e74fd40,"Université du Québec, Montreál, Canada; Western Michigan University, United States; University of Idaho, United States","Salahuddin, M.A., Université du Québec, Montreál, Canada; Al-Fuqaha, A., Western Michigan University, United States; Guizani, M., University of Idaho, United States","This article presents a concise view of vehicular clouds that incorporates various vehicular cloud models that have been proposed to date. Essentially, they all extend the traditional cloud and its utility computing functionalities across the entities in the vehicular ad hoc network. These entities include fixed roadside units, onboard units embedded in the vehicle, and personal smart devices of drivers and passengers. Cumulatively, these entities yield abundant processing, storage, sensing, and communication resources. However, vehicular clouds require novel resource provisioning techniques that can address the intrinsic challenges of dynamic demands for the resources and stringent QoS requirements. In this article, we show the benefits of reinforcement-learning-based techniques for resource provisioning in the vehicular cloud. The learning techniques can perceive long-term benefits and are ideal for minimizing the overhead of resource provisioning for vehicular clouds. © 2016 IEEE.",,Article,,Scopus,2-s2.0-84986220271
SCOPUS,[No author name available],UMAP 2016 - Proceedings of the 2016 Conference on User Modeling Adaptation and Personalization,2016,UMAP 2016 - Proceedings of the 2016 Conference on User Modeling Adaptation and Personalization,,,,,,359,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984903710&partnerID=40&md5=ff873ae9a3d19803d14b7a1a14a81b7f,,,"The proceedings contain 55 papers. The topics discussed include: analyzing and predicting task reminders; identifying grey sheep users in collaborative filtering: a distribution-based technique; on the value of reminders within e-commerce recommendations; reinforcement learning: the sooner the better, or the later the better?; automatic teacher modeling from live classroom audio; predicting individual differences for learner modeling in intelligent tutors from previous learner activities; gender differences in facial expressions of affect during learning; gender difference in the credibility perception of mobile websites: a mixed method approach; personalizing reminders to personality for melanoma self-checking; harnessing crowdsourced recommendation preference data from casual gameplay; and predicting customer satisfaction in customer support conversations in social media using affective features.",,Conference Review,,Scopus,2-s2.0-84984903710
SCOPUS,"Daskalaki E., Diem P., Mougiakakou S.G.",Model-free machine learning in biomedicine: Feasibility study in type 1 diabetes,2016,PLoS ONE,11,7, e0158722,,,,,10.1371/journal.pone.0158722,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979704088&doi=10.1371%2fjournal.pone.0158722&partnerID=40&md5=ef957c6dffd5fa2b20f63df7b81047f6,"Diabetes Technology Research Group, ARTORG Center for Biomedical Engineering Research, University of Bern, Murtenstrasse 50, Bern, Switzerland; Division of Endocrinology, Diabetes and Clinical Nutrition, Bern University Hospital Inselspital, Bern, Switzerland","Daskalaki, E., Diabetes Technology Research Group, ARTORG Center for Biomedical Engineering Research, University of Bern, Murtenstrasse 50, Bern, Switzerland; Diem, P., Division of Endocrinology, Diabetes and Clinical Nutrition, Bern University Hospital Inselspital, Bern, Switzerland; Mougiakakou, S.G., Diabetes Technology Research Group, ARTORG Center for Biomedical Engineering Research, University of Bern, Murtenstrasse 50, Bern, Switzerland, Division of Endocrinology, Diabetes and Clinical Nutrition, Bern University Hospital Inselspital, Bern, Switzerland","Although reinforcement learning (RL) is suitable for highly uncertain systems, the applicability of this class of algorithms to medical treatment may be limited by the patient variability which dictates individualised tuning for their usually multiple algorithmic parameters. This study explores the feasibility of RL in the framework of artificial pancreas development for type 1 diabetes (T1D). In this approach, an Actor-Critic (AC) learning algorithm is designed and developed for the optimisation of insulin infusion for personalised glucose regulation. AC optimises the daily basal insulin rate and insulin:carbohydrate ratio for each patient, on the basis of his/her measured glucose profile. Automatic, personalised tuning of AC is based on the estimation of information transfer (IT) from insulin to glucose signals. Insulinto-glucose IT is linked to patient-specific characteristics related to total daily insulin needs and insulin sensitivity (SI). The AC algorithm is evaluated using an FDA-accepted T1D simulator on a large patient database under a complex meal protocol, meal uncertainty and diurnal SI variation. The results showed that 95.66% of time was spent in normoglycaemia in the presence of meal uncertainty and 93.02% when meal uncertainty and SI variation were simultaneously considered. The time spent in hypoglycaemia was 0.27% in both cases. The novel tuning method reduced the risk of severe hypoglycaemia, especially in patients with low SI. © 2016 Daskalaki et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,Article,Open Access,Scopus,2-s2.0-84979704088
SCOPUS,"Tsiakas K., Papakostas M., Chebaa B., Ebert D., Karkaletsis V., Makedon F.",An interactive learning and adaptation framework for adaptive robot assisted therapy,2016,ACM International Conference Proceeding Series,29-June-2016,,,,,,1,10.1145/2910674.2935857,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005990789&doi=10.1145%2f2910674.2935857&partnerID=40&md5=cc7971a64b3859d4fd701146e04a14f9,"CSE Department, University of Texas at Arlington, United States; Department of Psychology, HERACLEIA Lab, University of Texas at Arlington, United States; National Center for Scientific Research Demokritos, Athens, Greece","Tsiakas, K., CSE Department, University of Texas at Arlington, United States; Papakostas, M., CSE Department, University of Texas at Arlington, United States; Chebaa, B., Department of Psychology, HERACLEIA Lab, University of Texas at Arlington, United States; Ebert, D., CSE Department, University of Texas at Arlington, United States; Karkaletsis, V., National Center for Scientific Research Demokritos, Athens, Greece; Makedon, F., CSE Department, University of Texas at Arlington, United States","In this paper, we present an interactive learning and adaptation framework. The framework combines Interactive Reinforcement Learning methods to effectively adapt and refine a learned policy to cope with new users. We argue that implicit feedback provided by the primary user and guidance from a secondary user can be integrated to the adaptation mechanism, resulting at a tailored and safe interaction. We illustrate this framework with a use case in Robot Assisted Therapy, presenting a Robot Yoga Trainer that monitors a yoga training session, adjusting the session parameters based on human motion activity recognition and evaluation through depth data, to assist the user complete the session, following a Reinforcement Learning approach. Copyright 2016 is held by the owner/author(s).",Adaptive Robot Assisted Therapy; Interactive Reinforcement Learning; Policy Adaptation,Conference Paper,,Scopus,2-s2.0-85005990789
SCOPUS,"Tsiakas K., Abellanoza C., Makedon F.",Interactive learning and adaptation for robot assisted therapy for people with dementia,2016,ACM International Conference Proceeding Series,29-June-2016,,,,,,,10.1145/2910674.2935849,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006062180&doi=10.1145%2f2910674.2935849&partnerID=40&md5=ec3f5c09483c1d6eab071621bde732a1,"CSE Department, HERACLEIA Lab, University of Texas at Arlington, United States; Department of Psychology, Cognitive Neuroscience of Memory Lab, University of Texas at Arlington, United States","Tsiakas, K., CSE Department, HERACLEIA Lab, University of Texas at Arlington, United States; Abellanoza, C., Department of Psychology, Cognitive Neuroscience of Memory Lab, University of Texas at Arlington, United States; Makedon, F., CSE Department, HERACLEIA Lab, University of Texas at Arlington, United States","In this paper, we present an adaptive cognitive music game designed to monitor and improve the attention levels of peo- ple with dementia. The goal of this game is to provide a customized protocol based on user needs and preferences, following the Reinforcement Learning (RL) framework. The game adjusts its parameters (e.g., diffculty level) so as to help the user complete the task successfully, while keeping them engaged. The main contribution of this paper is an interactive learning and adaptation framework that enables and facilitates the adaptation of the robot behavior towards new users, providing a safe, tailored and effcient interac- tion. Copyright 2016 is held by the owner/author(s).",Interactive Reinforcement Learning; Music Therapy; Policy Adaptation; Robot Assisted Therapy; Robot Learning and Behavior Adaptation,Conference Paper,,Scopus,2-s2.0-85006062180
SCOPUS,"Tegelund B., Son H., Lee D.",A task-oriented service personalization scheme for smart environments using reinforcement learning,2016,"2016 IEEE International Conference on Pervasive Computing and Communication Workshops, PerCom Workshops 2016",,,7457110,,,,2,10.1109/PERCOMW.2016.7457110,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966666615&doi=10.1109%2fPERCOMW.2016.7457110&partnerID=40&md5=baf5e68163fa278f820db16929ce294f,"School of Computer Science, KAIST, Daejeon, South Korea","Tegelund, B., School of Computer Science, KAIST, Daejeon, South Korea; Son, H., School of Computer Science, KAIST, Daejeon, South Korea; Lee, D., School of Computer Science, KAIST, Daejeon, South Korea","Users want IoT environments to provide them with personalized support. These environments therefore need to be able to learn user preferences, such as what temperature the room should be or which lights should be turned on. We propose a novel system that separates the tasks of learning a user's preferences and realizing them within the environment. The system is able to capture user preferences by reinterpreting the problem as an optimization problem and applying inverse reinforcement learning to it. The system is shown to be able to accurately extract simple preferences given a small number of user demonstrations. These preferences are then realized by actuates devices running reinforcement learning-based agents to provide an environment consistent with the learnt preferences, also in situations not included in any user demonstration. © 2016 IEEE.",,Conference Paper,,Scopus,2-s2.0-84966666615
SCOPUS,"Buzzi M.C., Buzzi M., Perrone E., Rapisarda B., Senette C.",Learning games for the cognitively impaired people,2016,W4A 2016 - 13th Web for All Conference,,, a30,,,,,10.1145/2899475.2899487,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983554779&doi=10.1145%2f2899475.2899487&partnerID=40&md5=a718dc59778be489c52825e92098f1e0,"CNR-IIT, Via Moruzzi, 1, Pisa, Italy","Buzzi, M.C., CNR-IIT, Via Moruzzi, 1, Pisa, Italy; Buzzi, M., CNR-IIT, Via Moruzzi, 1, Pisa, Italy; Perrone, E., CNR-IIT, Via Moruzzi, 1, Pisa, Italy; Rapisarda, B., CNR-IIT, Via Moruzzi, 1, Pisa, Italy; Senette, C., CNR-IIT, Via Moruzzi, 1, Pisa, Italy","Learning environments have been profoundly reshaped by pervasive technology. New educational methodologies take full advantage of ICT in a mobile customized user-friendly environment, to support learning and stimulate individuals'potential. Unfortunately, technology-enhanced learning tools are not often designed with accessibility in mind, although they can greatly benefit the personal empowerment and inclusion of special-needs people. To address this gap, a Web platform has been created for delivering accessible games to people with Down syndrome. Since personalization, orderliness and positive reinforcement are crucial to learning in these subjects, the platform offers a personalized safe environment for learning, conforming to behavioral analysis principles. Learning analytics are incorporated in the platform for easy monitoring of student progress via Web interfaces. The participatory design driving the development of the learning platform allowed the customization of the games'discriminative stimuli, difficulty levels and reinforcement, as well as the creation of a game ""engine"" to easily set up new personalized exercises. These customization features make the game platform usable by a larger audience, including individuals with learning difficulties and autism. © 2016 ACM.",Cognitive games; Computer-enhanced learning; People with special needs; Web applications,Conference Paper,,Scopus,2-s2.0-84983554779
SCOPUS,"Hochberg I., Feraru G., Kozdoba M., Mannor S., Tennenholtz M., Yom-Tov E.",Encouraging physical activity in patients with diabetes through automatic personalized feedback via reinforcement learning improves glycemic control,2016,Diabetes Care,39,4,,e59,e60,,6,10.2337/dc15-2340,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962053103&doi=10.2337%2fdc15-2340&partnerID=40&md5=6a9f3b67dee5f793093684afee87229a,"Rambam Health Care Campus, Haifa, Israel; Faculty of Medicine, Technion-Israel Institute of Technology, Haifa, Israel; Faculty of Electrical Engineering, Technion-Israel Institute of Technology, Haifa, Israel; Faculty of Industrial Engineering and Management, Technion-Israel Institute of Technology, Haifa, Israel; Microsoft Research, Herzliya, Israel","Hochberg, I., Rambam Health Care Campus, Haifa, Israel; Feraru, G., Faculty of Medicine, Technion-Israel Institute of Technology, Haifa, Israel; Kozdoba, M., Faculty of Electrical Engineering, Technion-Israel Institute of Technology, Haifa, Israel; Mannor, S., Faculty of Electrical Engineering, Technion-Israel Institute of Technology, Haifa, Israel; Tennenholtz, M., Faculty of Industrial Engineering and Management, Technion-Israel Institute of Technology, Haifa, Israel; Yom-Tov, E., Microsoft Research, Herzliya, Israel",[No abstract available],,Letter,,Scopus,2-s2.0-84962053103
SCOPUS,Tsiakas K.,Facilitating safe adaptation of learning agents using interactive reinforcement learning,2016,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,106,109,,4,10.1145/2876456.2876457,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014170682&doi=10.1145%2f2876456.2876457&partnerID=40&md5=9691ccc76385d92e9a7f6aefae9ddc6c,"Computer Science and Engineering Dept., University of Texas, Arlington, United States; Institute of Informatics and Telecommunications, National Centre for Scientific Research Demokritos, Greece","Tsiakas, K., Computer Science and Engineering Dept., University of Texas, Arlington, United States, Institute of Informatics and Telecommunications, National Centre for Scientific Research Demokritos, Greece","In this paper, we propose a learning framework for the adaptation of an interactive agent to a new user. We focus on applications where safety and personalization are essential, as Rehabilitation Systems and Robot Assisted Therapy. We argue that interactive learning methods can be utilised and combined into the Reinforcement Learning framework, aiming at a safe and tailored interaction.",Adaptation; Interaction management; Interactive reinforcement learning; Learning agents,Conference Paper,,Scopus,2-s2.0-85014170682
SCOPUS,"Lotfy H.M.S., Khamis S.M.S., Aboghazalah M.M.",Multi-agents and learning: Implications for Webusage mining,2016,Journal of Advanced Research,7,2,,285,295,,1,10.1016/j.jare.2015.06.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959451312&doi=10.1016%2fj.jare.2015.06.005&partnerID=40&md5=df411efcfc997150cd8dc04bbd48d678,"Computer Science, Mathematics Department, Faculty of Science, AinShams University, Cairo, Egypt; Pure Math. and Computer Science, Menoufia University, Menoufia, Egypt","Lotfy, H.M.S., Computer Science, Mathematics Department, Faculty of Science, AinShams University, Cairo, Egypt; Khamis, S.M.S., Computer Science, Mathematics Department, Faculty of Science, AinShams University, Cairo, Egypt; Aboghazalah, M.M., Pure Math. and Computer Science, Menoufia University, Menoufia, Egypt","Characterization of user activities is an important issue in the design and maintenance of websites. Server weblog files have abundant information about the user's current interests. This information can be mined and analyzed therefore the administrators may be able to guide the users in their browsing activity so they may obtain relevant information in a shorter span of time to obtain user satisfaction. Web-based technology facilitates the creation of personally meaningful and socially useful knowledge through supportive interactions, communication and collaboration among educators, learners and information. This paper suggests a new methodology based on learning techniques for a Web-based Multiagent-based application to discover the hidden patterns in the user's visited links. It presents a new approach that involves unsupervised, reinforcement learning, and cooperation between agents. It is utilized to discover patterns that represent the user's profiles in a sample website into specific categories of materials using significance percentages. These profiles are used to make recommendations of interesting links and categories to the user. The experimental results of the approach showed successful user pattern recognition, and cooperative learning among agents to obtain user profiles. It indicates that combining different learning algorithms is capable of improving user satisfaction indicated by the percentage of precision, recall, the progressive category weight and F1-measure. © 2015.Production and hosting by Elsevier B.V.",Cooperative learning; Personalized web search; Recommendation system; Reinforcement learning; Unsupervised learning,Article,Open Access,Scopus,2-s2.0-84959451312
SCOPUS,"Baker T.E., Stockwell T., Barnes G., Haesevoets R., Holroyd C.B.",Reward Sensitivity of ACC as an Intermediate Phenotype between DRD4-521T and Substance Misuse,2016,Journal of Cognitive Neuroscience,28,3,,460,471,,4,10.1162/jocn_a_00905,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956655137&doi=10.1162%2fjocn_a_00905&partnerID=40&md5=76c344095241734de621297ce06fecb7,"University of Victoria, Canada; University of Montreal, Canada","Baker, T.E., University of Victoria, Canada, University of Montreal, Canada; Stockwell, T., University of Victoria, Canada; Barnes, G., University of Victoria, Canada; Haesevoets, R., University of Victoria, Canada; Holroyd, C.B., University of Victoria, Canada","The development and expression of the midbrain dopamine system is determined in part by genetic factors that vary across individuals such that dopamine-related genes are partly responsible for addiction vulnerability. However, a complete account of how dopamine-related genes predispose individuals to drug addiction remains to be developed. Adopting an intermediate phenotype approach, we investigated whether reward-related electrophysiological activity of ACC—a cortical region said to utilize dopamine reward signals to learn the value of extended, context-specific sequences of goal-directed behaviors—mediates the influence of multiple dopamine-related functional polymorphisms over substance use. We used structural equation modeling to examine whether two related electrophysiological phenomena associated with the control and reinforcement learning functions of ACC—theta power and the reward positivity— mediated the relationship between the degree of substance misuse and genetic polymorphisms that regulate dopamine processing in frontal cortex. Substance use data were collected from 812 undergraduate students. One hundred ninety-six returned on a subsequent day to participate in an electrophysiological experiment and to provide saliva samples for DNA analysis. We found that these electrophysiological signals mediated a relationship between the DRD4-521T dopamine receptor genotype and substance misuse. Our results provide a theoretical framework that bridges the gap between genes and behavior in drug addiction and illustrate how future interventions might be individually tailored for specific genetic and neurocognitive profiles. © 2016 Massachusetts Institute of Technology.",,Article,,Scopus,2-s2.0-84956655137
SCOPUS,"Wang X., Zhang M., Ren F., Ito T.",GongBroker: A broker model for power trading in smart grid markets,2016,"Proceedings - 2015 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology, WI-IAT 2015",2,,7397309,21,24,,2,10.1109/WI-IAT.2015.108,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014289867&doi=10.1109%2fWI-IAT.2015.108&partnerID=40&md5=2353698fbdf722a21e19569da1a359c2,"School of Computing and Information Technology, University of Wollongon, Australia; Department of Computer Science and Engineering, Nagoya Institute of Technology, Japan","Wang, X., School of Computing and Information Technology, University of Wollongon, Australia; Zhang, M., School of Computing and Information Technology, University of Wollongon, Australia; Ren, F., School of Computing and Information Technology, University of Wollongon, Australia; Ito, T., Department of Computer Science and Engineering, Nagoya Institute of Technology, Japan","The Smart Grid market is dynamic and complex, and brokers are widely introduced to better manage this market. This paper proposes an intelligent broker model - GongBroker, with smart trading strategies to cope with the dynamics and complexity. GongBroker first predicts the short-term demands of various consumers, and then buys energy from the wholesale market through auctions, and sells energy to various consumers in the retail market. In order to predict the customer demand, a data-driven method is proposed. All the consumers are hierarchically clustered according to their historical energy consumptions, and different prediction methods are tailored for different customer clusters to predict one-day-ahead hourly energy demand. Based on the predicted demand, the GongBroker employs a Markov Decision Process for the one-day-ahead auction in the wholesale market. To compete with other brokers, GongBroker uses independent reinforcement learning processes to optimize prices for different types of consumers. Experimental results demonstrate that GongBroker not only is competitive in making profit, but also keeps a good supply-demand balance. © 2015 IEEE.",Broker Model; Data-driven; Reinforcement Learning; Smart Grid Market,Conference Paper,,Scopus,2-s2.0-85014289867
SCOPUS,"Zhou L., Brunskill E.",Latent contextual bandits and their application to personalized recommendations for new users,2016,IJCAI International Joint Conference on Artificial Intelligence,2016-January,,,3646,3653,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006132706&partnerID=40&md5=85ce893966ccab257cc8d48bc9141540,"Computer Science Department, Carnegie Mellon University, United States","Zhou, L., Computer Science Department, Carnegie Mellon University, United States; Brunskill, E., Computer Science Department, Carnegie Mellon University, United States","Personalized recommendations for new users, also known as the cold-start problem, can be formulated as a contextual bandit problem. Existing contextual bandit algorithms generally rely on features alone to capture user variability. Such methods are inefficient in learning new users' interests. In this paper we propose Latent Contextual Bandits. We consider both the benefit of leveraging a set of learned latent user classes for new users, and how we can learn such latent classes from prior users. We show that our approach achieves a better regret bound than existing algorithms. We also demonstrate the benefit of our approach using a large real world dataset and a preliminary user study.",,Conference Paper,,Scopus,2-s2.0-85006132706
SCOPUS,"Zhao T., King I.",Locality-sensitive linear bandit model for online social recommendation,2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9947 LNCS,,,80,90,,1,10.1007/978-3-319-46687-3_9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992623509&doi=10.1007%2f978-3-319-46687-3_9&partnerID=40&md5=9050adb209e548fd851f01c9385bc3ba,"Shenzhen Key Laboratory of Rich Media Big Data Analytics and Applications, Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong","Zhao, T., Shenzhen Key Laboratory of Rich Media Big Data Analytics and Applications, Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China, Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong; King, I., Shenzhen Key Laboratory of Rich Media Big Data Analytics and Applications, Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China, Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong","Recommender systems provide personalized suggestions by learning users’ preference based on their historical feedback. To alleviate the heavy relying on historical data, several online recommendation methods are recently proposed and have shown the effectiveness in solving data sparsity and cold start problems in recommender systems. However, existing online recommendation methods neglect the use of social connections among users, which has been proven as an effective way to improve recommendation accuracy in offline settings. In this paper, we investigate how to leverage social connections to improve online recommendation performance. In particular, we formulate the online social recommendation task as a contextual bandit problem and propose a Locality-sensitive Linear Bandit (LS.Lin) method to solve it. The proposed model incorporates users’ local social relations into a linear contextual bandit model and is capable to deal with the dynamic changes of user preference and the network structure. We provide a theoretical analysis to the proposed LS.Lin method and then demonstrate its improved performance for online social recommendation in empirical studies compared with baseline methods. © Springer International Publishing AG 2016.",Linear bandits; Online learning; Social recommendation,Conference Paper,,Scopus,2-s2.0-84992623509
SCOPUS,"Gordon G., Spaulding S., Korywestlund J., Lee J.J., Plummer L., Martinez M., Das M., Breazeal C.",Affective personalization of a social robot tutor for children's second language skills,2016,"30th AAAI Conference on Artificial Intelligence, AAAI 2016",,,,3951,3957,,16,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007268746&partnerID=40&md5=1d1e6a4e8fcc81aca41b3aa559b142ac,"MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Industrial Engineering Department, Curiosity Lab, Tel-Aviv Univerisity, Israel","Gordon, G., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States, Industrial Engineering Department, Curiosity Lab, Tel-Aviv Univerisity, Israel; Spaulding, S., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Korywestlund, J., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Lee, J.J., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Plummer, L., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Martinez, M., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Das, M., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States; Breazeal, C., MIT Media Lab, Personal Robots Group, 20 Ames Street E15-468, Cambridge, MA, United States","Though substantial research has been dedicated towards using technology to improve education, no current methods are as effective as one-on-one tutoring. A critical, though relatively understudied, aspect of effective tutoring is modulating the student's affective state throughout the tutoring session in order to maximize long-term learning gains. We developed an integrated experimental paradigm in which children play a second-language learning game on a tablet, in collaboration with a fully autonomous social robotic learning companion. As part of the system, we measured children's valence and engagement via an automatic facial expression analysis system. These signals were combined into a reward signal that fed into the robot's affective reinforcement learning algorithm. Over several sessions, the robot played the game and personalized its motivational strategies (using verbal and non-verbal actions) to each student. We evaluated this system with 34 children in preschool classrooms for a duration of two months. We saw that (1) children learned new words from the repeated tutoring sessions, (2) the affective policy personalized to students over the duration of the study, and (3) students who interacted with a robot that personalized its affective feedback strategy showed a significant increase in valence, as compared to students who interacted with a nonpersonalizing robot. This integrated system of tablet-based educational content, affective sensing, affective policy learning, and an autonomous social robot holds great promise for a more comprehensive approach to personalized tutoring. © Copyright 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,Conference Paper,,Scopus,2-s2.0-85007268746
SCOPUS,"Qian W., Yang Y.",Randomized allocation with arm elimination in a bandit problem with covariates,2016,Electronic Journal of Statistics,10,1,,242,270,,,10.1214/15-EJS1104,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959527338&doi=10.1214%2f15-EJS1104&partnerID=40&md5=562215ccad1c16d336da80cfc0856d7e,"School of Mathematical Sciences, Rochester Institute of Technology, Rochester, NY, United States; School of Statistics, University of Minnesota, Minneapolis, MN, United States","Qian, W., School of Mathematical Sciences, Rochester Institute of Technology, Rochester, NY, United States; Yang, Y., School of Statistics, University of Minnesota, Minneapolis, MN, United States","Motivated by applications in personalized web services and clinical research, we consider a multi-armed bandit problem in a setting where the mean reward of each arm is associated with some covariates. A multi-stage randomized allocation with arm elimination algorithm is proposed to combine the flexibility in reward function modeling and a theoretical guarantee of a cumulative regret minimax rate. When the function smoothness parameter is unknown, the algorithm is equipped with a histogram estimation based smoothness parameter selector using Lepski’s method, and is shown to maintain the regret minimax rate up to a logarithmic factor under a “self-similarity” condition. © 2016, Institute of Mathematical Statistics. All right reserved.",Adaptive estimation; Contextual bandit problem; MABC; Nonparametric bandit; Regret bound,Article,,Scopus,2-s2.0-84959527338
SCOPUS,"Wang P., Rowe J., Mott B., Lester J.",Decomposing drama management in educational interactive narrative: A modular reinforcement learning approach,2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10045 LNCS,,,270,282,,1,10.1007/978-3-319-48279-8_24,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996879693&doi=10.1007%2f978-3-319-48279-8_24&partnerID=40&md5=3e9d8ada18e9ba06d96af278df3be0e5,"North Carolina State University, Raleigh, NC, United States","Wang, P., North Carolina State University, Raleigh, NC, United States; Rowe, J., North Carolina State University, Raleigh, NC, United States; Mott, B., North Carolina State University, Raleigh, NC, United States; Lester, J., North Carolina State University, Raleigh, NC, United States","Recent years have seen growing interest in data-driven approaches to personalized interactive narrative generation and drama management. Reinforcement learning (RL) shows particular promise for training policies to dynamically shape interactive narratives based on corpora of player-interaction data. An important open question is how to design reinforcement learning-based drama managers in order to make effective use of player interaction data, which is often expensive to gather and sparse relative to the vast state and action spaces required by drama management. We investigate an offline optimization framework for training modular reinforcement learning-based drama managers in an educational interactive narrative, CRYSTAL ISLAND. We leverage importance sampling to evaluate drama manager policies derived from different decompositional representations of the interactive narrative. Empirical results show significant improvements in drama manager quality from adopting an optimized modular RL decomposition compared to competing representations. © Springer International Publishing AG 2016.",Drama management; Educational interactive narrative; Intelligent narrative technologies; Modular reinforcement learning,Conference Paper,,Scopus,2-s2.0-84996879693
SCOPUS,Narvekar S.,Curriculum learning in reinforcement learning,2016,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,,,1528,1529,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014268042&partnerID=40&md5=b74fdfa19965b0191b6a269bb83923c6,"Department of Computer Science, University of Texas at Austin, Austin, TX, United States","Narvekar, S., Department of Computer Science, University of Texas at Austin, Austin, TX, United States","Transfer learning in reinforcement learning is an area of research that seeks to speed up or improve learning of a complex target task, by leveraging knowledge from one or more source tasks. This thesis will extend the concept of transfer learning to curriculum learning, where the goal is to design a sequence of source tasks for an agent to train on, such that final performance or learning speed is improved. We discuss completed work on this topic, including an approach for modeling transferability between tasks, and methods for semi-automatically generating source tasks tailored to an agent and the characteristics of a target domain. Finally, we also present ideas for future work. Copyright © 2016, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",Curriculum learning; Reinforcement learning; Transfer learning,Conference Paper,,Scopus,2-s2.0-85014268042
SCOPUS,"Narvekar S., Sinapov J., Leonetti M., Stone P.",Source task creation for curriculum learning,2016,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,,,566,574,,5,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014299386&partnerID=40&md5=1f7f26c873913eaa841534ac92cb8a78,"Department of Computer Science, University of Texas at Austin, Austin, TX, United States","Narvekar, S., Department of Computer Science, University of Texas at Austin, Austin, TX, United States; Sinapov, J., Department of Computer Science, University of Texas at Austin, Austin, TX, United States; Leonetti, M., Department of Computer Science, University of Texas at Austin, Austin, TX, United States; Stone, P., Department of Computer Science, University of Texas at Austin, Austin, TX, United States","Transfer learning in reinforcement learning has been an active area of research over the past decade. In transfer learning, training on a source task is leveraged to speed up or otherwise improve learning on a target task. This paper presents the more ambitious problem of curriculum learning in reinforcement learning, in which the goal is to design a sequence of source tasks for an agent to train on, such that final performance or learning speed is improved. We take the position that each stage of such a curriculum should be tailored to the current ability of the agent in order to promote learning new behaviors. Thus, as a first step towards creating a curriculum, the trainer must be able to create novel, agent-specific source tasks. We explore how such a space of useful tasks can be created using a parameterized model of the domain and observed trajectories on the target task. We experimentally show that these methods can be used to form components of a curriculum and that such a curriculum can be used successfully for transfer learning in 2 challenging multiagent reinforcement learning domains. Copyright © 2016, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",Curriculum learning; Reinforcement learning; Transfer learning,Conference Paper,,Scopus,2-s2.0-85014299386
SCOPUS,[No author name available],"20th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, PAKDD 2016",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9652 LNAI,,,1,571,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988012804&partnerID=40&md5=e15878e50794c63cc5bbf3494dabe8da,,,"The proceedings contain 44 papers. The special focus in this conference is on Spatiotemporal, Image Data, Anomaly Detection, Clustering and Novel Models. The topics include: Denoising time series by way of a flexible model for phase space reconstruction; distributed sequential pattern mining in large scale uncertain databases; a deep dynamic memory model for predictive medicine; indoor positioning system for smart homes based on decision trees and passive RFID; deep feature extraction from trajectories for transportation mode estimation; online learning for accurate real-time map matching; multi-hypergraph incidence consistent sparse coding for image data clustering; robust multi-view manifold ranking for image retrieval; image representation optimization based on locally aggregated descriptors; reusing extracted knowledge in genetic programming to solve complex texture image classification problems; personal credit profiling via latent user behavior dimensions on social media; linear upper confidence bound algorithm for contextual bandit problem with piled rewards; incremental hierarchical clustering of stochastic pattern-based symbolic data; computing hierarchical summary of the data streams; unsupervised parameter estimation for one-class support vector machines; frequent pattern outlier detection without exhaustive mining; ensembles of interesting subgroups for discovering high potential employees; dynamic grouped mixture models for intermittent multivariate sensor data; a fast algorithm for DBSCAN-based clustering on high dimensional data and a rule based open information extraction method using cascaded finite-state transducer.",,Conference Review,,Scopus,2-s2.0-84988012804
SCOPUS,[No author name available],"8th International Conference on Social Robotics, ICSR 2016",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9979 LNAI,,,1,1017,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992488253&partnerID=40&md5=fc741ea6b5d011290522d517dd1e8ba6,,,"The proceedings contain 98 papers. The special focus in this conference is on Social Robotics. The topics include: Adaptive robot assisted therapy using interactive reinforcement learning; personalization framework for adaptive robotic feeding assistance; user evaluation of an interactive learning framework for single-arm and dual-arm robots; formalizing normative robot behavior; infinite personality space for non-fungible robots; responsive social agents; ethical decision making in robots; how facial expressions and small talk may influence trust in a robot; a study on trust in a robotic suitcase; the more human-like, the more humans like; designing a social robot to assist in medication sorting; other-oriented robot deception; ethically-guided emotional responses for social robots; emotion in robots using convolutional neural networks; qualitative user reactions to a hand-clapping humanoid robot; head and face design for a new humanoid service robot; collaborative visual object tracking via hierarchical structure; developing an interactive gaze algorithm for android robots; a novel parallel pinching and self-adaptive grasping robotic hand; social robots and teaching music to autistic children; examine the potential of robots to teach autistic children emotional concepts; a low-cost upper-torso social robot acting as a sign language teaching assistant; automatic adaptation of online language lessons for robot tutoring; what teachers think about teaching and learning with education robots; co-design and robots; iterative design of a system for programming socially interactive service robots and about the psychological superpowers of robots.",,Conference Review,,Scopus,2-s2.0-84992488253
SCOPUS,"Curran W., Brys T., Aha D., Taylor M., Smart W.D.",Dimensionality reduced reinforcement learning for assistive robots,2016,AAAI Fall Symposium - Technical Report,FS-16-01 - FS-16-05,,,25,31,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025818100&partnerID=40&md5=03d1c87d8c5fe85436796dd60feb5b9d,"Oregon State University, Corvallis, United States; Vrije Universiteit Brussel, Belgium; Navy Center for Applied Research in AI, United States; Washington State University, Pullman, Washington, United States","Curran, W., Oregon State University, Corvallis, United States; Brys, T., Vrije Universiteit Brussel, Belgium; Aha, D., Navy Center for Applied Research in AI, United States; Taylor, M., Washington State University, Pullman, Washington, United States; Smart, W.D., Oregon State University, Corvallis, United States","State-of-the-art personal robots need to perform complex manipulation tasks to be viable in assistive scenarios. However, many of these robots, like the PR2, use manipulators with high degrees-of-freedom, and the problem is made worse in bimanual manipulation tasks. The complexity of these robots lead to large dimensional state spaces, which are difficult to learn in. We reduce the state space by using demonstrations to discover a representative low-dimensional hyperplane in which to learn. This allows the agent to converge quickly to a good policy. We call this Dimensionality Reduced Reinforcement Learning (DRRL). However, when performing dimensionality reduction, not all dimensions can be fully represented. We extend this work by first learning in a single dimension, and then transferring that knowledge to a higher-dimensional hyperplane. By using our Iterative DRRL (IDRRL) framework with an existing learning algorithm, the agent converges quickly to a better policy by iterating to increasingly higher dimensions. IDRRL is robust to demonstration quality and can learn efficiently using few demonstrations. We show that adding IDRRL to the Q-Learning algorithm leads to faster learning on a set of mountain car tasks and the robot swimmers problem. Copyright © 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,Conference Paper,,Scopus,2-s2.0-85025818100
SCOPUS,"Zheng H., Jumadinova J.",OWLS: Observational wireless life-enhancing system,2016,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,,,1425,1426,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014134480&partnerID=40&md5=26e364abd3feeb0a7316192d9b9e02f8,"Allegheny College, 520 North Main Street, Allegheny, PA, United States","Zheng, H., Allegheny College, 520 North Main Street, Allegheny, PA, United States; Jumadinova, J., Allegheny College, 520 North Main Street, Allegheny, PA, United States","Socially assistive robotics technologies for individuals, who have been affected by age-related disabilities and similar types of disorders, have become popular options for facilitating natural independence and uninterrupted mobility. Wireless wearable sensor systems enable proactive personal health management and the ubiquitous monitoring of vital signs to keep an active watch on immediate health conditions. In this paper, we develop a system, called OWLS, where multiple wearable sensors, software agents, robots and health analysis technology, have been integrated into a single personal therapy solution (SPTS). Our system uses a reinforcement learning algorithm to make decisions about the user's current health conditions, and to take appropriate actions, as necessary (i.e, contacting outside parties). We show that the approach of non-invasive monitoring, when combined with an alert system, makes this a desirable SPTS in future health care. Copyright © 2016, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",,Conference Paper,,Scopus,2-s2.0-85014134480
SCOPUS,"Ahrndt S., Lützenberger M., Prochnow S.M.",Using personality models as prior knowledge to accelerate learning about stress-coping preferences (demonstration),2016,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,,,1485,1487,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014298076&partnerID=40&md5=ae6bb87e160503ce6a9421e9baa40182,"DAI-Labor, Technische Universität Berlin, Ernst-Reuter-Platz 7, Berlin, Germany; German Turkish Advanced Research Centre for ICT, Ernst-Reuter-Platz 7, Berlin, Germany","Ahrndt, S., DAI-Labor, Technische Universität Berlin, Ernst-Reuter-Platz 7, Berlin, Germany; Lützenberger, M., DAI-Labor, Technische Universität Berlin, Ernst-Reuter-Platz 7, Berlin, Germany; Prochnow, S.M., German Turkish Advanced Research Centre for ICT, Ernst-Reuter-Platz 7, Berlin, Germany","The management of (dis)stress is an important factor for a long and healthy life. Yet, stress affects people differently and everyone manages stress in different ways. In this paper we introduce PeSA, the Personality-enabled Stress Assistant, an agent-based application that accounts for this individualism. PeSA merges several agent techniques: Reinforcement learning is used to learn about preferences of the users, prior knowledge and knowledge transfer is applied to accelerate the learning process, agent mirroring helps to enable communication and offline functionalities. Based on these mechanisms, PeSA guides through stressful phases by proposing coping strategies that are tailored to the personality of each individual user. Users can assess these advices and thus provide a reward or punishment signal that helps PeSA to improve its suggestions. Copyright © 2016, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",Human-agent teamwork; Human-behaviour models; Reinforcement learning,Conference Paper,,Scopus,2-s2.0-85014298076
SCOPUS,"Li Z., Ge T.",Stochastic data acquisition for answering queries as time goes by,2016,Proceedings of the VLDB Endowment,10,3,,277,288,,,10.14778/3021924.3021942,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020435578&doi=10.14778%2f3021924.3021942&partnerID=40&md5=9106aa67ac44a645b64311764b00579d,"University of Massachusetts, Lowell, United States","Li, Z., University of Massachusetts, Lowell, United States; Ge, T., University of Massachusetts, Lowell, United States","Data and actions are tightly coupled. On one hand, data analysis results trigger decision making and actions. On the other hand, the action of acquiring data is the very first step in the whole data processing pipeline. Data acquisition almost always has some costs, which could be either monetary costs or computing resource costs such as sensor battery power, network transfers, or I/O costs. Using outdated data to answer queries can avoid the data acquisition costs, but there is a penalty of potentially inaccurate results. Given a sequence of incoming queries over time, we study the problem of sequential decision making on when to acquire data and when to use existing versions to answer each query. We propose two approaches to solve this problem using reinforcement learning and tailored locality-sensitive hashing. A systematic empirical study using two real-world datasets shows that our approaches are effective and efficient. © 2016. VLDB Endowment.",,Conference Paper,,Scopus,2-s2.0-85020435578
SCOPUS,"Maity R.K., Lakshminarayanan C., Padakandla S., Bhatnagar S.",Shaping proto-value functions using rewards,2016,Frontiers in Artificial Intelligence and Applications,285,,,1690,1691,,,10.3233/978-1-61499-672-9-1690,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013127793&doi=10.3233%2f978-1-61499-672-9-1690&partnerID=40&md5=3722c7c017b2866e389de907804ab4af,"Dept. of Computer Science and Automation, Indian Institute of Science, Bangalore, India","Maity, R.K., Dept. of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Lakshminarayanan, C., Dept. of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Padakandla, S., Dept. of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Bhatnagar, S., Dept. of Computer Science and Automation, Indian Institute of Science, Bangalore, India","In reinforcement learning (RL), an important sub-problem is learning the value function, which is chiefly influenced by the architecture used to represent value functions. is often expressed as a linear combination of a pre-selected set of basis functions. These basis functions are either selected in an ad-hoc manner or are tailored to the RL task using the domain knowledge. Selecting basis functions in an ad-hoc manner does not give a good approximation of value function while choosing functions using domain knowledge introduces dependency on the task. Thus, a desirable scenario is to have a method to choose basis functions that are task independent, but which also provide a good approximation for the value function. In this paper, we propose a novel task-independent basis function construction method that uses the topology of the underlying state space and the reward structure to build the reward-based Proto Value Functions (RPVFs). The approach we propose gives good approximation for the value function and enhanced learning performance. The performance is demonstrated via experiments on grid-world tasks. © 2016 The Authors and IOS Press.",,Conference Paper,,Scopus,2-s2.0-85013127793
SCOPUS,"Grimm F., Naros G., Gharabaghi A.",Closed-loop task difficulty adaptation during virtual reality reach-to-grasp training assisted with an exoskeleton for stroke rehabilitation,2016,Frontiers in Neuroscience,10,NOV,518,,,,8,10.3389/fnins.2016.00518,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009810403&doi=10.3389%2ffnins.2016.00518&partnerID=40&md5=81fb7c532c89357ddb73a579d4d5fd91,"Division of Functional and Restorative Neurosurgery, Centre for Integrative Neuroscience, Eberhard Karls University of Tuebingen, Tuebingen, Germany","Grimm, F., Division of Functional and Restorative Neurosurgery, Centre for Integrative Neuroscience, Eberhard Karls University of Tuebingen, Tuebingen, Germany; Naros, G., Division of Functional and Restorative Neurosurgery, Centre for Integrative Neuroscience, Eberhard Karls University of Tuebingen, Tuebingen, Germany; Gharabaghi, A., Division of Functional and Restorative Neurosurgery, Centre for Integrative Neuroscience, Eberhard Karls University of Tuebingen, Tuebingen, Germany","Stroke patients with severe motor deficits of the upper extremity may practice rehabilitation exercises with the assistance of a multi-joint exoskeleton. Although this technology enables intensive task-oriented training, it may also lead to slacking when the assistance is too supportive. Preserving the engagement of the patients while providing ""assistance-as-needed"" during the exercises, therefore remains an ongoing challenge. We applied a commercially available seven degree-of-freedom arm exoskeleton to provide passive gravity compensation during task-oriented training in a virtual environment. During this 4-week pilot study, five severely affected chronic stroke patients performed reach-to-grasp exercises resembling activities of daily living. The subjects received virtual reality feedback from their three-dimensional movements. The level of difficulty for the exercise was adjusted by a performance-dependent real-time adaptation algorithm. The goal of this algorithm was the automated improvement of the range of motion. In the course of 20 training and feedback sessions, this unsupervised adaptive training concept led to a progressive increase of the virtual training space (p < 0.001) in accordance with the subjects' abilities. This learning curve was paralleled by a concurrent improvement of real world kinematic parameters, i.e., range of motion (p = 0.008), accuracy of movement (p = 0.01), and movement velocity (p < 0.001). Notably, these kinematic gains were paralleled by motor improvements such as increased elbow movement (p = 0.001), grip force (p < 0.001), and upper extremity Fugl-Meyer-Assessment score from 14.3 ï¿½ 5 to 16.9 ï¿½ 6.1 (p = 0.026). Combining gravity-compensating assistance with adaptive closed-loop feedback in virtual reality provides customized rehabilitation environments for severely affected stroke patients. This approach may facilitate motor learning by progressively challenging the subject in accordance with the individual capacity for functional restoration. It might be necessary to apply concurrent restorative interventions to translate these improvements into relevant functional gains of severely motor impaired patients in activities of daily living. ï¿½ 2016 Grimm, Naros and Gharabaghi.",Hemiparesis; Individualized therapy; Motor recovery; Reinforcement learning; Robot-assisted rehabilitation; Robotic rehabilitation; Upper-limb assistance,Article,,Scopus,2-s2.0-85009810403
SCOPUS,"Molnos A., Lesecq S., Mottin J., Puschini D.",Investigation of Q-learning applied to DVFS management of a System-on-Chip,2016,IFAC-PapersOnLine,49,5,,278,284,,1,10.1016/j.ifacol.2016.07.126,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991111239&doi=10.1016%2fj.ifacol.2016.07.126&partnerID=40&md5=56c52e02deb63ed6679f3dc7913f9969,"Univ. Grenoble Alpes, F-38000 Grenoble, France, CEA, LETI, MINATEC Campus, Grenoble, France","Molnos, A., Univ. Grenoble Alpes, F-38000 Grenoble, France, CEA, LETI, MINATEC Campus, Grenoble, France; Lesecq, S., Univ. Grenoble Alpes, F-38000 Grenoble, France, CEA, LETI, MINATEC Campus, Grenoble, France; Mottin, J., Univ. Grenoble Alpes, F-38000 Grenoble, France, CEA, LETI, MINATEC Campus, Grenoble, France; Puschini, D., Univ. Grenoble Alpes, F-38000 Grenoble, France, CEA, LETI, MINATEC Campus, Grenoble, France","This paper presents a new Q-learning based strategy to manage Dynamic Voltage Frequency Scaling (DVFS) on a system on chip (SoC) such that the energy consumption is reduced. We address software applications with throughput constraints. The proposed Q-learning formulation has two main advantages: it has a reduced state space to limit the overhead and it embeds a new reward function tailored for throughput-constrained applications. The DVFS manager is evaluated on a test chip executing an HMAX object recognition application. We perform an experimental investigation of the main Q-learning parameters. The results suggest that the proposed method reduces the energy consumed with up to 44% at the cost of occasionally increasing the number of throughput violations, when compared to two state-of-the-art feedback controllers that address the same application domain. © 2016",DVFS; Reinforcement learning; system-of-chip energy management,Article,,Scopus,2-s2.0-84991111239
SCOPUS,[No author name available],"13th European Conference on Multi-Agent Systems, EUMAS 2015 and 3rd International Conference on Agreement Technologies, AT 2015",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9571,,,1,472,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964031669&partnerID=40&md5=a72b2265cf8d8df2d1f167091255f70b,,,"The proceedings contain 37 papers. The special focus in this conference is on Coordination and Planning. The topics include: Reducing risk in norm-based systems; from public plans to global solutions in multiagent planning; intelligent people flow coordination in smart spaces; customized document research by a stigmergic approach using agents and artifacts; collaborative framework for monitoring reliability of distributed components of composed services; graph patterns, reinforcement learning and models of reputation for improving coalition formation in collaborative multi-agent systems; multiagent model for agile context inference based on articial immune systems and sparse distributed representations; factored MDPs for optimal prosumer decision-making in continuous state spaces; composing swarm robot formations based on their distributions using mobile agents; group-based pricing to shape demand in real-time electricity markets; human rating methods on multi-agent systems; learning in multi agent social environments with opponent models; a particle swarm optimization metaheuristic for the blocking flow shop scheduling problem; towards an agent-based negotiation scheme for scheduling electric vehicles charging; agreement technologies applied to transmission towers maintenance; a fuzzy logic adaptive reasoning agent for energy trading; a dialectical approach to enable decision making in online trading; argumentation-based hybrid recommender system for recommending learning objects; identifying malicious behavior in multi-party bipolar argumentation debates; probabilistic argumentation, a small step for uncertainty, a giant step for complexity; modeling social deviance in artificial agent societies and modeling and enforcing semantic obligations for access control.",,Conference Review,,Scopus,2-s2.0-84964031669
SCOPUS,[No author name available],"1st International Early Research Career Enhancement School on Biologically Inspired Cognitive Architectures, FIERCES on BICA 2016",2016,Advances in Intelligent Systems and Computing,449,,,1,304,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964091555&partnerID=40&md5=79cfa34fe11e05f2d64eed01d4614126,,,The proceedings contain 38 papers. The special focus in this conference is on Biologically Inspired Cognitive Architectures BICA for Young Scientists. The topics include: The cognitive architecture within the natural-constructive approach; models of autonomous cognitive agents; differentiation of groundwater tax rates as an element of improving the economic mechanism in the state groundwater extraction management; character reasoning of the social network users on the basis of the content contained on their personal pages; bayesian optimization of spiking neural network parameters to solving the time series classification task; simulation of learning in neuronal culture; biologically plausible saliency detection model; active adaptation of expert-based suggestions in ladieswear recommender system lookbooksclub via reinforcement learning; visual analytics support for carbon nanotube design automation; a model of neurodynamics of hippocampal formation neurons performing spatial processing based on even cyclic inhibitory networks; feature selection for time-series prediction in case of undetermined estimation; a new approach for semantic cognitive maps creation and evaluation based on affix relations; on alternative instruments for the fMRI data analysis; application of hopfield neural network to the N-queens problem; simulation of a fear-like state on a model of dopamine system of rat brain; spatial and temporal parameters of eye movements during viewing of affective images; MEG data analysis using the empirical mode decomposition method and dynamic intelligent systems integration and evolution of intelligent control systems architectures.,,Conference Review,,Scopus,2-s2.0-84964091555
SCOPUS,"Collingbourne L., Seah W.K.G.",Teaching project management using a real-world group project,2015,"Proceedings - Frontiers in Education Conference, FIE",2014,,7344301,,,,1,10.1109/FIE.2015.7344301,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960336257&doi=10.1109%2fFIE.2015.7344301&partnerID=40&md5=72a41db75c4bc0929684c42488d5301d,"School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand","Collingbourne, L., School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; Seah, W.K.G., School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand","It is well established that an effective pedagogy for project management requires students to get real-world experience. The challenge in providing this when teaching undergraduate engineers is the dichotomy between achieving realism and maintaining sufficient simplicity to make the course tractable. A real-world group technology project at Victoria University of Wellington (VUW) in New Zealand establishes essential non-technical attributes required by the engineering profession while covering key elements of the project management body of knowledge (PMBOK). This paper first shows how the project covers the knowledge required in project management and then presents the results of two years of data collected from students' reflection on their own learning. We have established a pleasing congruence across the years against the specific learning topics of team working skills, communication skills and personal working skills with an improvement in project management skills. A key finding emerging from our analysis is the importance of reinforcement learning and reflective learning. We show a key link between these two learning mechanisms and the project pedagogy. Further analysis shows the link between the project pedagogy and four skill areas acquired. Finally, our research has identified specific areas for us to focus on for subsequent years. © 2015 IEEE.",graduate attributes; project management; real-world project,Conference Paper,,Scopus,2-s2.0-84960336257
SCOPUS,"Etkin A., Büchel C., Gross J.J.",The neural bases of emotion regulation,2015,Nature Reviews Neuroscience,16,11,,693,700,,131,10.1038/nrn4044,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944740173&doi=10.1038%2fnrn4044&partnerID=40&md5=13c04bd08c4c85acdbb1093d59a0fae3,"Department of Psychiatry and Behavioral Sciences, Stanford Neurosciences Institute, Stanford University, 401 Quarry Road, Stanford, CA, United States; Veterans Affairs Palo Alto Healthcare System, Sierra Pacific Mental Illness, Research, Education and Clinical Center (MIRECC), 3801 Miranda Ave, Palo Alto, CA, United States; Department of Systems Neuroscience, University Medical Center Hamburg- Eppendorf, Martinistr 52, Hamburg, Germany; Department of Psychology, Stanford University, 450 Serra Mall, Stanford, CA, United States","Etkin, A., Department of Psychiatry and Behavioral Sciences, Stanford Neurosciences Institute, Stanford University, 401 Quarry Road, Stanford, CA, United States, Veterans Affairs Palo Alto Healthcare System, Sierra Pacific Mental Illness, Research, Education and Clinical Center (MIRECC), 3801 Miranda Ave, Palo Alto, CA, United States; Büchel, C., Department of Systems Neuroscience, University Medical Center Hamburg- Eppendorf, Martinistr 52, Hamburg, Germany; Gross, J.J., Department of Psychology, Stanford University, 450 Serra Mall, Stanford, CA, United States","Emotions are powerful determinants of behaviour, thought and experience, and they may be regulated in various ways. Neuroimaging studies have implicated several brain regions in emotion regulation, including the ventral anterior cingulate and ventromedial prefrontal cortices, as well as the lateral prefrontal and parietal cortices. Drawing on computational approaches to value-based decision-making and reinforcement learning, we propose a unifying conceptual framework for understanding the neural bases of diverse forms of emotion regulation. © 2015 Macmillan Publishers Limited.",,Review,,Scopus,2-s2.0-84944740173
SCOPUS,"Lanzi P.L., Loiacono D.",XCSF with tile coding in discontinuous action-value landscapes,2015,Evolutionary Intelligence,8,2-3,,117,132,,1,10.1007/s12065-015-0129-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929840754&doi=10.1007%2fs12065-015-0129-7&partnerID=40&md5=8308915ac08dcb9ce69ff618441ae49b,"Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milan, Italy","Lanzi, P.L., Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milan, Italy; Loiacono, D., Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milan, Italy","Tile coding is an effective reinforcement learning method that uses a rather ingenious generalization mechanism based on (1) a carefully designed parameter setting and (2) the assumption that nearby states in the problem space will correspond to similar payoff predictions in the action-value function. Previously, we extended XCSF with tile coding prediction and compared it to tabular tile coding, showing that (1) XCSF performs as well as parameter-optimized tile coding, while also (2) evolving individualized parameter settings in each problem subspace. Our comparison was based on a set of well-known reinforcement learning environments (2D Gridworld and the Mountain Car) that involved no action-value discontinuities and so posed no challenge to tabular tile coding. In this paper, we go a step further and test XCSF with tile coding on a set of problems designed to challenge tile coding by introducing discontinuities in the action value landscape. The new testbed (called MazeWorld) extends 2D Gridworld with impenetrable obstacles, a conceptually simple modification that can dramatically increase the problem complexity for tabular tile coding. We compare four versions of XCSF with tile coding (each adapting a different set of parameters) to tabular tile coding on four problems of increasing complexity. We show that our system (1) needs fewer training problems than standard tile coding to reach an optimal policy; (2) it can evolve adequate coding parameters in each subspace without any previous knowledge; and that (3) even when XCSF is not allowed to evolve these parameters, the genetic algorithm will still adapt classifier conditions to properly decompose the problem into subspaces thus being much less sensitive to the parameter settings than tabular tile coding. © 2015, Springer-Verlag Berlin Heidelberg.",Learning classifier systems; Reinforcement learning; Tile coding; XCSF,Article,,Scopus,2-s2.0-84929840754
SCOPUS,"Buduru A.B., Yau S.S.",An Effective Approach to Continuous User Authentication for Touch Screen Smart Devices,2015,"Proceedings - 2015 IEEE International Conference on Software Quality, Reliability and Security, QRS 2015",,,7272936,219,226,,5,10.1109/QRS.2015.40,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962122735&doi=10.1109%2fQRS.2015.40&partnerID=40&md5=ff328a683cfc37f5c40e6c2272d6e99e,"Information Assurance Center, and School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, United States","Buduru, A.B., Information Assurance Center, and School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, United States; Yau, S.S., Information Assurance Center, and School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, United States","Due to the rapid increase in the use of personal smart devices, more sensitive data is stored and viewed on these smart devices. This trend makes it easier for attackers to access confidential data by physically compromising (including stealing) these smart devices. Currently, most personal smart devices employ one of the one-time user authentication schemes, such as four-to-six digits, fingerprint or pattern-based schemes. These authentication schemes are often not good enough for securing personal smart devices because the attackers can easily extract all the confidential data from the smart device by breaking such schemes, or by keeping the authenticated session open on a physically compromised smart device. In addition, existing re-authentication or continuous authentication techniques for protecting personal smart devices use centralized architecture and require servers at a centralized location to train and update the learning model used for continuous authentication, which impose additional communication overhead. In this paper, an approach is presented to generating and updating the authentication model on the user's smart device with user's gestures, instead of a centralized server. There are two major advantages in this approach. One is that this approach continuously learns and authenticates finger gestures of the user in the background without requiring the user to provide specific gesture inputs. The other major advantage is to have better authentication accuracy by treating uninterrupted user finger gestures over a short time interval as a single gesture for continuous user authentication. © 2015 IEEE.",adaptive continuous user authentication; and user re-authentication; reinforcement learning; Touch-screen smart devices,Conference Paper,,Scopus,2-s2.0-84962122735
SCOPUS,"Van Moffaert K., Brys T., Nowe A.",Risk-sensitivity through multi-objective reinforcement learning,2015,"2015 IEEE Congress on Evolutionary Computation, CEC 2015 - Proceedings",,,7257098,1746,1753,,1,10.1109/CEC.2015.7257098,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963533551&doi=10.1109%2fCEC.2015.7257098&partnerID=40&md5=2c8508bdf1a32fa0113bff91761da6b4,"Department of Computer Science, Vrije Universiteit Brussel, Brussels, Belgium","Van Moffaert, K., Department of Computer Science, Vrije Universiteit Brussel, Brussels, Belgium; Brys, T., Department of Computer Science, Vrije Universiteit Brussel, Brussels, Belgium; Nowe, A., Department of Computer Science, Vrije Universiteit Brussel, Brussels, Belgium","Usually in reinforcement learning, the goal of the agent is to maximize the expected return. However, in practical applications, algorithms that solely focus on maximizing the mean return could be inappropriate as they do not account for the variability of their solutions. Thereby, a variability measure could be included to accommodate for a risk-sensitive setting, i.e. where the system engineer can explicitly define the tolerated level of variance. Our approach is based on multi-objectivization where a standard single-objective environment is extended with one (or more) additional objectives. More precisely, we augment the standard feedback signal of an environment with an additional objective that defines the variance of the solution. We highlight that our algorithm, named risk-sensitive Pareto Q-learning, is (1) specifically tailored to learn a set of Pareto non-dominated policies that trade-off these two objectives. Additionally (2), the algorithm can also retrieve every policy that has been learned throughout the state-action space. This in contrast to standard risk-sensitive approaches where only a single trade-off between mean and variance is learned at a time. © 2015 IEEE.",,Conference Paper,,Scopus,2-s2.0-84963533551
SCOPUS,"Tang L., Jiang Y., Li L., Zeng C., Li T.",Personalized recommendation via parameter-free contextual bandits,2015,SIGIR 2015 - Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,,,,323,332,,8,10.1145/2766462.2767707,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953732355&doi=10.1145%2f2766462.2767707&partnerID=40&md5=10663d72e54cf9a059b4485b18011be2,"School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States","Tang, L., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Jiang, Y., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Li, L., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Zeng, C., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Li, T., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States","Personalized recommendation services have gained increasing popularity and attention in recent years as most useful information can be accessed online in real-time. Most online recommender systems try to address the information needs of users by virtue of both user and content information. Despite extensive recent advances, the problem of personalized recommendation remains challenging for at least two reasons. First, the user and item repositories undergo frequent changes, which makes traditional recommendation algorithms ineffective. Second, the so-called cold-start problem is difficult to address, as the information for learning a recommendation model is limited for new items or new users. Both challenges are formed by the dilemma of exploration and exploitation. In this paper, we formulate personalized recommendation as a contextual bandit problem to solve the exploration/exploitation dilemma. Specifically in our work, we propose a parameter-free bandit strategy, which employs a principled resampling approach called online bootstrap, to derive the distribution of estimated models in an online manner. Under the paradigm of probability matching, the proposed algorithm randomly samples a model from the derived distribution for every recommendation. Extensive empirical experiments on two real-world collections of web data (including online advertising and news recommendation) demonstrate the effectiveness of the proposed algorithm in terms of the click-through rate. The experimental results also show that this proposed algorithm is robust in the cold-start situation, in which there is no sufficient data or knowledge to tune the parameters. © 2015 ACM.",Bootstrapping; Contextual bandit; Personalization; Probability matching; Recommender systems,Conference Paper,,Scopus,2-s2.0-84953732355
SCOPUS,"De Paula M., Ávila L.O., Martínez E.C.",Controlling blood glucose variability under uncertainty using reinforcement learning and Gaussian processes,2015,Applied Soft Computing Journal,35,,3038,310,332,,3,10.1016/j.asoc.2015.06.041,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937399738&doi=10.1016%2fj.asoc.2015.06.041&partnerID=40&md5=44631a8b252ce7b708616dfd8199006e,"INTELYMEC - Centro de Investigaciones en Física e Ingeniería del Centro - CIFICEN - CONICET, Facultad de Ingeniería, Universidad Nacional del Centro de la Provincia de Buenos Aires - UNCPBA, Av. del Valle 5737, Olavarría, Argentina; INGAR (CONICET - UTN), Avellaneda 3657, Santa Fe, Argentina","De Paula, M., INTELYMEC - Centro de Investigaciones en Física e Ingeniería del Centro - CIFICEN - CONICET, Facultad de Ingeniería, Universidad Nacional del Centro de la Provincia de Buenos Aires - UNCPBA, Av. del Valle 5737, Olavarría, Argentina; Ávila, L.O., INGAR (CONICET - UTN), Avellaneda 3657, Santa Fe, Argentina; Martínez, E.C., INGAR (CONICET - UTN), Avellaneda 3657, Santa Fe, Argentina","Abstract Automated control of blood glucose (BG) concentration with a fully automated artificial pancreas will certainly improve the quality of life for insulin-dependent patients. Closed-loop insulin delivery is challenging due to inter- and intra-patient variability, errors in glucose sensors and delays in insulin absorption. Responding to the varying activity levels seen in outpatients, with unpredictable and unreported food intake, and providing the necessary personalized control for individuals is a challenging task for existing control algorithms. A novel approach for controlling glycemic variability using simulation-based learning is presented. A policy iteration algorithm that combines reinforcement learning with Gaussian process approximation is proposed. To account for multiple sources of uncertainty, a control policy is learned off-line using an Ito's stochastic model of the glucose-insulin dynamics. For safety and performance, only relevant data are sampled through Bayesian active learning. Results obtained demonstrate that a generic policy is both safe and efficient for controlling subject-specific variability due to a patient's lifestyle and its distinctive metabolic response. © 2015 Elsevier B.V.",Artificial pancreas; Diabetes; Gaussian processes; Policy iteration; Reinforcement learning; Stochastic optimal control,Article,,Scopus,2-s2.0-84937399738
SCOPUS,"Tsiakas K., Huber M., Makedon F.",A multimodal adaptive session manager for physical rehabilitation exercising,2015,"8th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2015 - Proceedings",,, a33,,,,6,10.1145/2769493.2769507,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956977207&doi=10.1145%2f2769493.2769507&partnerID=40&md5=0f798bcebf296279e4bbce0786ed18c2,"HERACLEIA Lab, Computer Science and Engineering Department, University of Texas, Arlington, United States; Computer Science and Engineering Department, University of Texas, Arlington, United States","Tsiakas, K., HERACLEIA Lab, Computer Science and Engineering Department, University of Texas, Arlington, United States; Huber, M., Computer Science and Engineering Department, University of Texas, Arlington, United States; Makedon, F., HERACLEIA Lab, Computer Science and Engineering Department, University of Texas, Arlington, United States","Physical exercising is an essential part of any rehabilitation plan. The subject must be committed to a daily exercising routine, as well as to a frequent contact with the therapist. Rehabilitation plans can be quite expensive and time-consuming. On the other hand, tele-rehabilitation systems can be really helpful and efficient for both subjects and therapists. In this paper, we present ReAdapt, an adaptive module for a tele-rehabilitation system that takes into consideration the progress and performance of the exercising utilizing multisensing data and adjusts the session difficulty resulting to a personalized session. Multimodal data such as speech, facial expressions and body motion are being collected during the exercising and feed the system to decide on the exercise and session difficulty. We formulate the problem as a Markov Decision Process and apply a Reinforcement Learning algorithm to train and evaluate the system on simulated data. © 2015 ACM.",Markov Decision Process; Multimodal adaptive systems; Personalized rehabilitation systems; Reinforcement Learning,Conference Paper,,Scopus,2-s2.0-84956977207
SCOPUS,"Garofalo S., di Pellegrino G.",Individual differences in the influence of task-irrelevant Pavlovian cues on human behavior,2015,Frontiers in Behavioral Neuroscience,9,JUNE,163,,,11,19,10.3389/fnbeh.2015.00163,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933039584&doi=10.3389%2ffnbeh.2015.00163&partnerID=40&md5=90df2c854310a8deae8890e93288872d,"Center for Studies and Research in Cognitive Neuroscience, Department of Psychology, University of Bologna, Cesena, Italy; Department of Psychiatry, University of Cambridge, Cambridge, United Kingdom; Behavioural and Clinical Neuroscience Institute, Department of Psychology, University of Cambridge, Cambridge, United Kingdom","Garofalo, S., Center for Studies and Research in Cognitive Neuroscience, Department of Psychology, University of Bologna, Cesena, Italy, Department of Psychiatry, University of Cambridge, Cambridge, United Kingdom, Behavioural and Clinical Neuroscience Institute, Department of Psychology, University of Cambridge, Cambridge, United Kingdom; di Pellegrino, G., Center for Studies and Research in Cognitive Neuroscience, Department of Psychology, University of Bologna, Cesena, Italy","Pavlovian-to-instrumental transfer (PIT) refers to the process of a Pavlovian rewardpaired cue acquiring incentive motivational proprieties that drive choices. It represents a crucial phenomenon for understanding cue-controlled behavior, and it has both adaptive and maladaptive implications (i.e., drug-taking). In animals, individual differences in the degree to which such cues bias performance have been identified in two types of individuals that exhibit distinct Conditioned Responses (CR) during Pavlovian conditioning: Sign-Trackers (ST) and Goal-Trackers (GT). Using an appetitive PIT procedure with a monetary reward, the present study investigated, for the first time, the extent to which such individual differences might affect the influence of rewardpaired cues in humans. In a first task, participants learned an instrumental response leading to reward; then, in a second task, a visual Pavlovian cue was associated with the same reward; finally, in a third task, PIT was tested by measuring the preference for the reward-paired instrumental response when the task-irrelevant reward-paired cue was presented, in the absence of the reward itself. In ST individuals, but not in GT individuals, reward-related cues biased behavior, resulting in an increased likelihood to perform the instrumental response independently paired with the same reward when presented with the task-irrelevant reward-paired cue, even if the reward itself was no longer available (i.e., stronger PIT effect). This finding has important implications for developing individualized treatment for maladaptive behaviors, such as addiction. © 2015 Garofalo and di Pellegrino.",Cue-controlled behavior; Goal-Tracker; Pavlovian-to-instrumental transfer; Reinforcement learning; Sign-Tracker,Article,,Scopus,2-s2.0-84933039584
SCOPUS,[No author name available],Proceedings of the National Conference on Artificial Intelligence,2015,Proceedings of the National Conference on Artificial Intelligence,4,,,,,4402,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960122297&partnerID=40&md5=25ead027fcdf560516e96c620730135c,,,The proceedings contain 674 papers. The topics discussed include: efficient top-fc shortest-path distance queries on large networks by pruned landmark labeling; inferring same-as facts from linked data: an iterative import-by-query approach; a personalized interest-forgetting Markov model for recommendations; sequence-form algorithm for computing Stackelberg equilibria in extensive-form games; combining compact representation and incremental generation in large games with sequential strategies; a sparse combined regression-classification formulation for learning a physiological alternative to clinical post-traumatic stress disorder scores; unsupervised cross-domain transfer in policy gradient reinforcement learning via manifold alignment; some fixed parameter tractability results for planning with non-acyclic domain-transition graphs; and speech adaptation in extended ambient intelligence environments.,,Conference Review,,Scopus,2-s2.0-84960122297
SCOPUS,[No author name available],Proceedings of the National Conference on Artificial Intelligence,2015,Proceedings of the National Conference on Artificial Intelligence,6,,,,,4402,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961207418&partnerID=40&md5=0d7fac0a1698671088a023909014571c,,,The proceedings contain 674 papers. The topics discussed include: efficient top-fc shortest-path distance queries on large networks by pruned landmark labeling; inferring same-as facts from linked data: an iterative import-by-query approach; a personalized interest-forgetting Markov model for recommendations; sequence-form algorithm for computing Stackelberg equilibria in extensive-form games; combining compact representation and incremental generation in large games with sequential strategies; a sparse combined regression-classification formulation for learning a physiological alternative to clinical post-traumatic stress disorder scores; unsupervised cross-domain transfer in policy gradient reinforcement learning via manifold alignment; some fixed parameter tractability results for planning with non-acyclic domain-transition graphs; and speech adaptation in extended ambient intelligence environments.,,Conference Review,,Scopus,2-s2.0-84961207418
SCOPUS,[No author name available],Proceedings of the National Conference on Artificial Intelligence,2015,Proceedings of the National Conference on Artificial Intelligence,3,,,,,4402,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959933996&partnerID=40&md5=8edddf1b81982b3894990a4151e10682,,,The proceedings contain 674 papers. The topics discussed include: efficient top-fc shortest-path distance queries on large networks by pruned landmark labeling; inferring same-as facts from linked data: an iterative import-by-query approach; a personalized interest-forgetting Markov model for recommendations; sequence-form algorithm for computing Stackelberg equilibria in extensive-form games; combining compact representation and incremental generation in large games with sequential strategies; a sparse combined regression-classification formulation for learning a physiological alternative to clinical post-traumatic stress disorder scores; unsupervised cross-domain transfer in policy gradient reinforcement learning via manifold alignment; some fixed parameter tractability results for planning with non-acyclic domain-transition graphs; and speech adaptation in extended ambient intelligence environments.,,Conference Review,,Scopus,2-s2.0-84959933996
SCOPUS,[No author name available],Proceedings of the National Conference on Artificial Intelligence,2015,Proceedings of the National Conference on Artificial Intelligence,2,,,,,4402,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959895494&partnerID=40&md5=f370a2e0f3edf57100b717a7d9aedc16,,,The proceedings contain 674 papers. The topics discussed include: efficient top-fc shortest-path distance queries on large networks by pruned landmark labeling; inferring same-as facts from linked data: an iterative import-by-query approach; a personalized interest-forgetting Markov model for recommendations; sequence-form algorithm for computing Stackelberg equilibria in extensive-form games; combining compact representation and incremental generation in large games with sequential strategies; a sparse combined regression-classification formulation for learning a physiological alternative to clinical post-traumatic stress disorder scores; unsupervised cross-domain transfer in policy gradient reinforcement learning via manifold alignment; some fixed parameter tractability results for planning with non-acyclic domain-transition graphs; and speech adaptation in extended ambient intelligence environments.,,Conference Review,,Scopus,2-s2.0-84959895494
SCOPUS,[No author name available],Proceedings of the National Conference on Artificial Intelligence,2015,Proceedings of the National Conference on Artificial Intelligence,5,,,,,4402,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961214152&partnerID=40&md5=f67c1a87a92fe0732c325203568dcc40,,,The proceedings contain 674 papers. The topics discussed include: efficient top-fc shortest-path distance queries on large networks by pruned landmark labeling; inferring same-as facts from linked data: an iterative import-by-query approach; a personalized interest-forgetting Markov model for recommendations; sequence-form algorithm for computing Stackelberg equilibria in extensive-form games; combining compact representation and incremental generation in large games with sequential strategies; a sparse combined regression-classification formulation for learning a physiological alternative to clinical post-traumatic stress disorder scores; unsupervised cross-domain transfer in policy gradient reinforcement learning via manifold alignment; some fixed parameter tractability results for planning with non-acyclic domain-transition graphs; and speech adaptation in extended ambient intelligence environments.,,Conference Review,,Scopus,2-s2.0-84961214152
SCOPUS,[No author name available],Proceedings of the National Conference on Artificial Intelligence,2015,Proceedings of the National Conference on Artificial Intelligence,1,,,,,4402,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959481294&partnerID=40&md5=eca1601358052c3bff50ca5e36793229,,,The proceedings contain 674 papers. The topics discussed include: efficient top-fc shortest-path distance queries on large networks by pruned landmark labeling; inferring same-as facts from linked data: an iterative import-by-query approach; a personalized interest-forgetting Markov model for recommendations; sequence-form algorithm for computing Stackelberg equilibria in extensive-form games; combining compact representation and incremental generation in large games with sequential strategies; a sparse combined regression-classification formulation for learning a physiological alternative to clinical post-traumatic stress disorder scores; unsupervised cross-domain transfer in policy gradient reinforcement learning via manifold alignment; some fixed parameter tractability results for planning with non-acyclic domain-transition graphs; and speech adaptation in extended ambient intelligence environments.,,Conference Review,,Scopus,2-s2.0-84959481294
SCOPUS,"Chen Y., Hofmann K.",Online learning to rank: Absolute vs. relative,2015,WWW 2015 Companion - Proceedings of the 24th International Conference on World Wide Web,,,,19,20,,5,10.1145/2740908.2742718,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968542091&doi=10.1145%2f2740908.2742718&partnerID=40&md5=a5678748f6782e9a44832ddac4643955,"University College London, United Kingdom; Microsoft Research, United States","Chen, Y., University College London, United Kingdom; Hofmann, K., Microsoft Research, United States","Online learning to rank holds great promise for learning personalized search result rankings. First algorithms have been proposed, namely absolute feedback approaches, based on contextual bandits learning; and relative feedback approaches, based on gradient methods and inferred preferences between complete result rankings. Both types of approaches have shown promise, but they have not previously been compared to each other. It is therefore unclear which type of approach is the most suitable for which online learning to rank problems. In this work we present the first empirical comparison of absolute and relative online learning to rank approaches.",Information retrieval; Learning to rank; Online learning,Conference Paper,,Scopus,2-s2.0-84968542091
SCOPUS,"Wang Y., Wang F., Xu K., Zhang Q., Zhang S., Zheng X.",Neural control of a tracking task via attention-gated reinforcement learning for brain-machine interfaces,2015,IEEE Transactions on Neural Systems and Rehabilitation Engineering,23,3,6863657,458,467,,5,10.1109/TNSRE.2014.2341275,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929330639&doi=10.1109%2fTNSRE.2014.2341275&partnerID=40&md5=272bbcb07328e30c279d66aedce4eb4f,"Qiushi Academy for Advanced Studies, Key Laboratory of Biomedical Engineering of Ministry of Education, Zhejiang University, Hangzhou, China; Qiushi Academy for Advanced Studies, Department of Biomedical Engineering, Zhejiang University, Hangzhou, China","Wang, Y., Qiushi Academy for Advanced Studies, Key Laboratory of Biomedical Engineering of Ministry of Education, Zhejiang University, Hangzhou, China; Wang, F., Qiushi Academy for Advanced Studies, Department of Biomedical Engineering, Zhejiang University, Hangzhou, China; Xu, K., Qiushi Academy for Advanced Studies, Department of Biomedical Engineering, Zhejiang University, Hangzhou, China; Zhang, Q., Qiushi Academy for Advanced Studies, Department of Biomedical Engineering, Zhejiang University, Hangzhou, China; Zhang, S., Qiushi Academy for Advanced Studies, Key Laboratory of Biomedical Engineering of Ministry of Education, Zhejiang University, Hangzhou, China; Zheng, X., Qiushi Academy for Advanced Studies, Key Laboratory of Biomedical Engineering of Ministry of Education, Zhejiang University, Hangzhou, China","Reinforcement learning (RL)-based brain machine interfaces (BMIs) enable the user to learn from the environment through interactions to complete the task without desired signals, which is promising for clinical applications. Previous studies exploited Q-learning techniques to discriminate neural states into simple directional actions providing the trial initial timing. However, the movements in BMI applications can be quite complicated, and the action timing explicitly shows the intention when to move. The rich actions and the corresponding neural states form a large state-action space, imposing generalization difficulty on Q-learning. In this paper, we propose to adopt attention-gated reinforcement learning (AGREL) as a new learning scheme for BMIs to adaptively decode high-dimensional neural activities into seven distinct movements (directional moves, holdings and resting) due to the efficient weight-updating. We apply AGREL on neural data recorded from M1 of a monkey to directly predict a seven-action set in a time sequence to reconstruct the trajectory of a center-out task. Compared to Q-learning techniques, AGREL could improve the target acquisition rate to 90.16% in average with faster convergence and more stability to follow neural activity over multiple days, indicating the potential to achieve better online decoding performance for more complicated BMI tasks. © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Attention-gated reinforcement learning (AGREL); Brain-machine interfaces (BMIS); Neural control; Trajectory tracking,Article,,Scopus,2-s2.0-84929330639
SCOPUS,"Hwang K.-S., Jiang W.-C., Chen Y.-J.",Model learning and knowledge sharing for a multiagent system with Dyna-Q learning,2015,IEEE Transactions on Cybernetics,45,5,6871355,964,976,,9,10.1109/TCYB.2014.2341582,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027918761&doi=10.1109%2fTCYB.2014.2341582&partnerID=40&md5=c39dd2adb2355b56de8ca1c78a770acb,"Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan; Department of Electrical Engineering, National Chung Cheng University, Chiayi, Taiwan","Hwang, K.-S., Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan; Jiang, W.-C., Department of Electrical Engineering, National Chung Cheng University, Chiayi, Taiwan; Chen, Y.-J., Department of Electrical Engineering, National Chung Cheng University, Chiayi, Taiwan","In a multiagent system, if agents' experiences could be accessible and assessed between peers for environmental modeling, they can alleviate the burden of exploration for unvisited states or unseen situations so as to accelerate the learning process. Since how to build up an effective and accurate model within a limited time is an important issue, especially for complex environments, this paper introduces a model-based reinforcement learning method based on a tree structure to achieve efficient modeling and less memory consumption. The proposed algorithm tailored a Dyna-Q architecture to multiagent systems by means of a tree structure for modeling. The tree-model built from real experiences is used to generate virtual experiences such that the elapsed time in learning could be reduced. As well, this model is suitable for knowledge sharing. This paper is inspired by the concept of knowledge sharing methods in multiagent systems where an agent could construct a global model from scattered local models held by individual agents. Consequently, it can increase modeling accuracy so as to provide valid simulated experiences for indirect learning at the early stage of learning. To simplify the sharing process, the proposed method applies resampling techniques to grafting partial branches of trees containing required and useful experiences disseminated from experienced peers, instead of merging the whole trees. The simulation results demonstrate that the proposed sharing method can achieve the objectives of sample efficiency and learning acceleration in multiagent cooperation applications. © 2013 IEEE.",Decision tree; Dyna-Q; model sharing; multiagent system,Article,,Scopus,2-s2.0-85027918761
SCOPUS,"Zhao Y.-Q., Zeng D., Laber E.B., Kosorok M.R.",New Statistical Learning Methods for Estimating Optimal Dynamic Treatment Regimes,2015,Journal of the American Statistical Association,110,510,,583,598,,27,10.1080/01621459.2014.937488,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936797778&doi=10.1080%2f01621459.2014.937488&partnerID=40&md5=4be5602bc2c4dc8e467b01449fb353a8,"Department of Biostatistics and Medical Informatics, University of Wisconsin-MadisonWI, United States; Department of Biostatistics, University of North Carolina at Chapel HillNC, United States; Department of Statistics, North Carolina State UniversityNC, United States; Department of Statistics and Operations Research, University of North Carolina at Chapel Hill, Chapel Hill, NC, United States","Zhao, Y.-Q., Department of Biostatistics and Medical Informatics, University of Wisconsin-MadisonWI, United States; Zeng, D., Department of Biostatistics, University of North Carolina at Chapel HillNC, United States; Laber, E.B., Department of Statistics, North Carolina State UniversityNC, United States; Kosorok, M.R., Department of Statistics and Operations Research, University of North Carolina at Chapel Hill, Chapel Hill, NC, United States","Dynamic treatment regimes (DTRs) are sequential decision rules for individual patients that can adapt over time to an evolving illness. The goal is to accommodate heterogeneity among patients and find the DTR which will produce the best long-term outcome if implemented. We introduce two new statistical learning methods for estimating the optimal DTR, termed backward outcome weighted learning (BOWL), and simultaneous outcome weighted learning (SOWL). These approaches convert individualized treatment selection into an either sequential or simultaneous classification problem, and can thus be applied by modifying existing machine learning techniques. The proposed methods are based on directly maximizing over all DTRs a nonparametric estimator of the expected long-term outcome; this is fundamentally different than regression-based methods, for example, Q-learning, which indirectly attempt such maximization and rely heavily on the correctness of postulated regression models.  We prove that the resulting rules are consistent, and provide finite sample bounds for the errors using the estimated rules. Simulation results suggest the proposed methods produce superior DTRs compared with Q-learning especially in small samples. We illustrate the methods using data from a clinical trial for smoking cessation. Supplementary materials for this article are available online. © 2015 American Statistical Association.",Classification; Personalized medicine; Q-learning; Reinforcement learning; Risk bound; Support vector machine,Article,,Scopus,2-s2.0-84936797778
SCOPUS,[No author name available],"International Conference on Intelligent User Interfaces, Proceedings IUI",2015,"International Conference on Intelligent User Interfaces, Proceedings IUI",29-March-2015,,,,,158,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958697830&partnerID=40&md5=2dd9e31f0e153e7e6b00a21384f274fb,,,"The proceedings contain 36 papers. The topics discussed include: automatic generation and insertion of assessment items in online video courses; MuLS: an open source EEG acquisition and streaming server for quick and simple prototyping and recording; user-interfaces for incremental recipient and response time predictions in asynchronous messaging; OfficeHours: a system for student supervisor matching through reinforcement learning; WallSHOP: multiuser interaction with public digital signage using mobile devices for personalized shopping; a task-centered interface for on-line collaboration in science; interactive querying over large network data: scalability, visualization, and interaction design; robot companions and smartpens for improved social communication of dementia patients; data privacy and security considerations for personal assistants for learning (PAL); and Mindminer: quantifying entity similarity via interactive distance metric learning.",,Conference Review,,Scopus,2-s2.0-84958697830
SCOPUS,"Qi H., Ma S., Jia N., Wang G.",Experiments on individual strategy updating in iterated snowdrift game under random rematching,2015,Journal of Theoretical Biology,368,,,1,12,,2,10.1016/j.jtbi.2014.12.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922607512&doi=10.1016%2fj.jtbi.2014.12.008&partnerID=40&md5=667741f583b8724be5eb3adf098c4c50,"Institute of Systems Engineering, College of Management and Economics, Tianjin University, Tianjin, China","Qi, H., Institute of Systems Engineering, College of Management and Economics, Tianjin University, Tianjin, China; Ma, S., Institute of Systems Engineering, College of Management and Economics, Tianjin University, Tianjin, China; Jia, N., Institute of Systems Engineering, College of Management and Economics, Tianjin University, Tianjin, China; Wang, G., Institute of Systems Engineering, College of Management and Economics, Tianjin University, Tianjin, China","How do people actually play the iterated snowdrift games, particularly under random rematching protocol is far from well explored. Two sets of laboratory experiments on snowdrift game were conducted to investigate human strategy updating rules. Four groups of subjects were modeled by experience-weighted attraction learning theory at individual-level. Three out of the four groups (75%) passed model validation. Substantial heterogeneity is observed among the players who update their strategies in four typical types, whereas rare people behave like belief-based learners even under fixed pairing. Most subjects (63.9%) adopt the reinforcement learning (or alike) rules; but, interestingly, the performance of averaged reinforcement learners suffered. It is observed that two factors seem to benefit players in competition, i.e., the sensitivity to their recent experiences and the overall consideration of forgone payoffs. Moreover, subjects with changing opponents tend to learn faster based on their own recent experience, and display more diverse strategy updating rules than they do with fixed opponent. These findings suggest that most of subjects do apply reinforcement learning alike updating rules even under random rematching, although these rules may not improve their performance. The findings help evolutionary biology researchers to understand sophisticated human behavioral strategies in social dilemmas. © 2015 Elsevier Ltd.",Evolutionary game theory; Experience-weighted attraction learning; Experimental economics; Matching protocol,Article,,Scopus,2-s2.0-84922607512
SCOPUS,"Li K., Meng M.Q.-H.",Personalizing a Service Robot by Learning Human Habits from Behavioral Footprints,2015,Engineering,1,1,,79,84,,,10.15302/J-ENG-2015024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988727903&doi=10.15302%2fJ-ENG-2015024&partnerID=40&md5=bd73d8218407187c4a2a349249f2a84c,"California Institute of Technology, Pasadena, CA, United States; The Chinese University of Hong Kong, Hong Kong, China","Li, K., California Institute of Technology, Pasadena, CA, United States; Meng, M.Q.-H., The Chinese University of Hong Kong, Hong Kong, China","For a domestic personal robot, personalized services are as important as predesigned tasks, because the robot needs to adjust the home state based on the operator's habits. An operator's habits are composed of cues, behaviors, and rewards. This article introduces behavioral footprints to describe the operator's behaviors in a house, and applies the inverse reinforcement learning technique to extract the operator's habits, represented by a reward function. We implemented the proposed approach with a mobile robot on indoor temperature adjustment, and compared this approach with a baseline method that recorded all the cues and behaviors of the operator. The result shows that the proposed approach allows the robot to reveal the operator's habits accurately and adjust the environment state accordingly. © 2015 THE AUTHORS. Published by Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company",behavioral footprints; habit learning; personalized robot,Article,Open Access,Scopus,2-s2.0-84988727903
SCOPUS,"Raghuveer V.R., Tripathy B.K., Singh T., Khanna S.",Reinforcement learning approach towards effective content recommendation in MOOC environments,2015,"Proceedings of the 2014 IEEE International Conference on MOOCs, Innovation and Technology in Education, IEEE MITE 2014",,,7020289,285,289,,3,10.1109/MITE.2014.7020289,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923522467&doi=10.1109%2fMITE.2014.7020289&partnerID=40&md5=a43bd0edbb6057325c6eba2df30c5c44,"SCSE, VIT University, Vellore, Tamilnadu, India; CSE, SCSE, VIT University, Vellore, Tamilnadu, India","Raghuveer, V.R., SCSE, VIT University, Vellore, Tamilnadu, India; Tripathy, B.K., SCSE, VIT University, Vellore, Tamilnadu, India; Singh, T., CSE, SCSE, VIT University, Vellore, Tamilnadu, India; Khanna, S., CSE, SCSE, VIT University, Vellore, Tamilnadu, India","Understanding the Learner requirements is an important aspect of any learning environment as it helps to recommend the LOs in a more personalized manner. With the growing demand for MOOCs offered by coursera, edx, etc. The learner information plays a vital role in understanding the extent to which the learners can gain out of such courses. The Learning Management Systems (LMS) across the web uses the explicit (rating, performance, etc.) and implicit feedback (LOs used) obtained through interaction with the learners to derive such information. As the requirements of the learners varies with the individual's interest and learning background, a common approach for recommending LOs may not cater the needs of all the learners. To overcome this issue, this paper proposes reinforcement learning based algorithm to analyze the learner information (derived from both implicit and explicit feedback) and generate the knowledge on the learner's requirements and capabilities inside a specific learning context. The reinforcement learning system (RILS) implemented as a part of this work utilizes the knowledge thus generated in order to recommend the appropriate LOs for the learners. The results have highlighted that the knowledge derived from the learning information analysis proved to effective in generating personalized recommendation policies that can cater the context specific requirements of the learners. © 2014 IEEE.",learning context; learning experience; LO recommendation; MOOC; Reinforcement Learning,Conference Paper,,Scopus,2-s2.0-84923522467
SCOPUS,"Theocharous G., Thomas P.S., Ghavamzadeh M.",Personalized ad recommendation systems for life-time value optimization with guarantees,2015,IJCAI International Joint Conference on Artificial Intelligence,2015-January,,,1806,1812,,7,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949758918&partnerID=40&md5=7653e3556c282c0be9fdbdfaf07f4fb5,"Adobe Research, United States; UMassAmherst, United States; INRIA, France","Theocharous, G., Adobe Research, United States; Thomas, P.S., Adobe Research, United States, UMassAmherst, United States; Ghavamzadeh, M., Adobe Research, United States, INRIA, France","In this paper, we propose a framework for using reinforcement learning (RL) algorithms to learn good policies for personalized ad recommendation (PAR) systems. The RL algorithms take into account the long-term effect of an action, and thus, could be more suitable than myopic techniques like supervised learning and contextual bandit, for modern PAR systems in which the number of returning visitors is rapidly growing. However, while myopic techniques have been well-studied in PAR systems, the RL approach is still in its infancy, mainly due to two fundamental challenges: how to compute a good RL strategy and how to evaluate a solution using historical data to ensure its ""safety"" before deployment. In this paper, we propose to use a family of off-policy evaluation techniques with statistical guarantees to tackle both these challenges. We apply these methods to a real PAR problem, both for evaluating the final performance and for optimizing the parameters of the RL algorithm. Our results show that a RL algorithm equipped with these offpolicy evaluation techniques outperforms the myopic approaches. Our results also give fundamental insights on the difference between the click through rate (CTR) and life-time value (LTV) metrics for evaluating the performance of a PAR algorithm.",,Conference Paper,,Scopus,2-s2.0-84949758918
SCOPUS,"Durand A., Pineau J.",Adaptive treatment allocation using sub-sampled Gaussian processes,2015,AAAI Fall Symposium - Technical Report,FS-15-04,,,9,11,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964573656&partnerID=40&md5=534281ed1c527f1c0b6fa9b0c046b90e,"Department of Electrical Engineering and Computer Engineering Universite, Laval, QC, Canada; School of Computer Science, McGill University, Montreal, Canada","Durand, A., Department of Electrical Engineering and Computer Engineering Universite, Laval, QC, Canada; Pineau, J., School of Computer Science, McGill University, Montreal, Canada","Personalized medicine targets the customization of treatment strategies to patients' individual characteristics. Here we consider the problem of optimizing personalized pharmacological treatment strategies for cancer. We focus primarily on developing effective strategies to collect the data necessary for the construction of personalized treatments. We formulate this problem as a contextual bandit and present a new algorithm based on repeated sub-sampling for robust data collection in this framework. We present a case study showing experiments on a simulation setting, built from real data collected in a previous animal experiments. Promising results in this case study have since lead us to deploy this strategy in a partner wet lab to allocate treatments for the next phase of animal experiments. Copyright © 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,Conference Paper,,Scopus,2-s2.0-84964573656
SCOPUS,"Neumann D., Mansi T., Itu L., Georgescu B., Kayvanpour E., Sedaghat-Hamedani F., Haas J., Katus H., Meder B., Steidl S., Hornegger J., Comaniciu D.",Vito – a generic agent for multi-physics model personalization: Application to heart modeling,2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9350,,,442,449,,1,10.1007/978-3-319-24571-3_53,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950986600&doi=10.1007%2f978-3-319-24571-3_53&partnerID=40&md5=c5628b61c47dad9007b6e8161dc760f4,"Imaging and Computer Vision, Siemens Corporate Technology, Princeton, NJ, Romania; Pattern Recognition Lab, FAU Erlangen-Nürnberg, Germany; Imaging and Computer Vision, Siemens Corporate Technology, Romania; Department of Internal Medicine III, University Hospital Heidelberg, Germany","Neumann, D., Imaging and Computer Vision, Siemens Corporate Technology, Princeton, NJ, Romania, Pattern Recognition Lab, FAU Erlangen-Nürnberg, Germany; Mansi, T., Imaging and Computer Vision, Siemens Corporate Technology, Princeton, NJ, Romania; Itu, L., Imaging and Computer Vision, Siemens Corporate Technology, Romania; Georgescu, B., Imaging and Computer Vision, Siemens Corporate Technology, Princeton, NJ, Romania; Kayvanpour, E., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Sedaghat-Hamedani, F., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Haas, J., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Katus, H., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Meder, B., Department of Internal Medicine III, University Hospital Heidelberg, Germany; Steidl, S., Pattern Recognition Lab, FAU Erlangen-Nürnberg, Germany; Hornegger, J., Pattern Recognition Lab, FAU Erlangen-Nürnberg, Germany; Comaniciu, D., Imaging and Computer Vision, Siemens Corporate Technology, Princeton, NJ, Romania","Precise estimation of computational physiological model parameters from patient data is one of the main hurdles towards their clinical applicability. Designing robust estimation algorithms is often a tedious and model-specific process. We propose to use, for the first time to our knowledge, artificial intelligence (AI) concepts to learn how to personalize a computational model, inspired by how an expert manually personalizes. We reformulate the parameter estimation problem in terms of Markov decision process and reinforcement learning. In an off-line phase, the artificial agent, called Vito, automatically learns a representative state-action-state model through data-driven exploration of the computational model under consideration. In other words, Vito learns how the model behaves under change of parameters and how to personalize it. Vito then controls the on-line personalization by exploiting its automatically derived action policy. Because the algorithm is model-independent, personalizing a completely new model would require only adjusting some simple parameters of the agent and defining the observations to match, without the full knowledge of the model itself. Vito was evaluated on two challenging problems: the inverse problem of cardiac electrophysiology and the personalization of a lumped-parameter whole-body circulation model. Obtained results suggested that Vito could achieve equivalent goodness of fit than standard methods, while being more robust (up to 25% higher success rates) and with faster (up to three times) convergence rate. Our AI approach could thus make model personalization algorithms generalizable and self-adaptable to any patient, like a human operator. © Springer International Publishing Switzerland 2015.",,Conference Paper,,Scopus,2-s2.0-84950986600
SCOPUS,"Casanueva I., Hain T., Christensen H., Marxer R., Green P.",Knowledge transfer between speakers for personalised dialogue management,2015,"SIGDIAL 2015 - 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Proceedings of the Conference",,,,12,21,,5,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988391769&partnerID=40&md5=dfddd45f3d809c20ad6bebb66483451d,"Department of Computer Science, University of Sheffield, United Kingdom","Casanueva, I., Department of Computer Science, University of Sheffield, United Kingdom; Hain, T., Department of Computer Science, University of Sheffield, United Kingdom; Christensen, H., Department of Computer Science, University of Sheffield, United Kingdom; Marxer, R., Department of Computer Science, University of Sheffield, United Kingdom; Green, P., Department of Computer Science, University of Sheffield, United Kingdom","Model-free reinforcement learning has been shown to be a promising data driven approach for automatic dialogue policy optimization, but a relatively large amount of dialogue interactions is needed before the system reaches reasonable performance. Recently, Gaussian process based reinforcement learning methods have been shown to reduce the number of dialogues needed to reach optimal performance, and pre-training the policy with data gathered from different dialogue systems has further reduced this amount. Following this idea, a dialogue system designed for a single speaker can be initialised with data from other speakers, but if the dynamics of the speakers are very different the model will have a poor performance. When data gathered from different speakers is available, selecting the data from the most similar ones might improve the performance. We propose a method which automatically selects the data to transfer by defining a similarity measure between speakers, and uses this measure to weight the influence of the data from each speaker in the policy model. The methods are tested by simulating users with different severities of dysarthria interacting with a voice enabled environmental control system.. © 2015 Association for Computational Linguistics.",,Conference Paper,,Scopus,2-s2.0-84988391769
SCOPUS,"Alipour M.M., Razavi S.N.",A new multiagent reinforcement learning algorithm to solve the symmetric traveling salesman problem,2015,Multiagent and Grid Systems,11,2,,107,119,,1,10.3233/MGS-150232,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940043679&doi=10.3233%2fMGS-150232&partnerID=40&md5=8efa63586c346e4fd35fbbb59b2e42dd,"Department of Computer Engineering, Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran","Alipour, M.M., Department of Computer Engineering, Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran; Razavi, S.N., Department of Computer Engineering, Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran","Travelling salesman problem (TSP) looks simple, however it is an important combinatorial problem. Its computational intractability has attracted a number of heuristic approaches to generate satisfactory, if not optimal solutions. In this paper, we present a new algorithm for the Symmetric TSP using Multiagent Reinforcement Learning (MARL) approach. Each agent in the multiagent system is an autonomous entity with personal declarative memory and behavioral components which are used to tour construction and then constructed tour of each agent is improved by 2-opt local search heuristic as tour improvement heuristic in order to reach optimal or near-optimal solutions in a reasonable time. The experiments in this paper are performed using the 29 datasets obtained from the TSPLIB. Also, the experimental results of the proposed method are compared with some well-known methods in the field. Our experimental results indicate that the proposed approach has a good performance with respect to the quality of the solution and the speed of computation.",2-opt local search heuristic; Multiagent reinforcement learning (MARL); Traveling salesman problem (TSP),Article,,Scopus,2-s2.0-84940043679
SCOPUS,"De Paula M., Acosta G.G., Martínez E.C.",On-line policy learning and adaptation for real-time personalization of an artificial pancreas,2015,Expert Systems with Applications,42,4,,2234,2255,,4,10.1016/j.eswa.2014.10.038,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910646834&doi=10.1016%2fj.eswa.2014.10.038&partnerID=40&md5=0b2a6928c88bfd78c305b765e1fa2e2b,"INTELYMEC, Centro de Investigaciones en Física e Ingeniería del Centro - CIFICEN, CONICET, Av. del Valle 5737, Olavarría, Argentina; UNCPBA-Facultad de Ingeniería, Universidad Nacional del Centro de la Provincia de Buenos Aires, Av. del Valle 5737, Olavarría, Argentina; INGAR (CONICET-UTN), Avellaneda 3657, Santa Fe, Argentina","De Paula, M., INTELYMEC, Centro de Investigaciones en Física e Ingeniería del Centro - CIFICEN, CONICET, Av. del Valle 5737, Olavarría, Argentina; Acosta, G.G., UNCPBA-Facultad de Ingeniería, Universidad Nacional del Centro de la Provincia de Buenos Aires, Av. del Valle 5737, Olavarría, Argentina; Martínez, E.C., INGAR (CONICET-UTN), Avellaneda 3657, Santa Fe, Argentina","The dynamic complexity of the glucose-insulin metabolism in diabetic patients is the main obstacle towards widespread use of an artificial pancreas. The significant level of subject-specific glycemic variability requires continuously adapting the control policy to successfully face daily changes in patient's metabolism and lifestyle. In this paper, an on-line selective reinforcement learning algorithm that enables real-time adaptation of a control policy based on ongoing interactions with the patient so as to tailor the artificial pancreas is proposed. Adaptation includes two online procedures: on-line sparsification and parameter updating of the Gaussian process used to approximate the control policy. With the proposed sparsification method, the support data dictionary for on-line learning is modified by checking if in the arriving data stream there exists novel information to be added to the dictionary in order to personalize the policy. Results obtained in silico experiments demonstrate that on-line policy learning is both safe and efficient for maintaining blood glucose variability within the normoglycemic range. © 2014 Elsevier Ltd. All rights reserved.",Diabetes; Gaussian processes; Glycemic variability; On-line sparsification; Policy learning; Reinforcement learning,Article,,Scopus,2-s2.0-84910646834
SCOPUS,"Yang H., Sloan M., Wang J.",Dynamic information retrieval modeling,2015,WSDM 2015 - Proceedings of the 8th ACM International Conference on Web Search and Data Mining,,,,409,410,,3,10.1145/2684822.2697038,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928710591&doi=10.1145%2f2684822.2697038&partnerID=40&md5=b9991c18a43a17193ce69d06e791c428,"Georgetown University, United Kingdom; University College London, United Kingdom","Yang, H., Georgetown University, United Kingdom; Sloan, M., University College London, United Kingdom; Wang, J., University College London, United Kingdom","In Dynamic Information Retrieval modeling we model dy-namic systems which change or adapt over time or a se-quence of events using a range of techniques from artificial intelligence and reinforcement learning. Many of the open problems in current IR research can be described as dynamic systems, for instance, session search or computational adver-tising. State of the art research provides solutions to these problems that are responsive to a changing environment, learn from past interactions and predict future utility. Ad-vances in IR interface, personalization and ad display de-mand models that can react to users in real time and in an intelligent, contextual way. The objective of this half-day tutorial is to provide a comprehensive and up-to-date introduction to Dynamic In-formation Retrieval Modeling. We motivate a conceptual model linking static, interactive and dynamic retrieval and use this to define dynamics within the context of IR. We then cover a number of algorithms and techniques from the artificial intelligence (AI) and online learning literature such as Markov Decision Processes (MDP) [1], their partially ob-servable variation (POMDP) [5] and multi-armed bandits [7]. Following this we describe how to identify dynamics in an IR problem and demonstrate how to model them using the described techniques. The remainder of the tutorial will then cover an array of state-of-the-art research on dynamic systems in IR and how they can be modeled using using dynamic IR [2, 6]. We use research on session search [3], multi-page search [4] and online advertising [8] as in-depth examples of such work. This tutorial is of relevance to IR practitioners and re-searchers, where we will present the merits of dynamic infor-mation retrieval modeling and introduce the relevant tech-niques. The content will be of particular interest to re-searchers working in the areas of statistical modeling, per-sonalization and recommendation, and is also relevant to practitioners in Web search, online advertising and anyone who works with big data. After this tutorial, attendees will: • Be able to identify the dynamics in an IR system •Be able to model these dynamics using techniques from AI and reinforcement learning • Have knowledge of the state-of-the-art research in dy-namic information retrieval modeling. Copyright © 2015 ACM.",Dynamic information retrieval modeling; Probabilistic rel-evance model; Reinforcement learning,Conference Paper,,Scopus,2-s2.0-84928710591
SCOPUS,[No author name available],"SIGDIAL 2015 - 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Proceedings of the Conference",2015,"SIGDIAL 2015 - 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Proceedings of the Conference",,,,,,460,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988311567&partnerID=40&md5=4c57cef40ae0ec7d54b4a5ca66e63ead,,,The proceedings contain 59 papers. The topics discussed include: human-machine dialogue as a stochastic game; knowledge transfer between speakers for personalized dialogue management; miscommunication recovery in physically situated dialogue; reinforcement learning in multi-party trading dialog; exploring the effects of redundancy within a tutorial dialogue system: restating students' responses; a discursive grid approach to model local coherence in multi-document summaries; belief tracking with stacked relational trees; which synthetic voice should I choose for an evocative task?; and exploiting knowledge base to generate responses for natural language dialog listening agents.,,Conference Review,,Scopus,2-s2.0-84988311567
SCOPUS,Wiebe J.,How to explore to maximize future return (invited talk),2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9091,,,,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945580124&partnerID=40&md5=00f775fd04c10724bcac7afdde539271,"Department of Computing Science, University of Alberta, Edmonton, Canada","Wiebe, J., Department of Computing Science, University of Alberta, Edmonton, Canada","With access to huge-scale distributed systems and more data than ever before, learning systems that learn to make good predictions break yesterday’s records on a daily basis. Although prediction problems are important, predicting what to do has its own challenges, which calls for specialized solution methods. In this talk, by means of some examples based on recent work on reinforcement learning, I will illustrate the unique opportunities and challenges that arise when a system must learn to make good decisions to maximize long-term return. In particular, I will start by demonstrating that passive data collection inevitably leads to catastrophic data sparsity in sequential decision making problems (no amount of data is big enough!), while clever algorithms, tailored to this setting, can escape data sparsity, learning essentially arbitrarily faster than what is possible under passive data collection. I will also describe current attempts to scale up such clever algorithms to work on large-scale problems. Amongst the possible approaches, I will discuss the role of sparsity to address this challenge in the practical, yet mathematically elegant setting of “linear bandits”. Interestingly, while in the related linear prediction problem, sparsity allows one to deal with huge dimensionality in a seamless fashion, the status of this question in the bandit setting is much less understood. © Springer International Publishing Switzerland 2015.",,Conference Paper,,Scopus,2-s2.0-84945580124
SCOPUS,[No author name available],Procedia Computer Science,2015,Procedia Computer Science,71,,,,,265,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962648606&partnerID=40&md5=65f505e2890a50d0d760d6f7f93d82e4,,,The proceedings contain 38 papers. The topics discussed include: Estimating Human Movements Using Memory of Errors; Constructing phenomenal knowledge in an unknown noumenal reality; modeling biological agents beyond the reinforcement-learning paradigm; development of a self-evolving conscious system; fly-the-bee: a game imitating concept learning in bees; a generic software platform for brain-inspired cognitive computing; the thermal grill illusion: a study using a consciousness system; pleasant and unpleasant states in a robot; semiautonomous control of personal mobility based on passenger's collision avoidance judgment timing; a comparison among cognitive architectures: a theoretical analysis; guidelines for designing artifacts for the dual-process; an approach for the binding problem based on brain-oriented autonomous adaptation system with object handling functions; model-based behavioral causality analysis of handball with delayed transfer entropy; and integrating a cognitive framework for knowledge representation and categorization in diverse cognitive architectures.,,Conference Review,,Scopus,2-s2.0-84962648606
SCOPUS,"Wu Q., Qiu J., Ding G.",Machine learning methods for big spectrum data processing,2015,Shuju Caiji Yu Chuli/Journal of Data Acquisition and Processing,30,4,,703,713,,3,10.16337/j.1004-9037.2015.04.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941894431&doi=10.16337%2fj.1004-9037.2015.04.001&partnerID=40&md5=e9115001bb97be1c11c6026d71376b47,"College of Communications Engineering, PLA University of Science and Technology, Nanjing, China","Wu, Q., College of Communications Engineering, PLA University of Science and Technology, Nanjing, China; Qiu, J., College of Communications Engineering, PLA University of Science and Technology, Nanjing, China; Ding, G., College of Communications Engineering, PLA University of Science and Technology, Nanjing, China","With the rapid development of the mobile Internet and the Internet of Things, the number of personal wireless devices has grown exponentially, resulting in the increase of massive spectrum data. Therefore, the big spectrum data are literally formed. Meanwhile, the spectrum deficit is also increasingly precarious. Effective big spectrum data processing is significant in improving the spectrum utilization. Firstly, from a perspective of wireless communication, a definition of big spectrum data is presented and its characteristics are also analyzed. Then, promising machine learning methods to analyze and utilize the big spectrum data are summarized, such as, the distributed and parallel learning, extreme learning machine, kernel-based learning, deep learning, reinforcement learning, game learning, and transfer learning. Finally, several open issues and research trends are addressed. ©, 2015, Journal of Data Acquisition and Processing. All right reserved.",Big data; Big spectrum data; Data mining; Internet of Things; Machine learning; Wireless communication,Article,,Scopus,2-s2.0-84941894431
SCOPUS,"Su P.-H., Wu C.-H., Lee L.-S.",A recursive dialogue game for personalized computer-aided pronunciation training,2015,IEEE/ACM Transactions on Audio Speech and Language Processing,23,1,,127,141,,6,10.1109/TASLP.2014.2375572,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923277426&doi=10.1109%2fTASLP.2014.2375572&partnerID=40&md5=d368ad490ffcaf553610ef2d6245d08f,"Department of Engineering, University of Cambridge, Cambridge, United Kingdom; Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan","Su, P.-H., Department of Engineering, University of Cambridge, Cambridge, United Kingdom; Wu, C.-H., Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan; Lee, L.-S., Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan","Learning languages in addition to the native language is very important for all people in the globalized world today, and computer-aided pronunciation training (CAPT) is attractive since the software can be used anywhere at any time, and repeated as many times as desired. In this paper, we introduce the immersive interaction scenario offered by spoken dialogues to CAPT by proposing a recursive dialogue game to make CAPT personalized. A number of tree-structured sub-dialogues are linked sequentially and recursively as the script for the game. The system policy at each dialogue turn is to select in real-time along the dialogue the best training sentence for each specific individual learner within the dialogue script, considering the learner's learning status and the future possible dialogue paths in the script, such that the learner can have the scores for all pronunciation units considered reaching a predefined standard in a minimum number of turns. The purpose here is that those pronunciation units poorly produced by the specific learner can be offered with more practice opportunities in the future sentences along the dialogue, which enables the learner to improve the pronunciation without having to repeat the same training sentences many times. This makes the learning process for each learner completely personalized. The dialogue policy is modeled by Markov decision process (MDP) with high-dimensional continuous state space, and trained with fitted value iteration using a huge number of simulated learners. These simulated leaners have the behavior similar to real learners, and were generated from a corpus of real learner data. The experiments demonstrated very promising results and a real cloud-based system is also successfully implemented. © 2014 IEEE.",Computer-aided pronunciation training (CAPT); computer-assisted language learning; dialogue game; Markov decision process; reinforcement learning,Article,,Scopus,2-s2.0-84923277426
SCOPUS,"Pearson D., Donkin C., Tran S.C., Most S.B., Le Pelley M.E.",Cognitive control and counterproductive oculomotor capture by reward-related stimuli,2015,Visual Cognition,23,1-2,,41,66,,19,10.1080/13506285.2014.994252,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928938476&doi=10.1080%2f13506285.2014.994252&partnerID=40&md5=1fa3d3bce5bf4240038cf63769c4d855,"School of Psychology, University of New South Wales, Sydney, NSW, Australia","Pearson, D., School of Psychology, University of New South Wales, Sydney, NSW, Australia; Donkin, C., School of Psychology, University of New South Wales, Sydney, NSW, Australia; Tran, S.C., School of Psychology, University of New South Wales, Sydney, NSW, Australia; Most, S.B., School of Psychology, University of New South Wales, Sydney, NSW, Australia; Le Pelley, M.E., School of Psychology, University of New South Wales, Sydney, NSW, Australia","Two experiments investigated the extent to which value-modulated oculomotor capture is subject to top-down control. In these experiments, participants were never required to look at the reward-related stimuli; indeed, doing so was directly counterproductive because it caused omission of the reward that would otherwise have been obtained. In Experiment 1, participants were explicitly informed of this omission contingency. Nevertheless, they still showed counterproductive oculomotor capture by reward-related stimuli, suggesting that this effect is relatively immune to cognitive control. Experiment 2 more directly tested whether this capture is controllable by comparing the performance of participants who either had or had not been explicitly informed of the omission contingency. There was no evidence that value-modulated oculomotor capture differed between the two conditions, providing further evidence that this effect proceeds independently of cognitive control. Taken together, the results of the present research provide strong evidence for the automaticity and cognitive impenetrability of value-modulated attentional capture. © 2015 Taylor & Francis.",Attentional capture; Eye movements; Reinforcement learning; Reward learning; Visual attention,Article,,Scopus,2-s2.0-84928938476
SCOPUS,"Bagdure N., Ambudkar B.",Reducing delay during vertical handover,2015,"Proceedings - 1st International Conference on Computing, Communication, Control and Automation, ICCUBEA 2015",,,7155834,200,204,,,10.1109/ICCUBEA.2015.44,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943327269&doi=10.1109%2fICCUBEA.2015.44&partnerID=40&md5=1c9d3c83bfa03dafc3887b5993bd8707,"Department of Electronics Engineering, Pad. Dr. D. Y. P. I. E. T., Pune, Maharashtra, India","Bagdure, N., Department of Electronics Engineering, Pad. Dr. D. Y. P. I. E. T., Pune, Maharashtra, India; Ambudkar, B., Department of Electronics Engineering, Pad. Dr. D. Y. P. I. E. T., Pune, Maharashtra, India","The next generation of wireless networks will provide heterogeneous wireless technologies to a mobile user, in which they can move using terminals with multiple access interfaces and non-real-time or real-time services. The most important issue in such environment is the Always Best Connected (ABC) concept allowing the best connectivity to applications anywhere at any time. With the growing of communication world lot of research is carried out in the field of wireless networks specifically next generation networks. The integration and interoperability of various wireless networks will lead to number of challenges. One of the challenges is the handovers across various/heterogeneous wireless networks as well as reduction of call drop probability. Number of researches has proposed solutions for this challenge. This paper proposes a solution to this problem for the improvement of end-to-end Quality of Service supporting high data rates, real time transmission over wide area and enabling users to specify their personal preferences using Markov Decision Process (MDP). The proposed method uses delay as its basic parameter to select a network during handovers. The selection of the network amongst the existing wireless network considers the ongoing application of the user during the handover. Through simulations we show that our algorithm reduces call drop probability and satisfies user's requirements. © 2015 IEEE.",Markov decision process; Reinforcement learning; Reward; Transition probability; Vertical handover,Conference Paper,,Scopus,2-s2.0-84943327269
SCOPUS,"Ferretti S., Mirri S., Prandi C., Salomoni P.",Exploiting reinforcement learning to profile users and personalize web pages,2014,"Proceedings - IEEE 38th Annual International Computers, Software and Applications Conference Workshops, COMPSACW 2014",,,6903138,252,257,,3,10.1109/COMPSACW.2014.45,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931075549&doi=10.1109%2fCOMPSACW.2014.45&partnerID=40&md5=c2a5c8e9023aa6654a71e345b1523080,"Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy","Ferretti, S., Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy; Mirri, S., Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy; Prandi, C., Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy; Salomoni, P., Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy","In this paper, we present a Web content adaptation system that is able to automatically adapt textual elements of Web pages, based on the user profile and preferences. The system employs Web intelligence to perform these automatic adaptations on single elements composing a Web page. In particular, a reinforcement learning algorithm, i.e. Q-learning, based on the idea of reward/punishment is utilized as the machine learning system that manages the user profile. Based on it, the user profile is updated, so that automatic adaptations can be effectively performed while surfing the Web. We created a simulation scenario to test our approach over different users with specific preferences and/or different kinds of disabilities. Simulation results confirm the viability of the proposal. © 2014 IEEE.",content adaptation; legibility; reinforcement learning; user profiling; Web personalization,Conference Paper,,Scopus,2-s2.0-84931075549
SCOPUS,"Shum H.P.H., Hoyet L., Ho E.S.L., Komura T., Multon F.",Natural preparation behavior synthesis,2014,Computer Animation and Virtual Worlds,25,5-6,,531,542,,,10.1002/cav.1546,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908543309&doi=10.1002%2fcav.1546&partnerID=40&md5=5d313de575efc8dbfeef4e15176ae509,"Northumbria University, United Kingdom; Trinity College Dublin, Ireland; Hong Kong Baptist University, Hong Kong; University of Edinburgh, United Kingdom; University Rennes 2, France","Shum, H.P.H., Northumbria University, United Kingdom; Hoyet, L., Trinity College Dublin, Ireland; Ho, E.S.L., Hong Kong Baptist University, Hong Kong; Komura, T., University of Edinburgh, United Kingdom; Multon, F., University Rennes 2, France","Humans adjust their movements in advance to prepare for the forthcoming action, resulting in efficient and smooth transitions. However, traditional computer animation approaches such as motion graphs simply concatenate a series of actions without taking into account the following one. In this paper, we propose a new method to produce preparation behaviors using reinforcement learning. As an offline process, the system learns the optimal way to approach a target and to prepare for interaction. A scalar value called the level of preparation is introduced, which represents the degree of transition from the initial action to the interacting action. To synthesize the movements of preparation, we propose a customized motion blending scheme based on the level of preparation, which is followed by an optimization framework that adjusts the posture to keep the balance. During runtime, the trained controller drives the character to move to a target with the appropriate level of preparation, resulting in a humanlike behavior. We create scenes in which the character has to move in a complex environment and to interact with objects, such as crawling under and jumping over obstacles while walking. The method is useful not only for computer animation but also for real-time applications such as computer games, in which the characters need to accomplish a series of tasks in a given environment. Copyright © 2013 John Wiley & Sons, Ltd.",Motion blending; Motion synthesis; Posture optimization; Preparation behavior; Reinforcement learning,Article,,Scopus,2-s2.0-84908543309
SCOPUS,"Erev I., Roth A.E.","Maximization, learning, and economic behavior",2014,Proceedings of the National Academy of Sciences of the United States of America,111,SUPPL.3,,10818,10825,,34,10.1073/pnas.1402846111,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904623675&doi=10.1073%2fpnas.1402846111&partnerID=40&md5=69d7ba87bf82a1c7d278af694eecaf43,"Industrial Engineering and Management, Technion, Haifa 32000, Israel; Warwick Business School, University of Warwick, Coventry CV4 7AL, United Kingdom; Department of Economics, Stanford University, Stanford, CA 94305-6072, United States","Erev, I., Industrial Engineering and Management, Technion, Haifa 32000, Israel, Warwick Business School, University of Warwick, Coventry CV4 7AL, United Kingdom; Roth, A.E., Department of Economics, Stanford University, Stanford, CA 94305-6072, United States","The rationality assumption that underlies mainstream economic theory has proved to be a useful approximation, despite the fact that systematic violations to its predictions can be found. That is, the assumption of rational behavior is useful in understanding the ways in which many successful economic institutions function, although it is also true that actual human behavior falls systematically short of perfect rationality. We consider a possible explanation of this apparent inconsistency, suggesting that mechanisms that rest on the rationality assumption are likely to be successful when they create an environment in which the behavior they try to facilitate leads to the best payoff for all agents on average, and most of the time. Review of basic learning research suggests that, under these conditions, people quickly learn to maximize expected return. This review also shows that there are many situations in which experience does not increase maximization. In many cases, experience leads people to underweight rare events. In addition, the current paper suggests that it is convenient to distinguish between two behavioral approaches to improve economic analyses. The first, and more conventional approach among behavioral economists and psychologists interested in judgment and decision making, highlights violations of the rational model and proposes descriptive models that capture these violations. The second approach studies human learning to clarify the conditions under which people quickly learn to maximize expected return. The current review highlights one set of conditions of this type and shows how the understanding of these conditions can facilitate market design.",Contingencies of reinforcements; Decisions from experience; Experience-description gap; Mechanism design; Reinforcement learning,Review,,Scopus,2-s2.0-84904623675
SCOPUS,"Mahmood T., Mujtaba G., Venturini A.",Dynamic personalization in conversational recommender systems,2014,Information Systems and e-Business Management,12,2,,213,238,,9,10.1007/s10257-013-0222-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953638450&doi=10.1007%2fs10257-013-0222-3&partnerID=40&md5=4692e5b4f1f7c8a9be659d012a702aee,"Department of Computer Science, National University of Computer and Emerging Sciences (NUCES), Shah Lateef Town, National Highway, Karachi, Pakistan; Sukkur Institute of Business Administration, Airport Road, Sukkur, Sindh, Pakistan; ECTRL Solutions SRL, Via Solteri 38, Trento, Italy","Mahmood, T., Department of Computer Science, National University of Computer and Emerging Sciences (NUCES), Shah Lateef Town, National Highway, Karachi, Pakistan; Mujtaba, G., Sukkur Institute of Business Administration, Airport Road, Sukkur, Sindh, Pakistan; Venturini, A., ECTRL Solutions SRL, Via Solteri 38, Trento, Italy","Conversational recommender systems are E-Commerce applications which interactively assist online users to acquire their interaction goals during their sessions. In our previous work, we have proposed and validated a methodology for conversational systems which autonomously learns the particular web page to display to the user, at each step of the session. We employed reinforcement learning to learn an optimal strategy, i.e., one that is personalized for a real user population. In this paper, we extend our methodology by allowing it to autonomously learn and update the optimal strategy dynamically (at run-time), and individually for each user. This learning occurs perpetually after every session, as long as the user continues her interaction with the system. We evaluate our approach in an off-line simulation with four simulated users, as well as in an online evaluation with thirteen real users. The results show that an optimal strategy is learnt and updated for each real and simulated user. For each simulated user, the optimal behavior is reasonably adapted to this user’s characteristics, but converges after several hundred sessions. For each real user, the optimal behavior converges only in several sessions. It provides assistance only in certain situations, allowing many users to buy several products together in shorter time and with more page-views and lesser number of query executions. We prove that our approach is novel and show how its current limitations can catered. © Springer-Verlag Berlin Heidelberg 2013.",Conversational recommender systems; Dynamic personalization; Individual user; Off-line simulation; On-line experiment; Optimal strategy; Real users; Reinforcement learning,Article,,Scopus,2-s2.0-84953638450
SCOPUS,"Osili U.O., Paulson A.",Crises and confidence: Systemic banking crises and depositor behavior,2014,Journal of Financial Economics,111,3,,646,660,,8,10.1016/j.jfineco.2013.11.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893678217&doi=10.1016%2fj.jfineco.2013.11.002&partnerID=40&md5=95bbc174fc50998df2c38a801cd6c9a0,"Indiana University-Purdue University at Indianapolis, United States; Federal Reserve Bank of Chicago, 230 S. LaSalle Street, Chicago, IL 60604, United States","Osili, U.O., Indiana University-Purdue University at Indianapolis, United States; Paulson, A., Federal Reserve Bank of Chicago, 230 S. LaSalle Street, Chicago, IL 60604, United States","We show that individuals who have experienced a systemic banking crisis are 11 percentage points less likely to use banks in the U.S. than otherwise similar individuals who emigrated from the same country but did not live through a crisis. This finding is robust to controlling for exposure to other macroeconomic events and to various methods for addressing potential bias due to migrant self-selection. Consistent with the view that personal experience plays an important role in decision-making, the effects are larger for individuals who were older and more likely to have had wealth entrusted to the banking system at the time of the crisis and for people who experienced crises in countries without deposit insurance. © 2013 Elsevier B.V.",Confidence; Deposit insurance; Immigrants; Reinforcement learning; Systemic banking crises,Article,,Scopus,2-s2.0-84893678217
SCOPUS,[No author name available],"Proceedings - 2014 13th International Conference on Machine Learning and Applications, ICMLA 2014",2014,"Proceedings - 2014 13th International Conference on Machine Learning and Applications, ICMLA 2014",,,,,,643,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949087653&partnerID=40&md5=de15f2f689464558fba5b0b86e5b4d15,,,The proceedings contain 105 papers. The topics discussed include: Arctic Sea ice extent forecasting using support vector regression; budgeted learning for developing personalized treatment; modeling mutual information between voiceprint and optimal number of Mel-frequency cepstral coefficients in voice discrimination; high precision screening for android malware with dimensionality reduction; a hybrid genetic-programming swarm-optimization approach for examining the nature and stability of high frequency trading strategies; Bayesian nonparametric inverse reinforcement learning for switched Markov decision processes; varying coefficient models for analyzing the effects of risk factors on pregnant women's blood pressure; and ensemble statistical and heuristic models for unsupervised word alignment.,,Conference Review,,Scopus,2-s2.0-84949087653
SCOPUS,"Deng K., Greiner R., Murphy S.",Budgeted learning for developing personalized treatment,2014,"Proceedings - 2014 13th International Conference on Machine Learning and Applications, ICMLA 2014",,,7033084,7,14,,,10.1109/ICMLA.2014.8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946687434&doi=10.1109%2fICMLA.2014.8&partnerID=40&md5=1557babc5035bb55e187bcad65b2b317,"Department of Statistics, University of MichiganMI, United States; Department of Computer Science, University of AlbertaAB, Canada","Deng, K., Department of Statistics, University of MichiganMI, United States; Greiner, R., Department of Computer Science, University of AlbertaAB, Canada; Murphy, S., Department of Statistics, University of MichiganMI, United States","There is increased interest in using patient-specific information to personalize treatment. Personalized treatment decision rules can be learned using data from standard clinical trials, but such trials are very costly to run. This paper explores the use of budgeted learning techniques to design more efficient clinical trials, by effectively determining which type of patients to recruit, at each time, throughout the duration of the trial. We propose a Bayesian bandit model and discuss the computational challenges and issues pertaining to this approach. We compare our budgeted learning algorithm, which approximately minimizes the Bayes risk, using both simulated data and data modeled after a clinical trial for treating depressed individuals, with other plausible algorithms. We show that our budgeted learning algorithm demonstrated excellent performance across a wide variety of situations. © 2014 IEEE.",Active Learning; Bayesian; Budgeted Learning; Personalized Treatment; Reinforcement Learning,Conference Paper,,Scopus,2-s2.0-84946687434
SCOPUS,"Tang L., Jiang Y., Li L., Li T.",Ensemble contextual bandits for personalized recommendation,2014,RecSys 2014 - Proceedings of the 8th ACM Conference on Recommender Systems,,,,73,80,,17,10.1145/2645710.2645732,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908877655&doi=10.1145%2f2645710.2645732&partnerID=40&md5=ba570d5a0e504e972c6b955345f4d614,"School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States","Tang, L., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Jiang, Y., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Li, L., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States; Li, T., School of Computing and Information Sciences, Florida International University, 11200 S.W. 8th Street, Miami, FL, United States","The cold-start problem has attracted extensive attention among various online services that provide personalized recommendation. Many online vendors employ contextual bandit strategies to tackle the so-called exploration/exploitation dilemma rooted from the cold-start problem. However, due to high-dimensional user/item features and the underlying characteristics of bandit policies, it is often difficult for service providers to obtain and deploy an appropriate algorithm to achieve acceptable and robust economic profit. In this paper, we explore ensemble strategies of contextual bandit algorithms to obtain robust predicted click-through rate (CTR) of web objects. The ensemble is acquired by aggregating different pulling policies of bandit algorithms, rather than forcing the agreement of prediction results or learning a unified predictive model. To this end, we employ a meta-bandit paradigm that places a hyper bandit over the base bandits, to explicitly explore/exploit the relative importance of base bandits based on user feedbacks. Extensive empirical experiments on two real-world data sets (news recommendation and online advertising) demonstrate the effectiveness of our proposed approach in terms of CTR. Copyright © 2014 ACM.",Contextual bandit; CTR prediction; Ensemble recommendation; Meta learning; Personalized recommendation,Conference Paper,,Scopus,2-s2.0-84908877655
SCOPUS,[No author name available],RecSys 2014 - Proceedings of the 8th ACM Conference on Recommender Systems,2014,RecSys 2014 - Proceedings of the 8th ACM Conference on Recommender Systems,,,,,,452,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920528470&partnerID=40&md5=1bac501a71f1ccae19c46fcfe77e082c,,,"The proceedings contain 84 papers. The topics discussed include: exploiting sentiment homophily for link prediction; a robust model for paper reviewer assignment; context adaptation in interactive recommender systems; question recommendation with constraints for massive open online courses; attacking item-based recommender systems with power items; recommending with an agenda: active learning of private attributes using matrix factorization; ensemble contextual bandits for personalized recommendation; cold-start news recommendation with domain-dependent browse graph; item cold-start recommendations: learning local collective embeddings; improving the discriminative power of inferred content information using segmented virtual profile; ratings meet reviews, a combined approach to recommend; beyond clicks: dwell time for personalization; and evaluating recommender behavior for new users.",,Conference Review,,Scopus,2-s2.0-84920528470
SCOPUS,"Priscoli F.D., Fogliati L., Palo A., Pietrabissa A.",Dynamic class of service mapping for quality of experience control in future networks,2014,"Proceedings of World Telecommunications Congress 2014, WTC 2014",,,6840010,,,,6,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991230299&partnerID=40&md5=4500eb29ee0bf742576e369aad0b419a,"Department of Computer, Control and Management Engineering Antonio Ruberti, University of Rome la Sapienza, Rome, Italy","Priscoli, F.D., Department of Computer, Control and Management Engineering Antonio Ruberti, University of Rome la Sapienza, Rome, Italy; Fogliati, L., Department of Computer, Control and Management Engineering Antonio Ruberti, University of Rome la Sapienza, Rome, Italy; Palo, A., Department of Computer, Control and Management Engineering Antonio Ruberti, University of Rome la Sapienza, Rome, Italy; Pietrabissa, A., Department of Computer, Control and Management Engineering Antonio Ruberti, University of Rome la Sapienza, Rome, Italy","The present work introduces a novel approach to cope with some key limitations of the present communication networks. In particular, the need for effectively managing heterogeneous resources over heterogeneous networks while guaranteeing personalized Quality of Experience (QoE) requirements to the applications, claims for a frill cognitive approach which is realized through the introduction of an appropriate Future Internet architecture. The paper, taking this architecture in mind, introduces the innovative concept of dynamic association between applications and Classes of Services, performed by means of proper Reinforcement Learning algorithms. The resulting procedure guarantees QoE personalization, requires low processing capabilities and entails limited signalling overhead. © VDE VERLAG GMBH · Berlin · Offenbach.",Future Internet; Quality of Experience; Quality of Service; Reinforcement Learning,Conference Paper,,Scopus,2-s2.0-84991230299
SCOPUS,"Tuncel E., Zeid A., Kamarthi S.",Solving large scale disassembly line balancing problem with uncertainty using reinforcement learning,2014,Journal of Intelligent Manufacturing,25,4,,647,659,,15,10.1007/s10845-012-0711-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904384763&doi=10.1007%2fs10845-012-0711-0&partnerID=40&md5=cf5242ed0560e198a2a024f86478a8a9,"Department of Mechanical and Industrial Engineering, Northeastern University, 360 Huntington Avenue, Boston, MA 02115, United States","Tuncel, E., Department of Mechanical and Industrial Engineering, Northeastern University, 360 Huntington Avenue, Boston, MA 02115, United States; Zeid, A., Department of Mechanical and Industrial Engineering, Northeastern University, 360 Huntington Avenue, Boston, MA 02115, United States; Kamarthi, S., Department of Mechanical and Industrial Engineering, Northeastern University, 360 Huntington Avenue, Boston, MA 02115, United States","Due to increasing environmental concerns, manufacturers are forced to take back their products at the end of products' useful functional life. Manufacturers explore various options including disassembly operations to recover components and subassemblies for reuse, remanufacture, and recycle to extend the life of materials in use and cut down the disposal volume. However, disassembly operations are problematic due to high degree of uncertainty associated with the quality and configuration of product returns. In this research we address the disassembly line balancing problem (DLBP) using a Monte-Carlo based reinforcement learning technique. This reinforcement learning approach is tailored fit to the underlying dynamics of a DLBP. The research results indicate that the reinforcement learning based method is able to perform effectively, even on a complex large scale problem, within a reasonable amount of computational time. The proposed method performed on par or better than the benchmark methods for solving DLBP reported in the literature. Unlike other methods which are usually limited deterministic environments, the reinforcement learning based method is able to operate in deterministic as well as stochastic environments. © 2012 Springer Science+Business Media New York.",Cell phone; Disassembly; Disassembly line balancing; Heuristics; PC; Reinforcement learning,Article,,Scopus,2-s2.0-84904384763
SCOPUS,"Hiraoka T., Neubig G., Sakti S., Toda T., Nakamura S.",Reinforcement learning of cooperative persuasive dialogue policies using framing,2014,"COLING 2014 - 25th International Conference on Computational Linguistics, Proceedings of COLING 2014: Technical Papers",,,,1706,1717,,7,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959910680&partnerID=40&md5=b153841fdc1b686556212286f40ea1af,"Nara Institute of Science and Technology (NAIST), Nara, Japan","Hiraoka, T., Nara Institute of Science and Technology (NAIST), Nara, Japan; Neubig, G., Nara Institute of Science and Technology (NAIST), Nara, Japan; Sakti, S., Nara Institute of Science and Technology (NAIST), Nara, Japan; Toda, T., Nara Institute of Science and Technology (NAIST), Nara, Japan; Nakamura, S., Nara Institute of Science and Technology (NAIST), Nara, Japan","In this paper, we apply reinforcement learning for automatically learning cooperative persuasive dialogue system policies using framing, the use of emotionally charged statements common in persuasive dialogue between humans. In order to apply reinforcement learning, we describe a method to construct user simulators and reward functions specifically tailored to persuasive dialogue based on a corpus of persuasive dialogues between human interlocutors. Then, we evaluate the learned policy and the effect of framing through experiments both with a user simulator and with real users. The experimental evaluation indicates that applying reinforcement learning is effective for construction of cooperative persuasive dialogue systems which use framing.",,Conference Paper,,Scopus,2-s2.0-84959910680
SCOPUS,"Ferretti S., Mirri S., Prandi C., Salomoni P.",User centered and context dependent personalization through experiential transcoding,2014,"2014 IEEE 11th Consumer Communications and Networking Conference, CCNC 2014",,,6940520,486,491,,5,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948730632&partnerID=40&md5=1315b1a8306c9029d9996602efc75440,"Department of Computer Science and Engineering, University of Bologna, Bologna, Italy","Ferretti, S., Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Mirri, S., Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Prandi, C., Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Salomoni, P., Department of Computer Science and Engineering, University of Bologna, Bologna, Italy","The pervasive presence of devices exploited to use and deliver entertainment text-based content can make reading easier to get but more difficult to enjoy, in particular for people with reading-related disabilities. The main solutions that allow overcoming some of the difficulties experienced by users with specific and special needs are based on content adaptation and user profiling. This paper presents a system that aims to improve content legibility by exploiting experiential transcoding techniques. The system we propose tracks users' behaviors so as to provide a tailored adaptation of textual content, in order to fit the needs of a specific user on the several different devices she/he actually uses. © 2014IEEE.",Content adaptation; Device capabilities; Legibility; Reinforcement learning; User profiling,Conference Paper,,Scopus,2-s2.0-84948730632
SCOPUS,"Ondrúška P., Posner I.",The route not taken: Driver-centric estimation of electric vehicle range,2014,"Proceedings International Conference on Automated Planning and Scheduling, ICAPS",2014-January,January,,413,420,,11,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933050809&partnerID=40&md5=46b4fa6e96f048605ef4708bb7d219f6,"Mobile Robotics Group, University of Oxford, United Kingdom","Ondrúška, P., Mobile Robotics Group, University of Oxford, United Kingdom; Posner, I., Mobile Robotics Group, University of Oxford, United Kingdom","This paper addresses the challenge of efficiently and accurately predicting an electric vehicle's attainable range. Specifically, our approach accounts for a driver's generalised route preferences to provide up-to-date, personalised information based on estimates of the energy required to reach every possible destination in a map. We frame this task in the context of sequential decision making and show that energy consumption in reaching a particular destination can be formulated as policy evaluation in a Markov Decision Process. In particular, we exploit the properties of the model adopted for predicting likely energy consumption to every possible destination in a realistically sized map in real-time. The policy to be evaluated is learned and, over time, refined using Inverse Reinforcement Learning to provide for a life-long adaptive system. Our approach is evaluated using a publicly available dataset providing real trajectory data of 50 individuals spanning approximately 10,000 miles of travel. We show that by accounting for driver specific route preferences our system significantly reduces the relative error in energy prediction compared to more common, driver-agnostic heuristics such as shortest-path or shortest-time routes. Copyright © 2014, Association for the Advancement of Artificial Intelligence.",,Conference Paper,,Scopus,2-s2.0-84933050809
SCOPUS,"Marivate V., Chemali J., Brunskill E., Littmanf M.",Quantifying uncertainty in batch personalized sequential decision making,2014,AAAI Workshop - Technical Report,WS-14-08,,,26,30,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974800780&partnerID=40&md5=436733a7134fcc8b7ea47a3703f08edc,"Department of Computer Science, Rutgers University, United States; Department of Computer Science, Carnegie Mellon University, United States; Department of Computer Science, Brown University, United States","Marivate, V., Department of Computer Science, Rutgers University, United States; Chemali, J., Department of Computer Science, Carnegie Mellon University, United States; Brunskill, E., Department of Computer Science, Carnegie Mellon University, United States; Littmanf, M., Department of Computer Science, Brown University, United States","As the amount of data collected from individuals increases, there are more opportunities to use it to offer personalized experiences (e.g., using electronic health records to offer personalized treatments). We advocate applying techniques from batch reinforcement learning to predict the range of effectiveness that policies might have for individuals. We identify three sources of uncer-tainty and present a method that addresses all of them. It handles the uncertainty caused by population mismatch by modeling the data as a latent mixture of different subpopulations of individuals, it explicitly quantifies data sparsity by accounting for the limited data available about the underlying models, and incorporates intrinsic stochasticity to yield estimated percentile ranges of the effectiveness of a policy for a particular new individual. Using this approach, we highlight some interesting variability in policy effectiveness amongst HIV patients given a prior patient treatment dataset. Our approach highlights the potential benefit of taking into account individual variability and data limitations when performing batch policy evaluation for new individuals. © Copyright 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,Conference Paper,,Scopus,2-s2.0-84974800780
SCOPUS,"Wang Y., Wang X., Wang Y., Hsu D.",Exploration in interactive personalized music recommendation: A reinforcement learning approach,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",11,1,7,,,,15,10.1145/2623372,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906856108&doi=10.1145%2f2623372&partnerID=40&md5=da52cbcf39aaff70ff8fd51e7294ca75,"Department of Computer Science, National University of Singapore, Singapore 117417, Singapore; Computing Science Department, Institute of High Performance Computing, A STAR, Singapore 138632, Singapore","Wang, Y., Department of Computer Science, National University of Singapore, Singapore 117417, Singapore; Wang, X., Department of Computer Science, National University of Singapore, Singapore 117417, Singapore; Wang, Y., Computing Science Department, Institute of High Performance Computing, A STAR, Singapore 138632, Singapore; Hsu, D., Department of Computer Science, National University of Singapore, Singapore 117417, Singapore","Current music recommender systems typically act in a greedymanner by recommending songs with the highest user ratings. Greedy recommendation, however, is suboptimal over the long term: it does not actively gather information on user preferences and fails to recommend novel songs that are potentially interesting. A successful recommender system must balance the needs to explore user preferences and to exploit this information for recommendation. This article presents a new approach to music recommendation by formulating this exploration-exploitation trade-off as a reinforcement learning task. To learn user preferences, it uses Bayesian model that accounts for both audio content and the novelty of recommendations. A piecewise-linear approximation to the model and a variational inference algorithm help to speed up Bayesian inference. One additional benefit of our approach is a single unified model for both music recommendation and playlist generation. We demonstrate the strong potential of the proposed approach with simulation results and a user study. © 2014 ACM.",Application; Machine learning; Model; Music; Recommender systems,Article,,Scopus,2-s2.0-84906856108
SCOPUS,"Ponce H., Padilla R.",A hierarchical reinforcement learning based artificial intelligence for non-player characters in video games,2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),8857,,,172,183,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914125261&partnerID=40&md5=52586b31e9733b68ff9d157927322735,"Graduate School of Engineering, Tecnologico de Monterrey, Campus Ciudad de Mexico, Mexico City, Mexico; Universidad Panamericana, Mexico City, Mexico; Department of Computer Science, Tecnologico de Monterrey, Campus Ciudad de Mexico, Mexico City, Mexico","Ponce, H., Graduate School of Engineering, Tecnologico de Monterrey, Campus Ciudad de Mexico, Mexico City, Mexico, Universidad Panamericana, Mexico City, Mexico; Padilla, R., Department of Computer Science, Tecnologico de Monterrey, Campus Ciudad de Mexico, Mexico City, Mexico","Nowadays, video games conforms a huge industry that is always developing new technology. In particular, artificial intelligence techniques have been used broadly in the well-known non-player characters (NPC) given the opportunity to users to feel video games more real. This paper proposes the usage of the MaxQ-Q hierarchical reinforcement learning algorithm in non-player characters in order to increase the experience of the user in terms of naturalness. A case study of an NPC with the proposed artificial intelligence based algorithm in a first personal shooter video game was developed. Experimental results show that this implementation improves naturalness from the user’s point of view. In addition, the proposed MaxQ-Q based algorithm in NPCs allow to programmers a robust way to give artificial intelligence to them. © Springer International Publishing Switzerland 2014.",Hierarchical reinforcement learning; Human assessment; Naturalness; Non-player characters; Video games,Conference Paper,,Scopus,2-s2.0-84914125261
SCOPUS,"Ramacliandran A., Scassellati B.",Adapting difficulty levels in personalized robot-child tutoring interactions,2014,AAAI Workshop - Technical Report,WS-14-07,,,56,59,,3,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974815499&partnerID=40&md5=75adeaedcf63bcb3f1c3d765aa90e4d7,"Yale University, 51 Prospect St., New Haven, CT, United States","Ramacliandran, A., Yale University, 51 Prospect St., New Haven, CT, United States; Scassellati, B., Yale University, 51 Prospect St., New Haven, CT, United States",".Social roliots can be used to tutor children in one-on-one interactions. Because students have different learning needs, they consequenUy require complex, non-scripted teaching behaviors that adapt to the learning needs of each child. As a result of this, robot tutors are more effective given a means of adaptively customizing the pace and content of a student's curriculum. In this paper we propose a reinforcement learning-based approach that affords such capabilities to a tutoring robot, with the goals of fostering measurable learning gains and sustained engagement. We outline an architecture in which the robot uses reinforcement learning to adapt the difficulty of its exercises. Further, we describe a proposed study capable of evaluating the effectiveness of our Intelligent Tutoring System. © Copyright 2014, Association for the Advancement of Artificial Intelligence (www.aaia.org). All rights reserved.",,Conference Paper,,Scopus,2-s2.0-84974815499
SCOPUS,"Bischoff B., Nguyen-Tuong D., Van Hoof H., McHutchon A., Rasmussen C.E., Knoll A., Peters J., Deisenroth M.P.",Policy search for learning robot control using sparse data,2014,Proceedings - IEEE International Conference on Robotics and Automation,,,6907422,3882,3887,,4,10.1109/ICRA.2014.6907422,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929207181&doi=10.1109%2fICRA.2014.6907422&partnerID=40&md5=f8aa6c73923287c95292997791a6b353,"Cognitive Systems, Bosch Corporate Research, Germany; Intelligent Autonomous Systems, TU Darmstadt, Germany; Computational and Biological Learning Lab, Univ. of Cambridge, United Kingdom; Robotics and Embedded Systems, TU Munchen, Germany; Max Planck Institute for Intelligent Systems, Tübingen, Germany; Department of Computing, Imperial College London, United Kingdom","Bischoff, B., Cognitive Systems, Bosch Corporate Research, Germany; Nguyen-Tuong, D., Cognitive Systems, Bosch Corporate Research, Germany; Van Hoof, H., Intelligent Autonomous Systems, TU Darmstadt, Germany; McHutchon, A., Computational and Biological Learning Lab, Univ. of Cambridge, United Kingdom; Rasmussen, C.E., Computational and Biological Learning Lab, Univ. of Cambridge, United Kingdom; Knoll, A., Robotics and Embedded Systems, TU Munchen, Germany; Peters, J., Intelligent Autonomous Systems, TU Darmstadt, Germany, Max Planck Institute for Intelligent Systems, Tübingen, Germany; Deisenroth, M.P., Intelligent Autonomous Systems, TU Darmstadt, Germany, Department of Computing, Imperial College London, United Kingdom","In many complex robot applications, such as grasping and manipulation, it is difficult to program desired task solutions beforehand, as robots are within an uncertain and dynamic environment. In such cases, learning tasks from experience can be a useful alternative. To obtain a sound learning and generalization performance, machine learning, especially, reinforcement learning, usually requires sufficient data. However, in cases where only little data is available for learning, due to system constraints and practical issues, reinforcement learning can act suboptimally. In this paper, we investigate how model-based reinforcement learning, in particular the probabilistic inference for learning control method (Pilco), can be tailored to cope with the case of sparse data to speed up learning. The basic idea is to include further prior knowledge into the learning process. As Pilco is built on the probabilistic Gaussian processes framework, additional system knowledge can be incorporated by defining appropriate prior distributions, e.g. A linear mean Gaussian prior. The resulting Pilco formulation remains in closed form and analytically tractable. The proposed approach is evaluated in simulation as well as on a physical robot, the Festo Robotino XT. For the robot evaluation, we employ the approach for learning an object pick-up task. The results show that by including prior knowledge, policy learning can be sped up in presence of sparse data. © 2014 IEEE.",,Conference Paper,,Scopus,2-s2.0-84929207181
SCOPUS,"Haw R., Alam M.G.R., Hong C.S.",A context-aware content delivery framework for QoS in mobile cloud,2014,APNOMS 2014 - 16th Asia-Pacific Network Operations and Management Symposium,,,6996607,,,,,10.1109/APNOMS.2014.6996607,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941044465&doi=10.1109%2fAPNOMS.2014.6996607&partnerID=40&md5=cceffa9dfa231b42ba00a42e92bbba90,"Department of Computer Engineering, Kyung Hee University, Yong In, South Korea","Haw, R., Department of Computer Engineering, Kyung Hee University, Yong In, South Korea; Alam, M.G.R., Department of Computer Engineering, Kyung Hee University, Yong In, South Korea; Hong, C.S., Department of Computer Engineering, Kyung Hee University, Yong In, South Korea","According to increasing performance of mobile devices, like smart phone, tablet PC and etc, and diffusing network infrastructures, like LTE, WiFi and etc, various types of content delivery services based on PC services can serve into mobile devices using cloud. In this paper we proposed content delivery framework with SDN (Software Defined Networking) and CCN (Content Centric Networking) to improve content delivery QoS in mobile cloud environment. Additionally to serve autonomic optimal services, we proposed reinforcement learning based context-aware content delivery scheme. Using our framework, we can guarantee QoS to provide context-aware content delivery scheme. © 2014 IEEE.",Content centric network; Content delivery; Context-aware; Machine learning; Mobile cloud; Software defined networking,Conference Paper,,Scopus,2-s2.0-84941044465
SCOPUS,"Cao Y., Duan D., Cheng X., Yang L., Wei J.",QoS-oriented wireless routing for smart meter data collection: Stochastic learning on graph,2014,IEEE Transactions on Wireless Communications,13,8,6779690,4470,4482,,7,10.1109/TWC.2014.2314121,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906217327&doi=10.1109%2fTWC.2014.2314121&partnerID=40&md5=eb3849fd5bcd969b3d11a2566a0e0553,"Department of Electronics and Information Engineering, Huazhong University of Science and Technology, Wuhan 430074, China; Department of Electrical and Computer Engineering, University of Wyoming, Laramie, WY 82071, United States; School of Electronics Engineering and Computer Science, Peking University, Beijing 100871, China; Department of Electrical and Computer Engineering, Colorado State University, Fort Collins, CO 80523, United States","Cao, Y., Department of Electronics and Information Engineering, Huazhong University of Science and Technology, Wuhan 430074, China; Duan, D., Department of Electrical and Computer Engineering, University of Wyoming, Laramie, WY 82071, United States; Cheng, X., School of Electronics Engineering and Computer Science, Peking University, Beijing 100871, China; Yang, L., Department of Electrical and Computer Engineering, Colorado State University, Fort Collins, CO 80523, United States; Wei, J., Department of Electronics and Information Engineering, Huazhong University of Science and Technology, Wuhan 430074, China","To ensure resilient and reliable meter data collection that is essential for the smart grid operation, we propose a QoS-oriented wireless routing scheme. Specifically tailored for the heterogeneity of the meter data traffic in the smart grid, we first design a novel utility function that not only jointly accounts for system throughput and transmission latency, but also allows for flexible tradeoff between the two with a strict transmission latency constraint, as desired by various smart meter applications. Then, we model the interactions among smart meter data concentrators as a mixed-strategy network formation game. To avoid potential information exchange which is not always practical in meter data collection scenario, a stochastic reinforcement learning algorithm with only private and incomplete information is proposed to solve the network formation problem. Such a problem formulation, together with our proposed stochastic learning algorithm on graph, results in a steady probabilistic route. Both contributions are novel and unique in comparison with existing work on this topic. Another distinct feature of our approach is its capability of effectively maintaining the QoS of smart meter data collection, even when the network is under fault or attack, as verified by simulations. © 2002-2012 IEEE.",Network formation; Smart grid; Stochastic learning; Wireless routing,Article,,Scopus,2-s2.0-84906217327
SCOPUS,"Da Silva B.C., Konidaris G., Barto A.",Active learning of parameterized skills,2014,"31st International Conference on Machine Learning, ICML 2014",5,,,3736,3745,,4,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919785739&partnerID=40&md5=db1f1a55c3825562fd59e4582f00757c,"School of Computer Science, University of Massachusetts, Amherst, MA, United States; Computer Science and Artificial Intelligence Lab., MIT, Cambridge, MA, United States","Da Silva, B.C., School of Computer Science, University of Massachusetts, Amherst, MA, United States; Konidaris, G., Computer Science and Artificial Intelligence Lab., MIT, Cambridge, MA, United States; Barto, A., School of Computer Science, University of Massachusetts, Amherst, MA, United States","We introduce a method for actively learning parameterized skills. Parameterized skills are flexible behaviors that can solve any task drawn from a distribution of parameterized reinforcement learning problems. Approaches to learning such skills have been proposed, but limited attention has been given to identifying which training tasks allow for rapid skill acquisition. We construct a non-parametric Bayesian model of skill performance and derive analytical expressions for a novel acquisition criterion capable of identifying tasks that maximize expected improvement in skill performance. We also introduce a spatiotemporal kernel tailored for non-stationary skill performance models. The proposed method is agnostic to policy and skill representation and scales independently of task dimensionality. We evaluate it on a non-linear simulated catapult control problem over arbitrarily mountainous terrains. Copyright 2014 by the author(s).",,Conference Paper,,Scopus,2-s2.0-84919785739
SCOPUS,"Claeys M., Latre S., Famaey J., De Turck F.",Design and evaluation of a self-learning http adaptive video streaming client,2014,IEEE Communications Letters,18,4,6746772,716,719,,33,10.1109/LCOMM.2014.020414.132649,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899631071&doi=10.1109%2fLCOMM.2014.020414.132649&partnerID=40&md5=7e3b9235ea91dc3ddfb47d236627dc6c,"Dept. of Information Tech., Ghent Univ., IMinds, Belgium; Dept. of Mathematics and Computer Science, University of Antwerp, IMinds, Belgium","Claeys, M., Dept. of Information Tech., Ghent Univ., IMinds, Belgium; Latre, S., Dept. of Mathematics and Computer Science, University of Antwerp, IMinds, Belgium; Famaey, J., Dept. of Information Tech., Ghent Univ., IMinds, Belgium; De Turck, F., Dept. of Information Tech., Ghent Univ., IMinds, Belgium","HTTP Adaptive Streaming (HAS) is becoming the de facto standard for Over-The-Top (OTT)-based video streaming services such as YouTube and Netflix. By splitting a video into multiple segments of a couple of seconds and encoding each of these at multiple quality levels, HAS allows a video client to dynamically adapt the requested quality during the playout to react to network changes. However, state-of-the-art quality selection heuristics are deterministic and tailored to specific network configurations. Therefore, they are unable to cope with a vast range of highly dynamic network settings. In this letter, a novel Reinforcement Learning (RL)-based HAS client is presented and evaluated. The self-learning HAS client dynamically adapts its behaviour by interacting with the environment to optimize the Quality of Experience (QoE), the quality as perceived by the end-user. The proposed client has been thoroughly evaluated using a network-based simulator and is shown to outperform traditional HAS clients by up to 13% in a mobile network environment. © 2014 IEEE.",intelligent agent; learning systems; quality of service; Streaming media,Article,,Scopus,2-s2.0-84899631071
SCOPUS,[No author name available],"2014 6th International Conference on Computational Aspects of Social Networks, CASoN 2014",2014,"2014 6th International Conference on Computational Aspects of Social Networks, CASoN 2014",,,,,,66,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911881369&partnerID=40&md5=9c305409f41bca49dc4a931c655e78f3,,,The proceedings contain 11 papers. The topics discussed include: modeling dynamics of social networks: a survey; a new protein structure classification model; community detection on heterogeneous networks by multiple semantic-path clustering; group adjacency matrices: effective visualisation of community structure in large networks; direct generation of random graphs exactly realising a prescribed degree sequence; recipe tuning by reinforcement learning in the sands ecosystem; Internet of things communication reference model; discount heuristics and heterogeneous probabilities for optimal influence in social networks; an ANP-based method for modeling centrality in online social networks; maximization of entropy in a two layer asymmetry-coupled network; and hybrid parallel approach for personalized literature recommendation system.,,Conference Review,,Scopus,2-s2.0-84911881369
SCOPUS,"Modayil J., White A., Sutton R.S.",Multi-timescale nexting in a reinforcement learning robot,2014,Adaptive Behavior,22,2,,146,160,,16,10.1177/1059712313511648,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896357393&doi=10.1177%2f1059712313511648&partnerID=40&md5=bb7136a9a1b35419ad7b0a5bd5a80987,"Reinforcement Learning and Artificial Intelligence Laboratory, University of Alberta, Canada","Modayil, J., Reinforcement Learning and Artificial Intelligence Laboratory, University of Alberta, Canada; White, A., Reinforcement Learning and Artificial Intelligence Laboratory, University of Alberta, Canada; Sutton, R.S., Reinforcement Learning and Artificial Intelligence Laboratory, University of Alberta, Canada","The term 'nexting' has been used by psychologists to refer to the propensity of people and many other animals to continually predict what will happen next in an immediate, local, and personal sense. The ability to 'next' constitutes a basic kind of awareness and knowledge of one's environment. In this paper we present results with a robot that learns to next in real time, making thousands of predictions about sensory input signals at timescales from 0.1 to 8 seconds. Our predictions are formulated as a generalization of the value functions commonly used in reinforcement learning, where now an arbitrary function of the sensory input signals is used as a pseudo reward, and the discount rate determines the timescale. We show that six thousand predictions, each computed as a function of six thousand features of the state, can be learned and updated online ten times per second on a laptop computer, using the standard temporal-difference(λ) algorithm with linear function approximation. This approach is sufficiently computationally efficient to be used for real-time learning on the robot and sufficiently data efficient to achieve substantial accuracy within 30 minutes. Moreover, a single tile-coded feature representation suffices to accurately predict many different signals over a significant range of timescales. We also extend nexting beyond simple timescales by letting the discount rate be a function of the state and show that nexting predictions of this more general form can also be learned with substantial accuracy. General nexting provides a simple yet powerful mechanism for a robot to acquire predictive knowledge of the dynamics of its environment. © The Author(s) 2014.",predictive knowledge; Reinforcement learning; robotics; temporal difference learning,Article,,Scopus,2-s2.0-84896357393
SCOPUS,"Cavanagh J.F., Masters S.E., Bath K., Frank M.J.",Conflict acts as an implicit cost in reinforcement learning,2014,Nature Communications,5,,6394,,,,23,10.1038/ncomms6394,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922975531&doi=10.1038%2fncomms6394&partnerID=40&md5=a995090841dde725bac87fa45a86e1e2,"Department of Psychology, University of New Mexico, 1 University of New Mexico, Albuquerque, NM, United States; Department of Cognitive, Linguistic and Psychological Sciences, Brown University, 190 Thayer Street, Providence, RI, United States; Department of Neuroscience, Brown University, Box GL-N, Providence, RI, United States; Department of Psychiatry, Brown University, Box G-A1, Providence, RI, United States; Brown Institute for Brain Science, Brown University, Box 1953 2 Stimson Ave., Providence, RI, United States","Cavanagh, J.F., Department of Psychology, University of New Mexico, 1 University of New Mexico, Albuquerque, NM, United States; Masters, S.E., Department of Cognitive, Linguistic and Psychological Sciences, Brown University, 190 Thayer Street, Providence, RI, United States; Bath, K., Department of Neuroscience, Brown University, Box GL-N, Providence, RI, United States; Frank, M.J., Department of Cognitive, Linguistic and Psychological Sciences, Brown University, 190 Thayer Street, Providence, RI, United States, Department of Psychiatry, Brown University, Box G-A1, Providence, RI, United States, Brown Institute for Brain Science, Brown University, Box 1953 2 Stimson Ave., Providence, RI, United States","Conflict has been proposed to act as a cost in action selection, implying a general function of medio-frontal cortex in the adaptation to aversive events. Here we investigate if response conflict acts as a cost during reinforcement learning by modulating experienced reward values in cortical and striatal systems. Electroencephalography recordings show that conflict diminishes the relationship between reward-related frontal theta power and cue preference yet it enhances the relationship between punishment and cue avoidance. Individual differences in the cost of conflict on reward versus punishment sensitivity are also related to a genetic polymorphism associated with striatal D1 versus D2 pathway balance (DARPP-32). We manipulate these patterns with the D2 agent cabergoline, which induces a strong bias to amplify the aversive value of punishment outcomes following conflict. Collectively, these findings demonstrate that interactive cortico-striatal systems implicitly modulate experienced reward and punishment values as a function of conflict. © 2014 Macmillan Publishers Limited. All rights reserved.",,Article,,Scopus,2-s2.0-84922975531
SCOPUS,[No author name available],"Towards Autonomous Robotic Systems - 14th Annual Conference, TAROS 2013, Revised Selected Papers",2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),8069 LNAI,,,,,469,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904204701&partnerID=40&md5=2d828398b2bac6080d461a97e9ef77d9,,,The proceedings contain 49 papers. The topics discussed include: dynamic power sharing for self-reconfigurable modular robots; heuristically-accelerated reinforcement learning: a comparative analysis of performance; TREEBOT: tree recovering renewable energy robot; a personal robotic flying machine with vertical takeoff controlled by the human body movements; developing the cerebellar chip as a general control module for autonomous systems; UAV horizon tracking using memristors and cellular automata visual processing; vision-based cooperative localization for small networked robot teams; design of a modular knee-ankle-foot-orthosis using soft actuator for gait rehabilitation; evaluation of laser range-finder mapping for agricultural spraying vehicles; control-oriented nonlinear dynamic modelling of dielectric electro-active polymers; emotionally driven robot control architecture for human-robot interaction; and realtime simulation-in-the-loop control for agile ground vehicles.,,Conference Review,,Scopus,2-s2.0-84904204701
SCOPUS,"Oswald S., Kretzschmar H., Burgard W., Stachniss C.",Learning to give route directions from human demonstrations,2014,Proceedings - IEEE International Conference on Robotics and Automation,,,6907334,3303,3308,,4,10.1109/ICRA.2014.6907334,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929223719&doi=10.1109%2fICRA.2014.6907334&partnerID=40&md5=57f88279f8012446c263a065049955e6,"Department of Computer Science, University of Freiburg, Freiburg, Germany; University of Bonn, Institute of Geodesy and Geoinformation, Bonn, Germany","Oswald, S., Department of Computer Science, University of Freiburg, Freiburg, Germany; Kretzschmar, H., Department of Computer Science, University of Freiburg, Freiburg, Germany; Burgard, W., Department of Computer Science, University of Freiburg, Freiburg, Germany; Stachniss, C., Department of Computer Science, University of Freiburg, Freiburg, Germany, University of Bonn, Institute of Geodesy and Geoinformation, Bonn, Germany","For several applications, robots and other computer systems must provide route descriptions to humans. These descriptions should be natural and intuitive for the human users. In this paper, we present an algorithm that learns how to provide good route descriptions from a corpus of human-written directions. Using inverse reinforcement learning, our algorithm learns how to select the information for the description depending on the context of the route segment. The algorithm then uses the learned policy to generate directions that imitate the style of the descriptions provided by humans, thus taking into account personal as well as cultural preferences and special requirements of the particular user group providing the learning demonstrations. We evaluate our approach in a user study and show that the directions generated by our policy sound similar to human-given directions and substantially more natural than directions provided by commercial web services. © 2014 IEEE.",,Conference Paper,,Scopus,2-s2.0-84929223719
SCOPUS,"Miyazaki K., Takeno J.",The necessity of a secondary system in machine consciousness,2014,Energy Procedia,41,C,,15,22,,,10.1016/j.procs.2014.11.079,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922387968&doi=10.1016%2fj.procs.2014.11.079&partnerID=40&md5=58d9bd5452f514d7b024ff7cde3157d0,"National Institution for Academic degrees, University Evaluation, Kodaira, Tokyo, Japan; Meiji University, Kawasaki, Kanagawa, Japan","Miyazaki, K., National Institution for Academic degrees, University Evaluation, Kodaira, Tokyo, Japan; Takeno, J., Meiji University, Kawasaki, Kanagawa, Japan","Our research purpose is to realize a consciousness system on computers. In this paper, we focus on the relation between a primary system that directly interacts and learns the input-output relation within an environment, and a secondary system that continuously monitors and is able to direct the primary system. We hold that consciousness is not composed of a primary system alone, but that the employment of a secondary system is essential. The purpose of this paper is therefore to clarify the importance of a secondary system. We show by numerical experiments that the addition of a secondary system provides adaptability to a wider range of environmental changes than a primary system alone. Alternatively, we present an extraordinary case where a customized primary system fully adapts to an environment undergoing a particular type of change. Far from refuting the value of a secondary system, this special case serves to point out the importance of the effective design of a secondary system. Therefore, we confirm the value and usefulness of a secondary system in a machine consciousness system through these numerical experiments. © 2014 The Authors. Published by Elsevier B.V.",Consciousness system; Exploitation-oriented learning (XoL); Primary system; Reinforcement learning; Secondary system,Conference Paper,Open Access,Scopus,2-s2.0-84922387968
SCOPUS,"Miyazaki K., Takeno J.",The necessity of a secondary system in machine consciousness,2014,Procedia Computer Science,41,,,15,22,,3,10.1016/j.procs.2014.11.079,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939220757&doi=10.1016%2fj.procs.2014.11.079&partnerID=40&md5=3ce53d88cb483fa5999f27e16b540688,"National Institution for Academic Degrees, University Evaluation, Kodaira, Tokyo, Japan; Meiji University, Kawasaki, Kanagawa, Japan","Miyazaki, K., National Institution for Academic Degrees, University Evaluation, Kodaira, Tokyo, Japan; Takeno, J., Meiji University, Kawasaki, Kanagawa, Japan","Our research purpose is to realize a consciousness system on computers. In this paper, we focus on the relation between a primary system that directly interacts and learns the input-output relation within an environment, and a secondary system that continuously monitors and is able to direct the primary system. We hold that consciousness is not composed of a primary system alone, but that the employment of a secondary system is essential. The purpose of this paper is therefore to clarify the importance of a secondary system. We show by numerical experiments that the addition of a secondary system provides adaptability to a wider range of environmental changes than a primary system alone. Alternatively, we present an extraordinary case where a customized primary system fully adapts to an environment undergoing a particular type of change. Far from refuting the value of a secondary system, this special case serves to point out the importance of the effective design of a secondary system. Therefore, we confirm the value and usefulness of a secondary system in a machine consciousness system through these numerical experiments. © The Authors. Published by Elsevier B.V.",Consciousness system; Exploitation-oriented learning (XoL); Primary system; Reinforcement learning; Secondary system,Conference Paper,Open Access,Scopus,2-s2.0-84939220757
SCOPUS,"Collins A.G.E., Cavanagh J.F., Frank M.J.",Human EEG uncovers latent generalizable rule structure during learning,2014,Journal of Neuroscience,34,13,,4677,4685,,21,10.1523/JNEUROSCI.3900-13.2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897881519&doi=10.1523%2fJNEUROSCI.3900-13.2014&partnerID=40&md5=f000c5ff6973ec9892b793b53bf60707,"Cognitive, Linguistic, and Psychological Sciences, Brown University, Providence, RI 02912, United States; Department of Psychology, University of New Mexico, Albuquerque, NM 87131, United States","Collins, A.G.E., Cognitive, Linguistic, and Psychological Sciences, Brown University, Providence, RI 02912, United States; Cavanagh, J.F., Cognitive, Linguistic, and Psychological Sciences, Brown University, Providence, RI 02912, United States, Department of Psychology, University of New Mexico, Albuquerque, NM 87131, United States; Frank, M.J., Cognitive, Linguistic, and Psychological Sciences, Brown University, Providence, RI 02912, United States","Human cognition is flexible and adaptive, affording the ability to detect and leverage complex structure inherent in the environment and generalize this structure to novel situations. Behavioral studies show that humans impute structure into simple learning problems, even when this tendency affords no behavioral advantage. Here we used electroencephalography to investigate the neural dynamics indicative of such incidental latent structure. Event-related potentials over lateral prefrontal cortex, typically observed for instructed task rules, were stratified according to individual participants' constructed rule sets. Moreover, this individualized latent rule structure could be independently decoded from multielectrode pattern classification. Both neural markers were predictive of participants' ability to subsequently generalize rule structure to new contexts. These EEG dynamics reveal that the human brain spontaneously constructs hierarchically structured representations during learning of simple task rules. © 2014 the authors.",EEG; Prefrontal cortex; Reinforcement learning; Rules; Task-set,Article,,Scopus,2-s2.0-84897881519
SCOPUS,"Chakraborty B., Murphy S.A.",Dynamic treatment regimes,2014,Annual Review of Statistics and Its Application,1,,,447,464,,24,10.1146/annurev-statistics-022513-115553,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906077445&doi=10.1146%2fannurev-statistics-022513-115553&partnerID=40&md5=ce47ee4c06d6e2fbd95907910f5528cc,"Duke-NUS Graduate Medical School, National University of Singapore, Singapore, Singapore; Department of Statistics and Institute for Social Research, University of Michigan, Ann Arbor, MI 48109, United States","Chakraborty, B., Duke-NUS Graduate Medical School, National University of Singapore, Singapore, Singapore; Murphy, S.A., Department of Statistics and Institute for Social Research, University of Michigan, Ann Arbor, MI 48109, United States","A dynamic treatment regime consists of a sequence of decision rules, one per stage of intervention, that dictate how to individualize treatments to patients, based on evolving treatment and covariate history. These regimes are particularly useful for managing chronic disorDers and fit well into the larger paradigm of personalized medicine. They provide one way to operationalize a clinical decision support system. Statistics plays a key role in the construction of evidence-based dynamic treatment regimes-informing the best study design as well as efficient estimation and valid inference. Owing to the many novel methodological challenges this area offers, it has been growing in popularity among statisticians in recent years. In this article, we review the key developments in this exciting field of research. In particular, we discuss the sequential multiple assignment randomized trial designs, estimation techniques like Q-learning and marginal structural models, and several inference techniques designed to address the associated nonstandard asymptotics. We reference software whenever available. We also outline some important future directions. © 2014 by Annual Reviews.",Dynamic treatment regime; Nonregularity; Q-learning; Reinforcement learning; Sequential randomization,Article,,Scopus,2-s2.0-84906077445
SCOPUS,Orseau L.,Teleporting universal intelligent agents,2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),8598 LNAI,,,109,120,,2,10.1007/978-3-319-09274-4_11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905814917&doi=10.1007%2f978-3-319-09274-4_11&partnerID=40&md5=790e058f08f7a6b8603c7a63a7b9255e,"AgroParisTech, UMR 518 MIA, F-75005 Paris, France; INRA, UMR 518 MIA, F-75005 Paris, France","Orseau, L., AgroParisTech, UMR 518 MIA, F-75005 Paris, France, INRA, UMR 518 MIA, F-75005 Paris, France","When advanced AIs begin to choose their own destiny, one decision they will need to make is whether or not to transfer or copy themselves (software and memory) to new hardware devices. For humans this possibility is not (yet) available and so it is not obvious how such a question should be approached. Furthermore, the traditional single-agent reinforcement-learning framework is not adequate for exploring such questions, and so we base our analysis on the ""multi-slot"" framework introduced in a companion paper. In the present paper we attempt to understand what an AI with unlimited computational capacity might choose if presented with the option to transfer or copy itself to another machine. We consider two rigorously executed formal thought experiments deeply related to issues of personal identity: one where the agent must choose whether to be copied into a second location (called a""slot""), and another where the agent must make this choice when, after both copies exist, one of them will be deleted. These decisions depend on what the agents believe their futures will be, which in turn depends on the definition of their value function, and we provide formal results. © 2014 Springer International Publishing.",AIXI; identity; teleportation; Universal AI,Conference Paper,,Scopus,2-s2.0-84905814917
SCOPUS,"Strauss G.P., Waltz J.A., Gold J.M.",A review of reward processing and motivational impairment in schizophrenia,2014,Schizophrenia Bulletin,40,SUPPL. 2,,S107,S116,,109,10.1093/schbul/sbt197,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895746086&doi=10.1093%2fschbul%2fsbt197&partnerID=40&md5=f5b2699983cc16f3d2cd92e69f3d0f38,"Department of Psychology, State University of New York at Binghamton, PO Box 6000, Binghamton, NY 13902, United States; Department of Psychiatry, University of Maryland School of Medicine, Maryland Psychiatric Research Center, Baltimore, MD, United States","Strauss, G.P., Department of Psychology, State University of New York at Binghamton, PO Box 6000, Binghamton, NY 13902, United States; Waltz, J.A., Department of Psychiatry, University of Maryland School of Medicine, Maryland Psychiatric Research Center, Baltimore, MD, United States; Gold, J.M., Department of Psychiatry, University of Maryland School of Medicine, Maryland Psychiatric Research Center, Baltimore, MD, United States","This article reviews and synthesizes research on reward processing in schizophrenia, which has begun to provide important insights into the cognitive and neural mechanisms associated with motivational impairments. Aberrant cortical-striatal interactions may be involved with multiple reward processing abnormalities, including: (1) dopamine-mediated basal ganglia systems that support reinforcement learning and the ability to predict cues that lead to rewarding outcomes; (2) orbitofrontal cortex-driven deficits in generating, updating, and maintaining value representations; (3) aberrant effort-value computations, which may be mediated by disrupted anterior cingulate cortex and midbrain dopamine functioning; and (4) altered activation of the prefrontal cortex, which is important for generating exploratory behaviors in environments where reward outcomes are uncertain. It will be important for psychosocial interventions targeting negative symptoms to account for abnormalities in each of these reward processes, which may also have important interactions; suggestions for novel behavioral intervention strategies that make use of external cues, reinforcers, and mobile technology are discussed. © 2013 © The Author 2014. Published by Oxford University Press on behalf of the Maryland Psychiatric Research Center. All rights reserved.",anhedonia; avolition; motivation; negative symptoms; psychosis; reward,Review,,Scopus,2-s2.0-84895746086
SCOPUS,[No author name available],Recent Advances of Neural Network Models and Applications-Proceedings of the 23rd Workshop of the Italian Neural Networks Society (SIREN),2014,"Smart Innovation, Systems and Technologies",26,,,,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901302688&partnerID=40&md5=8901494971b0aa606d0466bebff42026,,,The proceedings contain 44 papers. The special focus in this conference is on Neural Networks. The topics include: Identifying emergent dynamical structures in network models; experimental guidelines for semantic-based regularization; a preliminary study on transductive extreme learning machines; avoiding the cluster hypothesis in SV classification of partially labeled data; learning capabilities of ELM-trained time-varying neural networks; a quality-driven ensemble approach to automatic model selection in clustering; an adaptive reference point approach to efficiently search large chemical databases; genetic art in perspective; proportionate algorithms for blind source separation; pupillometric study of the dysregulation of the autonomous nervous system by SVM networks; a memristor circuit using basic elements with memory capability; effects of pruning on phase-coding and storage capacity of a spiking network; predictive analysis of the seismicity level at campi flegrei volcano using a data-driven approach; robot localization by echo state networks using RSS; smart home task and energy resource scheduling based on nonlinear programming; data fusion using a factor graph for ship tracking in harbour scenarios; reinforcement learning for automated financial trading; a collaborative filtering recommender exploiting a SOM network; SVM tree for personalized transductive learning in bioinformatics classification problems; multi-country mortality analysis using self organizing maps; a fuzzy decision support system for the environmental risk assessment of genetically modified organisms; recent approaches in handwriting recognition with Markovian modelling and recurrent neural networks; the effects of hand gestures on psychosocial perception; the influence of positive and negative emotions on physiological responses and memory task scores; mood effects on the decoding of emotional voices; the ascending reticular activating system; conversational entrainment in the use of discourse markers; language and gender effect in decoding emotional information; preliminary experiments on automatic gender recognition based on online capital letters; end-user design of emotion-adaptive dialogue strategies for therapeutic purposes; modulation of cognitive goals and sensorimotor actions in face-to-face communication by emotional states; intended and unintended offence; conceptual spaces for emotion identification and alignment; emotions and moral judgment; contextual information and reappraisal of negative emotional events and deciding with (or without) the future in mind.,,Conference Review,,Scopus,2-s2.0-84901302688
SCOPUS,"Tang L., Rosales R., Singh A.P., Agarwal D.",Automatic ad format selection via contextual bandits,2013,"International Conference on Information and Knowledge Management, Proceedings",,,,1587,1594,,20,10.1145/2505515.2514700,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889567137&doi=10.1145%2f2505515.2514700&partnerID=40&md5=652c81fbee7adda7e08bcedb57239955,"School of Computer Science, Florida International Univ., 11200 S.W. 8th St., Miami, FL 33199, United States; Applied Relevance Science LinkedIn, 2029 Stierlin Ct., Mountain View, CA 94043, United States","Tang, L., School of Computer Science, Florida International Univ., 11200 S.W. 8th St., Miami, FL 33199, United States; Rosales, R., Applied Relevance Science LinkedIn, 2029 Stierlin Ct., Mountain View, CA 94043, United States; Singh, A.P., Applied Relevance Science LinkedIn, 2029 Stierlin Ct., Mountain View, CA 94043, United States; Agarwal, D., School of Computer Science, Florida International Univ., 11200 S.W. 8th St., Miami, FL 33199, United States","Visual design plays an important role in online display advertising: changing the layout of an online ad can increase or decrease its effectiveness, measured in terms of click-through rate (CTR) or total revenue. The decision of which layout to use for an ad involves a trade-off: using a layout provides feedback about its effectiveness (exploration), but collecting that feedback requires sacrificing the immediate reward of using a layout we already know is effective (exploitation). To balance exploration with exploitation, we pose automatic layout selection as a contextual bandit problem. There are many bandit algorithms, each generating a policy which must be evaluated. It is impractical to test each policy on live traffic. However, we have found that offline replay (a.k.a. exploration scavenging) can be adapted to provide an accurate estimator for the performance of ad layout policies at Linkedin, using only historical data about the effectiveness of layouts. We describe the development of our offline replayer, and benchmark a number of common bandit algorithms. Copyright 2013 ACM.",Bandit algorithms; Exploration/exploitation; Layout; Offline evaluation; Online advertising; Personalization; Recommender systems,Conference Paper,,Scopus,2-s2.0-84889567137
SCOPUS,"Lowery C., Faisal A.A.","Towards efficient, personalized anesthesia using continuous reinforcement learning for propofol infusion control",2013,"International IEEE/EMBS Conference on Neural Engineering, NER",,,6696208,1414,1417,,1,10.1109/NER.2013.6696208,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897712602&doi=10.1109%2fNER.2013.6696208&partnerID=40&md5=e9f5249dff389fdcf0b8c036cbdd1c44,"Department of Computing, Imperial College London, South Kensington Campus, London SW7 2AZ, United Kingdom; Department of Bioengineering, Imperial College London, South Kensington Campus, London SW7 2AZ, United Kingdom; MRC Clinical Sciences Centre, Hammersmith Hospital Campus, W12 0NN London, United Kingdom","Lowery, C., Department of Computing, Imperial College London, South Kensington Campus, London SW7 2AZ, United Kingdom; Faisal, A.A., Department of Computing, Imperial College London, South Kensington Campus, London SW7 2AZ, United Kingdom, Department of Bioengineering, Imperial College London, South Kensington Campus, London SW7 2AZ, United Kingdom, MRC Clinical Sciences Centre, Hammersmith Hospital Campus, W12 0NN London, United Kingdom","We demonstrate the use of reinforcement learning algorithms for efficient and personalized control of patients' depth of general anesthesia during surgical procedures - an important aspect for Neurotechnology. We used the continuous actor-critic learning automaton technique, which was trained and tested in silico using published patient data, physiological simulation and the bispectral index (BIS) of patient EEG. Our two-stage technique learns first a generic effective control strategy based on average patient data (factory stage) and can then fine-tune itself to individual patients (personalization stage). The results showed that the reinforcement learner as compared to a bang-bang controller reduced the dose of the anesthetic agent administered by 9.4% and kept the patient closer to the target state, as measured by RMSE (4.90 compared to 8.47). It also kept the BIS error within a narrow, clinically acceptable range 93.9% of the time. Moreover, the policy was trained using only 50 simulated operations. Being able to learn a control strategy this quickly indicates that the reinforcement learner could also adapt regularly to a patient's changing responses throughout a live operation and facilitate the task of anesthesiologists by prompting them with recommended actions. © 2013 IEEE.",,Conference Paper,,Scopus,2-s2.0-84897712602
SCOPUS,"Nguyen K.-H., Ock C.-Y.",Word sense disambiguation as a traveling salesman problem,2013,Artificial Intelligence Review,40,4,,405,427,,12,10.1007/s10462-011-9288-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890425797&doi=10.1007%2fs10462-011-9288-9&partnerID=40&md5=fae48f20c14c6054532d0e5c6fa0d6a0,"School of Electrical Engineering, University of Ulsan, 93 Daehakro, Nam-gu, Ulsan 680-749, South Korea","Nguyen, K.-H., School of Electrical Engineering, University of Ulsan, 93 Daehakro, Nam-gu, Ulsan 680-749, South Korea; Ock, C.-Y., School of Electrical Engineering, University of Ulsan, 93 Daehakro, Nam-gu, Ulsan 680-749, South Korea","Word sense disambiguation (WSD) is a difficult problem in Computational Linguistics, mostly because of the use of a fixed sense inventory and the deep level of granularity. This paper formulates WSD as a variant of the traveling salesman problem (TSP) to maximize the overall semantic relatedness of the context to be disambiguated. Ant colony optimization, a robust nature-inspired algorithm, was used in a reinforcement learning manner to solve the formulated TSP. We propose a novel measure based on the Lesk algorithm and Vector Space Model to calculate semantic relatedness. Our approach to WSD is comparable to state-of-the-art knowledge-based and unsupervised methods for benchmark datasets. In addition, we show that the combination of knowledge-based methods is superior to the most frequent sense heuristic and significantly reduces the difference between knowledge-based and supervised methods. The proposed approach could be customized for other lexical disambiguation tasks, such as Lexical Substitution or Word Domain Disambiguation. © 2011 Springer Science+Business Media B.V.",Ant colony optimization; Lesk algorithm; Semantic relatedness; Traveling salesman problem; Word sense disambiguation; WordNet,Article,,Scopus,2-s2.0-84890425797
SCOPUS,"Zhao Y., Zhao Q., Xia L., Cheng Z., Wang F., Song F.",A unified control framework of HVAC system for thermal and acoustic comforts in office building,2013,IEEE International Conference on Automation Science and Engineering,,,6653964,416,421,,3,10.1109/CoASE.2013.6653964,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891521654&doi=10.1109%2fCoASE.2013.6653964&partnerID=40&md5=941f3729bf858945af53cb29b15fb4df,"Center for Intelligent and Networked System, Department of Automation and TNList, Tsinghua University, Beijing, 100084, China; Department of Building Science, Tsinghua University, Beijing, 100084, China; United Technologies Research Center (China) Ltd., Shanghai, 200120, China","Zhao, Y., Center for Intelligent and Networked System, Department of Automation and TNList, Tsinghua University, Beijing, 100084, China; Zhao, Q., Center for Intelligent and Networked System, Department of Automation and TNList, Tsinghua University, Beijing, 100084, China; Xia, L., Center for Intelligent and Networked System, Department of Automation and TNList, Tsinghua University, Beijing, 100084, China; Cheng, Z., Center for Intelligent and Networked System, Department of Automation and TNList, Tsinghua University, Beijing, 100084, China; Wang, F., Department of Building Science, Tsinghua University, Beijing, 100084, China; Song, F., United Technologies Research Center (China) Ltd., Shanghai, 200120, China","Intelligent building system attracts more and more attention in both academic and industrial communities. Learning human comfort requirements and incorporating it into building control system is one of the important issues. In the traditional HVAC control system, the thermal comfort and the acoustic comfort are often conflicted and we lack of a scheme to trade off them well. In this paper, we propose a unified control framework based on reinforcement learning to balance the multiple dimension comforts, including the thermal and acoustic comforts. We utilize the user's complaints in thermal and acoustic sensations as feedback and combine the current environment and devices information to learn the personalized optimal control policy using online Q-learning. The challenge caused by the complaints is coped with an incorporated perception estimation scheme in the Q-learning reward design. Both simulation results and the field experimental results demonstrate the effectiveness of the algorithm, especially in the adaptivity to the individual tradeoff between thermal and acoustic comfort. © 2013 IEEE.",,Conference Paper,,Scopus,2-s2.0-84891521654
SCOPUS,[No author name available],"Service-Oriented Computing - 11th International Conference, ICSOC 2013, Proceedings",2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),8274 LNCS,,,,,710,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892385032&partnerID=40&md5=6f27803dd9df2357023b897819139a31,,,"The proceedings contain 65 papers. The topics discussed include: QoS-aware cloud service composition using time series; runtime enforcement of first-order LTL properties on data-aware business processes; personalized quality prediction for dynamic service management based on invocation patterns; open source versus proprietary software in service-orientation: the case of BPEL engines; on-the-fly adaptation of dynamic service-based systems: incrementality, reduction and reuse; critical path-based iterative heuristic for workflow scheduling in utility and cloud computing; task scheduling optimization in cloud computing applying multi-objective particle swarm optimization; automatically composing services by mining process knowledge from the web; multi-objective service composition using reinforcement learning; service discovery from observed behavior while guaranteeing deadlock freedom in collaborations; and priority-based human resource allocation in business processes.",,Conference Review,,Scopus,2-s2.0-84892385032
SCOPUS,"D'Ardenne K., Lohrenz T., Bartley K.A., Montague P.R.",Computational heterogeneity in the human mesencephalic dopamine system,2013,"Cognitive, Affective and Behavioral Neuroscience",13,4,,747,756,,18,10.3758/s13415-013-0191-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891421186&doi=10.3758%2fs13415-013-0191-5&partnerID=40&md5=88015b9b776289751a9ec8a31ec8fe8f,"Virginia Tech Carilion Research Institute, Roanoke, VA, United States; Pediatric Emergency Medicine, Baylor College of Medicine, Houston, TX, United States; Department of Physics, Virginia Tech, Blacksburg, VA, United States; Wellcome Centre for Neuroimaging, University College London, 12 Queen Square, London WC1N 3BG, United Kingdom; Human Neuroimaging Laboratory, Virginia Tech Carilion Research Institute, 2 Riverside Circle, Roanoke, VA 24016, United States","D'Ardenne, K., Virginia Tech Carilion Research Institute, Roanoke, VA, United States; Lohrenz, T., Virginia Tech Carilion Research Institute, Roanoke, VA, United States; Bartley, K.A., Pediatric Emergency Medicine, Baylor College of Medicine, Houston, TX, United States; Montague, P.R., Virginia Tech Carilion Research Institute, Roanoke, VA, United States, Department of Physics, Virginia Tech, Blacksburg, VA, United States, Wellcome Centre for Neuroimaging, University College London, 12 Queen Square, London WC1N 3BG, United Kingdom, Human Neuroimaging Laboratory, Virginia Tech Carilion Research Institute, 2 Riverside Circle, Roanoke, VA 24016, United States","Recent evidence in animals has indicated that the mesencephalic dopamine system is heterogeneous anatomically, molecularly, and functionally, and it has been suggested that the dopamine system comprises distinct functional systems. Identifying and characterizing these systems in humans will have widespread ramifications for understanding drug addiction and mental health disorders. Model-based studies in humans have suggested an analogous computational heterogeneity, in which dopaminergic targets in striatum encode both experience-based learning signals and counterfactual learning signals that are based on hypothetical information. We used brainstem-tailored fMRI to identify mesencephalic sources of experiential and counterfactual learning signals. Participants completed a decision-making task based on investing in markets. This sequential investment task generated experience-based learning signals, in the form of temporal difference (TD) reward prediction errors, and counterfactual learning signals, in the form of ""fictive errors."" Fictive errors are reinforcement learning signals based on hypothetical information about ""what could have been."" An additional learning signal was constructed to be relatable to a motivational salience signal. Blood oxygenation level dependent responses in regions of substantia nigra (SN) and ventral tegmental area (VTA), where dopamine neurons are located, coded for TD and fictive errors, and additionally were related to the motivational salience signal these results are highly consistent with animal electrophysiology and provide direct evidence that human SN and VTA heterogeneously handle important reward-harvesting computations. © 2013 The Author(s).",Brainstem fMRI; Decision making; Dopamine; Reinforcement learning; Reward; SN; VTA,Article,,Scopus,2-s2.0-84891421186
SCOPUS,"Karatzoglou A., Baltrunas L., Shi Y.",Learning to rank for recommender systems,2013,RecSys 2013 - Proceedings of the 7th ACM Conference on Recommender Systems,,,,493,494,,18,10.1145/2507157.2508063,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887600751&doi=10.1145%2f2507157.2508063&partnerID=40&md5=6fb877b463ecc336e9a285a543b4ae18,"Telefonica Research, Spain; Delft University of Technology, Netherlands","Karatzoglou, A., Telefonica Research, Spain; Baltrunas, L., Telefonica Research, Spain; Shi, Y., Delft University of Technology, Netherlands","Recommender system aim at providing a personalized list of items ranked according to the preferences of the user, as such ranking methods are at the core of many recommendation algorithms. The topic of this tutorial focuses on the cutting-edge algorithmic development in the area of recommender systems. This tutorial will provide an in depth picture of the progress of ranking models in the field, summarizing the strengths and weaknesses of existing methods, and discussing open issues that could be promising for future research in the community. A qualitative and quantitative comparison between different models will be provided while we will also highlight recent developments in the areas of Reinforcement Learning. © 2013 ACM.",Collaborative filtering; Learning to rank; Ranking; Recommender systems,Conference Paper,,Scopus,2-s2.0-84887600751
SCOPUS,"Ghahfarokhi B.S., Movahhedinia N.",A personalized QoE-aware handover decision based on distributed reinforcement learning,2013,Wireless Networks,19,8,,1807,1828,,5,10.1007/s11276-013-0572-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886583003&doi=10.1007%2fs11276-013-0572-2&partnerID=40&md5=163b8778ea5599a29885ace91718c438,"Department of Information Technology Engineering, University of Isfahan, Isfahan, Iran; Department of Computer Engineering, University of Isfahan, Isfahan, Iran","Ghahfarokhi, B.S., Department of Information Technology Engineering, University of Isfahan, Isfahan, Iran; Movahhedinia, N., Department of Computer Engineering, University of Isfahan, Isfahan, Iran","Recent developments in heterogeneous mobile networks and growing demands for variety of real-time and multimedia applications have emphasized the necessity of more intelligent handover decisions. Addressing the context knowledge of mobile devices, users, applications, and networks is the subject of context-aware handoff decision as a recent effort to this aim. However, user perception has not been attended adequately in the area of context-aware handover decision making. Mobile users may have different judgments about the Quality of Service (QoS) depending on their environmental conditions, and personal and psychological characteristics. This reality has been exploited in this paper to introduce a personalized user-centric handoff decision method to decide about the time and target of handover based on User Perceived Quality (UPQ) feedbacks. The UPQ degradations are mainly for the sake of (1) exiting the coverage of the serving Point of Attachment (PoA) or (2) QoS degradation of serving access network. Using UPQ metric, the proposed method obviates the necessity of being aware about rapidly varying network QoS parameters and overcomes the complexity and overhead of gathering and managing some other context information. Moreover, considering the underlying network and geographical map, the proposed method is able to inherently exploit the trajectory information of mobile users for handover decision. UPQ degradation is not only due to the user behaviour, but also due to the behaviours of others users. As such, multi-agent reinforcement learning paradigm has been considered for target PoA selection. The employed decision algorithm is based on WoLF-PHC learning method where UPQ is used as a delayed reward for training. The proposed handoff decision has been implemented under IEEE 802.21 framework using NS2 network simulator. The results have shown better performance of the proposed method comparing to conventional methods assuming regular movement of mobile users. © 2013 Springer Science+Business Media New York.",Context-aware handover; Distributed reinforcement learning; QoE-aware handover; User perceived quality,Article,,Scopus,2-s2.0-84886583003
SCOPUS,"Daskalaki E., Diem P., Mougiakakou S.G.",Personalized tuning of a reinforcement learning control algorithm for glucose regulation,2013,"Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS",,,6610293,3487,3490,,4,10.1109/EMBC.2013.6610293,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886538622&doi=10.1109%2fEMBC.2013.6610293&partnerID=40&md5=d501ab8fe0a01632497d70664dff169c,"Diabetes Technology Research Group, ARTORG Center for Biomedical Engineering Research, University of Bern, Bern 3010, Switzerland; Division of Endocrinology, Diabetes and Clinical Nutrition, Bern University Hospital Inselspital, Bern 3010, Switzerland","Daskalaki, E., Diabetes Technology Research Group, ARTORG Center for Biomedical Engineering Research, University of Bern, Bern 3010, Switzerland; Diem, P., Division of Endocrinology, Diabetes and Clinical Nutrition, Bern University Hospital Inselspital, Bern 3010, Switzerland; Mougiakakou, S.G., Diabetes Technology Research Group, ARTORG Center for Biomedical Engineering Research, University of Bern, Bern 3010, Switzerland","Artificial pancreas is in the forefront of research towards the automatic insulin infusion for patients with type 1 diabetes. Due to the high inter- and intra-variability of the diabetic population, the need for personalized approaches has been raised. This study presents an adaptive, patient-specific control strategy for glucose regulation based on reinforcement learning and more specifically on the Actor-Critic (AC) learning approach. The control algorithm provides daily updates of the basal rate and insulin-to-carbohydrate (IC) ratio in order to optimize glucose regulation. A method for the automatic and personalized initialization of the control algorithm is designed based on the estimation of the transfer entropy (TE) between insulin and glucose signals. The algorithm has been evaluated in silico in adults, adolescents and children for 10 days. Three scenarios of initialization to i) zero values, ii) random values and iii) TE-based values have been comparatively assessed. The results have shown that when the TE-based initialization is used, the algorithm achieves faster learning with 98%, 90% and 73% in the A+B zones of the Control Variability Grid Analysis for adults, adolescents and children respectively after five days compared to 95%, 78%, 41% for random initialization and 93%, 88%, 41% for zero initial values. Furthermore, in the case of children, the daily Low Blood Glucose Index reduces much faster when the TE-based tuning is applied. The results imply that automatic and personalized tuning based on TE reduces the learning period and improves the overall performance of the AC algorithm. © 2013 IEEE.",,Conference Paper,,Scopus,2-s2.0-84886538622
SCOPUS,"Ni Y., Zheng M., Bu J., Chen C., Wang D.",Personalized automatic image annotation based on reinforcement learning,2013,Proceedings - IEEE International Conference on Multimedia and Expo,,,6607456,,,,4,10.1109/ICME.2013.6607456,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885602903&doi=10.1109%2fICME.2013.6607456&partnerID=40&md5=3fd5114380b73bab1526d44f45c9ccf0,"Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science, Zhejiang University, Hangzhou 310027, China","Ni, Y., Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science, Zhejiang University, Hangzhou 310027, China; Zheng, M., Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science, Zhejiang University, Hangzhou 310027, China; Bu, J., Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science, Zhejiang University, Hangzhou 310027, China; Chen, C., Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science, Zhejiang University, Hangzhou 310027, China; Wang, D., Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science, Zhejiang University, Hangzhou 310027, China","With the rapidly increasing number of personal image collections on the web, it is of great importance to annotate these user-uploaded images in personalized manner. But personalized image annotation is largely ignored by the mainstream of image annotation research. In this paper, we focus on personalizing the automatic image annotation by proposing a general framework which jointly exploits the generic content-based image annotation, personal image tagging history and the content of personal history images. In our framework, two sets of candidate annotations are extracted for each image based on content-based annotation and personal image tagging history. Considering that the user's interest may not stay the same, when exploiting the personal image tagging history, we also take the content of personal history images into account to avoid the noise. To get the final annotations, we propose an unsupervised algorithm based on reinforcement learning to combine the above two candidate annotation sets. Encouraging results show that the proposed framework is effective and promising for personalizing automatic image annotation. © 2013 IEEE.",Automatic image annotation; Personal image tagging history; Personalization; Reinforcement learning,Conference Paper,,Scopus,2-s2.0-84885602903
SCOPUS,"Su P.-H., Wang Y.-B., Yu T.-H., Lee L.-S.",A dialogue game framework with personalized training using reinforcement learning for computer-assisted language learning,2013,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",,,6639266,8213,8217,,8,10.1109/ICASSP.2013.6639266,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890503056&doi=10.1109%2fICASSP.2013.6639266&partnerID=40&md5=ec6fce989ffa933613fb5f86db49f18c,"Graduate Institute of Communication Engineering, National Taiwan University, Taiwan; Graduate Institute of Electrical Engineering, National Taiwan University, Taiwan","Su, P.-H., Graduate Institute of Communication Engineering, National Taiwan University, Taiwan; Wang, Y.-B., Graduate Institute of Electrical Engineering, National Taiwan University, Taiwan; Yu, T.-H., Graduate Institute of Communication Engineering, National Taiwan University, Taiwan; Lee, L.-S., Graduate Institute of Communication Engineering, National Taiwan University, Taiwan, Graduate Institute of Electrical Engineering, National Taiwan University, Taiwan","We propose a framework for computer-assisted language learning as a pedagogical dialogue game. The goal is to offer personalized learning sentences on-line for each individual learner considering the learner's learning status, in order to strike a balance between more practice on poorly-pronounced units and complete practice on the whole set of pronunciation units. This objective is achieved using a Markov decision process (MDP) trained with reinforcement learning using simulated learners generated from real learner data. Preliminary experimental results on a subset of the example dialogue script show the effectiveness of the framework. © 2013 IEEE.",Computer-Assisted Language Learning; Dialogue Game; Markov Decision Process; Reinforcement Learning,Conference Paper,,Scopus,2-s2.0-84890503056
SCOPUS,"Zhang Z.C., Li S., Hu K.S., Huang H.Y., Zhao S.Y.",Markov decision process based adaptive web advertisements scheduling,2013,Advanced Materials Research,765-767,,,1436,1440,,,10.4028/www.scientific.net/AMR.765-767.1436,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885049231&doi=10.4028%2fwww.scientific.net%2fAMR.765-767.1436&partnerID=40&md5=8ff8e90a548dd3e734a60cb6c1df0296,"Department of Industrial Engineering, Dongguan University of Technology, Dongguan 523808, China","Zhang, Z.C., Department of Industrial Engineering, Dongguan University of Technology, Dongguan 523808, China; Li, S., Department of Industrial Engineering, Dongguan University of Technology, Dongguan 523808, China; Hu, K.S., Department of Industrial Engineering, Dongguan University of Technology, Dongguan 523808, China; Huang, H.Y., Department of Industrial Engineering, Dongguan University of Technology, Dongguan 523808, China; Zhao, S.Y., Department of Industrial Engineering, Dongguan University of Technology, Dongguan 523808, China","We study web advertisements scheduling problem by fully considering the interaction of web users and web advertisements publishing system. We construct a Markov Decision Process (MDP) based web advertisements scheduling model and schedule advertisements publishing during the whole process of web surfing by the users, thus we make maximal use of personal behavior characteristics of every web user in the scheduling model. We also track the user habit with reinforcement learning, solve the MDP model by TD(λ) algorithm combing the function approximator, and obtain adaptive online scheduling policies for web advertisements publishing. © (2013) Trans Tech Publications, Switzerland.",Reinforcement learning; Scheduling; Web advertisements,Conference Paper,,Scopus,2-s2.0-84885049231
SCOPUS,Lightle J.P.,Harmful lie aversion and lie discovery in noisy expert advice games,2013,Journal of Economic Behavior and Organization,93,,,347,362,,6,10.1016/j.jebo.2013.04.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884411295&doi=10.1016%2fj.jebo.2013.04.006&partnerID=40&md5=a97f7b89c26a02b4a84445edbf419758,"Florida State University, Department of Economics, l 113 Collegiate Loop, Tallahassee, FL 32306, United States","Lightle, J.P., Florida State University, Department of Economics, l 113 Collegiate Loop, Tallahassee, FL 32306, United States","This study tests whether individuals are reluctant to tell lies, or perhaps only ""harmful lies"", in a previously untested environment: an expert sending a message to a decision maker whose interpretation of that message is subject to error, i.e. a noisy sender-receiver game. In the Aligned treatment, the expert can send a ""white lie"" to the receiver, eliminating the negative effects of noise and improving both parties' payoffs. In the Conflict treatment, lies are harmful and the inability to commit to truthtelling destroys all meaningful communication in equilibrium unless there is a cost of lying. In the experiment, receivers are overly trusting and experts learn to take advantage of this. As experts gain experience they tell stronger and more frequent lies in both treatments, consistent with models of reinforcement learning. The findings suggest that neither harmful nor universal lie aversion is a factor when communication is noisy, provided individuals have time to discover their personal benefits of lies. © 2013 Elsevier B.V.",Cheap talk; Communication; Experts; Lie aversion; Overcommunication,Article,,Scopus,2-s2.0-84884411295
SCOPUS,"Pandey N., Tyagi R.K., Sahu S., Dwivedi A.",Learning algorithms for intelligent agents based e-learning system,2013,"Proceedings of the 2013 3rd IEEE International Advance Computing Conference, IACC 2013",,,6514369,1034,1039,,6,10.1109/IAdCC.2013.6514369,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879852018&doi=10.1109%2fIAdCC.2013.6514369&partnerID=40&md5=c05a1beba7383a9c174b8c72a8cb8cf1,"Department of Computer Science and Engineering, Ajay Kumar Garg Engineering College, Ghaziabad, India; Department of Information Technology, Krishna Institute of Engineering and Technology, Ghaziabad, India; Department of MCA, Raj Kumar Goel Engineering College, Ghaziabad, India","Pandey, N., Department of Computer Science and Engineering, Ajay Kumar Garg Engineering College, Ghaziabad, India; Tyagi, R.K., Department of Information Technology, Krishna Institute of Engineering and Technology, Ghaziabad, India; Sahu, S., Department of Computer Science and Engineering, Ajay Kumar Garg Engineering College, Ghaziabad, India; Dwivedi, A., Department of MCA, Raj Kumar Goel Engineering College, Ghaziabad, India","Requirements are the basic entity which makes the project to be successfully implemented. In the development of the software, theses requirements must be measured accurately and correctly. The requirements of the end users may be changed with respect to different individual personality. As different requirements are given by different customers, all of them cannot be processed by single software system. In such a case, it is essential to make the changes in the software systems automatically which fulfills all the requirements of the user. Such condition is met by the systems with intelligent agents. In this paper, algorithms of intelligent agents adviser agent, personalization agent and content managing agent are proposed to automatically gather the requirements of the students and fulfill them. The algorithms are structured using reinforcement learning. Theses algorithm makes the agents to sense the needs of the students and evolve in the course of their operation. These intelligent agents are applied after the deployment of the e-learning software. All the algorithms have been implemented successfully. © 2013 IEEE.",E-Learning; Intelligent Agent; Reinforcement Learnin; Requirement Engineering,Conference Paper,,Scopus,2-s2.0-84879852018
SCOPUS,"Papangelis A., Karkaletsis V., Huang H.",Towards adaptive dialogue systems for assistive living environments,2013,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,29,32,,1,10.1145/2451176.2451185,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875830088&doi=10.1145%2f2451176.2451185&partnerID=40&md5=53894c815dba91d26bada3c5954a70c6,"Deptartment of Computer Science and Engineering, University of Texas, Arlington, United States; Institute of Informatics and Telecommunications, National Center for Scientific Research Demokritos, Greece","Papangelis, A., Deptartment of Computer Science and Engineering, University of Texas, Arlington, United States; Karkaletsis, V., Institute of Informatics and Telecommunications, National Center for Scientific Research Demokritos, Greece; Huang, H., Deptartment of Computer Science and Engineering, University of Texas, Arlington, United States","Adaptive Dialogue Systems can be seen as smart interfaces that typically use natural language (spoken or written) as a means of communication. They are being used in many applications, such as customer service, in-car interfaces, even in rehabilitation, and therefore it is essential that these systems are robust, scalable and quickly adaptable in order to cope with changing user or system needs or environmental conditions. Making Dialogue Systems adaptive means overcoming several challenges, such as scalability or lack of training data. Achieving adaptation online has thus been an even greater challenge. We propose to build such a system, that will operate in an Assistive Living Environment and provide its services as a coach to patients that need to perform rehabilitative exercises. We are currently in the process of developing it, using Robot Operating System on a robotic platform.",Adaptive dialogue systems; Dialogue management; Personalization; Reinforcement learning,Conference Paper,,Scopus,2-s2.0-84875830088
SCOPUS,"Chatzidimitriou K.C., Mitkas P.A.",Adaptive reservoir computing through evolution and learning,2013,Neurocomputing,103,,,198,209,,5,10.1016/j.neucom.2012.09.022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870392112&doi=10.1016%2fj.neucom.2012.09.022&partnerID=40&md5=3754cbca67fac087b95af3b50d04c59b,"Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki 54124, Greece","Chatzidimitriou, K.C., Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki 54124, Greece; Mitkas, P.A., Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki 54124, Greece","The development of real-world, fully autonomous agents would require mechanisms that would offer generalization capabilities from experience, suitable for a large range of machine learning tasks, like those from the areas of supervised and reinforcement learning. Such capacities could be offered by parametric function approximators that could either model the environment or the agent's policy. To promote autonomy, these structures should be adapted to the problem at hand with no or little human expert input. Towards this goal, we propose an adaptive function approximator method for developing appropriate neural networks in the form of reservoir computing systems through evolution and learning. Our neuro-evolution of augmenting reservoirs approach comprises of several ideas, successful on their own, in an effort to develop an algorithm that could handle a large range of problems, more efficiently. In particular, we use the neuro-evolution of augmented topologies algorithm as a meta-search method for the adaptation of echo state networks for handling problems to be encountered by autonomous entities. We test our approach on several test-beds from the realms of time series prediction and reinforcement learning. We compare our methodology against similar state-of-the-art algorithms with promising results. © 2012 Elsevier B.V.",Echo state networks; Evolutionary computation; Neuroevolution; Reinforcement learning; Reservoir computing,Article,,Scopus,2-s2.0-84870392112
SCOPUS,"Zhao Y., Zeng D.",Recent development on statistical methods for personalized medicine discovery,2013,Frontiers of Medicine in China,7,1,,102,110,,10,10.1007/s11684-013-0245-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874810512&doi=10.1007%2fs11684-013-0245-7&partnerID=40&md5=57299256c0f308af89fd3cc6054d33dd,"Department of Biostatistics and Medical Informatics, University of Wisconsin-Madison, 600 Highland Ave., Madison, WI 53792, United States; Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States","Zhao, Y., Department of Biostatistics and Medical Informatics, University of Wisconsin-Madison, 600 Highland Ave., Madison, WI 53792, United States; Zeng, D., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States","It is well documented that patients can show significant heterogeneous responses to treatments so the best treatment strategies may require adaptation over individuals and time. Recently, a number of new statistical methods have been developed to tackle the important problem of estimating personalized treatment rules using single-stage or multiple-stage clinical data. In this paper, we provide an overview of these methods and list a number of challenges. © 2013 Higher Education Press and Springer-Verlag Berlin Heidelberg.",dynamic treatment regimes; personalized medicine; Q-learning; reinforcement learning,Review,,Scopus,2-s2.0-84874810512
SCOPUS,"Daskalaki E., Diem P., Mougiakakou S.G.",An Actor-Critic based controller for glucose regulation in type 1 diabetes,2013,Computer Methods and Programs in Biomedicine,109,2,,116,125,,6,10.1016/j.cmpb.2012.03.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873458229&doi=10.1016%2fj.cmpb.2012.03.002&partnerID=40&md5=9fd626af8395921d3a16e711438dc11d,"ARTORG Center for Biomedical Engineering Research, Diabetes Technology Research Group, University of Bern, Murtenstrasse 50, 3010 Bern, Switzerland; Division of Endocrinology, Diabetes and Clinical Nutrition, University Hospital, Inselspital, University of Bern, 3010 Bern, Switzerland","Daskalaki, E., ARTORG Center for Biomedical Engineering Research, Diabetes Technology Research Group, University of Bern, Murtenstrasse 50, 3010 Bern, Switzerland; Diem, P., Division of Endocrinology, Diabetes and Clinical Nutrition, University Hospital, Inselspital, University of Bern, 3010 Bern, Switzerland; Mougiakakou, S.G., ARTORG Center for Biomedical Engineering Research, Diabetes Technology Research Group, University of Bern, Murtenstrasse 50, 3010 Bern, Switzerland","A novel adaptive approach for glucose control in individuals with type 1 diabetes under sensor-augmented pump therapy is proposed. The controller, is based on Actor-Critic (AC) learning and is inspired by the principles of reinforcement learning and optimal control theory. The main characteristics of the proposed controller are (i) simultaneous adjustment of both the insulin basal rate and the bolus dose, (ii) initialization based on clinical procedures, and (iii) real-time personalization. The effectiveness of the proposed algorithm in terms of glycemic control has been investigated in silico in adults, adolescents and children under open-loop and closed-loop approaches, using announced meals with uncertainties in the order of ±25% in the estimation of carbohydrates.The results show that glucose regulation is efficient in all three groups of patients, even with uncertainties in the level of carbohydrates in the meal. The percentages in the A. +. B zones of the Control Variability Grid Analysis (CVGA) were 100% for adults, and 93% for both adolescents and children.The AC based controller seems to be a promising approach for the automatic adjustment of insulin infusion in order to improve glycemic control. After optimization of the algorithm, the controller will be tested in a clinical trial. © 2012 Elsevier Ireland Ltd.",Actor-Critic; Closed-loop control; Glucose control; Reinforcement learning,Article,,Scopus,2-s2.0-84873458229
SCOPUS,"Cohen Y., Schneidman E.",High-order feature-based mixture models of classification learning predict individual learning curves and enable personalized teaching,2013,Proceedings of the National Academy of Sciences of the United States of America,110,2,,684,689,,4,10.1073/pnas.1211606110,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872186192&doi=10.1073%2fpnas.1211606110&partnerID=40&md5=6691bc98916c6238b952fd11589c72bd,"Department of Neurobiology, Weizmann Institute of Science, Rehovot 76100, Israel","Cohen, Y., Department of Neurobiology, Weizmann Institute of Science, Rehovot 76100, Israel; Schneidman, E., Department of Neurobiology, Weizmann Institute of Science, Rehovot 76100, Israel","Pattern classification learning tasks are commonly used to explore learning strategies in human subjects. The universal and individual traits of learning such tasks reflect our cognitive abilities and have been of interest both psychophysically and clinically. From a computational perspective, these tasks are hard, because the number of patterns and rules one could consider even in simple cases is exponentially large. Thus, when we learn to classify we must use simplifying assumptions and generalize. Studies of human behavior in probabilistic learning tasks have focused on rules in which pattern cues are independent, and also described individual behavior in terms of simple, single-cue, feature-based models. Here, we conducted psychophysical experiments in which people learned to classify binary sequences according to deterministic rules of different complexity, including high-order, multicue-dependent rules. We show that human performance on such tasks is very diverse, but that a class of reinforcement learning-like models that use a mixture of features captures individual learning behavior surprisinglywell. Thesemodels reflect the important role of subjects' priors, and their reliance on high-order features even when learning a low-order rule. Further, we show that these models predict future individual answers to a high degree of accuracy. We then use these models to build personally optimized teaching sessions and boost learning.",Inference; Information; Maximum entropy,Article,,Scopus,2-s2.0-84872186192
SCOPUS,"Xie N., Hachiya H., Sugiyama M.",Artist agent: A reinforcement learning approach to automatic stroke generation in oriental ink painting,2013,IEICE Transactions on Information and Systems,E96-D,5,,1134,1144,,2,10.1587/transinf.E96.D.1134,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878232055&doi=10.1587%2ftransinf.E96.D.1134&partnerID=40&md5=42a89e33d745f1d465ad15ab23290171,"Tokyo Institute of Technology, Tokyo, 152-8550, Japan","Xie, N., Tokyo Institute of Technology, Tokyo, 152-8550, Japan; Hachiya, H., Tokyo Institute of Technology, Tokyo, 152-8550, Japan; Sugiyama, M., Tokyo Institute of Technology, Tokyo, 152-8550, Japan","Oriental ink painting, called Sumi-e, is one of the most distinctive painting styles and has attracted artists around the world. Major challenges in Sumi-e simulation are to abstract complex scene information and reproduce smooth and natural brush strokes. To automatically generate such strokes, we propose to model the brush as a reinforcement learning agent, and let the agent learn the desired brush-trajectories by maximizing the sum of rewards in the policy search framework. To achieve better performance, we provide elaborate design of actions, states, and rewards specifically tailored for a Sumi-e agent. The effectiveness of our proposed approach is demonstrated through experiments on Sumi-e simulation. Copyright © 2013 The Institute of Electronics, Information and Communication Engineers.",Painterly rendering; Policy gradient; Reinforcement learning; Stroke-based rendering,Article,,Scopus,2-s2.0-84878232055
SCOPUS,"Bothe M.K., Dickens L., Reichel K., Tellmann A., Ellger B., Westphal M., Faisal A.A.",The use of reinforcement learning algorithms to meet the challenges of an artificial pancreas.,2013,Expert review of medical devices,10,5,,661,673,,9,10.1586/17434440.2013.827515,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901870499&doi=10.1586%2f17434440.2013.827515&partnerID=40&md5=5e57f84d94e8248985013780a9d9b267,"Fresenius Kabi Deutschland GmbH, Else-Kröner-Strasse 1, 61352 Bad Homburg, Germany.","Bothe, M.K., Fresenius Kabi Deutschland GmbH, Else-Kröner-Strasse 1, 61352 Bad Homburg, Germany.; Dickens, L.; Reichel, K.; Tellmann, A.; Ellger, B.; Westphal, M.; Faisal, A.A.","Blood glucose control, for example, in diabetes mellitus or severe illness, requires strict adherence to a protocol of food, insulin administration and exercise personalized to each patient. An artificial pancreas for automated treatment could boost quality of glucose control and patients' independence. The components required for an artificial pancreas are: i) continuous glucose monitoring (CGM), ii) smart controllers and iii) insulin pumps delivering the optimal amount of insulin. In recent years, medical devices for CGM and insulin administration have undergone rapid progression and are now commercially available. Yet, clinically available devices still require regular patients' or caregivers' attention as they operate in open-loop control with frequent user intervention. Dosage-calculating algorithms are currently being studied in intensive care patients [1] , for short overnight control to supplement conventional insulin delivery [2] , and for short periods where patients rest and follow a prescribed food regime [3] . Fully automated algorithms that can respond to the varying activity levels seen in outpatients, with unpredictable and unreported food intake, and which provide the necessary personalized control for individuals is currently beyond the state-of-the-art. Here, we review and discuss reinforcement learning algorithms, controlling insulin in a closed-loop to provide individual insulin dosing regimens that are reactive to the immediate needs of the patient.",,Review,,Scopus,2-s2.0-84901870499
SCOPUS,"Ávila-Sansores S., Orihuela-Espina F., Enrique-Sucar L.",Patient tailored virtual rehabilitation,2013,Biosystems and Biorobotics,1,,,879,883,,4,10.1007/978-3-642-34546-3_143,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014689131&doi=10.1007%2f978-3-642-34546-3_143&partnerID=40&md5=4843998cb02fe4a40767841dcace8bed,"Optics and Electronics, National Institute for Astrophysics, Puebla, Mexico","Ávila-Sansores, S., Optics and Electronics, National Institute for Astrophysics, Puebla, Mexico; Orihuela-Espina, F., Optics and Electronics, National Institute for Astrophysics, Puebla, Mexico; Enrique-Sucar, L., Optics and Electronics, National Institute for Astrophysics, Puebla, Mexico","Virtual rehabilitation should be adaptable to the patient need and progress. To do so, patient in-game performace and ability are monitored to maintain an adequate level of challenge. A novel adaptation strategy is proposed by which patient control and speed are dynamically interrogated to adjust the game difficulty. The strategy is based on a Markov decision process seeding a therapist-guided reinforcement learning algorithm. The optimal learning scheme for the algorithm is established (α = 0.5). Convergence to an optimal therapeutic plan is demonstrated for patients with non-deterministic behaviour. The proposed adaptation algorithm can enhance existing virtual reality-based motor rehabilitation platforms by tailoring the games response to the patient changing needs. © 2013, Springer-Verlag Berlin Heidelberg.",,Book Chapter,,Scopus,2-s2.0-85014689131
SCOPUS,[No author name available],"Intelligent Narrative Technologies - Papers from the 2013 AIIDE Workshop, Technical Report",2013,AAAI Workshop - Technical Report,WS-13-21,,,,,109,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898882064&partnerID=40&md5=a8bf5409d693649835c82bb55998dbae,,,"The proceedings contain 20 papers. The topics discussed include: improving goal recognition in interactive narratives with models of narrative discovery events; an authoring tool to derive valid interactive scenarios; evaluating the use of action templates to create suspense in narrative planning; identifying personal narratives in Chinese weblog posts; generating believable stories in large domains; integrating formal qualitative analysis techniques within a procedural narrative generation system; a conceptual blending approach to the generation of cognitive scripts for interactive narrative; evaluation, orientation, and action in interactive storytelling; a modular reinforcement learning framework for interactive narrative planning; PAStE: a platform for adaptive storytelling with events; and using expressive language generation to increase authorial leverage.",,Conference Review,,Scopus,2-s2.0-84898882064
SCOPUS,"Parkinson J., Haggard P.",Hedonic value of intentional action provides reinforcement for voluntary generation but not voluntary inhibition of action,2013,Consciousness and Cognition,22,4,,1253,1261,,2,10.1016/j.concog.2013.08.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883693383&doi=10.1016%2fj.concog.2013.08.009&partnerID=40&md5=e2b3ce30686d657af601b4e4bfbcc088,"Institute of Cognitive Neuroscience, University College London, London, United Kingdom","Parkinson, J., Institute of Cognitive Neuroscience, University College London, London, United Kingdom; Haggard, P., Institute of Cognitive Neuroscience, University College London, London, United Kingdom","Intentional inhibition refers to stopping oneself from performing an action at the last moment, a vital component of self-control. It has been suggested that intentional inhibition is associated with negative hedonic value, perhaps due to the frustration of cancelling an intended action. Here we investigate hedonic implications of the free choice to act or inhibit. Participants gave aesthetic ratings of arbitrary visual stimuli that immediately followed voluntary decisions to act or to inhibit action. We found that participants for whom decisions to act produced a strong positive hedonic value for the immediately following visual stimulus made more choices to act than those with weaker hedonic value for action. This finding is consistent with reinforcement learning of action decisions. However, participants who experienced inhibition as generating more positive hedonic value did not choose to inhibit more than other participants. Thus, voluntary inhibition of action did not act as reinforcement for future inhibitory behaviour. Our finding that inhibition of action lacks motivational capacity may explain why self-control is both difficult and limited. © 2013.",Action; Decision making; Free will; Hedonic value; Inhibition; Intention; Reinforcement; Reward; Volition,Article,Open Access,Scopus,2-s2.0-84883693383
SCOPUS,"Monfardini E., Gaveau V., Boussaoud D., Hadj-Bouziane F., Meunier M.",Social learning as a way to overcome choice-induced preferences? Insights from humans and rhesus macaques,2012,Frontiers in Neuroscience,,SEP, Article 127,,,,10,10.3389/fnins.2012.00127,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870896027&doi=10.3389%2ffnins.2012.00127&partnerID=40&md5=c877f35a31016240c353c28d7736ba31,"INSERM, U1028, Impact Team, Lyon Neuroscience Research Center, Lyon, France; CNRS, UMR5292, Impact Team, Lyon Neuroscience Research Center, Lyon, France; University Lyon, Lyon, France; Institut de Médecine Environnementale, Paris, France; Institut de Neuroscience des Systèmes, UMR 1106, INSERM, Aix-Marseille Université, Marseille, France","Monfardini, E., INSERM, U1028, Impact Team, Lyon Neuroscience Research Center, Lyon, France, CNRS, UMR5292, Impact Team, Lyon Neuroscience Research Center, Lyon, France, University Lyon, Lyon, France, Institut de Médecine Environnementale, Paris, France; Gaveau, V., INSERM, U1028, Impact Team, Lyon Neuroscience Research Center, Lyon, France, CNRS, UMR5292, Impact Team, Lyon Neuroscience Research Center, Lyon, France, University Lyon, Lyon, France; Boussaoud, D., Institut de Neuroscience des Systèmes, UMR 1106, INSERM, Aix-Marseille Université, Marseille, France; Hadj-Bouziane, F., INSERM, U1028, Impact Team, Lyon Neuroscience Research Center, Lyon, France, CNRS, UMR5292, Impact Team, Lyon Neuroscience Research Center, Lyon, France, University Lyon, Lyon, France; Meunier, M., INSERM, U1028, Impact Team, Lyon Neuroscience Research Center, Lyon, France, CNRS, UMR5292, Impact Team, Lyon Neuroscience Research Center, Lyon, France, University Lyon, Lyon, France","Much theoretical attention is currently devoted to social learning. Yet, empirical studies formally comparing its effectiveness relative to individual learning are rare. Here, we focus on free choice, which is at the heart of individual reward-based learning, but absent in social learning. Choosing among two equally valued options is known to create a preference for the selected option in both humans and monkeys.We thus surmised that social learning should be more helpful when choice-induced preferences retard individual learning than when they optimize it.To test this prediction, the same task requiring to find which among two items concealed a reward was applied to rhesus macaques and humans. The initial trial was individual or social, rewarded or unrewarded. Learning was assessed on the second trial. Choice-induced preference strongly affected individual learning. Monkeys and humans performed much more poorly after an initial negative choice than after an initial positive choice. Comparison with social learning verified our prediction. For negative outcome, social learning surpassed or at least equaled individual learning in all subjects. For positive outcome, the predicted superiority of individual learning did occur in a majority of subjects (5/6 monkeys and 6/12 humans). A minority kept learning better socially though, perhaps due to a more dominant/aggressive attitude toward peers. Poor learning from errors due to over-valuation of personal choices is among the decision-making biases shared by humans and animals. The present study suggests that choice-immune social learning may help curbing this potentially harmful tendency. Learning from successes is an easier path. The present data suggest that whether one tends to walk it alone or with a peer's help might depend on the social dynamics within the actor/observer dyad. © 2012 Monfardini, Gaveau, Boussaoud, Hadj-Bouziane and Meunier.",Choice-induced preference; Cognitive biases; Humans; Reinforcement learning; Rhesus macaques; Social learning,Article,,Scopus,2-s2.0-84870896027
SCOPUS,"Wang L., Gao Y., Cao C., Wang Li.",Towards a general supporting framework for self-adaptive software systems,2012,Proceedings - International Computer Software and Applications Conference,,,6341568,158,163,,3,10.1109/COMPSACW.2012.38,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870801132&doi=10.1109%2fCOMPSACW.2012.38&partnerID=40&md5=036aaf69b1a7c0f6c77995f79a58b336,"State Key Laboratory for Novel Software Technology, Nanjing University, 210046 Nanjing, China","Wang, L., State Key Laboratory for Novel Software Technology, Nanjing University, 210046 Nanjing, China; Gao, Y., State Key Laboratory for Novel Software Technology, Nanjing University, 210046 Nanjing, China; Cao, C., State Key Laboratory for Novel Software Technology, Nanjing University, 210046 Nanjing, China; Wang, Li., State Key Laboratory for Novel Software Technology, Nanjing University, 210046 Nanjing, China","When confronted with their internal and environmental dynamics, various software systems increasingly require self-adaptation capabilities. The vision above requires the self-adaptation approach to tackle these challenges and some software systems have their own specific implementations. However, in this paper a general supporting framework is proposed for software systems to self-adapt with running environmental dynamics and meanwhile fulfill various user requirements. Three key issues are covered in the framework: 1) the overall control architecture, which adopts the double closed-loop style and respectively includes the self-adaptation loop and the self-learning loop; 2) a general descriptive language, which is a application-independent and unified language to represent self-adaptation knowledge about target systems; 3) three implementation mechanisms, including forward reasoning, planning and reinforcement learning using feedback, which are supported by the above descriptive language and executed at runtime in different modules. Finally, one scenario of ondemand services of massive data mining tasks is selected and the case study demonstrates how the framework is customized as required and how the approach works. © 2012 IEEE.",Double Closed-loop Control Architecture; General Descriptive Language; Hierarchical Task Network; Rete Algorithm; Self- Adaptive Software Systems; Self-Adaptive Supporting Framework,Conference Paper,,Scopus,2-s2.0-84870801132
SCOPUS,"Daltayanni M., Wang C., Akella R.",A fast interactive search system for healthcare services,2012,"Annual SRII Global Conference, SRII",,,6311035,525,534,,3,10.1109/SRII.2012.65,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870849937&doi=10.1109%2fSRII.2012.65&partnerID=40&md5=789da4298c90d43dd9b79126c649886b,"Technology and Information Management, University of California, Santa Cruz, United States","Daltayanni, M., Technology and Information Management, University of California, Santa Cruz, United States; Wang, C., Technology and Information Management, University of California, Santa Cruz, United States; Akella, R., Technology and Information Management, University of California, Santa Cruz, United States","In this paper we describe the design, development, and evaluation of a general human-machine interaction search system, and its potential and use in the context of a collaboration project with SAP and Saffron. The objective of a specialized version of the system is to provide medical and healthcare information services to users via interactive search for personalized patient needs. Patients usually have questions regarding healthcare, including those which concern illness symptoms, duration and types of treatment, possible drug effects, and more. Authorized personnel would often be ideal in responding to such needs; however they could potentially be very expensive, and not easy to support and maintain. If patients could have access to information at their home, by means of i-phone or online access, this could save time, doctor office visit expenses, as well as valuable and restricted medical time. What is more, information concerning other anonymized and similar patient cases provides knowledge and perspective on a wide range of patient issues. From the doctors' perspective, they typically need to spend time on differential analysis about new patient cases: study symptoms, research possible causes, rank results by emergency priority and treat them accordingly. A search system that would direct a doctor (or patient/user) to similar patient cases would save significant amount of manual search time. The powerful new feature of this system is the storage and mining of past patient cases knowledge, to create metadata to be used in the subsequent retrieval of relevant documents. Finally, the interactive search system would speed up identification of rare cases; for instance, symptoms that do not appear commonly in past cases may require special treatment or expert referral. We build a model which dynamically learns medical needs of interacting MDs and patients. The model works on free or unstructured text, allowing disambiguation of vague words and flexibility in describing medical needs. In addition, both experts with an advanced knowledge of medical terminology, and beginning users using basic medical terms, can achieve high search relevance. Furthermore, our approach obviates the need for the assignment of tags or labels, such as treatment, symptoms, causes, to documents, to respond effectively to user queries. In particular, we build a temporal difference algorithm to predict user's information needs by incorporating both current and predicted knowledge into learning the user profile. Our source of information about the user consists of submitted queries and feedback on the returned results. We tested our system on publicly available medical data (OhsuMed TREC dataset 2002) and we achieved a significant improvement in retrieval accuracy, compared to the literature. We provide quantitative results as well as demonstration screenshots which illustrate a) the value of interaction (user time spent with system versus results accuracy), b) the value of using medical terminology understanding, when compared with simple general words, and c) the value of allowing the maximum number of feedback submissions to vary. © 2012 IEEE.",Healthcare information services; Interactive retrieval; Medical data retrieval; Reinforcement learning; Temporal difference,Conference Paper,,Scopus,2-s2.0-84870849937
SCOPUS,"Sharma V.K., Shukla S.S.P., Singh V.",A tailored Q- Learning for routing in wireless sensor networks,2012,"Proceedings of 2012 2nd IEEE International Conference on Parallel, Distributed and Grid Computing, PDGC 2012",,,6449899,663,668,,4,10.1109/PDGC.2012.6449899,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874448922&doi=10.1109%2fPDGC.2012.6449899&partnerID=40&md5=b247adc66f9cb5fc92fd5da379ae5d84,"Department of Computer Science and Engineering, Jaypee Polytechnic and Training Centre, Rewa, M.P, India; Department of Computer Science and Engineering Line, Rewa Institute Technology, Rewa, M.P, India","Sharma, V.K., Department of Computer Science and Engineering, Jaypee Polytechnic and Training Centre, Rewa, M.P, India; Shukla, S.S.P., Department of Computer Science and Engineering, Jaypee Polytechnic and Training Centre, Rewa, M.P, India; Singh, V., Department of Computer Science and Engineering Line, Rewa Institute Technology, Rewa, M.P, India",Wireless sensor networks (WSNs) have major importance in distributed sensing applications. The important concern in the intend of wireless sensor networks is battery consumption which usually rely on non-renewable sources of energy. In this paper we have proposed a tailored Q-Learning algorithm for routing scheme in wireless sensor network. Our primary goal is to make an efficient routing algorithm with help of modified Q-Learning approach to minimize the energy consumption utilized by sensor nodes. This approach is a modified version of existing Q-Learning method for WSN that leads to the convergence problem. © 2012 IEEE.,Convergence Problem; Q-Learning; Reinforcement learning; WSN Flooding Routing Protocol,Conference Paper,,Scopus,2-s2.0-84874448922
SCOPUS,"Li H., Lau J., Alonso R.",Discovering virtual interest groups across chat rooms,2012,KMIS 2012 - Proceedings of the International Conference on Knowledge Management and Information Sharing,,,,152,157,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881594185&partnerID=40&md5=c6e60adc01232386122bd4ff27e82e5f,"SAIC, Inc., 1710 SAIC Drive, McLean, VA 22102, United States","Li, H., SAIC, Inc., 1710 SAIC Drive, McLean, VA 22102, United States; Lau, J., SAIC, Inc., 1710 SAIC Drive, McLean, VA 22102, United States; Alonso, R., SAIC, Inc., 1710 SAIC Drive, McLean, VA 22102, United States","Chat has becoming an increasingly popular communication tool in our everyday life. When the number of related concurrent chat rooms gets large, tracking them 24x7 becomes very difficult. To address this research problem, we have developed VIGIR (Virtual Interest Group & Information Recommender), a tool for automatic chat room monitoring. The tool builds adaptive interest models for chat users, which are used to provide a number of personalized services including finding virtual interest groups (VIGs) for chat users. Dynamic identification of the VIG addresses the distributed user collaboration challenge, which is acute problem especially in military operations. VIGIR extends our prior work in user interest modeling into the domain of real-time text-based communications. We have evaluated the effectiveness of VIGIR in two studies. The first is a user-centred evaluation where we have achieved a precision at 60% and recall at 80% for VIG identification. In the second study using military chat data, we have demonstrated an average precision of 45% to 50%. In addition, we have shown that the precision for predicting VIG increases over time as more data become available.",Chat; IRC; Machine learning; Reinforcement learning; User modeling; Virtual interest group; XMPP,Conference Paper,,Scopus,2-s2.0-84881594185
SCOPUS,"Moling O., Baltrunas L., Ricci F.",Optimal radio channel recommendations with explicit and implicit feedback,2012,RecSys'12 - Proceedings of the 6th ACM Conference on Recommender Systems,,,,75,82,,6,10.1145/2365952.2365971,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867388659&doi=10.1145%2f2365952.2365971&partnerID=40&md5=5a2334482844dda8d0e8d5a453984015,"Free University of Bozen-Bolzano, Piazza Domenicani 3, Bolzano, Italy; Telefonica Research, Plaza se E. Lluchi Martin 5, Barcelona, Spain","Moling, O., Free University of Bozen-Bolzano, Piazza Domenicani 3, Bolzano, Italy; Baltrunas, L., Telefonica Research, Plaza se E. Lluchi Martin 5, Barcelona, Spain; Ricci, F., Free University of Bozen-Bolzano, Piazza Domenicani 3, Bolzano, Italy","The very large majority of recommender systems are running as server-side applications, and they are controlled by the content provider, i.e., who provides the recommended items. This paper focuses on a different scenario: the user is supposed to be able to access content from multiple providers, in our application they offer radio channels, and it is up to a personal recommender installed on the clients' side to decide which channel to select and recommend to the user. We exploit the implicit feedback derived from the user's listening behavior, and we model channel recommendation as a sequential decision making problem. We have implemented a personal RS that integrates reinforcement learning techniques to decide what channel to play every time the user asks for a new music track or the current track finishes playing. In a live user study we show that the proposed system can sequentially select the next channel to play such that the users listen to the streamed tracks for a larger fraction, and for more time, compared to a baseline system not exploiting implicit feedback. Copyright © 2012 by the Association for Computing Machinery, Inc. (ACM).",Implicit feedback; Reinforcement learning; Sequential music recommendations,Conference Paper,,Scopus,2-s2.0-84867388659
SCOPUS,"Yue Y., Hong S.A., Guestrin C.",Hierarchical exploration for accelerating contextual bandits,2012,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",2,,,1895,1902,,8,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867112205&partnerID=40&md5=4dc1f9081d737b65dfff6e12543f0bba,"iLab, H. John Heinz III College, Carnegie Mellon University, Pittsburgh, PA 15213, United States; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States","Yue, Y., iLab, H. John Heinz III College, Carnegie Mellon University, Pittsburgh, PA 15213, United States; Hong, S.A., School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States; Guestrin, C., School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States","Contextual bandit learning is an increasingly popular approach to optimizing recommender systems via user feedback, but can be slow to converge in practice due to the need for exploring a large feature space. In this paper, we propose a coarse-to-fine hierarchical approach for encoding prior knowledge that drastically reduces the amount of exploration required. Intuitively, user preferences can be reasonably embedded in a coarse low-dimensional feature space that can be explored efficiently, requiring exploration in the high-dimensional space only as necessary. We introduce a bandit algorithm that explores within this coarse-to-fine spectrum, and prove performance guarantees that depend on how well the coarse space captures the user's preferences. We demonstrate substantial improvement over conventional bandit algorithms through extensive simulation as well as a live user study in the setting of personalized news recommendation. Copyright 2012 by the author(s)/owner(s).",,Conference Paper,,Scopus,2-s2.0-84867112205
SCOPUS,"Xie N., Hachiya H., Sugiyama M.",Artist agent: A reinforcement learning approach to automatic stroke generation in oriental ink painting,2012,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",1,,,153,160,,5,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867137193&partnerID=40&md5=3326dc65d41217d8a8f1e874539a9791,"Department of Computer Science, Tokyo Institute of Technology, Tokyo, Japan","Xie, N., Department of Computer Science, Tokyo Institute of Technology, Tokyo, Japan; Hachiya, H., Department of Computer Science, Tokyo Institute of Technology, Tokyo, Japan; Sugiyama, M., Department of Computer Science, Tokyo Institute of Technology, Tokyo, Japan","Oriental ink painting, called Sumi-e, is one of the most appealing painting styles that has attracted artists around the world. Major challenges in computer-based Sumi-e simulation are to abstract complex scene information and draw smooth and natural brush strokes. To automatically generate such strokes, we propose to model a brush as a reinforcement learning agent, and learn desired brush-trajectories by maximizing the sum of rewards in the policy search framework. We also elaborate on the design of actions, states, and rewards tailored for a Sumi-e agent. The effectiveness of our proposed approach is demonstrated through simulated Sumi-e experiments. Copyright 2012 by the author(s)/owner(s).",,Conference Paper,,Scopus,2-s2.0-84867137193
SCOPUS,[No author name available],"Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2012, Proceedings",2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),7524 LNAI,PART 2,,,,1794,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866856631&partnerID=40&md5=7932fb656703071a8fcb17badba8df50,,,The proceedings contain 119 papers. The topics discussed include: declarative modeling for machine learning and data mining; solving problems with visual analytics: challenges and applications; discovering descriptive tile trees: by mining optimal geometric subtiles; efficient discovery of association rules and frequent itemsets through sampling with tight performance guarantees; combining subjective probabilities and data in training Markov logic networks; a note on extending generalization bounds for binary large-margin classifiers to multiple classes; fairness-aware classifier with prejudice remover regularizer; a live comparison of methods for personalized article recommendation at Forbes.com; fast reinforcement learning with large action sets using error-correcting output codes for MDP factorization; opinion formation by voter model with temporal decay dynamics; and discriminative factor alignment across heterogeneous feature space.,,Conference Review,,Scopus,2-s2.0-84866856631
SCOPUS,[No author name available],"Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2012, Proceedings",2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),7523 LNAI,PART 1,,,,1794,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866856734&partnerID=40&md5=9a9af96c2485750439013c7a132d27df,,,The proceedings contain 119 papers. The topics discussed include: declarative modeling for machine learning and data mining; solving problems with visual analytics: challenges and applications; discovering descriptive tile trees: by mining optimal geometric subtiles; efficient discovery of association rules and frequent itemsets through sampling with tight performance guarantees; combining subjective probabilities and data in training Markov logic networks; a note on extending generalization bounds for binary large-margin classifiers to multiple classes; fairness-aware classifier with prejudice remover regularizer; a live comparison of methods for personalized article recommendation at Forbes.com; fast reinforcement learning with large action sets using error-correcting output codes for MDP factorization; opinion formation by voter model with temporal decay dynamics; and discriminative factor alignment across heterogeneous feature space.,,Conference Review,,Scopus,2-s2.0-84866856734
SCOPUS,"Dulac-Arnold G., Denoyer L., Preux P., Gallinari P.",Sequential approaches for learning datum-wise sparse representations,2012,Machine Learning,89,1-2,,87,122,,6,10.1007/s10994-012-5306-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865273017&doi=10.1007%2fs10994-012-5306-7&partnerID=40&md5=711ea68e179921d4c90e65895e356bde,"UPMC, LIP6, Université Pierre et Marie Curie, 4 Place Jussieu Case 169, Paris 75005, France; LIFL (UMR CNRS) and INRIA Lille Nord-Europe, Université de Lille, Villeneuve d'Ascq, France","Dulac-Arnold, G., UPMC, LIP6, Université Pierre et Marie Curie, 4 Place Jussieu Case 169, Paris 75005, France; Denoyer, L., UPMC, LIP6, Université Pierre et Marie Curie, 4 Place Jussieu Case 169, Paris 75005, France; Preux, P., LIFL (UMR CNRS) and INRIA Lille Nord-Europe, Université de Lille, Villeneuve d'Ascq, France; Gallinari, P., UPMC, LIP6, Université Pierre et Marie Curie, 4 Place Jussieu Case 169, Paris 75005, France","In supervised classification, data representation is usually considered at the dataset level: one looks for the ""best"" representation of data assuming it to be the same for all the data in the data space. We propose a different approach where the representations used for classification are tailored to each datum in the data space. One immediate goal is to obtain sparse datum-wise representations: our approach learns to build a representation specific to each datum that contains only a small subset of the features, thus allowing classification to be fast and efficient. This representation is obtained by way of a sequential decision process that sequentially chooses which features to acquire before classifying a particular point; this process is learned through algorithms based on Reinforcement Learning. The proposed method performs well on an ensemble of medium-sized sparse classification problems. It offers an alternative to global sparsity approaches, and is a natural framework for sequential classification problems. The method extends easily to a whole family of sparsity-related problem which would otherwise require developing specific solutions. This is the case in particular for cost-sensitive and limited-budget classification, where feature acquisition is costly and is often performed sequentially. Finally, our approach can handle non-differentiable loss functions or combinatorial optimization encountered in more complex feature selection problems. © 2012 The Author(s).",Classification; Features selection; Reinforcement learning; Sequential models; Sparsity,Article,,Scopus,2-s2.0-84865273017
SCOPUS,"Pape L., Oddo C.M., Controzzi M., Cipriani C., Förster A., Carrozza M.C., Schmidhuber J.",Learning tactile skills through curious exploration,2012,Frontiers in Neurorobotics,,JULY, Article 6,,,,20,10.3389/fnbot.2012.00006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866027577&doi=10.3389%2ffnbot.2012.00006&partnerID=40&md5=10f5035af6eb9cf4b2a2d65efcc15ce3,"Istituto Dalle Molle di Studi sull'Intelligenza Artificiale, Università della Svizzera Italiana, Lugano, Switzerland; The BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy","Pape, L., Istituto Dalle Molle di Studi sull'Intelligenza Artificiale, Università della Svizzera Italiana, Lugano, Switzerland; Oddo, C.M., The BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy; Controzzi, M., The BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy; Cipriani, C., The BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy; Förster, A., Istituto Dalle Molle di Studi sull'Intelligenza Artificiale, Università della Svizzera Italiana, Lugano, Switzerland; Carrozza, M.C., The BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy; Schmidhuber, J., Istituto Dalle Molle di Studi sull'Intelligenza Artificiale, Università della Svizzera Italiana, Lugano, Switzerland","We present curiosity-driven, autonomous acquisition of tactile exploratory skills on a biomimetic robot finger equipped with an array of microelectromechanical touch sensors. Instead of building tailored algorithms for solving a specific tactile task, we employ a more general curiosity-driven reinforcement learning approach that autonomously learns a set of motor skills in absence of an explicit teacher signal. In this approach, the acquisition of skills is driven by the information content of the sensory input signals relative to a learner that aims at representing sensory inputs using fewer and fewer computational resources. We show that, from initially random exploration of its environment, the robotic system autonomously develops a small set of basic motor skills that lead to different kinds of tactile input. Next, the system learns how to exploit the learned motor skills to solve supervised texture classification tasks. Our approach demonstrates the feasibility of autonomous acquisition of tactile skills on physical robotic platforms through curiosity-driven reinforcement learning, overcomes typical difficulties of engineered solutions for active tactile exploration and underactuated control, and provides a basis for studying developmental learning through intrinsic motivation in robots. © 2012 Pape, Oddo, Controzzi, Cipriani, Förster, Carrozza and Schmidhuber.",Active learning; Biomimetic robotics; Curiosity; Intrinsic motivation; Reinforcement learning; Skill learning; Tactile sensing,Article,,Scopus,2-s2.0-84866027577
SCOPUS,"Modayil J., White A., Sutton R.S.",Multi-timescale nexting in a reinforcement learning robot,2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),7426 LNAI,,,299,309,,8,10.1007/978-3-642-33093-3_30,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866006400&doi=10.1007%2f978-3-642-33093-3_30&partnerID=40&md5=23a4ca68ff90db39525283184c36db51,"Reinforcement Learning and Artificial Intelligence Laboratory, University of Alberta, Edmonton, AB, Canada","Modayil, J., Reinforcement Learning and Artificial Intelligence Laboratory, University of Alberta, Edmonton, AB, Canada; White, A., Reinforcement Learning and Artificial Intelligence Laboratory, University of Alberta, Edmonton, AB, Canada; Sutton, R.S., Reinforcement Learning and Artificial Intelligence Laboratory, University of Alberta, Edmonton, AB, Canada","The term ""nexting"" has been used by psychologists to refer to the propensity of people and many other animals to continually predict what will happen next in an immediate, local, and personal sense. The ability to ""next"" constitutes a basic kind of awareness and knowledge of one's environment. In this paper we present results with a robot that learns to next in real time, predicting thousands of features of the world's state, including all sensory inputs, at timescales from 0.1 to 8 seconds. This was achieved by treating each state feature as a reward-like target and applying temporal-difference methods to learn a corresponding value function with a discount rate corresponding to the timescale. We show that two thousand predictions, each dependent on six thousand state features, can be learned and updated online at better than 10Hz on a laptop computer, using the standard TD(λ) algorithm with linear function approximation. We show that this approach is efficient enough to be practical, with most of the learning complete within 30 minutes. We also show that a single tile-coded feature representation suffices to accurately predict many different signals at a significant range of timescales. Finally, we show that the accuracy of our learned predictions compares favorably with the optimal off-line solution. © 2012 Springer-Verlag.",,Conference Paper,,Scopus,2-s2.0-84866006400
SCOPUS,"Raje R.R., Mukhopadhyay S., Phatak S., Shastri R., Gallege L.S.",Software service selection by multi-level matching and reinforcement learning,2012,"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering",87 LNICST,,,310,324,,,10.1007/978-3-642-32615-8_31,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869594563&doi=10.1007%2f978-3-642-32615-8_31&partnerID=40&md5=0cd9062b73a1835a5e2626173a36dbc9,"Indiana University Purdue University Indianapolis, Indianapolis, IN 46202, United States","Raje, R.R., Indiana University Purdue University Indianapolis, Indianapolis, IN 46202, United States; Mukhopadhyay, S., Indiana University Purdue University Indianapolis, Indianapolis, IN 46202, United States; Phatak, S., Indiana University Purdue University Indianapolis, Indianapolis, IN 46202, United States; Shastri, R., Indiana University Purdue University Indianapolis, Indianapolis, IN 46202, United States; Gallege, L.S., Indiana University Purdue University Indianapolis, Indianapolis, IN 46202, United States","The software realization of distributed systems is typically achieved as loose coalitions of independently created services. The selection of such services, to act as building blocks of a distributed system, is a critical task that requires discovery and matching activities. This selection task is generally based on simple matching techniques and without any notion of customization. This paper presents a method to achieve the service discovery process using the principles of multilevel matching based on multi-level specifications and customization based on reinforcement learning techniques. In this method, services are selected dynamically using an on-line performance-based reinforcement feedback. In contrast to methods which require the services to actually carry out a task before being selected, in the method proposed in this paper, service selection is carried out using only specification matching, thereby eliminating a large amount of redundant computation. Experimental results are presented in the context of a information classification system. These experiments demonstrate that a high degree of performance can be achieved at a much reduced computational cost using only multi-level specification-matching based reinforcement feedback signals. © 2012 ICST Institute for Computer Science, Social Informatics and Telecommunications Engineering.",acquaintances; classification; discovery; multi-level specifications; reinforcement learning; software services,Conference Paper,,Scopus,2-s2.0-84869594563
SCOPUS,"Llorente M.S., Guerrero S.E.",Increasing retrieval quality in conversational recommenders,2012,IEEE Transactions on Knowledge and Data Engineering,24,10,5871618,1876,1888,,9,10.1109/TKDE.2011.116,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865313639&doi=10.1109%2fTKDE.2011.116&partnerID=40&md5=a8894ecf5900614630cc1b5df3226b4b,"Department Matematica Aplicada i Analisi, Universitat de Barcelona, Gran Via de les Corts Catalanes, Barcelona 585-08007, Spain; Computer Vision Center, Universitat Autònoma de Barcelona, Bellatera 08193, Spain","Llorente, M.S., Department Matematica Aplicada i Analisi, Universitat de Barcelona, Gran Via de les Corts Catalanes, Barcelona 585-08007, Spain; Guerrero, S.E., Department Matematica Aplicada i Analisi, Universitat de Barcelona, Gran Via de les Corts Catalanes, Barcelona 585-08007, Spain, Computer Vision Center, Universitat Autònoma de Barcelona, Bellatera 08193, Spain","A major task of research in conversational recommender systems is personalization. Critiquing is a common and powerful form of feedback, where a user can express her feature preferences by applying a series of directional critiques over the recommendations instead of providing specific preference values. Incremental Critiquing (IC) is a conversational recommender system that uses critiquing as a feedback to efficiently personalize products. The expectation is that in each cycle the system retrieves the products that best satisfy the user's soft product preferences from a minimal information input. In this paper, we present a novel technique that increases retrieval quality based on a combination of compatibility and similarity scores. Under the hypothesis that a user learns during the recommendation process, we propose two novel exponential Reinforcement Learning (RL) approaches for compatibility that take into account both the instant at which the user makes a critique and the number of satisfied critiques. Moreover, we consider that the impact of features on the similarity differs according to the preferences manifested by the user. We propose a Global Weighting (GW) approach that uses a common weight for nearest cases in order to focus on groups of relevant products. We show that our methodology significantly improves recommendation efficiency in four data sets of different sizes in terms of session length in comparison with state-of-the-art approaches. Moreover, our recommender shows higher robustness against noisy user data when compared to classical approaches. © 2012 IEEE.",case-based reasoning; Conversational recommender systems; critiquing elicitation; personalization,Article,,Scopus,2-s2.0-84865313639
SCOPUS,"Kelly J., Gooding P., Pratt D., Ainsworth J., Welford M., Tarrier N.",Intelligent real-time therapy: Harnessing the power of machine learning to optimise the delivery of momentary cognitivebehavioural interventions,2012,Journal of Mental Health,21,4,,404,414,,22,10.3109/09638237.2011.638001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864214034&doi=10.3109%2f09638237.2011.638001&partnerID=40&md5=a00355d44c25f890ea793fe20c824beb,"Greater Manchester West Mental Health NHS Foundation Trust, Department of Psychology, Prestwich Hospital, Prestwich M25 3BL, United Kingdom; School of Psychological Sciences, University of Manchester, Manchester, United Kingdom; School of Community Based Medicine, University of Manchester, Manchester, United Kingdom; Department of Psychology, Institute of Psychiatry, King's College London, London, United Kingdom","Kelly, J., Greater Manchester West Mental Health NHS Foundation Trust, Department of Psychology, Prestwich Hospital, Prestwich M25 3BL, United Kingdom, School of Psychological Sciences, University of Manchester, Manchester, United Kingdom; Gooding, P., School of Psychological Sciences, University of Manchester, Manchester, United Kingdom; Pratt, D., School of Psychological Sciences, University of Manchester, Manchester, United Kingdom; Ainsworth, J., School of Community Based Medicine, University of Manchester, Manchester, United Kingdom; Welford, M., Greater Manchester West Mental Health NHS Foundation Trust, Department of Psychology, Prestwich Hospital, Prestwich M25 3BL, United Kingdom; Tarrier, N., School of Psychological Sciences, University of Manchester, Manchester, United Kingdom, Department of Psychology, Institute of Psychiatry, King's College London, London, United Kingdom","Background Experience sampling methodology (ESM) [Csikszentmihalyi, M. & Larson, R. (1987). Validity and reliability of the experience-sampling method. Journal of Nervous and Mental Disease, 175(9), 526536] has been used to elucidate the cognitivebehavioural mechanisms underlying the development and maintenance of complex mental disorders as well as mechanisms involved in resilience from such states. We present an argument for the development of intelligent real-time therapy (iRTT). Machine learning and reinforcement learning specifically may be used to optimise the delivery of interventions by observing and altering the timing of real-time therapies based on ongoing ESM measures.Aims The aims of the present article are to outline the principles of iRTT and to consider how it would be applied to complex problems such as suicide prevention.Methods Relevant literature was identified through use of PychInfo.Results iRTT may provide an important and ecologically valid adjunct to traditional CBT, providing a means of balancing population-based data with individual data, thus addressing the ""knowledgepractice gap"" [Tarrier, N. (2010b). The cognitive and behavioral treatment of PTSD, what is known and what is known to be unknown: How not to fall into the practice gap. Clinical Psychology: Science and Practice, 17(2), 134143] and facilitating the delivery of interventions in situ, thereby addressing the ""therapyreal-world gap"".Conclusions iRTT may provide a platform for the development of individualised and multifaceted momentary intervention strategies that are ecologically valid and aimed at attenuating pathological pathways to complex mental health problems and amplifying pathways associated with resilience. © 2012 Informa UK, Ltd.",Cognitive behaviour therapy (CBT); Ecological momentary intervention (EMI); Experience sampling methodology (ESM); Intelligent real-time therapy (iRTT); Smartphone; Suicide prevention,Article,,Scopus,2-s2.0-84864214034
SCOPUS,"Young B., Ghirlanda S., Grasso F.W.",Parallel implementation of instinctual and learning neural mechanisms in a simulated mobile robot,2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),7375 LNAI,,,298,308,,,10.1007/978-3-642-31525-1_26,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864023295&doi=10.1007%2f978-3-642-31525-1_26&partnerID=40&md5=e3e1a751618d3dcae7806ade3a230c20,"BioMimetic and Cognitive Robotics Laboratory, Brooklyn College, Brooklyn, NY, United States; Department of Psychology, Brooklyn College, Brooklyn, NY, United States; Centre for the Study of Cultural Evolution, Stockholm University, Sweden","Young, B., BioMimetic and Cognitive Robotics Laboratory, Brooklyn College, Brooklyn, NY, United States; Ghirlanda, S., Department of Psychology, Brooklyn College, Brooklyn, NY, United States, Centre for the Study of Cultural Evolution, Stockholm University, Sweden; Grasso, F.W., BioMimetic and Cognitive Robotics Laboratory, Brooklyn College, Brooklyn, NY, United States, Department of Psychology, Brooklyn College, Brooklyn, NY, United States","The question of how biological learning and instinctive neural mechanisms interact with each other in the course of development to produce novel, adaptive behaviors was explored via a robotic simulation. Instinctive behavior in the agent was implemented in a hard-wired network which produced obstacle avoidance. Phototactic behavior was produced in two serially connected plastic layers. A self-organizing feature map was combined with a reinforcement learning layer to produce a learning network. The reinforcement came from an internally generated signal. Both the adaptive and fixed networks supplied motor control signals to the robot motors. The sizes of the self-organizing layer, reinforcement layer, and the complexity of the environment were varied and effects on robot phototactic efficiency and accuracy in the mature networks were measured. A significant interaction of the three independent variables was found, supporting the idea that organisms evolve distinct combinations of instinctive and plastic neural mechanisms which are tailored to the demands of the environment in which their species evolved. © 2012 Springer-Verlag.",evolution; instinct; learning neural network; robots,Conference Paper,,Scopus,2-s2.0-84864023295
SCOPUS,"Ong S.C.W., Grinberg Y., Pineau J.",Goal-directed online learning of predictive models,2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),7188 LNAI,,,18,29,,3,10.1007/978-3-642-29946-9_6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861691760&doi=10.1007%2f978-3-642-29946-9_6&partnerID=40&md5=cb911c2456dde19107bbd0f1fe48349f,"School of Computer Science, McGill University, Montreal, QC, Canada","Ong, S.C.W., School of Computer Science, McGill University, Montreal, QC, Canada; Grinberg, Y., School of Computer Science, McGill University, Montreal, QC, Canada; Pineau, J., School of Computer Science, McGill University, Montreal, QC, Canada","We present an algorithmic approach for integrated learning and planning in predictive representations. The approach extends earlier work on predictive state representations to the case of online exploration, by allowing exploration of the domain to proceed in a goal-directed fashion and thus be more efficient. Our algorithm interleaves online learning of the models, with estimation of the value function. The framework is applicable to a variety of important learning problems, including scenarios such as apprenticeship learning, model customization, and decision-making in non-stationary domains. © 2012 Springer-Verlag.",model-based reinforcement learning; online learning; predictive state representation,Conference Paper,,Scopus,2-s2.0-84861691760
SCOPUS,"May B.C., Korda N., Lee A., Leslie D.S.",Optimistic bayesian sampling in contextual-bandit problems,2012,Journal of Machine Learning Research,13,,,2069,2106,,40,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864939787&partnerID=40&md5=7096900388c4f8aba3fc7a7151cd9980,"School of Mathematics, University of Bristol, Bristol, BS8 1TW, United Kingdom; Oxford-Man Institute, University of Oxford, Walton Well Road, Oxford, OX2 6ED, United Kingdom","May, B.C., School of Mathematics, University of Bristol, Bristol, BS8 1TW, United Kingdom; Korda, N., Oxford-Man Institute, University of Oxford, Walton Well Road, Oxford, OX2 6ED, United Kingdom; Lee, A., Oxford-Man Institute, University of Oxford, Walton Well Road, Oxford, OX2 6ED, United Kingdom; Leslie, D.S., School of Mathematics, University of Bristol, Bristol, BS8 1TW, United Kingdom","In sequential decision problems in an unknown environment, the decision maker often faces a dilemma over whether to explore to discover more about the environment, or to exploit current knowledge. We address the exploration-exploitation dilemma in a general setting encompassing both standard and contextualised bandit problems. The contextual bandit problem has recently resurfaced in attempts to maximise click-through rates in web based applications, a task with significant commercial interest. In this article we consider an approach of Thompson (1933) which makes use of samples from the posterior distributions for the instantaneous value of each action. We extend the approach by introducing a new algorithm, Optimistic Bayesian Sampling (OBS), in which the probability of playing an action increases with the uncertainty in the estimate of the action value. This results in better directed exploratory behaviour. We prove that, under unrestrictive assumptions, both approaches result in optimal behaviour with respect to the average reward criterion of Yang and Zhu (2002). We implement OBS and measure its performance in simulated Bernoulli bandit and linear regression domains, and also when tested with the task of personalised news article recommendation on a Yahoo! Front Page Today Module data set. We find that OBS performs competitively when compared to recently proposed benchmark algorithms and outperforms Thompson's method throughout. © 2012 Benedict C. May, Nathan Korda, Anthony Lee and David S. Leslie.",Contextual bandits; Exploration-exploitation; Multi-armed bandits; Sequential allocation; Thompson sampling,Article,,Scopus,2-s2.0-84864939787
SCOPUS,"Bouneffouf D., Bouzeghoub A., Gançarski A.L.",Hybrid-ε-greedy for mobile context-aware recommender system,2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),7301 LNAI,PART 1,,468,479,,11,10.1007/978-3-642-30217-6_39,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861444817&doi=10.1007%2f978-3-642-30217-6_39&partnerID=40&md5=ad6ea70f5fbea80eaaa0f3864a7fd509,"Department of Computer Science, Télécom SudParis, UMR CNRS Samovar, 91011 Evry Cedex, France","Bouneffouf, D., Department of Computer Science, Télécom SudParis, UMR CNRS Samovar, 91011 Evry Cedex, France; Bouzeghoub, A., Department of Computer Science, Télécom SudParis, UMR CNRS Samovar, 91011 Evry Cedex, France; Gançarski, A.L., Department of Computer Science, Télécom SudParis, UMR CNRS Samovar, 91011 Evry Cedex, France","The wide development of mobile applications provides a considerable amount of data of all types. In this sense, Mobile Context-aware Recommender Systems (MCRS) suggest the user suitable information depending on her/his situation and interests. Our work consists in applying machine learning techniques and reasoning process in order to adapt dynamically the MCRS to the evolution of the user's interest. To achieve this goal, we propose to combine bandit algorithm and case-based reasoning in order to define a contextual recommendation process based on different context dimensions (social, temporal and location). This paper describes our ongoing work on the implementation of a MCRS based on a hybrid-ε-greedy algorithm. It also presents preliminary results by comparing the hybrid-ε-greedy and the standard ε-greedy algorithm. © 2012 Springer-Verlag.",contextual bandit; exploration/exploitation dilemma; Machine learning; personalization; recommender systems,Conference Paper,,Scopus,2-s2.0-84861444817
SCOPUS,[No author name available],WSDM 2012 - Proceedings of the 5th ACM International Conference on Web Search and Data Mining,2012,WSDM 2012 - Proceedings of the 5th ACM International Conference on Web Search and Data Mining,,,,,,786,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858060495&partnerID=40&md5=9326af29cce390bfd331415d2516a70e,,,"The proceedings contain 81 papers. The topics discussed include: spatially-aware indexing for image object retrieval; Auralist: introducing serendipity into music recommendation; beyond co-occurrence: discovering and visualizing tag relationships from geo-spatial and temporal similarities; overcoming browser cookie churn with clustering; of hammers and nails: an empirical comparison of three paradigms for processing large graphs; mining slang and urban opinion words and phrases from cQA services: an optimization approach; characterizing web content, user interests, and search behavior by reading level and topic; selecting actions for resource-bounded information extraction using reinforcement learning; overlapping clusters for distributed computation; personalized click model through collaborative filtering; extracting search-focused key N-grams for relevance ranking in web search; and tapping into knowledge base for concept feedback: leveraging ConceptNet to improve search results for difficult queries.",,Conference Review,,Scopus,2-s2.0-84858060495
SCOPUS,"Kastanis I., Slater M.",Reinforcement learning utilizes proxemics: An avatar learns to manipulate the position of people in immersive virtual reality,2012,ACM Transactions on Applied Perception,9,1,3,,,,5,10.1145/2134203.2134206,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859474974&doi=10.1145%2f2134203.2134206&partnerID=40&md5=83e08cb9da9427c493cfb59a2296e9f2,"University College London, United Kingdom; Facultat de Psicologia, Campus de Mundet - Edifici Teatre, Universitat de Barcelona, Passeig de la Vall d'Hebron 171, 08035 Barcelona, Spain; Facultat de Psicologia, Campus de Mundet - Edifici Teatre, ICREA-Universitat de Barcelona, Passeig de la Vall d'Hebron 171, 08035 Barcelona, Spain","Kastanis, I., Facultat de Psicologia, Campus de Mundet - Edifici Teatre, Universitat de Barcelona, Passeig de la Vall d'Hebron 171, 08035 Barcelona, Spain; Slater, M., University College London, United Kingdom, Facultat de Psicologia, Campus de Mundet - Edifici Teatre, ICREA-Universitat de Barcelona, Passeig de la Vall d'Hebron 171, 08035 Barcelona, Spain","A reinforcement learning (RL) method was used to train a virtual character to move participants to a specified location. The virtual environment depicted an alleyway displayed through a wide field-of-view head-tracked stereo head-mounted display. Based on proxemics theory, we predicted that when the character approached within a personal or intimate distance to the participants, they would be inclined to move backwards out of the way. We carried out a between-groups experiment with 30 female participants, with 10 assigned arbitrarily to each of the following three groups: In the Intimate condition the character could approach within 0.38m and in the Social condition no nearer than 1.2m. In the Random condition the actions of the virtual character were chosen randomly from among the same set as in the RL method, and the virtual character could approach within 0.38m. The experiment continued in each case until the participant either reached the target or 7 minutes had elapsed. The distributions of the times taken to reach the target showed significant differences between the three groups, with 9 out of 10 in the Intimate condition reaching the target significantly faster than the 6 out of 10 who reached the target in the Social condition. Only 1 out of 10 in the Random condition reached the target. The experiment is an example of applied presence theory: we rely on the many findings that people tend to respond realistically in immersive virtual environments, and use this to get people to achieve a task of which they had been unaware. This method opens up the door for many such applications where the virtual environment adapts to the responses of the human participants with the aim of achieving particular goals. © 2012 ACM 1544-3558/2012/03- ART3 ©10.00.",Experimentation; Human Factors,Article,,Scopus,2-s2.0-84859474974
SCOPUS,"Cui H., Turan O., Sayer P.",Learning-based ship design optimization approach,2012,CAD Computer Aided Design,44,3,,186,195,,13,10.1016/j.cad.2011.06.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855691014&doi=10.1016%2fj.cad.2011.06.011&partnerID=40&md5=77cd692740e8a4769780a062c5bfb89a,"Department of Naval Architecture and Marine Engineering, University of Strathclyde, Glasgow G4 0LZ, United Kingdom","Cui, H., Department of Naval Architecture and Marine Engineering, University of Strathclyde, Glasgow G4 0LZ, United Kingdom; Turan, O., Department of Naval Architecture and Marine Engineering, University of Strathclyde, Glasgow G4 0LZ, United Kingdom; Sayer, P., Department of Naval Architecture and Marine Engineering, University of Strathclyde, Glasgow G4 0LZ, United Kingdom","With the development of computer applications in ship design, optimization, as a powerful approach, has been widely used in the design and analysis process. However, the running time, which often varies from several weeks to months in the current computing environment, has been a bottleneck problem for optimization applications, particularly in the structural design of ships. To speed up the optimization process and adjust the complex design environment, ship designers usually rely on their personal experience to assist the design work. However, traditional experience, which largely depends on the designer's personal skills, often makes the design quality very sensitive to the experience and decreases the robustness of the final design. This paper proposes a new machine-learning-based ship design optimization approach, which uses machine learning as an effective tool to give direction to optimization and improves the adaptability of optimization to the dynamic design environment. The natural human learning process is introduced into the optimization procedure to improve the efficiency of the algorithm. Q-learning, as an approach of reinforcement learning, is utilized to realize the learning function in the optimization process. The multi-objective particle swarm optimization method, multi-agent system, and CAE software are used to build an integrated optimization system. A bulk carrier structural design optimization was performed as a case study to evaluate the suitability of this method for real-world application. © 2011 Elsevier Ltd. All rights reserved.",Machine learning; Multi-objective optimization; Ship design; Structure analysis; Structure optimization,Article,,Scopus,2-s2.0-84855691014
SCOPUS,"Goldberg Y., Kosorok M.R.",Q-learning with censored data,2012,Annals of Statistics,40,1,,529,560,,24,10.1214/12-AOS968,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861349230&doi=10.1214%2f12-AOS968&partnerID=40&md5=a0bad44aa6961dc76c7819dc739d8b8f,"Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States","Goldberg, Y., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States; Kosorok, M.R., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States","We develop methodology for a multistage decision problem with flexible number of stages in which the rewards are survival times that are subject to censoring. We present a novel Q-learning algorithm that is adjusted for censored data and allows a flexible number of stages. We provide finite sample bounds on the generalization error of the policy learned by the algorithm, and show that when the optimal Q-function belongs to the approximation space, the expected survival time for policies obtained by the algorithm converges to that of the optimal policy. We simulate a multistage clinical trial with flexible number of stages and apply the proposed censored-Q-learning algorithm to find individualized treatment regimens. The methodology presented in this paper has implications in the design of personalized medicine trials in cancer and in other life-threatening diseases. © Institute of Mathematical Statistics, 2012.",Generalization error; Q-learning; Reinforcement learning; Survival analysis,Article,,Scopus,2-s2.0-84861349230
SCOPUS,"Johnson S., Zaiane O.",Intelligent feedback polarity and timing selection in the Shufti Intelligent Tutoring System,2012,"Proceedings of the 20th International Conference on Computers in Education, ICCE 2012",,,,56,60,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896328874&partnerID=40&md5=2f69d64ebf1e2474954ed1c9e97c26e4,"Alberta Innovates Center for Machine Learning, University of Alberta, Canada","Johnson, S., Alberta Innovates Center for Machine Learning, University of Alberta, Canada; Zaiane, O., Alberta Innovates Center for Machine Learning, University of Alberta, Canada","It is well known that the training of medical students is a long and arduous process. Students master many areas of knowledge in a relatively short amount of time in order to become experts in their chosen field. The Socratic Method used in the latter stages of medical education, where a physician directly monitors a group of students, is inherently restrictive due to the limited number of cases and length of the students' rotations. Innovative Intelligent Tutoring techniques offer a solution to this problem. This paper outlines the overall structure and design of Shufti, an Intelligent Tutoring System (ITS) focused on mammography and medical imaging. Shufti's aim is to provide medical students with an improved learning environment, exposing them to a broad range of examples supported by customized feedback and hints driven by an adaptive Reinforcement Learning system and Clustering Techniques.",Breast cancer; Data mining; Feedback; Hints; Intelligent Tutoring System; Machine learning; Reinforcement learning; Serious games,Conference Paper,,Scopus,2-s2.0-84896328874
SCOPUS,"Agogino A.K., Tumer K.",A multiagent approach to managing air traffic flow,2012,Autonomous Agents and Multi-Agent Systems,24,1,,1,25,,31,10.1007/s10458-010-9142-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855309978&doi=10.1007%2fs10458-010-9142-5&partnerID=40&md5=2a6f989a67a241ed85211b1c24408346,"University of California, Santa Cruz, CA, United States; Oregon State University, Corvallis, OR, United States","Agogino, A.K., University of California, Santa Cruz, CA, United States; Tumer, K., Oregon State University, Corvallis, OR, United States","Intelligent air traffic flow management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today. FAA estimates put weather, routing decisions and airport condition induced delays at 1,682,700 h in 2007 (FAA OPSNET Data, US Department of Transportation website, http://www.faa.gov/data_statistics/), resulting in a staggering economic loss of over $41 billion (Joint Economic Commission Majority Staff, Your flight has been delayed again, 2008). New solutions to the flow management are needed to accommodate the threefold increase in air traffic anticipated over the next two decades. Indeed, this is a complex problem where the interactions of changing conditions (e. g., weather), conflicting priorities (e. g., different airlines), limited resources (e. g., air traffic controllers) and heavy volume (e. g., over 40,000 flights over the US airspace) demand an adaptive and robust solution. In this paper we explore a multiagent algorithm where agents use reinforcement learning (RL) to reduce congestion through local actions. Each agent is associated with a fix (a specific location in 2D space) and has one of three actions: setting separation between airplanes, ordering ground delays or performing reroutes. We simulate air traffic using FACET which is an air traffic flow simulator developed at NASA and used extensively by the FAA and industry. Our FACET simulations on both artificial and real historical data from the Chicago and New York airspaces show that agents receiving personalized rewards reduce congestion by up to 80% over agents receiving a global reward and by up to 90% over a current industry approach (Monte Carlo estimation). © 2010 The Author(s).",Agent coordination; Air traffic control; Multiagent learning,Article,,Scopus,2-s2.0-84855309978
SCOPUS,"Suay H.B., Chernova S.",A comparison of two algorithms for robot learning from demonstration,2011,"Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics",,,6084052,2495,2500,,2,10.1109/ICSMC.2011.6084052,https://www.scopus.com/inward/record.uri?eid=2-s2.0-83755174421&doi=10.1109%2fICSMC.2011.6084052&partnerID=40&md5=cddc21f31bfba29772d4b041d67358f4,"Robotics Engineering Program, Worcester Polytechnic Institute, Worcester, MA 01609, United States","Suay, H.B., Robotics Engineering Program, Worcester Polytechnic Institute, Worcester, MA 01609, United States; Chernova, S., Robotics Engineering Program, Worcester Polytechnic Institute, Worcester, MA 01609, United States","Robot learning from demonstration focuses on algorithms that enable a robot to learn a policy from demonstrations performed by a teacher, typically a human expert. This paper presents an experimental evaluation of two learning from demonstration algorithms, Interactive Reinforcement Learning and Behavior Networks. We evaluate the performance of these algorithms using a humanoid robot and discuss the relative advantages and drawbacks of these methods with respect to learning time, number of demonstrations, ease of implementation and other metrics. Our results show that Behavior Networks rely on a greater degree of domain knowledge and programmer expertise, requiring very precise definitions for behavior pre- and post-conditions. By contrast Interactive RL requires a relatively simple implementation based only on the robot's sensor data and actions. However, Behavior Networks leverage the pre-coded knowledge to effectively reduce learning time and the required number of human interactions to learn the task. © 2011 IEEE.",Learning and Adaptive Systems; Personal Robots,Conference Paper,,Scopus,2-s2.0-83755174421
SCOPUS,"Castro-González A., Amirabdollahian F., Polani D., Malfaz M., Salichs M.A.","Robot self-preservation and adaptation to user preferences in game play, a preliminary study",2011,"2011 IEEE International Conference on Robotics and Biomimetics, ROBIO 2011",,,6181679,2491,2498,,1,10.1109/ROBIO.2011.6181679,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860704585&doi=10.1109%2fROBIO.2011.6181679&partnerID=40&md5=6e4b62542718de128051050ad896ea71,"RoboticsLab., Carlos III University of Madrid, 28911, Leganés, Madrid, Spain; Adaptive Systems Research Group, University of Hertfordshire, Hatfield, United Kingdom","Castro-González, A., RoboticsLab., Carlos III University of Madrid, 28911, Leganés, Madrid, Spain; Amirabdollahian, F., Adaptive Systems Research Group, University of Hertfordshire, Hatfield, United Kingdom; Polani, D., Adaptive Systems Research Group, University of Hertfordshire, Hatfield, United Kingdom; Malfaz, M., RoboticsLab., Carlos III University of Madrid, 28911, Leganés, Madrid, Spain; Salichs, M.A., RoboticsLab., Carlos III University of Madrid, 28911, Leganés, Madrid, Spain","It is expected that in a near future, personal robots will be endowed with enough autonomy to function and live in an individual's home. This is while commercial robots are designed with default configuration and factory settings which may often be different to an individual's operating preferences. This paper presents how reinforcement learning is applied and utilised towards personalisation of a robot's behaviour. Two-level reinforcement learning has been implemented: first level is in charge of energy autonomy, i.e. how to survive, and second level is involved in adapting robot's behaviour to user's preferences. In both levels Q-learning algorithm has been applied. First level actions have been learnt in a simulated environment and then the results have been transferred to the real robot. Second level has been fully implemented in the real robot and learnt by human-robot interaction. Finally, experiments showing the performance of the system are presented. © 2011 IEEE.",,Conference Paper,,Scopus,2-s2.0-84860704585
SCOPUS,"Fisher R., Simmons R.",Smartphone interruptibility using density-weighted uncertainty sampling with reinforcement learning,2011,"Proceedings - 10th International Conference on Machine Learning and Applications, ICMLA 2011",1,,6147012,436,441,,18,10.1109/ICMLA.2011.128,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857814070&doi=10.1109%2fICMLA.2011.128&partnerID=40&md5=3846ad4a050dc87a51f3ecd94b889988,"Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213-3815, United States; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213-3815, United States","Fisher, R., Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213-3815, United States; Simmons, R., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213-3815, United States","We present the In-Context application for smart-phones, which combines signal processing, active learning, and reinforcement learning to autonomously create a personalized model of interruptibility for incoming phone calls. We empirically evaluate the system, and show that we can obtain an average of 96.12% classification accuracy when predicting interruptibility after a week of training. In contrast to previous work, we leverage density-weighted uncertainty sampling combined with a reinforcement learning framework applied to passively collected data to achieve comparable or superior classification accuracy using many fewer queries issued to the user. © 2011 IEEE.",Active learning; interruptibility; mobile devices; reinforcement learning,Conference Paper,,Scopus,2-s2.0-84857814070
SCOPUS,"Zhao Y., Zeng D., Socinski M.A., Kosorok M.R.",Reinforcement learning strategies for clinical trials in nonsmall cell lung cancer,2011,Biometrics,67,4,,1422,1433,,50,10.1111/j.1541-0420.2011.01572.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-83655181241&doi=10.1111%2fj.1541-0420.2011.01572.x&partnerID=40&md5=64ad4676c2bde9eeed5337ac997d331a,"Global Biostatistics and Epidemiology, Amgen Inc., One Amgen Center Drive, Thousand Oaks, CA 91320, United States; Department of Biostatistics, University of North Carolina at Chapel Hill, 3101 McGavran-Greenberg, CB 7420, Chapel Hill, NC 27599, United States; Department of Medicine, University of North Carolina at Chapel Hill, Physicians Office Building, 170 Manning Drive, Chapel Hill, NC 27599, United States","Zhao, Y., Global Biostatistics and Epidemiology, Amgen Inc., One Amgen Center Drive, Thousand Oaks, CA 91320, United States; Zeng, D., Department of Biostatistics, University of North Carolina at Chapel Hill, 3101 McGavran-Greenberg, CB 7420, Chapel Hill, NC 27599, United States; Socinski, M.A., Department of Medicine, University of North Carolina at Chapel Hill, Physicians Office Building, 170 Manning Drive, Chapel Hill, NC 27599, United States; Kosorok, M.R., Department of Biostatistics, University of North Carolina at Chapel Hill, 3101 McGavran-Greenberg, CB 7420, Chapel Hill, NC 27599, United States","Typical regimens for advanced metastatic stage IIIB/IV nonsmall cell lung cancer (NSCLC) consist of multiple lines of treatment. We present an adaptive reinforcement learning approach to discover optimal individualized treatment regimens from a specially designed clinical trial (a ""clinical reinforcement trial"") of an experimental treatment for patients with advanced NSCLC who have not been treated previously with systemic therapy. In addition to the complexity of the problem of selecting optimal compounds for first- and second-line treatments based on prognostic factors, another primary goal is to determine the optimal time to initiate second-line therapy, either immediately or delayed after induction therapy, yielding the longest overall survival time. A reinforcement learning method calledQ-learning is utilized, which involves learning an optimal regimen from patient data generated from the clinical reinforcement trial. Approximating theQ-function with time-indexed parameters can be achieved by using a modification of support vector regression that can utilize censored data. Within this framework, a simulation study shows that the procedure can extract optimal regimens for two lines of treatment directly from clinical data without prior knowledge of the treatment effect mechanism. In addition, we demonstrate that the design reliably selects the best initial time for second-line therapy while taking into account the heterogeneity of NSCLC across patients. © 2011, The International Biometric Society.",Adaptive design; Dynamic treatment regime; Individualized therapy; Multistage decision problems; Nonsmall cell lung cancer; Personalized medicine; Q-learning; Reinforcement learning; Support vector regression,Article,,Scopus,2-s2.0-83655181241
SCOPUS,"Van Moffaert K., Van Vreckem B., Mihaylov M., Nowe A.",A learning approach to the school bus routing problem,2011,Belgian/Netherlands Artificial Intelligence Conference,,,,,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874008961&partnerID=40&md5=e7b2d272e1fcee9c2b12161e58cb9536,"CoMo, Vrije Universiteit Brussel, Pleinlaan 2, Elsene, Belgium","Van Moffaert, K., CoMo, Vrije Universiteit Brussel, Pleinlaan 2, Elsene, Belgium; Van Vreckem, B., CoMo, Vrije Universiteit Brussel, Pleinlaan 2, Elsene, Belgium; Mihaylov, M., CoMo, Vrije Universiteit Brussel, Pleinlaan 2, Elsene, Belgium; Nowe, A., CoMo, Vrije Universiteit Brussel, Pleinlaan 2, Elsene, Belgium","In this paper, we introduce a solution to the School Bus Routing problem (SBRP), using a reinforcement learning technique that we previously applied in the domain of transportation logistics. This approach consists of bundling transportation requests between several locations in order to construct combinations of items in a cost-efficient way. We investigate how the combination of reinforcement learning and our novel bundling algorithm can be used to increase the efficiency in logistics and how it can be applied to other domains. In particular, we discuss how the SBRP can be transformed to resemble a problem in transportation logistics and thus solve the SBRP using our combined technique. We obtain results comparable to those presented in literature, namely from a cost minimization approach that is specifically tailored to the SBRP. We conclude that our reinforcement learning and bundling algorithms are flexible enough to be applied in different domains and offer significant reductions in the cost of the stakeholders.",,Conference Paper,,Scopus,2-s2.0-84874008961
SCOPUS,"Chasparis G.C., Shamma J.S., Rantzer A.",Perturbed learning automata in potential games,2011,Proceedings of the IEEE Conference on Decision and Control,,,6161294,2453,2458,,5,10.1109/CDC.2011.6161294,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860653554&doi=10.1109%2fCDC.2011.6161294&partnerID=40&md5=f42851a4b8eab2303788a23536a8c909,"Department of Automatic Control, Lund University, 221 00-SE Lund, Sweden; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA 30332, United States","Chasparis, G.C., Department of Automatic Control, Lund University, 221 00-SE Lund, Sweden; Shamma, J.S., School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA 30332, United States; Rantzer, A., Department of Automatic Control, Lund University, 221 00-SE Lund, Sweden","This paper presents a reinforcement learning algorithm and provides conditions for global convergence to Nash equilibria. For several reinforcement learning schemes, including the ones proposed here, excluding convergence to action profiles which are not Nash equilibria may not be trivial, unless the step-size sequence is appropriately tailored to the specifics of the game. In this paper, we sidestep these issues by introducing a new class of reinforcement learning schemes where the strategy of each agent is perturbed by a state-dependent perturbation function. Contrary to prior work on equilibrium selection in games, where perturbation functions are globally state dependent, the perturbation function here is assumed to be local, i.e., it only depends on the strategy of each agent. We provide conditions under which the strategies of the agents will converge to an arbitrarily small neighborhood of the set of Nash equilibria almost surely. We further specialize the results to a class of potential games. © 2011 IEEE.",,Conference Paper,,Scopus,2-s2.0-84860653554
SCOPUS,"Razavi R., Claussen H.",Self-configuring switched multi-element antenna system for interference mitigation in femtocell networks,2011,"IEEE International Symposium on Personal, Indoor and Mobile Radio Communications, PIMRC",,,6139947,237,242,,5,10.1109/PIMRC.2011.6139947,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857584963&doi=10.1109%2fPIMRC.2011.6139947&partnerID=40&md5=667dac81684e674b1621997aabc922b2,"Bell Laboratories, Alcatel-Lucent, Blanchardtown Industrial Park, Dublin 15, Ireland","Razavi, R., Bell Laboratories, Alcatel-Lucent, Blanchardtown Industrial Park, Dublin 15, Ireland; Claussen, H., Bell Laboratories, Alcatel-Lucent, Blanchardtown Industrial Park, Dublin 15, Ireland","This paper introduces a Switched Multi-Element Antenna (SMEA) solution for interference mitigation in femtocell networks. While the main objective is to protect the femtocell users against uplink interference, the downlink interference from femtocell base stations to the other users is simultaneously reduced as a by-product of this technique. A tailored form of reinforcement learning is used to allow for self-configuration of the femtocell base station and to adaptively select the optimal antenna configuration in a time varying environment. Compared to the traditional Omni-directional antenna systems, the results show an average of 2.5dB gain in uplink direction in terms of reduced transmission power and approximately 1dB of gain in the downlink channel. © 2011 IEEE.",Femtocell networks; Interference management; Multi-element antenna; Q-learning; Reinforcement learning; Self-configuration; WCDMA Femtocells,Conference Paper,,Scopus,2-s2.0-84857584963
SCOPUS,[No author name available],"Proceedings of the 14th IEEE International Multitopic Conference 2011, INMIC 2011",2011,"Proceedings of the 14th IEEE International Multitopic Conference 2011, INMIC 2011",,,,,,380,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858025376&partnerID=40&md5=f34d9c7e660c77cbd896026d3979aa68,,,The proceedings contain 68 papers. The topics discussed include: estimation of weights to combine trained neural networks using linear estimation techniques; diagnosis of liver disease induced by hepatitis virus using artificial neural networks; feature selection based image clustering using local discriminant model and global integration; heart disease classification ensemble optimization using genetic algorithm; gender classification using image processing techniques: a survey; a modified pheromone dominant ant colony algorithm for computer virus detection; application of fuzzy relational databases in medical informatics; analysis of adaptability of reinforcement learning approach; robot soccer framework for learning; and personalized versus non-personalized tag recommendation: a suitability study on three social networks.,,Conference Review,,Scopus,2-s2.0-84858025376
SCOPUS,"Tan C.H., Tan K.C., Tay A.",Dynamic game difficulty scaling using adaptive behavior-based AI,2011,IEEE Transactions on Computational Intelligence and AI in Games,3,4,5783334,289,301,,29,10.1109/TCIAIG.2011.2158434,https://www.scopus.com/inward/record.uri?eid=2-s2.0-83655198239&doi=10.1109%2fTCIAIG.2011.2158434&partnerID=40&md5=7fa85240bbb1652e86e75ac32fd70605,"Institute for Infocomm Research, Agency for Science Technology and Research (A STAR), Singapore 138632, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore 117576, Singapore","Tan, C.H., Institute for Infocomm Research, Agency for Science Technology and Research (A STAR), Singapore 138632, Singapore; Tan, K.C., Department of Electrical and Computer Engineering, National University of Singapore, Singapore 117576, Singapore; Tay, A., Department of Electrical and Computer Engineering, National University of Singapore, Singapore 117576, Singapore","Games are played by a wide variety of audiences. Different individuals will play with different gaming styles and employ different strategic approaches. This often involves interacting with nonplayer characters that are controlled by the game AI. From a developer's standpoint, it is important to design a game AI that is able to satisfy the variety of players that will interact with the game. Thus, an adaptive game AI that can scale the difficulty of the game according to the proficiency of the player has greater potential to customize a personalized and entertaining game experience compared to a static game AI. In particular, dynamic game difficulty scaling refers to the use of an adaptive game AI that performs game adaptations in real time during the game session. This paper presents two adaptive algorithms that use ideas from reinforcement learning and evolutionary computation to improve player satisfaction by scaling the difficulty of the game AI while the game is being played. The effects of varying the learning and mutation rates are examined and a general rule of thumb for the parameters is proposed. The proposed algorithms are demonstrated to be capable of matching its opponents in terms of mean scores and winning percentages. Both algorithms are able to generalize well to a variety of opponents. © 2009 IEEE.",Artificial intelligence; Behavior based; Car racing simulation; Game AI; Player satisfaction; Real-time adaptation,Article,,Scopus,2-s2.0-83655198239
SCOPUS,Li X.,Research of personal decision process using event-related potentials,2011,Proceedings of SPIE - The International Society for Optical Engineering,8205,,820526,,,,,10.1117/12.905918,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855189580&doi=10.1117%2f12.905918&partnerID=40&md5=5961f2673bb1234d199456b6bee3601d,"Institution of Information and Technology, Jiangxi Blue Sky University, 330098, Nanchang, China","Li, X., Institution of Information and Technology, Jiangxi Blue Sky University, 330098, Nanchang, China","To gain insights into the neural basis of such adaptive decision-making processes, we investigated the nature of learning process in humans playing a competitive game with binary choices, using a matching pennies game. As in reinforcement learning, the subject's choice during a competitive game was biased by its choice and reward history, as well as by the strategies of its opponent. Analyses of ERP data focused on the feedback-related negativity (FRN), we found that the magnitude of ERPs after losing to the computer opponent predicted whether subjects would change decision behavior on the subsequent trial. These findings provide novel evidence that humans engage a reinforcement learning process to adjust representations of competing decision options. © 2011 Copyright Society of Photo-Optical Instrumentation Engineers (SPIE).",Decision-making; ERP; event-related potential; feedback,Conference Paper,,Scopus,2-s2.0-81855189580
SCOPUS,"Nanduri V., Otieno W.",A New Water and Carbon Conscious Electricity Market Model for the Electricity-Water-Climate Change Nexus,2011,Electricity Journal,24,9,,64,74,,7,10.1016/j.tej.2011.09.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81155137757&doi=10.1016%2fj.tej.2011.09.021&partnerID=40&md5=10887ec2ca7863c2380da6bfe6a1dc5e,"Department of Industrial Engineering at the University of Wisconsin-Milwaukee, University of South Florida, United States","Nanduri, V., Department of Industrial Engineering at the University of Wisconsin-Milwaukee, University of South Florida, United States; Otieno, W., Department of Industrial Engineering at the University of Wisconsin-Milwaukee, University of South Florida, United States","With electricity, water, and climate change inextricably linked to each other, developing individualized policies or studying them in isolation is ineffectual and misguided. To understand the serious joint implications of the electricity-water-climate change nexus, the authors propose a conceptual framework of a joint carbon and water cap-and-trade model and present a multi-agent reinforcement learning-based predictive model to gain a deeper understanding of this nexus. © 2011 Elsevier Inc.",,Article,,Scopus,2-s2.0-81155137757
SCOPUS,[No author name available],"2011 RO-MAN - 20th IEEE International Symposium on Robot and Human Interactive Communication, Symposium Digest",2011,Proceedings - IEEE International Workshop on Robot and Human Interactive Communication,,,,,,503,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053045293&partnerID=40&md5=f8e84ea9dc806f38411390d51ebe2798,,,"The proceedings contain 80 papers. The topics discussed include: anthropomatics - the science of building smart artifacts for humans; human-robot interaction in the wild: land, marine, and aerial robots at Fukushima and Sendai; deployment of personal service robots for education and task services; developing skin-based technologies for interactive robots - challenges in design, development and the possible integration in therapeutic environments; effect of human guidance and state space size on interactive reinforcement learning; saliency-based boundary object detection in naturally complex scenes; how to walk a robot: a dog-leash human-robot interface; a behavior combination generating method for reflecting emotional probabilities using simulated annealing algorithm; multimodal controls for soldier/swarm interaction; recognizing situations that demand trust; and effects of responding to, initiating and ensuring joint attention in human-robot interaction.",,Conference Review,,Scopus,2-s2.0-80053045293
SCOPUS,"Ge Y., Qiu Q.",Dynamic thermal management for multimedia applications using machine learning,2011,Proceedings - Design Automation Conference,,,5981924,95,100,,25,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052678756&partnerID=40&md5=2c4a6edef9b5523f20383aad9c1d3426,"Department of Electrical and Computer Engineering, Binghamton University, United States","Ge, Y., Department of Electrical and Computer Engineering, Binghamton University, United States; Qiu, Q., Department of Electrical and Computer Engineering, Binghamton University, United States","Multimedia applications are expected to form the largest portion of workload in general purpose PC and portable devices. The ever-increasing computation intensity of multimedia applications elevates the processor temperature and consequently impairs the reliability and performance of the system. In this paper, we propose to perform dynamic thermal management using reinforcement learning algorithm for multimedia applications. The proposed learning model does not need any prior knowledge of the workload information or the system thermal and power characteristics. It learns the temperature change and workload switching patterns by observing the temperature sensor and event counters on the processor, and finds the management policy that provides good performance-thermal tradeoff during the runtime. We validated our model on a Dell personal computer with Intel Core 2 processor. Experimental results show that our approach provides considerable performance improvements with marginal increase in the percentage of thermal hotspot comparing to existing workload phase detection approach. © 2011 ACM.",Dynamic thermal management; multimedia application; reinforcement learning,Conference Paper,,Scopus,2-s2.0-80052678756
SCOPUS,"Malpani A., Ravindran B., Murthy H.",Personalized intelligent tutoring system using reinforcement learning,2011,"Proceedings of the 24th International Florida Artificial Intelligence Research Society, FLAIRS - 24",,,,561,562,,5,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052392052&partnerID=40&md5=6dc8d7ca7906b00d34f9338e4ba9a6bf,"Microsoft Corporation, India; Department of Computer Science and Engineering, IIT Madras, India","Malpani, A., Microsoft Corporation, India; Ravindran, B., Department of Computer Science and Engineering, IIT Madras, India; Murthy, H., Department of Computer Science and Engineering, IIT Madras, India","In this paper, we present a Personalized Intelligent Tutoring System that uses Reinforcement Learning techniques to implicitly learn teaching rules and provide instructions to students based on their needs. The system works on coarsely labeled data with minimum expert knowledge to ease extension to newer domains. Copyright © 2011, Association for the Advancement of Artificial Intelligence. All rights reserved.",,Conference Paper,,Scopus,2-s2.0-80052392052
SCOPUS,"Deng K., Pineau J., Murphy S.",Active learning for personalizing treatment,2011,IEEE SSCI 2011: Symposium Series on Computational Intelligence - ADPRL 2011: 2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning,,,5967348,32,39,,4,10.1109/ADPRL.2011.5967348,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052250780&doi=10.1109%2fADPRL.2011.5967348&partnerID=40&md5=0587089d6581a827974b615fa4ccb9bb,"Department of Statistics, University of Michigan, United Kingdom; Department of Computer Science, McGill University, Canada","Deng, K., Department of Statistics, University of Michigan, United Kingdom; Pineau, J., Department of Computer Science, McGill University, Canada; Murphy, S., Department of Statistics, University of Michigan, United Kingdom","The personalization of treatment via genetic biomarkers and other risk categories has drawn increasing interest among clinical researchers and scientists. A major challenge here is to construct individualized treatment rules (ITR), which recommend the best treatment for each of the different categories of individuals. In general, ITRs can be constructed using data from clinical trials, however these are generally very costly to run. In order to reduce the cost of learning an ITR, we explore active learning techniques designed to carefully decide whom to recruit, and which treatment to assign, throughout the online conduct of the clinical trial. As an initial investigation, we focus on simple ITRs that utilize a small number of subpopulation categories to personalize treatment. To minimize the maximal uncertainty regarding the treatment effects for each subpopulation, we propose the use of a minimax bandit model and provide an active learning policy for solving it. We evaluate our active learning policy using simulated data and data modeled after a clinical trial involving treatments for depressed individuals. We contrast this policy with other plausible active learning policies. The techniques presented in the paper may be generalized to tackle problems of efficient exploration in other domains. © 2011 IEEE.",,Conference Paper,,Scopus,2-s2.0-80052250780
SCOPUS,"Zhao J., Zhang H.",Multi-objective reinforcement learning algorithm and its improved convergency method,2011,"Proceedings of the 2011 6th IEEE Conference on Industrial Electronics and Applications, ICIEA 2011",,,5976002,2438,2445,,1,10.1109/ICIEA.2011.5976002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052232612&doi=10.1109%2fICIEA.2011.5976002&partnerID=40&md5=d09513f4d461e785564681b202ff9d86,"Department of Control Science and Engineering, Huazhong University of Science and Technology, China","Zhao, J., Department of Control Science and Engineering, Huazhong University of Science and Technology, China; Zhang, H., Department of Control Science and Engineering, Huazhong University of Science and Technology, China","This paper proposes a multi-objective reinforcement learning algorithm (MORLA) and uses simultaneous perturbation stochastic approximation (SPSA) to improve the convergence of it. Usually, reinforcement learning (RL) is used to design neurocontroller for control system with single objective. When facing multi-objective system, it is necessary to design the neurocontroller according to the personal preference. The MORLA can transform the multi-objective into synthetical objective and applies parallel genetic algorithm (PGA) to evolve the neurocontroller according to the synthetical objective. To establish the synthetical objective, the objective weight which represents the personal preference is calculated by solving the constrained optimization problem (COP) at the end of each generation. The COP requires not only the biggest variance of the synthetical objective in the population, but also requires the weight to fit the designer's preference. After acquiring the weights, the PGA can select the elitists from the population according to the designer's preference and design a satisfying neurocontroller by evolutionary operations. In addition, although GA has good global search ability, it descends slowly at local area. This paper applies SPSA algorithm to search optimal solution when GA is vibrating at local area. The SPSA converges fast by efficient gradient approximation that relies on measurements of the objective function. The hybrid algorithm accelerates the learning speed of reinforcement learning. At last, the MORLA is used to design neurocontroller for a speed-controlled induction motor drive with indirect vector control. With different personal preferences for the drive system, the simulation results show the feasibility and validity of the MORLA. © 2011 IEEE.",multi-objective reinforcement learning; speed-controlled; SPSA,Conference Paper,,Scopus,2-s2.0-80052232612
SCOPUS,[No author name available],"Help Me Help You: Bridging the Gaps in Human-Agent Collaboration - Papers from the AAAI Spring Symposium, Technical Report",2011,AAAI Spring Symposium - Technical Report,SS-11-05,,,,,70,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051539397&partnerID=40&md5=c8936bb7596612081b7c37a87369aa39,,,"The proceedings contain 12 papers. The topics discussed include: decentralized models for use in a real-world personal assistant agent scenario; mixed-initiative optimization in security games: a preliminary report; helping agents help their users despite imperfect speech recognition; a framework for teaching and executing verb phrases; human natural instruction of a simulated electronic student; help me to help you: how to learn intentions, actions and plans; reinforcement learning with human feedback in mountain car; spatial interactions between humans and assistive agents; a framework in which robots and humans help each other; just keep tweeting, dear: web-mining methods for helping a social robot understand user needs; and using human demonstrations to improve reinforcement learning.",,Conference Review,,Scopus,2-s2.0-80051539397
SCOPUS,Hartono P.,"Utilization of machine learning methods for assembling, training and understanding autonomous robots",2011,"4th International Conference on Human System Interaction, HSI 2011",,,5937399,398,402,,2,10.1109/HSI.2011.5937399,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79961201221&doi=10.1109%2fHSI.2011.5937399&partnerID=40&md5=668e13fc1921f6bfb0c834787f9a1f46,"Department of Mechanics and Information Technology, Chukyo University, Toyota, Japan","Hartono, P., Department of Mechanics and Information Technology, Chukyo University, Toyota, Japan","For decades human society has been supported by the proliferation of complex artifacts such as electronic appliances, personal vehicles and mass transportation systems, electrical and communications grids, and in the past few decades, Internet. In the very near future, robots will play increasingly important roles in our daily life. The increase in complexity of the tasks and sometimes physical forms or morphologies of the artifacts consequently requires complex assembling and controlling procedures of them, which soon will be unmanageable by the traditional manufacturing process. The aim of this paper is to give a brief review on the potentials of the non-traditional assembling of complex artifacts, which in this study is symbolized by the creation of autonomous robots. Methods in self-assembling modular robots, real time learning of autonomous robots and a method for giving the comprehensive understanding, albeit intuitively, to human will be explained through some physical experiments. © 2011 IEEE.",Autonomous Robots; Modular Robots; Perceptual Understanding; Reinforcement Learning; Self Assembling; Self-Organizing Map,Conference Paper,,Scopus,2-s2.0-79961201221
SCOPUS,"Yu T., Zhou B., Chan K.W., Chen L., Yang B.",Stochastic optimal relaxed automatic generation control in non-Markov environment based on multi-step Q(λ) learning,2011,IEEE Transactions on Power Systems,26,3,5706397,1272,1282,,43,10.1109/TPWRS.2010.2102372,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960889687&doi=10.1109%2fTPWRS.2010.2102372&partnerID=40&md5=12e82352452aed9cf2121488bbead254,"College of Electric Power, South China University of Technology, Guangzhou 510640, China; Department of Electrical Engineering, Hong Kong Polytechnic University, Hong Kong, Hong Kong; Guangdong Power Dispatch and Communication Center, China Southern Power Grid Company, Guangzhou 510600, China","Yu, T., College of Electric Power, South China University of Technology, Guangzhou 510640, China; Zhou, B., Department of Electrical Engineering, Hong Kong Polytechnic University, Hong Kong, Hong Kong; Chan, K.W., Department of Electrical Engineering, Hong Kong Polytechnic University, Hong Kong, Hong Kong; Chen, L., Guangdong Power Dispatch and Communication Center, China Southern Power Grid Company, Guangzhou 510600, China; Yang, B., College of Electric Power, South China University of Technology, Guangzhou 510640, China","This paper proposes a stochastic optimal relaxed control methodology based on reinforcement learning (RL) for solving the automatic generation control (AGC) under NERC's control performance standards (CPS). The multi-step Q(λ) learning algorithm is introduced to effectively tackle the long time-delay control loop for AGC thermal plants in non-Markov environment. The moving averages of CPS1/ACE are adopted as the state feedback input, and the CPS control and relaxed control objectives are formulated as multi-criteria reward function via linear weighted aggregate method. This optimal AGC strategy provides a customized platform for interactive self-learning rules to maximize the long-run discounted reward. Statistical experiments show that the RL theory based Q(λ) controllers can effectively enhance the robustness and dynamic performance of AGC systems, and reduce the number of pulses and pulse reversals while the CPS compliances are ensured. The novel AGC scheme also provides a convenient way of controlling the degree of CPS compliance and relaxation by online tuning relaxation factors to implement the desirable relaxed control. © 2011 IEEE.",AGC; CPS; multi-step Q(λ) learning; non-Markov environment; relaxed control; stochastic optimization,Article,,Scopus,2-s2.0-79960889687
SCOPUS,"Kleb J., Abecker A.",Disambiguating entity references within an ontological model,2011,ACM International Conference Proceeding Series,,,,,,,,10.1145/1988688.1988714,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960587711&doi=10.1145%2f1988688.1988714&partnerID=40&md5=bbe8b01aaafdf3f880b75ab16cdbda79,"FZI, Research Institute for Information Technology, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany","Kleb, J., FZI, Research Institute for Information Technology, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany; Abecker, A., FZI, Research Institute for Information Technology, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany","In our everyday conversations, entities (persons, companies, etc.) are referred to by natural language identifiers (NLIs). Humans employ personal experience and situational context to interpret such identifiers. However, due to ambiguity, even humans run the risk of misinterpretations. In our prior work, we presented a novel method to resolve entity references in texts under the aspect of ambiguity. We explore ontological background knowledge represented in an RDF(S) graph. The different interpretation possibilities lead to different subgraphs of the underlying ontology, each subgraph describing one consistent, non-ambiguous interpretation of the ambiguous NLIs within the ontological knowledge base. Our domain-independent approach is based on spreading activation and uses a semantic relational ranking. In this paper, we suggest three extensions to our original algorithm. First, we process in a two-step interpretation - -instead of the whole original input text - -at first hand smaller text windows in order to get more precise reference interpretations through a smaller local text context. Second, we extend the spreading-activation algorithm within the RDF(S) graph towards a bidirectional exploration of edges which shall speed-up the algorithm. Third we use reinforcement learning in order to take advantage of re-occurring information. We present first experimental results with these algorithmic extensions and derive directions for future work. © 2011 ACM.",graph; named entity; spreading activation,Conference Paper,,Scopus,2-s2.0-79960587711
SCOPUS,[No author name available],"Networked Digital Technologies - Third International Conference, NDT 2011, Proceedings",2011,Communications in Computer and Information Science,136 CCIS,,,,,447,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960218847&partnerID=40&md5=d783d8c45aff505623b42c523fd527ea,,,The proceedings contain 37 papers. The topics discussed include: authentication and authorization in web services; integrating access control mechanism with EXEL labeling scheme for XML document updating; an access control model for supporting XML document updating; accelerated particle swarm optimization and support vector machine for business optimization and applications; XRD metadata to make digital identity less visible and foster trusted collaborations across networked computing ecosystems; an overview of performance comparison of different TCP variants in IP and MPLS networks; routing in mobile ad-hoc networks as a reinforcement learning task; identifying usability issues in personal calendar tools; adaptive query processing for semantic interoperable information systems; and statistical character-based syntax similarity measurement for detecting biomedical syntax variations through named entity recognition.,,Conference Review,,Scopus,2-s2.0-79960218847
SCOPUS,"Baker T.E., Stockwell T., Barnes G., Holroyd C.B.","Individual differences in substance dependence: At the intersection of brain, behaviour and cognition",2011,Addiction Biology,16,3,,458,466,,21,10.1111/j.1369-1600.2010.00243.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959823204&doi=10.1111%2fj.1369-1600.2010.00243.x&partnerID=40&md5=e52c842a99576e3ddaf9e327c0554916,"Department of Psychology, University of Victoria, P.O. Box 3050 STN CSC, Victoria, BC V8W 3P5, Canada; Center of Addiction Research of British Columbia, University of Victoria, Canada; Child and Youth Care, University of Victoria, Canada","Baker, T.E., Department of Psychology, University of Victoria, P.O. Box 3050 STN CSC, Victoria, BC V8W 3P5, Canada; Stockwell, T., Department of Psychology, University of Victoria, P.O. Box 3050 STN CSC, Victoria, BC V8W 3P5, Canada, Center of Addiction Research of British Columbia, University of Victoria, Canada; Barnes, G., Child and Youth Care, University of Victoria, Canada; Holroyd, C.B., Department of Psychology, University of Victoria, P.O. Box 3050 STN CSC, Victoria, BC V8W 3P5, Canada","Recent theories of drug dependence propose that the transition from occasional recreational substance use to harmful use and dependence results from the impact of disrupted midbrain dopamine signals for reinforcement learning on frontal brain areas that implement cognitive control and decision-making.We investigated this hypothesis in humans using electrophysiological and behavioral measures believed to assay the integrity of midbrain dopamine system and its neural targets. Our investigation revealed two groups of dependent individuals, one characterized by disrupted dopamine-dependent reward learning and the other by disrupted error learning associated with depression-proneness. These results highlight important neurobiological and behavioral differences between two classes of dependent users that can inform the development of individually tailored treatment programs. © 2010 The Authors, Addiction Biology © 2010 Society for the Study of Addiction.",Addiction; Cognitive control; Event-related brain potentials; Feedback error-related negativity; Midbrain dopamine system; Reinforcement learning,Article,,Scopus,2-s2.0-79959823204
SCOPUS,"Denrell J., Le Mens G.",Seeking positive experiences can produce illusory correlations,2011,Cognition,119,3,,313,324,,8,10.1016/j.cognition.2011.01.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953324111&doi=10.1016%2fj.cognition.2011.01.007&partnerID=40&md5=0ff08a439f19bac8fc23297259212fb6,"Universitat Pompeu Fabra, Department of Economics and Business, Ramon Trias Fargas 25-27, 08005 Barcelona, Spain; Saïd Business School, Park End Street, Oxford Oxfordshire OX1 1HP, United Kingdom","Denrell, J., Saïd Business School, Park End Street, Oxford Oxfordshire OX1 1HP, United Kingdom; Le Mens, G., Universitat Pompeu Fabra, Department of Economics and Business, Ramon Trias Fargas 25-27, 08005 Barcelona, Spain","Individuals tend to select again alternatives about which they have positive impressions and to avoid alternatives about which they have negative impressions. Here we show how this sequential sampling feature of the information acquisition process leads to the emergence of an illusory correlation between estimates of the attributes of multi-attribute alternatives. The sign of the illusory correlation depends on how the decision maker combines estimates in making her sampling decisions. A positive illusory correlation emerges when evaluations are compensatory or disjunctive and a negative illusory correlation can emerge when evaluations are conjunctive. Our theory provides an alternative explanation for illusory correlations that does not rely on biased information processing nor selective attention to different pieces of information. It provides a new perspective on several well-established empirical phenomena such as the 'Halo' effect in personality perception, the relation between proximity and attitudes, and the in-group out-group bias in stereotype formation. © 2011 Elsevier B.V.",Adaptive behavior; Halo effect; Reinforcement learning; Sampling; Stereotype formation,Article,,Scopus,2-s2.0-79953324111
SCOPUS,"Kasanova Z., Waltz J.A., Strauss G.P., Frank M.J., Gold J.M.",Optimizing vs. Matching: Response Strategy in a Probabilistic Learning Task is associated with Negative Symptoms of Schizophrenia,2011,Schizophrenia Research,127,1-3,,215,222,,10,10.1016/j.schres.2010.12.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952316333&doi=10.1016%2fj.schres.2010.12.003&partnerID=40&md5=f0df20a2c8bca795481cdcbab687e5bb,"Department of Psychiatry, Maryland Psychiatric Research Center, University of Maryland School of Medicine, PO Box 21247, Baltimore, MD 21228, United States; Department of Cognitive, Linguistic and Psychological Sciences, Providence, RI, United States; Department of Psychiatry and Human Behavior, Brown University, Providence, RI, United States","Kasanova, Z., Department of Psychiatry, Maryland Psychiatric Research Center, University of Maryland School of Medicine, PO Box 21247, Baltimore, MD 21228, United States; Waltz, J.A., Department of Psychiatry, Maryland Psychiatric Research Center, University of Maryland School of Medicine, PO Box 21247, Baltimore, MD 21228, United States; Strauss, G.P., Department of Psychiatry, Maryland Psychiatric Research Center, University of Maryland School of Medicine, PO Box 21247, Baltimore, MD 21228, United States; Frank, M.J., Department of Cognitive, Linguistic and Psychological Sciences, Providence, RI, United States, Department of Psychiatry and Human Behavior, Brown University, Providence, RI, United States; Gold, J.M., Department of Psychiatry, Maryland Psychiatric Research Center, University of Maryland School of Medicine, PO Box 21247, Baltimore, MD 21228, United States","Previous research indicates that behavioral performance in simple probability learning tasks can be organized into response strategy classifications that are thought to predict important personal characteristics and individual differences. Typically, relatively small proportion of subjects can be identified as optimizers for effectively exploiting the environment and choosing the more rewarding stimulus nearly all of the time. In contrast, the vast majority of subjects behaves sub-optimally and adopts the matching or super-matching strategy, apportioning their responses in a way that matches or slightly exceeds the probabilities of reinforcement. In the present study, we administered a two-choice probability learning paradigm to 51 individuals with schizophrenia (SZ) and 29 healthy controls (NC) to examine whether there are differences in the proportion of subjects falling into these response strategy classifications, and to determine whether task performance is differentially associated with symptom severity and neuropsychological functioning. Although the sample of SZ patients did not differ from NC in overall rate of learning or end performance, significant clinical differences emerged when patients were divided into optimizing, super-matching and matching subgroups based upon task performance. Patients classified as optimizers, who adopted the most advantageous learning strategy, exhibited higher levels of positive and negative symptoms than their matching and super-matching counterparts. Importantly, when both positive and negative symptoms were considered together, only negative symptom severity was a significant predictor of whether a subject would behave optimally, with each one standard deviation increase in negative symptoms increasing the odds of a patient being an optimizer by as much as 80%. These data provide a rare example of a greater clinical impairment being associated with better behavioral performance. © 2010 Elsevier B.V.",Exploration; Matching; Negative Symptoms; Probability Learning Task; Reinforcement Learning; Schizophrenia,Article,,Scopus,2-s2.0-79952316333
SCOPUS,"Samsudin K., Ahmad F.A., Mashohor S.",A highly interpretable fuzzy rule base using ordinal structure for obstacle avoidance of mobile robot,2011,Applied Soft Computing Journal,11,2,,1631,1637,,37,10.1016/j.asoc.2010.05.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751633864&doi=10.1016%2fj.asoc.2010.05.002&partnerID=40&md5=3eac299086b1245a7062038f727ccc06,"Intelligent System and Robotics Laboratory, Institute of Advanced Technology, Universiti Putra Malaysia, 43400 Serdang, Selangor, Malaysia; Department of Computer and Communication Systems, Faculty of Engineering, Universiti Putra Malaysia, 43400 Serdang, Selangor, Malaysia","Samsudin, K., Intelligent System and Robotics Laboratory, Institute of Advanced Technology, Universiti Putra Malaysia, 43400 Serdang, Selangor, Malaysia; Ahmad, F.A., Department of Computer and Communication Systems, Faculty of Engineering, Universiti Putra Malaysia, 43400 Serdang, Selangor, Malaysia; Mashohor, S., Department of Computer and Communication Systems, Faculty of Engineering, Universiti Putra Malaysia, 43400 Serdang, Selangor, Malaysia","Conventional fuzzy logic controller is applicable when there are only two fuzzy inputs with usually one output. Complexity increases when there are more than one inputs and outputs making the system unrealizable. The ordinal structure model of fuzzy reasoning has an advantage of managing high-dimensional problem with multiple input and output variables ensuring the interpretability of the rule set. This is achieved by giving an associated weight to each rule in the defuzzification process. In this work, a methodology to design an ordinal fuzzy logic controller with application for obstacle avoidance of Khepera mobile robot is presented. The implementation will show that ordinal structure fuzzy is easier to design with highly interpretable rules compared to conventional fuzzy controller. In order to achieve high accuracy, a specially tailored Genetic Algorithm (GA) approach for reinforcement learning has been proposed to optimize the ordinal structure fuzzy controller. Simulation results demonstrated improved obstacle avoidance performance in comparison with conventional fuzzy controllers. Comparison of direct and incremental GA for optimization of the controller is also presented. © 2010 Elsevier B.V. All rights reserved.",Fuzzy logic; Genetic algorithm; Mobile robot; Obstacle avoidance; Ordinal structure,Conference Paper,,Scopus,2-s2.0-78751633864
SCOPUS,"de Jong S., Tuyls K.",Human-inspired computational fairness,2011,Autonomous Agents and Multi-Agent Systems,22,1,,103,126,,11,10.1007/s10458-010-9122-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651242484&doi=10.1007%2fs10458-010-9122-9&partnerID=40&md5=6377a01ddb7d901ab98beeefaa7b1365,"Computational Modelling Lab, Vrije Universiteit Brussel, Pleinlaan 2, 1050 Brussels, Belgium; Department of Knowledge Engineering, Maastricht University, PO Box 616, 6200 MD Maastricht, Netherlands","de Jong, S., Computational Modelling Lab, Vrije Universiteit Brussel, Pleinlaan 2, 1050 Brussels, Belgium, Department of Knowledge Engineering, Maastricht University, PO Box 616, 6200 MD Maastricht, Netherlands; Tuyls, K., Department of Knowledge Engineering, Maastricht University, PO Box 616, 6200 MD Maastricht, Netherlands","In many common tasks for multi-agent systems, assuming individually rational agents leads to inferior solutions. Numerous researchers found that fairness needs to be considered in addition to individual reward, and proposed valuable computational models of fairness. In this paper, we argue that there are two opportunities for improvement. First, existing models are not specifically tailored to addressing a class of tasks named social dilemmas, even though such tasks are quite common in the context of multi-agent systems. Second, the models generally rely on the assumption that all agents will and can adhere to these models, which is not always the case. We therefore present a novel computational model, i.e., human-inspired computational fairness. Upon being confronted with social dilemmas, humans may apply a number of fully decentralized sanctioning mechanisms to ensure that optimal, fair solutions emerge, even though some participants may be deciding purely on the basis of individual reward. In this paper, we show how these human mechanisms may be computationally modelled, such that fair and optimal solutions emerge from agents being confronted with social dilemmas. © 2010 The Author(s).",Fairness; Human-inspired mechanisms; Multi-agent systems; Reinforcement learning; Social dilemmas,Article,,Scopus,2-s2.0-78651242484
SCOPUS,"Chi C.-Y., Tsai R.T.-H., Lai J.-Y., Hsu J.Y.-J.",A reinforcement learning approach to emotion-based automatic playlist generation,2010,"Proceedings - International Conference on Technologies and Applications of Artificial Intelligence, TAAI 2010",,,5695433,60,65,,5,10.1109/TAAI.2010.21,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951754440&doi=10.1109%2fTAAI.2010.21&partnerID=40&md5=d42bfd1648749e0374aab450cb5ccfcd,"Dept. of CSIE, National Taiwan University, Taipei, Taiwan; Dept. of CSE, Yuan Ze University, Chung-Li, Taiwan","Chi, C.-Y., Dept. of CSIE, National Taiwan University, Taipei, Taiwan; Tsai, R.T.-H., Dept. of CSE, Yuan Ze University, Chung-Li, Taiwan; Lai, J.-Y., Dept. of CSE, Yuan Ze University, Chung-Li, Taiwan; Hsu, J.Y.-J., Dept. of CSIE, National Taiwan University, Taipei, Taiwan","A novel trend emerged in music exploration is to organize and search songs according to their emotions. However, research on automatic playlist generation (APG) primarily focuses on metadata and audio similarity. Mainstream solutions view APG as a static problem. This paper argues that the APG problem is better modeled as a continuous optimization problem, and proposes an adaptive preference model for personalized APG based on emotions. The main idea is to collect a user's behavior in music playing, e.g., rating, skipping and replaying, as immediate feedback in learning the user's preferences for music emotion within a playlist. Reinforcement learning is adopted to learn the user's current preferences, which are used to generate personalized playlists. Learning parameters are tuned by simulation of two hypothetical users. A two-month user study is conducted to evaluate the APG solutions. The results show that the proposed approach reduces the Miss Ratio by 10% in comparison with the baseline approach. © 2010 IEEE.",Automatic playlist generation; Reinforcement learning; Song emotion,Conference Paper,,Scopus,2-s2.0-79951754440
SCOPUS,"Gasic M., Jurčíček F., Keizer S., Mairesse F., Thomson B., Yu K., Young S.",Gaussian processes for fast policy optimisation of POMDP-based dialogue managers,2010,Proceedings of the SIGDIAL 2010 Conference: 11th Annual Meeting of the Special Interest Group onDiscourse and Dialogue,,,,201,204,,33,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857755225&partnerID=40&md5=53959c32cfe4d3fe63d3cc1543d0da65,"Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom","Gasic, M., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom; Jurčíček, F., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom; Keizer, S., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom; Mairesse, F., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom; Thomson, B., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom; Yu, K., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom; Young, S., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB2 1PZ, United Kingdom","Modelling dialogue as a Partially Observable Markov Decision Process (POMDP) enables a dialogue policy robust to speech understanding errors to be learnt. However, a major challenge in POMDP policy learning is to maintain tractability, so the use of approximation is inevitable. We propose applying Gaussian Processes in Reinforcement learning of optimal POMDP dialogue policies, in order (1) to make the learning process faster and (2) to obtain an estimate of the uncertainty of the approximation. We first demonstrate the idea on a simple voice mail dialogue task and then apply this method to a real-world tourist information dialogue task. © 2010 Association for Computational Linguistics.",,Conference Paper,,Scopus,2-s2.0-84857755225
SCOPUS,"Samin R.E., Khamis A., Kasmani R.Md., Isa S.",Forecasting sunspot numbers with Recurrent Neural Networks (RNN) using 'sunspot neural forecaster' system,2010,"Proceedings - 2010 2nd International Conference on Advances in Computing, Control and Telecommunication Technologies, ACT 2010",,,5675853,10,14,,,10.1109/ACT.2010.50,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751638681&doi=10.1109%2fACT.2010.50&partnerID=40&md5=ce015317eb2780ecdf92030923ec3af2,"Faculty of Electrical and Electronics Engineering, Universiti Malaysia Pahang, Kuantan, Pahang, Malaysia; Faculty of Science, Arts and Heritage, Universiti Tun Hussein Onn Malaysia, Batu Pahat, Johor, Malaysia","Samin, R.E., Faculty of Electrical and Electronics Engineering, Universiti Malaysia Pahang, Kuantan, Pahang, Malaysia; Khamis, A., Faculty of Science, Arts and Heritage, Universiti Tun Hussein Onn Malaysia, Batu Pahat, Johor, Malaysia; Kasmani, R.Md., Faculty of Science, Arts and Heritage, Universiti Tun Hussein Onn Malaysia, Batu Pahat, Johor, Malaysia; Isa, S., Faculty of Science, Arts and Heritage, Universiti Tun Hussein Onn Malaysia, Batu Pahat, Johor, Malaysia","This paper presents the investigations of forecasting performance of different type of Recurrent Neural Networks (RNN) in forecasting the sunspot numbers. Recurrent Neural Network will be used in this investigation by using different learning algorithms, sunspot data models and RNN transfer functions. Simulations are done using Matlab 7 where customized Graphic User Interface (GUI) called 'Sunspot Neural Forecaster' have been developed for analysis. A complete analysis for different learning algorithms, sunspot data models and RNN transfer functions are examined in terms of Mean Square Error (MSE) and correlation analysis. Finally, the best optimized RNN parameters will be used to forecast the sunspot numbers. © 2010 IEEE.",Mean square error (mse); Recurrent neural networks (rnn); Sunspot numbers,Conference Paper,,Scopus,2-s2.0-78751638681
SCOPUS,"Hochman G., Yechiam E., Bechara A.",Recency gets larger as lesions move from anterior to posterior locations within the ventromedial prefrontal cortex,2010,Behavioural Brain Research,213,1,,27,34,,8,10.1016/j.bbr.2010.04.023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953362266&doi=10.1016%2fj.bbr.2010.04.023&partnerID=40&md5=aa7b60b245add046b6257561846611c7,"Max Wertheimer Minerva Center for Cognitive Studies, Faculty of Industrial Engineering and Management, Technion - Israel Institute of Technology, Haifa 32000, Israel; USC Neuroscience, University of Southern California, Los Angeles, CA 90033, United States","Hochman, G., Max Wertheimer Minerva Center for Cognitive Studies, Faculty of Industrial Engineering and Management, Technion - Israel Institute of Technology, Haifa 32000, Israel; Yechiam, E., Max Wertheimer Minerva Center for Cognitive Studies, Faculty of Industrial Engineering and Management, Technion - Israel Institute of Technology, Haifa 32000, Israel; Bechara, A., USC Neuroscience, University of Southern California, Los Angeles, CA 90033, United States","In the past two decades neuroimaging research has substantiated the important role of the prefrontal cortex (PFC) in decision-making. In the current study, we use the complementary lesion based approach to deepen our knowledge concerning the specific cognitive mechanisms modulated by prefrontal activity. Specifically, we assessed the brain substrates implicated in two decision making dimensions in a sample of prefrontal cortex patients: (a) the tendency to differently weigh recent compared to past experience; and (b) the tendency to differently weigh gains compared to losses. The participants performed the Iowa Gambling Task, a complex experience-based decision-making task [3], which was analyzed with a formal cognitive model (the Expectancy-Valance model; [12]). The results indicated that decisions become influenced by more recent, as opposed to older, events when the damage reaches the posterior sectors of the ventromedial prefrontal cortex (VMPC). Furthermore, the degree of this recency deficit was related to the size of the lesion. These results suggest that the posterior area of the prefrontal cortex directly modulates the capacity to use time-delayed information. In contrast, we did not find similar modulation for the sensitivity to gains versus losses. © 2010 Elsevier B.V.",Cognitive models; Recency; Reinforcement learning; Risk taking,Article,,Scopus,2-s2.0-77953362266
SCOPUS,"Wright R., Gemelli N.",Adaptive state space abstraction using neuroevolution,2010,Communications in Computer and Information Science,67 CCIS,,,84,96,,,10.1007/978-3-642-11819-7_7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957571304&doi=10.1007%2f978-3-642-11819-7_7&partnerID=40&md5=3745801dbd637a95644b184783ea7b30,"Air Force Research Laboratory Information Directorate, 525 Brooks Rd., Rome, NY 13441, United States","Wright, R., Air Force Research Laboratory Information Directorate, 525 Brooks Rd., Rome, NY 13441, United States; Gemelli, N., Air Force Research Laboratory Information Directorate, 525 Brooks Rd., Rome, NY 13441, United States","In this paper, we present a new machine learning algorithm, RL-SANE, which uses a combination of neuroevolution (NE) and traditional reinforcement learning (RL) techniques to improve learning performance. RL-SANE is an innovative combination of the neuroevolutionary algorithm NEAT[9] and the RL algorithm Sarsa(λ)[12]. It uses the special ability of NEAT to generate and train customized neural networks that provide a means for reducing the size of the state space through state aggregation. Reducing the size of the state space through aggregation enables Sarsa(λ) to be applied to much more difficult problems than standard tabular based approaches. Previous similar work in this area, such as in Whiteson and Stone [15] and Stanley and Miikkulainen [10], have shown positive results. This paper gives a brief overview of neuroevolutionary methods, introduces the RL-SANE algorithm, presents a comparative analysis of RL-SANE to other neuroevolutionary algorithms, and concludes with a discussion of enhancements that need to be made to RL-SANE. © 2010 Springer-Verlag Berlin Heidelberg.",Evolutionary Algorithms; NeuroEvolution; Reinforcement Learning; State Abstraction,Conference Paper,,Scopus,2-s2.0-77957571304
SCOPUS,"Levine S., Krähenbühl P., Thrun S., Koltun V.",Gesture controllers,2010,ACM Transactions on Graphics,29,4,124,,,,37,10.1145/1778765.1778861,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956388261&doi=10.1145%2f1778765.1778861&partnerID=40&md5=c5626a3962e7e2f070413dd84ee11ad0,"Stanford University, United States","Levine, S., Stanford University, United States; Krähenbühl, P., Stanford University, United States; Thrun, S., Stanford University, United States; Koltun, V., Stanford University, United States","We introduce gesture controllers, a method for animating the body language of avatars engaged in live spoken conversation. A gesture controller is an optimal-policy controller that schedules gesture animations in real time based on acoustic features in the user's speech. The controller consists of an inference layer, which infers a distribution over a set of hidden states from the speech signal, and a control layer, which selects the optimal motion based on the inferred state distribution. The inference layer, consisting of a specialized conditional random field, learns the hidden structure in body language style and associates it with acoustic features in speech. The control layer uses reinforcement learning to construct an optimal policy for selecting motion clips from a distribution over the learned hidden states. The modularity of the proposed method allows customization of a character's gesture repertoire, animation of non-human characters, and the use of additional inputs such as speech recognition or direct user control. © 2010 ACM.",Data-driven animation; Gesture synthesis; Human animation; Nonverbal behavior generation; Optimal control,Article,,Scopus,2-s2.0-77956388261
SCOPUS,"Kardan A.A., Speily O.R.B.",Smart lifelong learning system based on Q-learning,2010,ITNG2010 - 7th International Conference on Information Technology: New Generations,,,5501486,1086,1091,,1,10.1109/ITNG.2010.140,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955296677&doi=10.1109%2fITNG.2010.140&partnerID=40&md5=2c5363d1f7fd350f4a3d1dc735613424,"Advanced E-Learning Technologies Lab., AmirKabir University of Technology, Tehran, Iran","Kardan, A.A., Advanced E-Learning Technologies Lab., AmirKabir University of Technology, Tehran, Iran; Speily, O.R.B., Advanced E-Learning Technologies Lab., AmirKabir University of Technology, Tehran, Iran","The majority of current web-based learning systems are closed learning environments where courses and learning materials are fixed and the only dynamic aspect is the organization of the material that can be adapted to allow a relatively individualized learning environment. In this paper, we propose an evolving web-based learning system which can adapt itself to its learners. More specifically, the novelty with respect to the system lies in its ability to find relevant content on the web, and its ability to personalize and adapt this content based on the system's observation of its learners and the accumulated ratings given by the learners. Hence, although learners do not have direct interaction with the open Web, the system can retrieve relevant information related to them and their situated learning characteristics. Lifelong learning scenarios have particular differences in their need for personalized recommendations that make not possible reusing existing general approaches of recommender systems. The paper describes those challenges and we propose a hybrid technique based on machine learning to recognize learner preferences and predict theirs required contents with high accuracy. © 2010 IEEE.",Learning promotion; Lifelong learning; Machine learning; Q learning; Recommender systems; Reinforcement learning,Conference Paper,,Scopus,2-s2.0-77955296677
SCOPUS,"Matignon L., Laurent G.J., Le Fort-Piat N., Chapuis Y.-A.",Designing decentralized controllers for distributed-air-jet mems-based micromanipulators by reinforcement learning,2010,Journal of Intelligent and Robotic Systems: Theory and Applications,59,2,,145,166,,8,10.1007/s10846-010-9396-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955654600&doi=10.1007%2fs10846-010-9396-9&partnerID=40&md5=96e32a868d4dae09fd21ae5869018a42,"FEMTO-ST/UFC-ENSMM-UTBM-CNRS, Université de Franche-Comté, Besançon, France; InESS/ULP-CNRS, Université Louis Pasteur, Strasbourg, France","Matignon, L., FEMTO-ST/UFC-ENSMM-UTBM-CNRS, Université de Franche-Comté, Besançon, France; Laurent, G.J., FEMTO-ST/UFC-ENSMM-UTBM-CNRS, Université de Franche-Comté, Besançon, France; Le Fort-Piat, N., FEMTO-ST/UFC-ENSMM-UTBM-CNRS, Université de Franche-Comté, Besançon, France; Chapuis, Y.-A., InESS/ULP-CNRS, Université Louis Pasteur, Strasbourg, France","Distributed-air-jet MEMS-based systems have been proposed to manipulate small parts with high velocities and without any friction problems. The control of such distributed systems is very challenging and usual approaches for contact arrayed system don't produce satisfactory results. In this paper, we investigate reinforcement learning control approaches in order to position and convey an object. Reinforcement learning is a popular approach to find controllers that are tailored exactly to the system without any prior model. We show how to apply reinforcement learning in a decentralized perspective and in order to address the global-local trade-off. The simulation results demonstrate that the reinforcement learning method is a promising way to design control laws for such distributed systems. © 2010 Springer Science+Business Media B.V.",Decentralized control; Distributed control; MEMS-based actuator array; Reinforcement learning; Smart surface,Article,,Scopus,2-s2.0-77955654600
SCOPUS,"Levine S., Krähenbühl P., Thrun S., Koltun V.",Gesture controllers,2010,"ACM SIGGRAPH 2010 Papers, SIGGRAPH 2010",,,124,,,,9,10.1145/1778765.1778861,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939181995&doi=10.1145%2f1778765.1778861&partnerID=40&md5=258e4b91511e1e3be0b5a63afecbbcdf,"Stanford University, United States","Levine, S., Stanford University, United States; Krähenbühl, P., Stanford University, United States; Thrun, S., Stanford University, United States; Koltun, V., Stanford University, United States","We introduce gesture controllers, a method for animating the body language of avatars engaged in live spoken conversation. A gesture controller is an optimal-policy controller that schedules gesture animations in real time based on acoustic features in the user's speech. The controller consists of an inference layer, which infers a distribution over a set of hidden states from the speech signal, and a control layer, which selects the optimal motion based on the inferred state distribution. The inference layer, consisting of a specialized conditional random field, learns the hidden structure in body language style and associates it with acoustic features in speech. The control layer uses reinforcement learning to construct an optimal policy for selecting motion clips from a distribution over the learned hidden states. The modularity of the proposed method allows customization of a character's gesture repertoire, animation of non-human characters, and the use of additional inputs such as speech recognition or direct user control. © 2010 ACM.",Data-driven animation; Gesture synthesis; Human animation; Nonverbal behavior generation; Optimal control,Conference Paper,,Scopus,2-s2.0-84939181995
SCOPUS,"Li L., Chu W., Langford J., Schapire R.E.",A contextual-bandit approach to personalized news article recommendation,2010,"Proceedings of the 19th International Conference on World Wide Web, WWW '10",,,,661,670,,350,10.1145/1772690.1772758,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954641643&doi=10.1145%2f1772690.1772758&partnerID=40&md5=0deba3e304c09a3db001bfeeca14cd3f,"Yahoo Labs., United States; Dept. of Computer Science, Princeton University, United States","Li, L., Yahoo Labs., United States; Chu, W., Yahoo Labs., United States; Langford, J., Yahoo Labs., United States; Schapire, R.E., Dept. of Computer Science, Princeton University, United States","Personalized web services strive to adapt their services (advertisements, news articles, etc.) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation. In this work, we model personalized recommendation of news articles as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks. The contributions of this work are three-fold. First, we propose a new, general contextual bandit algorithm that is computationally efficient and well motivated from learning theory. Second, we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic. Finally, using this offline evaluation method, we successfully applied our new algorithm to a Yahoo Front Page Today Module dataset containing over 33 million events. Results showed a 12.5% click lift compared to a standard context-free bandit algorithm, and the advantage becomes even greater when data gets more scarce. © 2010 International World Wide Web Conference Committee (IW3C2).",contextual bandit; exploration/exploitation dilemma; personalization; recommender systems; web service,Conference Paper,,Scopus,2-s2.0-77954641643
SCOPUS,"Chi M., Vanlehn K., Litman D., Jordan P.",Inducing effective pedagogical strategies using learning context features,2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),6075 LNCS,,,147,158,,10,10.1007/978-3-642-13470-8_15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954602915&doi=10.1007%2f978-3-642-13470-8_15&partnerID=40&md5=7383a8c4df03d5465e9cffbeffaa8641,"Machine Learning Department, Carnegie Mellon University, PA 15213, United States; School of Computing and Informatics, Arizona State University, AZ 85287, United States; Department of Computer Science, University of Pittsburgh, PA 15260, United States; Department of Biomedical Informatics, University of Pittsburgh, Pittsburgh, PA 15260, United States","Chi, M., Machine Learning Department, Carnegie Mellon University, PA 15213, United States; Vanlehn, K., School of Computing and Informatics, Arizona State University, AZ 85287, United States; Litman, D., Department of Computer Science, University of Pittsburgh, PA 15260, United States; Jordan, P., Department of Biomedical Informatics, University of Pittsburgh, Pittsburgh, PA 15260, United States","Effective pedagogical strategies are important for e-learning environments. While it is assumed that an effective learning environment should craft and adapt its actions to the user's needs, it is often not clear how to do so. In this paper, we used a Natural Language Tutoring System named Cordillera and applied Reinforcement Learning (RL) to induce pedagogical strategies directly from pre-existing human user interaction corpora. 50 features were explored to model the learning context. Of these features, domain-oriented and system performance features were the most influential while user performance and background features were rarely selected. The induced pedagogical strategies were then evaluated on real users and results were compared with pre-existing human user interaction corpora. Overall, our results show that RL is a feasible approach to induce effective, adaptive pedagogical strategies by using a relatively small training corpus. Moreover, we believe that our approach can be used to develop other adaptive and personalized learning environments. © 2010 Springer-Verlag.",,Conference Paper,,Scopus,2-s2.0-77954602915
SCOPUS,"Hu X., Sun C., Zhang B.",Design of recurrent neural networks for solving constrained least absolute deviation problems,2010,IEEE Transactions on Neural Networks,21,7,5487384,1073,1086,,18,10.1109/TNN.2010.2048123,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954563179&doi=10.1109%2fTNN.2010.2048123&partnerID=40&md5=89b6a9456374fa8c76f6ffd6d0839616,"State Key Laboratory of Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China; School of Automation, Southeast University, Nanjing 210096, China","Hu, X., State Key Laboratory of Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China; Sun, C., State Key Laboratory of Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China; Zhang, B., School of Automation, Southeast University, Nanjing 210096, China","Recurrent neural networks for solving constrained least absolute deviation (LAD) problems or L1-norm optimization problems have attracted much interest in recent years. But so far most neural networks can only deal with some special linear constraints efficiently. In this paper, two neural networks are proposed for solving LAD problems with various linear constraints including equality, two-sided inequality and bound constraints. When tailored to solve some special cases of LAD problems in which not all types of constraints are present, the two networks can yield simpler architectures than most existing ones in the literature. In particular, for solving problems with both equality and one-sided inequality constraints, another network is invented. All of the networks proposed in this paper are rigorously shown to be capable of solving the corresponding problems. The different networks designed for solving the same types of problems possess the same structural complexity, which is due to the fact these architectures share the same computing blocks and only differ in connections between some blocks. By this means, some flexibility for circuits realization is provided. Numerical simulations are carried out to illustrate the theoretical results and compare the convergence rates of the networks. © 2006 IEEE.",L1-norm optimization; least absolute deviation (LAD); minimax optimization; recurrent neural network (RNN); stability analysis,Article,,Scopus,2-s2.0-77954563179
SCOPUS,"Moxley E., Mei T., Manjunath B.S.",Video annotation through search and graph reinforcement mining,2010,IEEE Transactions on Multimedia,12,3,5398917,184,193,,46,10.1109/TMM.2010.2041101,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949676020&doi=10.1109%2fTMM.2010.2041101&partnerID=40&md5=40dc9c7359b3a167e775851aea9ef93b,"University of California at Santa Barbara, Santa Barbara, CA 93106, United States; Microsoft Research Asia, Beijing 100190, China","Moxley, E., University of California at Santa Barbara, Santa Barbara, CA 93106, United States; Mei, T., Microsoft Research Asia, Beijing 100190, China; Manjunath, B.S., University of California at Santa Barbara, Santa Barbara, CA 93106, United States","Unlimited vocabulary annotation of multimedia documents remains elusive despite progress solving the problem in the case of a small, fixed lexicon. Taking advantage of the repetitive nature of modern information and online media databases with independent annotation instances, we present an approach to automatically annotate multimedia documents that uses mining techniques to discover new annotations from similar documents and to filter existing incorrect annotations. The annotation set is not limited to words that have training data or for which models have been created. It is limited only by the words in the collective annotation vocabulary of all the database documents. A graph reinforcement method driven by a particular modality (e.g., visual) is used to determine the contribution of a similar document to the annotation target. The graph supplies possible annotations of a different modality (e.g., text) that can be mined for annotations of the target. Experiments are performed using videos crawled from YouTube. A customized precision-recall metric shows that the annotations obtained using the proposed method are superior to those originally existing for the document. These extended, filtered tags are also superior to a state-of-the-art semi-supervised technique for graph reinforcement learning on the initial user-supplied annotations. © 2006 IEEE.",Data mining; Graph theory; Video annotation; Video content analysis,Article,,Scopus,2-s2.0-77949676020
SCOPUS,[No author name available],"8th International Conference on Simulated Evolution and Learning, SEAL 2010",2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),6457 LNCS,,,,,715,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037650690&partnerID=40&md5=59293a3bc7165cde3074960b3a97deff,,,The proceedings contain 80 papers. The special focus in this conference is on Simulated Evolution and Learning. The topics include: On the Flexible Applied Boundary and Support Conditions of Compliant Mechanisms Using Customized Evolutionary Algorithm; intensification Strategies for Extremal Optimisation; comparing Two Constraint Handling Techniques in a Binary-Coded Genetic Algorithm for Optimization Problems; evolving Stories: Tree Adjoining Grammar Guided Genetic Programming for Complex Plot Generation; Improving Differential Evolution by Altering Steps in EC; a Dynamic Island-Based Genetic Algorithms Framework; solving the Optimal Coverage Problem in Wireless Sensor Networks Using Evolutionary Computation Algorithms; a Comparative Study of Different Variants of Genetic Algorithms for Constrained Optimization; Evolutionary FCMAC-BYY Applied to Stream Data Analysis; Optimal μ-Distributions for the Hypervolume Indicator for Problems with Linear Bi-objective Fronts: Exact and Exhaustive Results; UNIFAC Group Interaction Prediction for Ionic Liquid-Thiophene Based Systems Using Genetic Algorithm; HIER-HEIR: An Evolutionary System with Hierarchical Representation and Operators Applied to Fashion Design; a Population Diversity-Oriented Gene Expression Programming for Function Finding; evolutionary Optimization of Catalysts Assisted by Neural-Network Learning; dominance-Based Pareto-Surrogate for Multi-Objective Optimization; learning Cellular Automata Rules for Pattern Reconstruction Task; evolving Fuzzy Rules: Evaluation of a New Approach; a Niched Genetic Programming Algorithm for Classification Rules Discovery in Geographic Databases; Artificial Neural Network Modeling for Estimating the Depth of Penetration and Weld Bead Width from the Infra Red Thermal Image of the Weld Pool during A-TIG Welding; swarm Reinforcement Learning Method Based on an Actor-Critic Method; a Parallel Algorithm for Solving Large Convex Minimax Problems; car Setup Optimisation.,,Conference Review,,Scopus,2-s2.0-85037650690
SCOPUS,"Moradi S., Mohsenian-Rad A.H., Wong V.W.S.",An RL-based scheduling algorithm for video traffic in high-rate wireless personal area networks,2009,Computer Networks,53,18,,2997,3010,,3,10.1016/j.comnet.2009.07.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449529532&doi=10.1016%2fj.comnet.2009.07.012&partnerID=40&md5=f2f8311900ac740326aa5a6c4a386bab,"Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, Canada","Moradi, S., Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, Canada; Mohsenian-Rad, A.H., Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, Canada; Wong, V.W.S., Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, Canada","The emerging high-rate wireless personal area network (WPAN) technology is capable of supporting high-speed and high-quality real-time multimedia applications. In particular, video streams are deemed to be a dominant traffic type, and require quality of service (QoS) support. However, in the current IEEE 802.15.3 standard for MAC (media access control) of high-rate WPANs, the implementation details of some key issues such as scheduling and QoS provisioning have not been addressed. In this paper, we first propose a Markov decision process (MDP) model for optimal scheduling for video flows in high-rate WPANs. Using this model, we also propose a scheduler that incorporates compact state space representation, function approximation, and reinforcement learning (RL). Simulation results show that our proposed RL scheduler achieves nearly optimal performance and performs better than F-SRPT, EDD + SRPT, and PAP scheduling algorithms in terms of a lower decoding failure rate. © 2009 Elsevier B.V. All rights reserved.",Markov decision process (MDP); QoS; Reinforcement learning; Scheduling; Ultra-wide band (UWB); Wireless personal area networks,Article,,Scopus,2-s2.0-70449529532
SCOPUS,"Matignon L., Laurent G.J., Le Fort-Piat N.",Design of semi-decentralized control laws for distributed-air-jet micromanipulators by reinforcement learning,2009,"2009 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2009",,,5353902,3277,3283,,3,10.1109/IROS.2009.5353902,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249101087&doi=10.1109%2fIROS.2009.5353902&partnerID=40&md5=3b64f2f4f65423868108d674f8c95fa8,"FEMTO-ST/UFC-ENSMM-UTBM-CNRS, Université de Franche-Comté, Besançon, France","Matignon, L., FEMTO-ST/UFC-ENSMM-UTBM-CNRS, Université de Franche-Comté, Besançon, France; Laurent, G.J., FEMTO-ST/UFC-ENSMM-UTBM-CNRS, Université de Franche-Comté, Besançon, France; Le Fort-Piat, N., FEMTO-ST/UFC-ENSMM-UTBM-CNRS, Université de Franche-Comté, Besançon, France","Recently, a great deal of interest has been developed in learning in multi-agent systems to achieve decentralized control. Machine learning is a popular approach to find controllers that are tailored exactly to the system without any prior model. In this paper, we propose a semi-decentralized reinforcement learning control approach in order to position and convey an object on a contact-free MEMS-based distributed-manipulation system. The experimental results validate the semi-decentralized reinforcement learning method as a way to design control laws for such distributed systems. © 2009 IEEE.",,Conference Paper,,Scopus,2-s2.0-76249101087
SCOPUS,Gaweda A.E.,Machine learning in personalized anemia treatment,2009,"Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques",,,,265,276,,,10.4018/978-1-60566-766-9.ch012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898381465&doi=10.4018%2f978-1-60566-766-9.ch012&partnerID=40&md5=25913f90e423c1b3501bf0b18d7056c2,"Department of Medicine, Division of Nephrology, University of Louisville, United States","Gaweda, A.E., Department of Medicine, Division of Nephrology, University of Louisville, United States","This chapter presents application of reinforcement learning to drug dosing personalization in treatment of chronic conditions. Reinforcement learning is a machine learning paradigm that mimics the trialand-error skill acquisition typical for humans and animals. In treatment of chronic illnesses, finding the optimal dose amount for an individual is also a process that is usually based on trial-and-error. In this chapter, the author focuses on the challenge of personalized anemia treatment with recombinant human erythropoietin. The author demonstrates the application of a standard reinforcement learning method, called Q-learning, to guide the physician in selecting the optimal erythropoietin dose. The author further addresses the issue of random exploration in Q-learning from the drug dosing perspective and proposes a ""smart"" exploration method. Finally, the author performs computer simulations to compare the outcomes from reinforcement learning-based anemia treatment to those achieved by a standard dosing protocol used at a dialysis unit. © 2010, IGI Global.",,Book Chapter,,Scopus,2-s2.0-84898381465
SCOPUS,"Iima H., Kuroe Y.",Swarm reinforcement learning algorithm based on particle swarm optimization whose personal bests have lifespans,2009,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),5864 LNCS,PART 2,,169,178,,1,10.1007/978-3-642-10684-2_19,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249096804&doi=10.1007%2f978-3-642-10684-2_19&partnerID=40&md5=ddde3d9d2f0fa887419a160c5928a5d5,"Kyoto Institute of Technology, Matsugasaki, Sakyo-ku, Kyoto, Japan","Iima, H., Kyoto Institute of Technology, Matsugasaki, Sakyo-ku, Kyoto, Japan; Kuroe, Y., Kyoto Institute of Technology, Matsugasaki, Sakyo-ku, Kyoto, Japan","We recently proposed a swarm reinforcement learning algorithm based on particle swarm optimization (PSO) in order to find optimal policies rapidly. In this algorithm, multiple agents are prepared, and they learn not only by individual learning but also by an update procedure of PSO. In this procedure, state-action values are updated based on the personal best and the global best which are found by the agents so far. In this paper, we direct our attention to a problem that overvaluing personal bests brings inferior learning performance. In order not to update the state-action values based on the overvalued personal best, we propose a swarm reinforcement learning algorithm based on PSO in which the personal best of each agent has a lifespan. © 2009 Springer-Verlag Berlin Heidelberg.",Particle swarm optimization; Reinforcement learning; Swarm intelligence,Conference Paper,,Scopus,2-s2.0-76249096804
SCOPUS,"Mirea A.-M., Preda M.C.",Adaptive learning based on exercises fitness degree,2009,"Proceedings - 2009 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Workshops, WI-IAT Workshops 2009",3,,5284965,215,218,,,10.1109/WI-IAT.2009.266,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856816701&doi=10.1109%2fWI-IAT.2009.266&partnerID=40&md5=ecb451fadc7bba4c992e8b0c2120af7d,"Department of Computer Science, University of Craiova, Craiova, Romania","Mirea, A.-M., Department of Computer Science, University of Craiova, Craiova, Romania; Preda, M.C., Department of Computer Science, University of Craiova, Craiova, Romania","The paper considers the e-learning systems that provide personalized content to their users and that permanently adapt to the evolution of the users during their learning stages. Such systems help the students to consolidate their knowledge faster than other methods. The main contribution is the proposal of a mathematical model of an adaptive learning system with the mentioned characteristics. The model involves a multi step process where, at each stage, the performances of the student are measured and the system is adapting accordingly. © 2009 IEEE.",Adaptive control; Adaptive learning environments; Learning systems; Personalized learning; Reinforcement learning,Conference Paper,,Scopus,2-s2.0-84856816701
SCOPUS,"Choi J.J., Laibson D., Madrian B.C., Metrick A.",Reinforcement learning and savings behavior,2009,Journal of Finance,64,6,,2515,2534,,61,10.1111/j.1540-6261.2009.01509.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72249109179&doi=10.1111%2fj.1540-6261.2009.01509.x&partnerID=40&md5=4f2de206966b9a672be7e0bfe478ef04,"Yale School of Management and, NBER, United States; Harvard University, Department of Economics, NBER, United States; Harvard Kennedy School, NBER, United States; Yale School of Management, NBER, United States","Choi, J.J., Yale School of Management and, NBER, United States; Laibson, D., Harvard University, Department of Economics, NBER, United States; Madrian, B.C., Harvard Kennedy School, NBER, United States; Metrick, A., Yale School of Management, NBER, United States","We show that individual investors over-extrapolate from their personal experience when making savings decisions. Investors who experience particularly rewarding outcomes from 401(k) saving - a high average and/or low variance return - increase their 401(k) savings rate more than investors who have less rewarding experiences. This finding is not driven by aggregate time-series shocks, income effects, rational learning about investing skill, investor fixed effects, or time-varying investor-level heterogeneity that is correlated with portfolio allocations to stock, bond, and cash asset classes. We discuss implications for the equity premium puzzle and interventions aimed at improving household financial outcomes. © 2009 the American Finance Association.",,Article,,Scopus,2-s2.0-72249109179
SCOPUS,"Vučević N., Pérez-Romero J., Sallent O., Agustí R.",Joint radio resource management for LTE-UMTS coexistence scenarios,2009,"IEEE International Symposium on Personal, Indoor and Mobile Radio Communications, PIMRC",,,5450181,,,,6,10.1109/PIMRC.2009.5450181,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952847596&doi=10.1109%2fPIMRC.2009.5450181&partnerID=40&md5=24930b5327d36a1598cbac15b154b15c,"Dept. TSC, Universitat Politècnica de Catalunya (UPC), c/ Jordi Girona 1-3, 08034, Barcelona, Spain","Vučević, N., Dept. TSC, Universitat Politècnica de Catalunya (UPC), c/ Jordi Girona 1-3, 08034, Barcelona, Spain; Pérez-Romero, J., Dept. TSC, Universitat Politècnica de Catalunya (UPC), c/ Jordi Girona 1-3, 08034, Barcelona, Spain; Sallent, O., Dept. TSC, Universitat Politècnica de Catalunya (UPC), c/ Jordi Girona 1-3, 08034, Barcelona, Spain; Agustí, R., Dept. TSC, Universitat Politècnica de Catalunya (UPC), c/ Jordi Girona 1-3, 08034, Barcelona, Spain","Constantly increasing demand for throughput and quality in wireless communication systems leads to continuous research of wise radio resource management, because of the scarce availability of frequency bands and the consequent capacity limitations. In addition, technology evolution is addressed towards spectral efficient techniques that can offer higher data rates. This is the case of OFDMA (Orthogonal Frequency-Division Multiple Access), introduced by 3GPP as the technology for future Long Term Evolution (LTE). However, given the current penetration of legacy technologies such as UMTS (Universal Mobile Telecommunications System), operators will have to deal with the coexistence of multiple Radio Access Technologies (RATs), so that the exploitation of the complementarities between technologies through Joint Radio Resource Management (JRRM) mechanisms will be needed. In this paper we propose a novel dynamic JRRM algorithm for LTE-UMTS coexistence scenarios. The proposed mechanism is based on Reinforcement Learning (RL) which is considered to be a good candidate to achieve cognition in future reconfigurable networks. The proposed solution implements autonomous RL agents in each base station which decide on the allocation of the most suitable RAT to each user. We give a detailed description of the solution and analyze the behavior under various load conditions. We also demonstrate the capability of the algorithm to adjust in dynamic scenarios. ©2009 IEEE.",Joint radio resource management; LTE; Reinforcement learning; WCDMA,Conference Paper,,Scopus,2-s2.0-77952847596
SCOPUS,"Zhao Y., Kosorok M.R., Zeng D.",Reinforcement learning design for cancer clinical trials,2009,Statistics in Medicine,28,26,,3294,3315,,68,10.1002/sim.3720,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449449564&doi=10.1002%2fsim.3720&partnerID=40&md5=6d68df31db7c4ca684184e8a88243c93,"Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States","Zhao, Y., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States; Kosorok, M.R., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States; Zeng, D., Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United States","We develop reinforcement learning trials for discovering individualized treatment regimens for lifethreatening diseases such as cancer. A temporal-difference learning method called Q-learning is utilized that involves learning an optimal policy from a single training set of finite longitudinal patient trajectories. Approximating the Q-function with time-indexed parameters can be achieved by using support vector regression or extremely randomized trees. Within this framework, we demonstrate that the procedure can extract optimal strategies directly from clinical data without relying on the identification of any accurate mathematical models, unlike approaches based on adaptive design. We show that reinforcement learning has tremendous potential in clinical research because it can select actions that improve outcomes by taking into account delayed effects even when the relationship between actions and outcomes is not fully known. To support our claims, the methodology's practical utility is illustrated in a simulation analysis. In the immediate future, we will apply this general strategy to studying and identifying new treatments for advanced metastatic stage IIIB/IV non-small cell lung cancer, which usually includes multiple lines of chemotherapy treatment. Moreover, there is significant potential of the proposed methodology for developing personalized treatment strategies in other cancers, in cystic fibrosis, and in other life-threatening diseases. Copyright © 2009 John Wiley & Sons, Ltd.",Adaptive design; Clinical trials; Dynamic treatment regime; Extremely randomized trees; Multistage decision problems; Non-small cell lung cancer; Optimal policy; Reinforcement learning; Support vector regression,Article,,Scopus,2-s2.0-70449449564
SCOPUS,Gaweda A.E.,Improving management of Anemia in End Stage Renal Disease using Reinforcement Learning,2009,Proceedings of the International Joint Conference on Neural Networks,,,5179004,953,958,,2,10.1109/IJCNN.2009.5179004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449378417&doi=10.1109%2fIJCNN.2009.5179004&partnerID=40&md5=29bab9564895e7a687f94935ded716d4,"Department of Medicine, Division of Nephrology, University of Louisville, United States","Gaweda, A.E., Department of Medicine, Division of Nephrology, University of Louisville, United States",We present a Reinforcement Learning approach to elicit individualized dose adjustment policies for patients suffering Anemia due to End Stage Renal Disease. Our goal is to achieve stable steady-state anemia management in patients with exhibiting different levels of treatment response. The approach uses Q-learning with parsimonious parametric representation of the state-action value function. We show that this approach achieves stability even in highly responsive patients. © 2009 IEEE.,,Conference Paper,,Scopus,2-s2.0-70449378417
SCOPUS,"Bountourelis T., Reveliotis S.",Customized learning algorithms for episodic tasks with acyclic state spaces,2009,"2009 IEEE International Conference on Automation Science and Engineering, CASE 2009",,,5234189,627,634,,,10.1109/COASE.2009.5234189,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449135159&doi=10.1109%2fCOASE.2009.5234189&partnerID=40&md5=bfd995f2b9965136f23db2dda84df342,"School of Industrial and Systems Engineering, Georgia Institute of Technology, United States","Bountourelis, T., School of Industrial and Systems Engineering, Georgia Institute of Technology, United States; Reveliotis, S., School of Industrial and Systems Engineering, Georgia Institute of Technology, United States","The work presented in this paper provides a practical, customized learning algorithm for reinforcement learning tasks that evolve episodically over acyclic state spaces. The presented results are motivated by the Optimal Disassembly Planning (ODP) problem described in [14], and they complement and enhance some earlier developments on this problem that were presented in [15]. In particular, the proposed algorithm is shown to be a substantial improvement of the original algorithm developed in [15], in terms of, both, the involved computational effort and the attained performance, where the latter is measured by the accumulated reward. The new algorithm also leads to a robust performance gain over the typical Q-learning implementations for the considered problem context. © 2009 IEEE.",,Conference Paper,,Scopus,2-s2.0-70449135159
SCOPUS,"Wright R., Gemelli N.",State aggregation for reinforcement learning using neuroevolution,2009,ICAART 2009 - Proceedings of the 1st International Conference on Agents and Artificial Intelligence,,,,45,52,,3,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349454220&partnerID=40&md5=0eebdef21111af2a9eebacdf818ab11f,"Air Force Research Lab., Information Directorate, 525 Brooks Rd., Rome, 13441, United States","Wright, R., Air Force Research Lab., Information Directorate, 525 Brooks Rd., Rome, 13441, United States; Gemelli, N., Air Force Research Lab., Information Directorate, 525 Brooks Rd., Rome, 13441, United States","In this paper, we present a new machine learning algorithm, RL-SANE, which uses a combination of neuroevolution (NE) and traditional reinforcement learning (RL) techniques to improve learning performace. RL-SANE is an innovative combination of the neuroevolutionary algorithm NEAT(Stanley, 2004) and the RL algorithm Sarsa(λ)(Sutton and Barto, 1998). It uses the special ability of NEAT to generate and train customized neural networks that provide a means for reducing the size of the state space through state aggregation. Reducing the size of the state space through aggregation enables Sarsa(λ) to be applied to much more difficult problems than standard tabular based approaches. Previous similar work in this area, such as in Whiteson and Stone (Whiteson and Stone, 2006) and Stanley and Miikkulainen (Stanley and Miikkulainen, 2001), have shown positive and promising results. This paper gives a brief overview of neuroevolutionary methods, introduces the RL-SANE algorithm, presents a comparative analysis of RL-SANE to other neuroevolutionary algorithms, and concludes with a discussion of enhancements that need to be made to RL-SANE.",Evolutionary algorithms; NeuroEvolution; Reinforcement learning; State aggregation,Conference Paper,,Scopus,2-s2.0-70349454220
SCOPUS,"Atrash A., Pineau J.",A Bayesian reinforcement learning approach for customizing human-robot interfaces,2009,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,355,359,,10,10.1145/1502650.1502700,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953871643&doi=10.1145%2f1502650.1502700&partnerID=40&md5=265764ecab4726718eea0e4605b2a094,"School of Computer Science, McGill University, Montreal, QC H3A 2A7, Canada","Atrash, A., School of Computer Science, McGill University, Montreal, QC H3A 2A7, Canada; Pineau, J., School of Computer Science, McGill University, Montreal, QC H3A 2A7, Canada","Personal robots are becoming increasingly prevalent, which raises a number of interesting issues regarding the design and customization of interfaces to such platforms. The particular problem addressed by this paper is the use of learning methods to improve the quality and effectiveness of human-machine interaction onboard a robotic wheelchair. In support of this, we present a method for learning and adapting probabilistic models with the aid of a human operator. We use a Bayesian reinforcement learning framework, that allows us to mix learning and execution, as well as take advantage of prior information about the world. We address the problems of learning, handling a partially observable environment, and limiting the number of action requests. We demonstrate empirical feasibility of our approach on an interface for an autonomous wheelchair. Copyright 2009 ACM.",Activity & plan recognition; Intelligent assistants; Intelligent interfaces for ubiquitous computing,Conference Paper,,Scopus,2-s2.0-77953871643
SCOPUS,"Yao G., Zhang W., Wang J.",Research on intelligent home network model based on multi-level agents,2009,Gaojishu Tongxin/Chinese High Technology Letters,19,9,,919,925,,,10.3772/j.issn.1002-0470.2009.09.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350540861&doi=10.3772%2fj.issn.1002-0470.2009.09.008&partnerID=40&md5=297fa71683a5373e29a2b4d2b92e7aa1,"Graduate University of Chinese Academy of Sciences, Beijing 100049, China; Institute of Acoustics, Chinese Acad. of Sci., Beijing 100190, China","Yao, G., Graduate University of Chinese Academy of Sciences, Beijing 100049, China, Institute of Acoustics, Chinese Acad. of Sci., Beijing 100190, China; Zhang, W., Institute of Acoustics, Chinese Acad. of Sci., Beijing 100190, China; Wang, J., Institute of Acoustics, Chinese Acad. of Sci., Beijing 100190, China","This paper presents an intelligent home network model based on multi-level agents to solve the problem that the existing intelligent devices in home networks can not study users' habits efficiently so that they can not satisfy users' personalized QoS. A formal description for devices in intelligent home networks is given before the model is established. The overall agent in an intelligent home network summarizes the rules of service through the study of the long-term data of the family, which guide the work of equipment agents according to the life habits of the family. The equipment agents learn and detect the change of the family environments automatically and make the optimal choices using the reinforcement learning algorithms. An application scene of the multi-level agent model and the simulation result show that, by applying this model, the devices can study the habits of the user and provide personalized service to the user.",Intelligent home network; Multi-level agents; Reinforcement learning,Article,,Scopus,2-s2.0-70350540861
SCOPUS,"Martín-Guerrero J.D., Gomez F., Soria-Olivas E., Schmidhuber J., Climente-Martí M., Jiménez-Torres N.V.",A reinforcement learning approach for individualizing erythropoietin dosages in hemodialysis patients,2009,Expert Systems with Applications,36,6,,9737,9742,,17,10.1016/j.eswa.2009.02.041,https://www.scopus.com/inward/record.uri?eid=2-s2.0-64049101720&doi=10.1016%2fj.eswa.2009.02.041&partnerID=40&md5=3f10a7edf6be7a31b5baf5d6be7218ef,"Department of Electronic Engineering, University of Valencia, CL. Dr. Moliner, 50, 46100 Burjassot, Valencia, Spain; Istituto Dalle Molle di Studi sull'Intelligenza Artificiale (IDSIA), Lugano, Switzerland; Pharmacy Unit, University Hospital, Dr. Peset, Valencia, Spain; Pharmacy and Pharmaceutical Technology Department, University of Valencia, Spain","Martín-Guerrero, J.D., Department of Electronic Engineering, University of Valencia, CL. Dr. Moliner, 50, 46100 Burjassot, Valencia, Spain; Gomez, F., Istituto Dalle Molle di Studi sull'Intelligenza Artificiale (IDSIA), Lugano, Switzerland; Soria-Olivas, E., Department of Electronic Engineering, University of Valencia, CL. Dr. Moliner, 50, 46100 Burjassot, Valencia, Spain; Schmidhuber, J., Istituto Dalle Molle di Studi sull'Intelligenza Artificiale (IDSIA), Lugano, Switzerland; Climente-Martí, M., Pharmacy Unit, University Hospital, Dr. Peset, Valencia, Spain; Jiménez-Torres, N.V., Pharmacy Unit, University Hospital, Dr. Peset, Valencia, Spain, Pharmacy and Pharmaceutical Technology Department, University of Valencia, Spain","This paper presents a reinforcement learning (RL) approach for anemia management in patients undergoing chronic renal failure. Erythropoietin (EPO) is the treatment of choice for this kind of anemia but it is an expensive drug and with some dangerous side-effects that should be considered especially for patients who do not respond to the treatment. Therefore, an individualized treatment appears to be necessary. RL is a suitable approach to tackle this problem. Moreover, resulting policies are similar to medical protocols, and hence, they can easily be transferred to daily practice. A cohort of 64 patients are included in the study. An implementation of the Q-learning algorithm based on a state-aggregation table and another implementation using the multi-layer perceptron as a function approximator (Q-MLP) are compared with the protocols followed in the Nephrology Unit. The policy obtained by the Q-MLP approach outperforms the hospital policy in terms of the ratio of patients that are within the targeted range of hemoglobin (11.5-12.5 g/dl) at the end of the analyzed period, since an increase of 25% is observed. It ensures an improvement in patients' quality-of-life and considerable economic savings for the health care system due to both the expensiveness of EPO treatment and the costs incurred by the health care system in order to alleviate problems related to EPO over-dosing. It should be pointed out that the approach presented here is completely general, and therefore, it can be applied to any problem of drug dosage optimization. © 2009 Elsevier Ltd. All rights reserved.",Anemia; Chronic renal failure; Clinical pharmacokinetics; Erythropoietin; Reinforcement learning,Article,,Scopus,2-s2.0-64049101720
SCOPUS,"Riedmiller M., Gabel T., Hafner R., Lange S.",Reinforcement learning for robot soccer,2009,Autonomous Robots,27,1,,55,73,,83,10.1007/s10514-009-9120-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650996818&doi=10.1007%2fs10514-009-9120-4&partnerID=40&md5=6fa390732e9f6fea8e925083bf502037,"Department of Computer Science, Albert-Ludwigs-Universität Freiburg, Freiburg, Germany","Riedmiller, M., Department of Computer Science, Albert-Ludwigs-Universität Freiburg, Freiburg, Germany; Gabel, T., Department of Computer Science, Albert-Ludwigs-Universität Freiburg, Freiburg, Germany; Hafner, R., Department of Computer Science, Albert-Ludwigs-Universität Freiburg, Freiburg, Germany; Lange, S., Department of Computer Science, Albert-Ludwigs-Universität Freiburg, Freiburg, Germany","Batch reinforcement learning methods provide a powerful framework for learning efficiently and effectively in autonomous robots. The paper reviews some recent work of the authors aiming at the successful application of reinforcement learning in a challenging and complex domain. It discusses several variants of the general batch learning framework, particularly tailored to the use of multilayer perceptrons to approximate value functions over continuous state spaces. The batch learning framework is successfully used to learn crucial skills in our soccer-playing robots participating in the RoboCup competitions. This is demonstrated on three different case studies. © 2009 Springer Science+Business Media, LLC.",Autonomous learning robots; Batch reinforcement learning; Learning mobile robots; Neural control; RoboCup,Article,,Scopus,2-s2.0-67650996818
SCOPUS,"Hu J.-F., Li X.-F., Yin J.-H.",Event-related potential-based personal decision process,2009,Journal of Clinical Rehabilitative Tissue Engineering Research,13,26,,5074,5078,,,10.3969/j.issn.1673-8225.2009.26.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69749100603&doi=10.3969%2fj.issn.1673-8225.2009.26.016&partnerID=40&md5=6c84d43df855aa6451429654394c4504,"Institute of Information and Technology, Jiangxi Bluesky University, Nanchang 330098, Jiangxi Province, China","Hu, J.-F., Institute of Information and Technology, Jiangxi Bluesky University, Nanchang 330098, Jiangxi Province, China; Li, X.-F., Institute of Information and Technology, Jiangxi Bluesky University, Nanchang 330098, Jiangxi Province, China; Yin, J.-H., Institute of Information and Technology, Jiangxi Bluesky University, Nanchang 330098, Jiangxi Province, China","Decision making refers to an evaluation process of selecting a particular action from a number of alternative choices in a provided situation. However, in a multi-agent environment, where the outcomes of one's actions change dynamically because they are related to the behavior of others, it becomes difficult to make an optimal decision. Although game theory provides normative solutions for decision making in groups, how such decision-making strategies are altered by experience remains poorly understood. Optimal behavior in a competitive world requires the flexibility to adapt decision strategies based on recent outcomes. To explore the essence of such adaptive decision-making processes, we investigated the learning process in human playing a competitive game with binary choices, using a matching pennies game. In the present study, we tested the hypothesis that this flexibility emerges through a reinforcement learning process, and produced a reinforcement learning model. In addition, event-related brain potentials (ERPs) were recorded while subjects were playing the game. Analyses of ERP data focused on the feedback-related negativity (FRN), an outcome-locked potential thought to reflect a neural prediction error signal. Results show that the magnitude of ERPs after losing to the computer opponent predicted whether subjects would change decision behavior on the subsequent trial.",,Article,,Scopus,2-s2.0-69749100603
SCOPUS,"Zaidenberg S., Reignier P., Crowley J.L.",Reinforcement learning of context models for a ubiquitous personal assistant,2009,Advances in Soft Computing,51,,,254,264,,6,10.1007/978-3-540-85867-6_30,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149109424&doi=10.1007%2f978-3-540-85867-6_30&partnerID=40&md5=7b74657e0245e5a528b0a4029e063cd7,"Laboratoire LIG, 681 rue de la Passerelle, St-Martin d'Hères 38402, France","Zaidenberg, S., Laboratoire LIG, 681 rue de la Passerelle, St-Martin d'Hères 38402, France; Reignier, P., Laboratoire LIG, 681 rue de la Passerelle, St-Martin d'Hères 38402, France; Crowley, J.L., Laboratoire LIG, 681 rue de la Passerelle, St-Martin d'Hères 38402, France","Ubiquitous environments may become a reality in a foreseeable future and research is aimed on making them more and more adapted and comfortable for users. Our work consists on applying reinforcement learning techniques in order to adapt services provided by a ubiquitous assistant to the user. The learning produces a context model, associating actions to perceived situations of the user. Associations are based on feedback given by the user as a reaction to the behavior of the assistant. Our method brings a solution to some of the problems encountered when applying reinforcement learning to systems where the user is in the loop. For instance, the behavior of the system is completely incoherent at the be-ginning and needs time to converge. The user does not accept to wait that long to train the system. The user's habits may change over time and the assistant needs to integrate these changes quickly. We study methods to accelerate the reinforced learning process. © 2009 Springer-Verlag Berlin Heidelberg.",,Conference Paper,,Scopus,2-s2.0-58149109424
SCOPUS,Chen H.,A markov-based decision process model for wireless access agent,2009,Journal of Information and Computational Science,6,1,,415,422,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66149128446&partnerID=40&md5=ad9493ff0bfd400ce2d780a8a277aa4f,"Business School, Renmin University of China, Beijing 100872, China","Chen, H., Business School, Renmin University of China, Beijing 100872, China","The Personal Router is a mobile personal user agent whose task is to dynamically model the user, update its knowledge of a market of wireless service providers and select providers that satisfies the user's expected preferences. In this paper, we show how the user modeling problem can be represented as a Markov-based Decision Process (MDP) and suggest reinforcement learning and collaborative filtering as two candidate solution mechanisms for the information problem in the user modeling. © 2009 Binary Information Press.",Agent; Markov-based decision process; Personal router; Wireless access,Article,,Scopus,2-s2.0-66149128446
SCOPUS,"Xueli y., Zhi L., Changneng Z., Guangping Z., Zengrong L.",A design proposal of a game-based professional training system for highly dangerous professions,2009,Proceedings of the European Conference on Games-based Learning,2009-January,,,388,394,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938575896&partnerID=40&md5=6cd3db1e9a9c7a1b9429e5e93b4e8cab,"College of Computer and Software, Taiyuan University of Technology, China","Xueli, y., College of Computer and Software, Taiyuan University of Technology, China; Zhi, L., College of Computer and Software, Taiyuan University of Technology, China; Changneng, Z., College of Computer and Software, Taiyuan University of Technology, China; Guangping, Z., College of Computer and Software, Taiyuan University of Technology, China; Zengrong, L., College of Computer and Software, Taiyuan University of Technology, China","Recently, modern society frequently faces local wars, terrorism, earthquakes, fire accidents, epidemics and coal mining accidents. Members of highly dangerous professions must obtain rigorous training so that they could bear the great historical mission. Generally speaking, these professions include armed forces, special police force, fire department, astronauts and mine disaster rescue troop. The game-based professional training systems for highly dangerous professions have their own distinct requirements. The aim of game-based learning systems is not only the study of declarative knowledge, but also entraining procedural knowledge through repeated practice until it becomes an automatic skill. The result of highly dangerous professional training is extremely important, since if a trainee dose not master the basic knowledge and skill, they could be in grave danger; the trainee's mental qualities should be continuously prompted by the training system so that they could be act intuitively under the most execrable circumstance. Based on requirements analysis and taking the case of mining rescue into account, we divide the whole training system into three parts: machine learning subsystem, brain information subsystem and credit-assignment subsystem. The machine learning subsystem (as know as serious game subsystem), contains the audio-visual coherency analysis, semantic annotation of a scene based on association memory, cooperating management of audio-visual cross-modal signals, personalization rendering of a scene. The brain information subsystem includes functions for receiving, storing and analyzing trainee's trial data based on visual and auditory signals from EEG, sEMG and psychological tests. The credit-assignment subsystem involves trainee's profiles and effect evaluation which are sent from brain information subsystem to machine learning subsystem, while the plan of knowledge learning, the result of skills training and consequence of the desensitization trial are sent as the feedback to brain information subsystem. Therefore, the whole framework works as a reinforcement learning system. The kernel of this system is the cooperating learning schema of audio-visual cross-modal signals. Furthermore, in this system the main visual signals contain scene textures, 3D character animation, 3D scene animation, while the main auditory signals contain the realistic sound, the on-the-spot orders, the on-the-spot yells and background music. In the light of cognitive principles, the following factors should be considered when a game-based leaning system is designed: (1)The working memory including phonological loop and visuo-spatial sketchpad act as two slave systems, play the role of dual sensory channels so that semantic coherency of the visual and the auditory data could be combined with the prior knowledge to be formed as long-term memory; (2) A goal of cooperative learning for audio-visual cross-modal signals is to create an approach which can process verbal information(like the realistic sound and the on-the-spot orders) and non-verbal information(such as 3D character animation as well as 3D scene animation) from the two separate subsystems; (3) Schema acquisition (based on Theory of Cognitive Load -TCL) should be a primary means of learning, and the automation of cognitive process (including declarative knowledge procedural knowledge) will be used to reduce working memory load.",Audio-visual coherency; Game-based professional training; Highly dangerous professions; Theory of cognitive load; Working memory,Conference Paper,,Scopus,2-s2.0-84938575896
SCOPUS,"Tumer K., Agogino A.",Adaptive management of air traffic flow: A multiagent coordination approach,2008,Proceedings of the National Conference on Artificial Intelligence,3,,,1581,1584,,4,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57749098530&partnerID=40&md5=dbd59ebb6c6553c0f9800d71773ffaf3,"Oregon State University, 204 Rogers Hall, Corvallis, OR 97331, United States; UCSC, NASA Ames Research Center, Mailstop 269-3, Moffett Field, CA 94035, United States","Tumer, K., Oregon State University, 204 Rogers Hall, Corvallis, OR 97331, United States; Agogino, A., UCSC, NASA Ames Research Center, Mailstop 269-3, Moffett Field, CA 94035, United States","This paper summarizes recent advances in the application of multiagent coordination algorithms to air traffic flow management. Indeed, air traffic flow management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today. This problem is particularly complex as it requires the integration and/or coordination of many factors including: new data (e.g., changing weather info), potentially conflicting priorities (e.g., different airlines), limited resources (e.g., air traffic controllers) and very heavy traffic volume (e.g., over 40,000 flights over the US airspace). The multiagent approach assigns an agent to a navigational fix (a specific location in 2D space) and uses three separate actions to control the airspace: setting the separation between airplanes, setting ground holds that delay aircraft departures and rerouting aircraft. Agents then use reinforcement learning to learn the best set of actions. Results based on FACET (a commercial simulator) show that agents receiving personalized rewards reduce congestion by up to 80% over agents receiving a global reward and by up to 85% over a current industry approach (Monte Carlo estimation). These results show that with proper selection of agents, their actions and their reward structures, multiagent coordination algorithms can be successfully applied to complex real world domains. Copyright © 2008.",,Conference Paper,,Scopus,2-s2.0-57749098530
SCOPUS,"Taghipour N., Kardan A.",A hybrid web recommender system based on Q-learning,2008,Proceedings of the ACM Symposium on Applied Computing,,,,1164,1168,,11,10.1145/1363686.1363954,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56749160807&doi=10.1145%2f1363686.1363954&partnerID=40&md5=b3d254c9ed6f9435078120241cad1399,"Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran","Taghipour, N., Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran; Kardan, A., Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran","Different efforts have been made to address the problem of information overload on the Internet. Recommender systems aim at directing users through this information space, toward the resources that best meet their needs and interests. Web Content Recommendation has been an active application area for Information Filtering, Web Mining and Machine Learning research. Recent studies show that combining the conceptual and usage information can improve the quality of web recommendations. In this paper we exploit this idea to enhance a reinforcement learning framework, primarily devised for web recommendations based on web usage data. A hybrid web recommendation method is proposed by making use of the conceptual relationships among web resources to derive a novel model of the problem, enriched with semantic knowledge about the usage behavior. With our hybrid model for the web page recommendation problem we show the apt and flexibility of the reinforcement learning framework in the web recommendation domain, and demonstrate how it can be extended in order to incorporate various sources of information. We evaluate our method under different settings and show how this method can improve the overall quality of web recommendations. Copyright 2008 ACM.",Machine learning; Personalization; Recommender systems; Reinforcement learning; Web mining,Conference Paper,,Scopus,2-s2.0-56749160807
SCOPUS,"Martín-Guerrero J.D., Soria-Olivas E., Martínez-Sober M., Serrrano-López A.J., Magdalena-Benedito R., Gómez-Sanchis J.",Use of reinforcement learning in two real applications,2008,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),5323 LNAI,,,191,204,,,10.1007/978-3-540-89722-4_15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58449123856&doi=10.1007%2f978-3-540-89722-4_15&partnerID=40&md5=fb2f4ac752eba3eca8189bfc230bd0e6,"Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain","Martín-Guerrero, J.D., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain; Soria-Olivas, E., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain; Martínez-Sober, M., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain; Serrrano-López, A.J., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain; Magdalena-Benedito, R., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain; Gómez-Sanchis, J., Intelligent Data Analysis Laboratory, Department of Electronic Engineering, University of Valencia, Spain","In this paper, we present two sucessful applications of Reinforcement Learning (RL) in real life. First, the optimization of anemia management in patients undergoing Chronic Renal Failure is presented. The aim is to individualize the treatment (Erythropoietin dosages) in order to stabilize patients within a targeted range of Hemoglobin (Hb). Results show that the use of RL increases the ratio of patients within the desired range of Hb. Thus, patients' quality of life is increased, and additionally, Health Care System reduces its expenses in anemia management. Second, RL is applied to modify a marketing campaign in order to maximize long-term profits. RL obtains an individualized policy depending on customer characteristics that increases long-term profits at the end of the campaign. Results in both problems show the robustness of the obtained policies and suggest their use in other real-life problems. © 2008 Springer Berlin Heidelberg.",,Conference Paper,,Scopus,2-s2.0-58449123856
SCOPUS,"Kaustia M., Knüpfer S.",Do investors overweight personal experience? evidence from IPO subscriptions,2008,Journal of Finance,63,6,,2679,2702,,64,10.1111/j.1540-6261.2008.01411.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55949118727&doi=10.1111%2fj.1540-6261.2008.01411.x&partnerID=40&md5=38d5e3afd5363a1adc24a762e6bfcb59,"Helsinki School of Economics, Department of Accounting and Finance","Kaustia, M., Helsinki School of Economics, Department of Accounting and Finance; Knüpfer, S., Helsinki School of Economics, Department of Accounting and Finance","We find a strong positive link between past IPO returns and future subscriptions at the investor level in Finland. Our setting allows us to trace this effect to the returns personally experienced by investors; the effect is not explained by patterns related to the IPO cycle, or wealth effects. This behavior is consistent with reinforcement learning, where personally experienced outcomes are overweighted compared to rational Bayesian learning. The results provide a microfoundation for the argument that investor sentiment drives IPO demand. The paper also contributes to understanding how popular investment styles develop, and has implications for the marketing of financial products. © 2008 The American Finance Association.",,Article,,Scopus,2-s2.0-55949118727
SCOPUS,"Lin C.-W., Wang J.-S.",A digital circuit design of state-space recurrent neural networks,2008,"Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics",,,4811556,1838,1842,,,10.1109/ICSMC.2008.4811556,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69949122521&doi=10.1109%2fICSMC.2008.4811556&partnerID=40&md5=c2b5b56323768f5e7dc643fba041608f,"Department of Electrical Engineering, National Cheng Kung University, Tainan 701, Taiwan","Lin, C.-W., Department of Electrical Engineering, National Cheng Kung University, Tainan 701, Taiwan; Wang, J.-S., Department of Electrical Engineering, National Cheng Kung University, Tainan 701, Taiwan","This paper presents a digital circuit design of a state-space recurrent neural network (RNN). The proposed digital circuit design separates the datapath of the state-space RNN into a linear subcircuit and a nonlinear subcircuit. The linear subcircuit is realized by a matrix-vector multiplier while the nonlinear subcircuit by a customized nonlinear function computing unit. The throughput rate of the proposed RNN circuit is 36060.5 times faster than that of the software simulation using MATLAB®. The proposed state-space RNN digital design methodology not only possesses the advantages including high computing speed, small area and portability, but also increases the possibility of using the digital RNN circuit in real-world dynamic problems. © 2008 IEEE.",,Conference Paper,,Scopus,2-s2.0-69949122521
SCOPUS,"Agogino A.K., Tumer K.",Analyzing and visualizing multiagent rewards in dynamic and stochastic domains,2008,Autonomous Agents and Multi-Agent Systems,17,2,,320,338,,50,10.1007/s10458-008-9046-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51649111408&doi=10.1007%2fs10458-008-9046-9&partnerID=40&md5=8c7580869c4516766cd5b5f8b2d46fd2,"University of California, Santa Cruz, Santa Cruz, CA, United States; Oregon State University, 204 Rogers Hall, Corvallis, OR 97330, United States","Agogino, A.K., University of California, Santa Cruz, Santa Cruz, CA, United States; Tumer, K., Oregon State University, 204 Rogers Hall, Corvallis, OR 97330, United States","The ability to analyze the effectiveness of agent reward structures is critical to the successful design of multiagent learning algorithms. Though final system performance is the best indicator of the suitability of a given reward structure, it is often preferable to analyze the reward properties that lead to good system behavior (i.e., properties promoting coordination among the agents and providing agents with strong signal to noise ratios). This step is particularly helpful in continuous, dynamic, stochastic domains ill-suited to simple table backup schemes commonly used in TD(λ)/Q-learning where the effectiveness of the reward structure is difficult to distinguish from the effectiveness of the chosen learning algorithm. In this paper, we present a new reward evaluation method that provides a visualization of the tradeoff between the level of coordination among the agents and the difficulty of the learning problem each agent faces. This method is independent of the learning algorithm and is only a function of the problem domain and the agents' reward structure. We use this reward property visualization method to determine an effective reward without performing extensive simulations. We then test this method in both a static and a dynamic multi-rover learning domain where the agents have continuous state spaces and take noisy actions (e.g., the agents' movement decisions are not always carried out properly). Our results show that in the more difficult dynamic domain, the reward efficiency visualization method provides a two order of magnitude speedup in selecting good rewards, compared to running a full simulation. In addition, this method facilitates the design and analysis of new rewards tailored to the observational limitations of the domain, providing rewards that combine the best properties of traditional rewards. © 2008 Springer Science+Business Media, LLC.",Multiagent learning; Reinforcement learning; Reward analysis; Visualization,Article,,Scopus,2-s2.0-51649111408
SCOPUS,"Riedmiller M., Hafner R., Lange S., Lauer M.",Learning to dribble on a real robot by success and failure,2008,Proceedings - IEEE International Conference on Robotics and Automation,,,4543536,2207,2208,,6,10.1109/ROBOT.2008.4543536,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51649126825&doi=10.1109%2fROBOT.2008.4543536&partnerID=40&md5=673a4032945a12df25a67f5576bbad3e,"Dept. of Mathematics and Informatics, Institute of Computer Science, University of Osnabrück","Riedmiller, M., Dept. of Mathematics and Informatics, Institute of Computer Science, University of Osnabrück; Hafner, R., Dept. of Mathematics and Informatics, Institute of Computer Science, University of Osnabrück; Lange, S., Dept. of Mathematics and Informatics, Institute of Computer Science, University of Osnabrück; Lauer, M., Dept. of Mathematics and Informatics, Institute of Computer Science, University of Osnabrück","Learning directly on real world systems such as autonomous robots is a challenging task, especially if the training signal is given only in terms of success or failure (Reinforcement Learning). However, if successful, the controller has the advantage of being tailored exactly to the system it eventually has to control. Here we describe, how a neural network based RL controller learns the challenging task of ball dribbling directly on our Middle-Size robot. The learned behaviour was actively used throughout the RoboCup world championship tournament 2007 in Atlanta, where we won the first place. This contistutes another important step within our Brainstormers project. The goal of this project is to develop an intelligent control architecture for a soccer playing robot, that is able to learn more and more complex behaviours from scratch. ©2008 IEEE.",,Conference Paper,,Scopus,2-s2.0-51649126825
SCOPUS,"La Camera G., Richmond B.J.",Modeling the violation of reward maximization and invariance in reinforcement schedules,2008,PLoS Computational Biology,4,8, e1000131,,,,15,10.1371/journal.pcbi.1000131,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50949100487&doi=10.1371%2fjournal.pcbi.1000131&partnerID=40&md5=fb5b4a256d57aeb2818488fb299d5828,"National Institute of Mental Health, National Institutes of Health, Department of Health and Human Services, Bethesda, MD, United States","La Camera, G., National Institute of Mental Health, National Institutes of Health, Department of Health and Human Services, Bethesda, MD, United States; Richmond, B.J., National Institute of Mental Health, National Institutes of Health, Department of Health and Human Services, Bethesda, MD, United States","It is often assumed that animals and people adjust their behavior to maximize reward acquisition. In visually cued reinforcement schedules, monkeys make errors in trials that are not immediately rewarded, despite having to repeat error trials. Here we show that error rates are typically smaller in trials equally distant from reward but belonging to longer schedules (referred to as ""schedule length effect""). This violates the principles of reward maximization and invariance and cannot be predicted by the standard methods of Reinforcement Learning, such as the method of temporal differences. We develop a heuristic model that accounts for all of the properties of the behavior in the reinforcement schedule task but whose predictions are not different from those of the standard temporal difference model in choice tasks. In the modification of temporal difference learning introduced here, the effect of schedule length emerges spontaneously from the sensitivity to the immediately preceding trial. We also introduce a policy for general Markov Decision Processes, where the decision made at each node is conditioned on the motivation to perform an instrumental action, and show that the application of our model to the reinforcement schedule task and the choice task are special cases of this general theoretical framework. Within this framework, Reinforcement Learning can approach contextual learning with the mixture of empirical findings and principled assumptions that seem to coexist in the best descriptions of animal behavior. As examples, we discuss two phenomena observed in humans that often derive from the violation of the principle of invariance: ""framing,"" wherein equivalent options are treated differently depending on the context in which they are presented, and the ""sunk cost"" effect, the greater tendency to continue an endeavor once an investment in money, effort, or time has been made. The schedule length effect might be a manifestation of these phenomena in monkeys.",,Article,Open Access,Scopus,2-s2.0-50949100487
SCOPUS,Lee D.,Game theory and neural basis of social decision making,2008,Nature Neuroscience,11,4,,404,409,,139,10.1038/nn2065,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41149123141&doi=10.1038%2fnn2065&partnerID=40&md5=779a41c1bac083442ed4c4aa074053c3,"Yale University School of Medicine, Department of Neurobiology, SHM B404, 333 Cedar Street, New Haven, CT 06510, United States","Lee, D., Yale University School of Medicine, Department of Neurobiology, SHM B404, 333 Cedar Street, New Haven, CT 06510, United States","Decision making in a social group has two distinguishing features. First, humans and other animals routinely alter their behavior in response to changes in their physical and social environment. As a result, the outcomes of decisions that depend on the behavior of multiple decision makers are difficult to predict and require highly adaptive decision-making strategies. Second, decision makers may have preferences regarding consequences to other individuals and therefore choose their actions to improve or reduce the well-being of others. Many neurobiological studies have exploited game theory to probe the neural basis of decision making and suggested that these features of social decision making might be reflected in the functions of brain areas involved in reward evaluation and reinforcement learning. Molecular genetic studies have also begun to identify genetic mechanisms for personal traits related to reinforcement learning and complex social decision making, further illuminating the biological basis of social behavior. © 2008 Nature Publishing Group.",,Review,,Scopus,2-s2.0-41149123141
SCOPUS,"Kersting K., De Raedt L., Gutmann B., Karwath A., Landwehr N.",Relational sequence learning,2008,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),4911 LNAI,,,28,55,,8,10.1007/978-3-540-78652-8_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40249105911&doi=10.1007%2f978-3-540-78652-8_2&partnerID=40&md5=fec366043b3d7dcd69b6241a78a19d35,"CSAIL, Massachusetts Institute of Technology, 32 Vassar Street, Cambridge, MA 02139-4307, United States; Departement Computerwetenschappen, K.U. Leuven, Celestijnenlaan 200A - bus 2402, Heverlee B-3001, Belgium; Machine Learning Lab., Institute for Computer Science, University of Freiburg, Georges-Koehler Allee, Freiburg 79110, Germany","Kersting, K., CSAIL, Massachusetts Institute of Technology, 32 Vassar Street, Cambridge, MA 02139-4307, United States; De Raedt, L., Departement Computerwetenschappen, K.U. Leuven, Celestijnenlaan 200A - bus 2402, Heverlee B-3001, Belgium; Gutmann, B., Departement Computerwetenschappen, K.U. Leuven, Celestijnenlaan 200A - bus 2402, Heverlee B-3001, Belgium; Karwath, A., Machine Learning Lab., Institute for Computer Science, University of Freiburg, Georges-Koehler Allee, Freiburg 79110, Germany; Landwehr, N., Machine Learning Lab., Institute for Computer Science, University of Freiburg, Georges-Koehler Allee, Freiburg 79110, Germany","Sequential behavior and sequence learning are essential to intelligence. Often the elements of sequences exhibit an internal structure that can elegantly be represented using relational atoms. Applying traditional sequential learning techniques to such relational sequences requires one either to ignore the internal structure or to live with a combinatorial explosion of the model complexity. This chapter briefly reviews relational sequence learning and describes several techniques tailored towards realizing this, such as local pattern mining techniques, (hidden) Markov models, conditional random fields, dynamic programming and reinforcement learning. © 2008 Springer-Verlag Berlin Heidelberg.",,Article,,Scopus,2-s2.0-40249105911
SCOPUS,"Huajun Z., Jin Z., Rui W., Tan M.",Multi-objective reinforcement learning algorithm and its application in drive system,2008,IECON Proceedings (Industrial Electronics Conference),,,4757965,274,279,,,10.1109/IECON.2008.4757965,https://www.scopus.com/inward/record.uri?eid=2-s2.0-63149156232&doi=10.1109%2fIECON.2008.4757965&partnerID=40&md5=78db1241d4916bede52a1646c3ce7773,"Department of Control Science and Engineering, Huazhong University of Science and Technology, China; Department of Control Science and Engineering, Huazhong University of Science and Technology, Wuhan city, Hubei province, China","Huajun, Z., Department of Control Science and Engineering, Huazhong University of Science and Technology, China, Department of Control Science and Engineering, Huazhong University of Science and Technology, Wuhan city, Hubei province, China; Jin, Z., Department of Control Science and Engineering, Huazhong University of Science and Technology, China, Department of Control Science and Engineering, Huazhong University of Science and Technology, Wuhan city, Hubei province, China; Rui, W., Department of Control Science and Engineering, Huazhong University of Science and Technology, China, Department of Control Science and Engineering, Huazhong University of Science and Technology, Wuhan city, Hubei province, China; Tan, M., Department of Control Science and Engineering, Huazhong University of Science and Technology, China, Department of Control Science and Engineering, Huazhong University of Science and Technology, Wuhan city, Hubei province, China","Generally, reinforcement learning (RL) is used to design neurocontroller for control system with single objective. When facing multi-objective system, it is necessary to design the neurocontroller according to the personal preference. This paper proposed a multi-objective reinforcement learning algorithm (MORLA) to design neurocontroller with the personal preference. It transformed the multi-objective into synthetical objective and applied parallel genetic algorithm (PGA) to evolve the neurocontroller according to the synthetical objective. To establish the synthetical objective, the objective weight which represents the personal preference is calculated by solving the constrained optimization problem (COP) at the end of each generation. The COP requires not only the biggest variance of the synthetical objective in the population, but also requires the weight to fit the designer's preference. After acquiring the weights, the PGA can select the elitists from the population according to the designer's preference and design a satisfying neurocontroller by evolutionary operations. At last, the MORLA is used to design neurocontroller for a speed-controlled induction motor drive with indirect vector control. This paper designed several neurocontrollers with different personal preferences for the drive system. The simulation results show the feasibility and validity of the MORLA. © 2008 IEEE.",,Conference Paper,,Scopus,2-s2.0-63149156232
SCOPUS,"Turner K., Welch Z.T., Agogino A.",Aligning social welfare and agent preferences to alleviate traffic congestion,2008,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",2,,,646,653,,17,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899955497&partnerID=40&md5=9b77acafd443bb62e6e336d3822aa81c,"Oregon State University, 204 Rogers Hall, Corvailis, OR 97331, United States; UCSC, NASA Ames Res. Ctr., Mailstop 269-3, Moffett Field, CA 94035, United States","Turner, K., Oregon State University, 204 Rogers Hall, Corvailis, OR 97331, United States; Welch, Z.T., Oregon State University, 204 Rogers Hall, Corvailis, OR 97331, United States; Agogino, A., UCSC, NASA Ames Res. Ctr., Mailstop 269-3, Moffett Field, CA 94035, United States","Multi-agent coordination algorithms provide unique insights into the challenging problem of alleviating traffic congestion. What is particularly interesting in this class of problem is that no individual action (e.g., leave at a given time) is intrinsically ""bad"" but that combinations of actions among agents lead to undesirable outcomes. As a consequence, agents need to learn how to coordinate their actions with those of other agents, rather than learn a particular set of ""good"" actions. In general, the traffic problem can be approached from two distinct perspectives: (i) from a city manager's point of view, where the aim is to optimize a city wide objective function (e.g., minimize total city wide delays), and (ii) from the individual driver's point of view, where each driver is aiming to optimize a personal objective function (e.g., a ""timeliness"" function that minimizes the difference desired and actual arrival times at a destination). In many cases, these two objective functions are at odds with one another, where drivers aiming to optimize their own objectives yield to congestion and poor values of city objective functions. In this paper we present an objective shaping approach to both types of problems and study the system behavior that arises from the drivers' choices. We first show a top-down approach that provides incentives to drivers and leads to good values of the city manager's objective function. We then present a bottom-up approach that shows that drivers aiming to optimize their own personal timeliness objective lead to poor performance with respect to a city manager's objective function. Finally, we present the intriguing result that drivers that aim to optimize a modified version of their own timeliness function not only perform well in terms of the city manager's objective function, but also perform better with respect to their own original timeliness functions. Copyright © 2008, International Foundation for Autonomous Agents and Multi-agent Systems (www.ifaamas.org). All rights reserved.",Multi-agent systems; Optimization; Reinforcement learning; Traffic management; Transportation,Conference Paper,,Scopus,2-s2.0-84899955497
SCOPUS,"Mahmood T., Ricci F.",Learning and adaptivity in interactive recommender systems,2007,ACM International Conference Proceeding Series,258,,,75,84,,25,10.1145/1282100.1282114,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36849060222&doi=10.1145%2f1282100.1282114&partnerID=40&md5=3eb06672fc6e015b5b7949df5d904559,"University of Trento, Trento, Italy; Free University of Bozen-Bolzano, Bolzano, Italy","Mahmood, T., University of Trento, Trento, Italy; Ricci, F., Free University of Bozen-Bolzano, Bolzano, Italy","Recommender systems are intelligent E-commerce applications that assist users in a decision-making process by offering personalized product recommendations during an interaction session. Quite recently, conversational approaches have been introduced in order to support more interactive recommendation sessions. Notwithstanding the increased interactivity offered by these approaches, the system employs an interaction strategy that is specified apriori (at design time) and followed quite rigidly during the interaction. In this paper, we present a new type of recommender system which is capable of learning autonomously an adaptive interaction strategy for assisting the users in acquiring their interaction goals. We view the recommendation process as a sequential decision problem and we model it as a Markov Decision Process (MDP). We learn a model of the user behavior, and use it to acquire the adaptive strategy using Reinforcement Learning (RL) techniques. In this context, the system learns the optimal strategy by observing the consequences of its actions on the users and also on the final outcome of the recommendation session. We apply our approach within an existing travel recommender system which uses a rigid, non-adaptive support strategy for advising a user in refining a query to a travel product catalogue. The initial results demonstrate the value of our approach and show that our system is able to improve the non-adaptive strategy in order to learn an optimal (adaptive) recommendation strategy. Copyright 2007 ACM.",Adaptivity; Conversational recommender systems; Markov decision process; Reinforcement learning,Conference Paper,,Scopus,2-s2.0-36849060222
SCOPUS,"Taghipour N., Kardan A., Ghidary S.S.",Usage-based web recommendations: A reinforcement learning approach,2007,RecSys'07: Proceedings of the 2007 ACM Conference on Recommender Systems,,,,113,120,,31,10.1145/1297231.1297250,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149177829&doi=10.1145%2f1297231.1297250&partnerID=40&md5=0344c9539560bc03572a0e0f62a61992,"Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran","Taghipour, N., Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran; Kardan, A., Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran; Ghidary, S.S., Amirkabir University of Technology, Department of Computer Engineering, 424, Hafez, Tehran, Iran","Information overload is no longer news; the explosive growth of the Internet has made this issue increasingly serious for Web users. Users are very often overwhelmed by the huge amount of information and are faced with a big challenge to find the most relevant information in the right time. Recommender systems aim at pruning this information space and directing users toward the items that best meet their needs and interests. Web Recommendation has been an active application area in Web Mining and Machine Learning research. In this paper we propose a novel machine learning perspective toward the problem, based on reinforcement learning. Unlike other recommender systems, our system does not use the static patterns discovered from web usage data, instead it learns to make recommendations as the actions it performs in each situation. We model the problem as Q-Learning while employing concepts and techniques commonly applied in the web usage mining domain. We propose that the reinforcement learning paradigm provides an appropriate model for the recommendation problem, as well as a framework in which the system constantly interacts with the user and learns from her behavior. Our experimental evaluations support our claims and demonstrate how this approach can improve the quality of web recommendations. Copyright 2007 ACM.",Machine learning; Personalization; Recommender systems; Reinforcement learning; Web usage mining,Conference Paper,,Scopus,2-s2.0-42149177829
SCOPUS,"Kushida M., Takahashi K., Ueda H., Miyahara T.",A comparative study of parallel reinforcement learning methods with a PC cluster system,2007,"Proceedings - 2006 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT 2006 Main Conference Proceedings), IAT'06",,,4052954,416,419,,6,10.1109/IAT.2006.3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38949096069&doi=10.1109%2fIAT.2006.3&partnerID=40&md5=9c95a364e00344ac1a83ba04b0ea0039,"Faculty of Information Sciences, Hiroshima City University, 731-3194, Japan","Kushida, M., Faculty of Information Sciences, Hiroshima City University, 731-3194, Japan; Takahashi, K., Faculty of Information Sciences, Hiroshima City University, 731-3194, Japan; Ueda, H., Faculty of Information Sciences, Hiroshima City University, 731-3194, Japan; Miyahara, T., Faculty of Information Sciences, Hiroshima City University, 731-3194, Japan","This paper presents a comparative study of three parallel implementation models for reinforcement learning. Two of them utilize Q-learning, and the other one utilizes fuzzy Q-learning for agent learning. In order to evaluate performance and validity of the three method, a PC(personal computer) cluster system consisting of 16 PCs connected via Gigabit ethernet has been built. For communications to deliver data among PCs, MPI (Message Passing Interface) is employed. Experimental results are compared with one another to show the performance and characteristics of the three methods. © 2006 IEEE.",,Conference Paper,,Scopus,2-s2.0-38949096069
SCOPUS,"Ghosh H., Poornachander P., Mallik A., Chaudhury S.",Learning ontology for personalized video retrieval,2007,Proceedings of the ACM International Multimedia Conference and Exhibition,,,,39,46,,5,10.1145/1290067.1290075,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37849030482&doi=10.1145%2f1290067.1290075&partnerID=40&md5=f20e9386c1b589ba6b1a0af3b6d9de83,"Innovation Labs, Delhi, TCS Limited, 249 D and E Udyog Vihar Phase 4, Gurgaon 122016, India; Electrical Engineering Deptt., Indian Institute of Technology, Delhi, New Delhi 110016, India","Ghosh, H., Innovation Labs, Delhi, TCS Limited, 249 D and E Udyog Vihar Phase 4, Gurgaon 122016, India; Poornachander, P., Electrical Engineering Deptt., Indian Institute of Technology, Delhi, New Delhi 110016, India; Mallik, A., Electrical Engineering Deptt., Indian Institute of Technology, Delhi, New Delhi 110016, India; Chaudhury, S., Electrical Engineering Deptt., Indian Institute of Technology, Delhi, New Delhi 110016, India","This paper proposes a new method for using implicit user feedback from clickthrough data to provide personalized ranking of results in a video retrieval system. The annotation based search is complemented with a feature based ranking in our approach. The ranking algorithm uses belief revision in a Bayesian Network, which is derived from a multimedia ontology that captures the probabilistic association of a concept with expected video features. We have developed a content model for videos using discrete feature states to enable Bayesian reasoning and to alleviate on-line feature processing overheads. We propose a reinforcement learning algorithm for the parameters of the Bayesian Network with the implicit feedback obtained from the clickthrough data. Copyright 2007 ACM.",Bayesian network; Content modeling; Reinforcement learning; Video retrieval,Conference Paper,,Scopus,2-s2.0-37849030482
SCOPUS,"Moon A., Kang T., Kim H., Kim H.",A service recommendation using reinforcement learning for network-based robots in ubiquitous computing environments,2007,Proceedings - IEEE International Workshop on Robot and Human Interactive Communication,,,4415198,821,826,,3,10.1109/ROMAN.2007.4415198,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48749104576&doi=10.1109%2fROMAN.2007.4415198&partnerID=40&md5=19231e2f967dc365c53d9fb80724513e,"Electronice and Telecommunication Research Institute, 161 Gajeong-dong, Yuseong-gu, Daejeon, 305-350, South Korea","Moon, A., Electronice and Telecommunication Research Institute, 161 Gajeong-dong, Yuseong-gu, Daejeon, 305-350, South Korea; Kang, T., Electronice and Telecommunication Research Institute, 161 Gajeong-dong, Yuseong-gu, Daejeon, 305-350, South Korea; Kim, H., Electronice and Telecommunication Research Institute, 161 Gajeong-dong, Yuseong-gu, Daejeon, 305-350, South Korea; Kim, H., Electronice and Telecommunication Research Institute, 161 Gajeong-dong, Yuseong-gu, Daejeon, 305-350, South Korea","Ubiquitous Robotic Companion (URC) is a new concept for the network-based robot platform which can enable to be following its master wherever or whenever he/she be in order to provide necessary services. The robot platforms in present normally interest in providing services through the direct interaction in responding to the user's demands. On the other hand, URC services are required to be provided by the means of recognizing the circumstances and taking a user's preference into account. In this paper, we propose a service recommendation scheme for URC robots. The proposed service recommendation, developed based on the reinforcement learning, can be used to provide personalized services by learning users' preferences or tasks through the interaction with users. Using simulation for rapid testing, we evaluate of the proposed scheme under a variety of user modeling types and discount factors. ©2007 IEEE.",,Conference Paper,,Scopus,2-s2.0-48749104576
SCOPUS,"Rieser V., Lemon O.",Learning dialogue strategies for interactive database search,2007,"International Speech Communication Association - 8th Annual Conference of the International Speech Communication Association, Interspeech 2007",3,,,2041,2044,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56149117477&partnerID=40&md5=0223cd1a2d0ddeecbd0b38a429aa35eb,"Department of Computational Linguistics, Saarland University, Saarbrücken, Germany; School of Informatics, University of Edinburgh, Edinburgh, United Kingdom","Rieser, V., Department of Computational Linguistics, Saarland University, Saarbrücken, Germany; Lemon, O., School of Informatics, University of Edinburgh, Edinburgh, United Kingdom","We show how to learn optimal dialogue policies for a wide range of database search applications, concerning how many database search results to present to the user, and when to present them. We use Reinforcement Learning methods for a wide spectrum of different database simulations, turn penalty conditions, and noise conditions. Our objective is to show that our policy learning framework covers this spectrum. We can show that even for challenging cases learning significantly outperforms hand-coded policies tailored to the different operating situations. The polices are adaptive/context-sensitive in respect of both the overall operating situation (e.g. noise) and the local context of the interaction (e.g. user's last move). The learned policies produce an average relative increase in reward of 25.7% over the corresponding threshold-based hand-coded baseline policies.",Adaptive strategies; Database search; Dialogue systems; Multimodality; Reinforcement learning,Conference Paper,,Scopus,2-s2.0-56149117477
SCOPUS,"Tumer K., Agogino A.",Distributed agent-based air traffic flow management,2007,Proceedings of the International Conference on Autonomous Agents,,,255,342,349,,23,10.1145/1329125.1329434,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349116249&doi=10.1145%2f1329125.1329434&partnerID=40&md5=e15919384d9d2bae22084e828e135bbc,"Oregon State University, 204 Rogers Hall, Corvallis, OR 97331, United States; UCSC, NASA Ames Research Center, Mailstop 269-3, Moffett Field, CA 94035, United States","Tumer, K., Oregon State University, 204 Rogers Hall, Corvallis, OR 97331, United States; Agogino, A., UCSC, NASA Ames Research Center, Mailstop 269-3, Moffett Field, CA 94035, United States","Air traffic flow management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today. The FAA estimates that in 2005 alone, there were over 322,000 hours of delays at a cost to the industry in excess of three billion dollars. Finding reliable and adaptive solutions to the flow management problem is of paramount importance if the Next Generation Air Transportation Systems are to achieve the stated goal of accommodating three times the current traffic volume. This problem is particularly complex as it requires the integration and/or coordination of many factors including: new data (e.g., changing weather info), potentially conflicting priorities (e.g., different airlines), limited resources (e.g., air traffic controllers) and very heavy traffic volume (e.g., over 40,000 flights over the US airspace). In this paper we use FACET - an air traffic flow simulator developed at NASA and used extensively by the FAA and industry - to test a multi-agent algorithm for traffic flow management. An agent is associated with a fix (a specific location in 2D space) and its action consists of setting the separation required among the airplanes going though that fix. Agents use reinforcement learning to set this separation and their actions speed up or slow down traffic to manage congestion. Our FACET based results show that agents receiving personalized rewards reduce congestion by up to 45% over agents receiving a global reward and by up to 67% over a current industry approach (Monte Carlo estimation). © 2007 IFAAMAS.",Air traffic control; Multiagent systems; Optimization; Reinforcement learning,Conference Paper,,Scopus,2-s2.0-60349116249
SCOPUS,"Moradi S., Rad A.H.M., Wong V.W.S.",A novel scheduling algorithm for video traffic in high-rate WPANs,2007,GLOBECOM - IEEE Global Telecommunications Conference,,,4411054,742,747,,5,10.1109/GLOCOM.2007.144,https://www.scopus.com/inward/record.uri?eid=2-s2.0-39349116080&doi=10.1109%2fGLOCOM.2007.144&partnerID=40&md5=613f2c9394a3ec54192483ff93c79c96,"Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada","Moradi, S., Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada; Rad, A.H.M., Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada; Wong, V.W.S., Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada","The emerging high-rate wireless personal area network (WPAN) technology is capable of supporting high-speed and high-quality real-time multimedia applications. In particular, MPEG-4 video streams are deemed to be a widespread traffic type. However, in the current IEEE 802.15.3 standard for media access control (MAC) of high-rate WPANs, the implementation details of some key issues such as scheduling and quality of service (QoS) provisioning have not been addressed. In this paper, we first propose a mathematical model for the optimal scheduling scheme for MPEG-4 flows in high-rate WPANs. We also propose an RL scheduler based on the reinforcement learning (RL) technique. Simulation results show that our proposed RL scheduler achieves nearly optimal performance and performs better than F-SRPT [1], EDD+SRPT [2], and PAP [3] scheduling algorithms in terms of a lower decoding failure rate. © 2007 IEEE.",,Conference Paper,,Scopus,2-s2.0-39349116080
SCOPUS,"Zhu Y.-H., Xiao G., Li Z.-Y., Zhu F.",A location management scheme with reinforcement learning for PCS networks,2007,"2007 IEEE International Conference on Networking, Sensing and Control, ICNSC'07",,,4238994,224,227,,2,10.1109/ICNSC.2007.372781,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34748869523&doi=10.1109%2fICNSC.2007.372781&partnerID=40&md5=9bb5981ee4eb3f0c8aa4d6759ad585ff,"IEEE; College of Information Engineering, Zhejiang University of Technology, Hangzhou, 310032, China; College of Business Administration, Zhejiang University of Technology, Hangzhou, 310023, China; Dept. of Computer, International College, Zhejiang University of Technology, Hangzhou, 310023, China","Zhu, Y.-H., IEEE, College of Information Engineering, Zhejiang University of Technology, Hangzhou, 310032, China; Xiao, G., College of Information Engineering, Zhejiang University of Technology, Hangzhou, 310032, China; Li, Z.-Y., College of Business Administration, Zhejiang University of Technology, Hangzhou, 310023, China; Zhu, F., Dept. of Computer, International College, Zhejiang University of Technology, Hangzhou, 310023, China","Mobility management, consisting of location management and handoff management, is a challenge for Personal Communication Services (PCS) networks. Location management mainly involves two operations: location update and paging. A two-stage paging strategy with Reinforcement Learning is proposed. Under this scheme, each cell in a location area is given a preference. Besides, a small real number is used to decide whether to choose and page cells with higher preferences or with lower ones during the first paging stage so that the scheme can quickly adapt to the moving pattern of mobiles. Simulation results show that the proposed paging strategy can reduce the cost as much as 25%, compared with the paging strategy of the basic location management scheme used in the existing PCS networks. © 2007 IEEE.",Location management; Mobility management; Personal communication service (PCS) networks,Conference Paper,,Scopus,2-s2.0-34748869523
SCOPUS,"Vandenbroeck M., Verschelden G., Boonaert T., Haute L.V.",Changes in the digital divide: A case from Belgium: Colloquium,2007,British Journal of Educational Technology,38,4,,742,743,,4,10.1111/j.1467-8535.2007.00698.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250010931&doi=10.1111%2fj.1467-8535.2007.00698.x&partnerID=40&md5=0e7cb21f6ecfac56d03ef05ac6363b9f,"Ghent University, SocialWelfare Studies, Dunantlaan 2, B-9000 Gent, Belgium","Vandenbroeck, M., Ghent University, SocialWelfare Studies, Dunantlaan 2, B-9000 Gent, Belgium; Verschelden, G.; Boonaert, T.; Haute, L.V.","A major concern about the use of information and communication technology (ICT) in adult education is that the digital divide may reinforce the already existing learning divide, because it runs along the same lines of socio-economic class, gender and ethnicity. A case study of family day care providers (FDC) in Flanders, Belgium is presented to study the potentialities and problems with low professional status, as well as a target group for enhancing professionalism. A study was conducted to assess to what degree technology may be integrated in FDC provider's experience, cultures and beliefs. An analysis of covariance (ANCOVA) of the perceived PC skills as dependant variable and including education, previous work experiences with a PC, previous adult training in PC use, and age as independent variables showed that only age as independent variable was interrelated with PC skills. The result also showed that anxiety and motivation in the sample need to be considered as personal variables that may not run along tradition lines of the digital divide.",,Conference Paper,,Scopus,2-s2.0-34250010931
SCOPUS,"Pineau J., Bellemare M.G., Rush A.J., Ghizaru A., Murphy S.A.",Constructing evidence-based treatment strategies using methods from computer science,2007,Drug and Alcohol Dependence,88,SUPPL. 2,,S52,S60,,25,10.1016/j.drugalcdep.2007.01.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047273906&doi=10.1016%2fj.drugalcdep.2007.01.005&partnerID=40&md5=636602c8d15aa0f10a7e2b9c0a82f70d,"McGill University, School of Computer Science, 3480 University St., Montreal, Que. H3A 2A7, Canada; University of Texas, Southwestern Medical Center, 323 Harry Hines Blvd, Dallas, TX 75390, United States; University of Michigan, Institute for Social Research, 426 Thompson St, Ann Arbor, MI 48106, United States","Pineau, J., McGill University, School of Computer Science, 3480 University St., Montreal, Que. H3A 2A7, Canada; Bellemare, M.G., McGill University, School of Computer Science, 3480 University St., Montreal, Que. H3A 2A7, Canada; Rush, A.J., University of Texas, Southwestern Medical Center, 323 Harry Hines Blvd, Dallas, TX 75390, United States; Ghizaru, A., McGill University, School of Computer Science, 3480 University St., Montreal, Que. H3A 2A7, Canada; Murphy, S.A., University of Michigan, Institute for Social Research, 426 Thompson St, Ann Arbor, MI 48106, United States","This paper details a new methodology, instance-based reinforcement learning, for constructing adaptive treatment strategies from randomized trials. Adaptive treatment strategies are operationalized clinical guidelines which recommend the next best treatment for an individual based on his/her personal characteristics and response to earlier treatments. The instance-based reinforcement learning methodology comes from the computer science literature, where it was developed to optimize sequences of actions in an evolving, time varying system. When applied in the context of treatment design, this method provides the means to evaluate both the therapeutic and diagnostic effects of treatments in constructing an adaptive treatment strategy. The methodology is illustrated with data from the STAR*D trial, a multi-step randomized study of treatment alternatives for individuals with treatment-resistant major depressive disorder. © 2007 Elsevier Ireland Ltd. All rights reserved.",Clinical decision-making; Learning; Methodology; Sequential decisions; Treatment,Article,,Scopus,2-s2.0-34047273906
SCOPUS,"Murphy S.A., Collins L.M., Rush A.J.",Customizing treatment to the patient: Adaptive treatment strategies,2007,Drug and Alcohol Dependence,88,SUPPL. 2,,S1,S3,,48,10.1016/j.drugalcdep.2007.02.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047213941&doi=10.1016%2fj.drugalcdep.2007.02.001&partnerID=40&md5=e148da8ef07ce304ef8306445b05a86a,"Institute for Social Research, University of Michigan, 426 Thompson St, Ann Arbor, MI 48106-1248, United States; The Methodology Center, Department of Human Development and Family Studies, The Pennsylvania State University, University Park, PA 16802, United States; University of Texas, Southwestern Medical Center at Dallas, Dallas, TX 75390, United States","Murphy, S.A., Institute for Social Research, University of Michigan, 426 Thompson St, Ann Arbor, MI 48106-1248, United States; Collins, L.M., The Methodology Center, Department of Human Development and Family Studies, The Pennsylvania State University, University Park, PA 16802, United States; Rush, A.J., University of Texas, Southwestern Medical Center at Dallas, Dallas, TX 75390, United States",[No abstract available],Clinical decision support; Control engineering; Experimental design; Individualized care; Reinforcement learning; Stepped care; Structured treatment interruptions,Editorial,,Scopus,2-s2.0-34047213941
SCOPUS,[No author name available],"2nd International Conference on Computer Science and its Applications, CIIA 2009 [Proceedings of the 2nd Conference Internationale sur l'Informatique et Ses Applications, CIIA 2009]",2007,CEUR Workshop Proceedings,547,,,,,953,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897369782&partnerID=40&md5=b05d46663d4442d000f9c11e17d5c6f4,,,The proceedings contain 86 papers. The topics discussed include: practical use of an audiometer based on a computer software for hearing loss screening; ontology-driven method for ranking unexpected rules; color quantization and its impact on color histogram based image retrieval; a multi-agent framework for a web-based decision support system applied to manufacturing system; simulation of the navigation of a mobile robot by the Qlearning using artificial neuron networks; electronic corpora: as powerful tools in computational linguistic analyses; towards WSMO ontology specification from existing web services; dynamic scheduling in petroleum process using reinforcement learning; applying a discrete particle swarm optimization algorithm to database vertical partition; personal identification by fingerprints based on Gabor filters; and PNtools: a multi-language environment to integrate Petri nets tools.,,Conference Review,,Scopus,2-s2.0-84897369782
SCOPUS,"Chen J.-F., Lin W.-C., Bai H.-S., Chao H.-C.",A reinforcement self-learning model on an intelligent behavior avatar in a virtual world,2006,"Proceedings - IEEE International Conference on Sensor Networks, Ubiquitous, and Trustworthy Computing",2006 II,,1636185,268,274,,,10.1109/SUTC.2006.1636185,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845402688&doi=10.1109%2fSUTC.2006.1636185&partnerID=40&md5=cdb468c82990bb2275aad8e623c60ddf,"Department of Information Engineering, TamKang University, Taiwan; Department of Information Techonolgy, Tak-Ming College, Taiwan","Chen, J.-F., Department of Information Engineering, TamKang University, Taiwan; Lin, W.-C., Department of Information Techonolgy, Tak-Ming College, Taiwan; Bai, H.-S., Department of Information Engineering, TamKang University, Taiwan; Chao, H.-C., Department of Information Engineering, TamKang University, Taiwan","In this paper, a novel method for personal intelligent behavior avatar (IBA) is proposed to acquire autonomous behavior based on the interactions between user and smart objects in the virtual environment. In this method, the behavior decision model and the self-learning model are integrated by Bayesian Networks and reinforcement learning. The Bayesian Networks can treat interaction experiences using statistical processes, and the sureness of decision making is represented by certainty factors using stochastic reasoning. The reinforcement learning is implemented by learning experimentation or trial and error mechanisms to improve the performance of IBA through feedback. Therefore, the IBA makes a strategic decision that is approximated and appropriate to the user through the self-learning process by reinforcement learning. Finally, the feasibility of this method is investigated by imitating user's behavior and the results of self-learning process. The results of simulation show that the method is successful in imitating user's behavior and improving the performance of IBA. © 2006 IEEE.",Bayesian networks; Intelligent behavior avatar; Reinforcement learning; Virtual environment,Conference Paper,,Scopus,2-s2.0-33845402688
SCOPUS,[No author name available],"Research and Development in Intelligent Systems XXII - Proceedings of AI 2005, the 25th SGAI International Conference on Innovative Techniques and Applications of Artificial Intelligence",2006,"Research and Development in Intelligent Systems XXII - Proceedings of AI 2005, the 25th SGAI International Conference on Innovative Techniques and Applications of Artificial Intelligence",,,,,,378,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881402476&partnerID=40&md5=842e11fee8597640c83592e62ab5f3f9,,,The proceedings contain 26 papers. The topics discussed include: computational intelligence for bioinformatics: a knowledge engineering approach; robot docking based on omnidirectional vision and reinforcement learning; global EM learning offinite mixture models using the greedy elimination method; hierarchical knowledge-oriented specification for formation integration; the semantic web as a linguistic resource: opportunities for natural language generation; using simple ontologies to build personal webs of knowledge; exploring the noisy threshold function in designing Bayesian networks; collaborative recommending using formal concept analysis; formal concept analysis for knowledge refinement in case based reasoning; improved methods for extracting frequent itemsets from interim-support trees; and reliable instance classification with version spaces.,,Conference Review,,Scopus,2-s2.0-84881402476
SCOPUS,Zhang K.,Learn to coordinate with generic non-stationary opponents,2006,"Proceedings of the 5th IEEE International Conference on Cognitive Informatics, ICCI 2006",1,,4216463,558,565,,1,10.1109/COGINF.2006.365546,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650040559&doi=10.1109%2fCOGINF.2006.365546&partnerID=40&md5=9f1fc7d6744f4831c7401c734cbe7f23,"Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China","Zhang, K., Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China","Learning to coordinate with non-stationary opponents is a major challenge for adaptive agents. Most previous research investigated only restricted classes of such dynamic opponents. The main contribution of this paper is twofold: (i) A class of generic non-stationary opponents is introduced. The opponents keep mixed strategies which change with less regularity. It's showed that the independent reinforcement learners (ILs), which have neither prior knowledge nor opponent models, cannot coordinate well with this type of opponent, (ii) A new exploration strategy, the DAE (Detect and Explore) mechanism, is tailored for the ILs in such coordination tasks. This mechanism allows the ILs dynamically detect changes in the opponent's behavior and adjust their learning rate and exploration temperature. It's showed that ILs using this strategy are still able to converge in self-play, and are able to coordinate well with the non-stationary opponents. © 2006 IEEE.",Coordination game; Exploration strategy; Non-stationary opponent; Reinforcement learning,Conference Paper,,Scopus,2-s2.0-78650040559
SCOPUS,"Witt K., Daniels C., Daniel V., Schmitt-Eliassen J., Volkmann J., Deuschl G.",Patients with Parkinson's disease learn to control complex systems-an indication for intact implicit cognitive skill learning,2006,Neuropsychologia,44,12,,2445,2451,,16,10.1016/j.neuropsychologia.2006.04.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745944539&doi=10.1016%2fj.neuropsychologia.2006.04.013&partnerID=40&md5=c0a0717070a406cc65c19d0e09ac84a5,"Department of Neurology, University Medical Center Schleswig-Holstein, Campus Kiel, Germany","Witt, K., Department of Neurology, University Medical Center Schleswig-Holstein, Campus Kiel, Germany; Daniels, C., Department of Neurology, University Medical Center Schleswig-Holstein, Campus Kiel, Germany; Daniel, V., Department of Neurology, University Medical Center Schleswig-Holstein, Campus Kiel, Germany; Schmitt-Eliassen, J., Department of Neurology, University Medical Center Schleswig-Holstein, Campus Kiel, Germany; Volkmann, J., Department of Neurology, University Medical Center Schleswig-Holstein, Campus Kiel, Germany; Deuschl, G., Department of Neurology, University Medical Center Schleswig-Holstein, Campus Kiel, Germany","Implicit memory and learning mechanisms are composed of multiple processes and systems. Previous studies demonstrated a basal ganglia involvement in purely cognitive tasks that form stimulus response habits by reinforcement learning such as implicit classification learning. We will test the basal ganglia influence on two cognitive implicit tasks previously described by Berry and Broadbent, the sugar production task and the personal interaction task. Furthermore, we will investigate the relationship between certain aspects of an executive dysfunction and implicit learning. To this end, we have tested 22 Parkinsonian patients and 22 age-matched controls on two implicit cognitive tasks, in which participants learned to control a complex system. They interacted with the system by choosing an input value and obtaining an output that was related in a complex manner to the input. The objective was to reach and maintain a specific target value across trials (dynamic system learning). The two tasks followed the same underlying complex rule but had different surface appearances. Subsequently, participants performed an executive test battery including the Stroop test, verbal fluency and the Wisconsin card sorting test (WCST). The results demonstrate intact implicit learning in patients, despite an executive dysfunction in the Parkinsonian group. They lead to the conclusion that the basal ganglia system affected in Parkinson's disease does not contribute to the implicit acquisition of a new cognitive skill. Furthermore, the Parkinsonian patients were able to reach a specific goal in an implicit learning context despite impaired goal directed behaviour in the WCST, a classic test of executive functions. These results demonstrate a functional independence of implicit cognitive skill learning and certain aspects of executive functions. © 2006 Elsevier Ltd. All rights reserved.",Cognitive skill learning; Executive dysfunction; Implicit learning; Parkinson's disease,Article,,Scopus,2-s2.0-33745944539
SCOPUS,"Fujishiro T., Nakano H., Miyauchi A.",Parallel distributed profit sharing for PC cluster,2006,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),4131 LNCS - I,,,811,819,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749828133&partnerID=40&md5=ffbbc47e327e70287984d490431bc14d,"Musashi Institute of Technology, 1-28-1 Tamazutsumi, Setagayaku, Tokyo, 158-8557, Japan","Fujishiro, T., Musashi Institute of Technology, 1-28-1 Tamazutsumi, Setagayaku, Tokyo, 158-8557, Japan; Nakano, H., Musashi Institute of Technology, 1-28-1 Tamazutsumi, Setagayaku, Tokyo, 158-8557, Japan; Miyauchi, A., Musashi Institute of Technology, 1-28-1 Tamazutsumi, Setagayaku, Tokyo, 158-8557, Japan","This paper presents a parallel reinforcement learning method considered communication cost. In our method, each agent communicates only action sequences with a constant episode interval. As the communication interval is longer, communication cost is smaller, but parallelism is lower. Implementing our method on PC cluster, we investigate such trade-off characteristics. We show that computation time to learning can be reduced by properly adjusting the communication interval. © Springer-Verlag Berlin Heidelberg 2006.",,Conference Paper,,Scopus,2-s2.0-33749828133
SCOPUS,"Prommer T., Holzapfel H., Waibel A.",Rapid simulation-driven reinforcement learning of multimodal dialog strategies in human-robot interaction,2006,"INTERSPEECH 2006 and 9th International Conference on Spoken Language Processing, INTERSPEECH 2006 - ICSLP",4,,,1918,1921,,12,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44949181000&partnerID=40&md5=b035fc070c6373c68c94bedfd9b4194d,"Interactive Systems Labs., Universität Karlsruhe (TH), Germany","Prommer, T., Interactive Systems Labs., Universität Karlsruhe (TH), Germany; Holzapfel, H., Interactive Systems Labs., Universität Karlsruhe (TH), Germany; Waibel, A., Interactive Systems Labs., Universität Karlsruhe (TH), Germany","In this work we propose a procedure model for rapid automatic strategy learning in multimodal dialogs. Our approach is tailored for typical task-oriented human-robot dialog interactions, with no prior knowledge about the expected user and system dynamics being present. For such scenarios, we propose the use of stochastic dialog simulation for strategy learning, where the user and system error models are solely trained through the initial execution of an inexpensive Wizard-of-Oz experiment. We argue that for the addressed dialogs, already a small data corpus combined with a low-conditioned simulation model facilitates learning of strong and complex dialog strategies. To validate our overall approach, we empirically show the supremacy of the learned strategy over a hand-crafted strategy for a concrete human-robot dialog scenario. To the authors' knowledge, this work is the first to perform strategy learning from multimodal dialog simulation.",Multimodal human-robot dialogs; Strategy learning,Conference Paper,,Scopus,2-s2.0-44949181000
SCOPUS,"Srivihok A., Sukonmanee P.",Intelligent agent for E-tourism: Personalization travel support agent using reinforcement learning,2005,CEUR Workshop Proceedings,143,,,,,4,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883477666&partnerID=40&md5=b8091062d9e61ff7714e961f19c584d9,"Department of Computer Science, Faculty of Science, Kasetsart University, Bangkok 10900, Thailand","Srivihok, A., Department of Computer Science, Faculty of Science, Kasetsart University, Bangkok 10900, Thailand; Sukonmanee, P., Department of Computer Science, Faculty of Science, Kasetsart University, Bangkok 10900, Thailand","Web personalization and one to one marketing have been introduced as strategy and marketing tools. By using historical and present information of customers, organizations can learn, predict customer's behaviors and develop products to fit potential customers. In this study, a Personalization Travel Support System is introduced to manage traveling information for user. It provides the information that matches the users' interests. This system applies the Reinforcement Learning to analyze, learn customer behaviors and recommend products to meet customer interests. There are two learning approaches using in this study. First, Personalization Learner by Group Properties is learning from all users in one group to find the group interests of travel information by using given data on user ages and genders. Second, Personalization Learner by User Behavior: user profile, user behaviors and trip features will be analyzed to find the unique interest of each web user. The results from this study reveal that it is possible to develop Personalization Travel Support System. Using weighted trip features improve effectiveness and increase the accuracy of the personalized engine. Precision, Recall and Harmonic Mean of the learned system are higher than the original one. This study offers useful information regarding the areas of personalization of web support system.",Intelligent agent; Personalization; Recommendation algorithm; Reinforcement Learning,Conference Paper,,Scopus,2-s2.0-84883477666
SCOPUS,"Madani O., DeCoste D.",Contextual recommender problems,2005,"Proceedings of the 1st International Workshop on Utility-Based Data Mining, UBDM '05",,,,86,89,,9,10.1145/1089827.1089838,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751025354&doi=10.1145%2f1089827.1089838&partnerID=40&md5=02bb7c496352dcb4e276b940c4c3ea5f,"Yahoo Research, 74 N. Pasadena Ave, Pasadena, CA 91103, United States","Madani, O., Yahoo Research, 74 N. Pasadena Ave, Pasadena, CA 91103, United States; DeCoste, D., Yahoo Research, 74 N. Pasadena Ave, Pasadena, CA 91103, United States","The contextual recommender task is the problem of making useful offers, e.g., placing ads or related links on a web page, based on the context information, e.g., contents of the page and information about the user visiting, and information on the available alternatives, i.e., the advertisements or relevant links. In the case of ads for example, the goal is to select ads that result in high click rates, where the (ad) click rate is some unknown function of the attributes of the context and ad. We describe the task and make connections to related problems including recommender and multi-armed bandit problems. Copyright 2005 ACM.",data mining; exploration-exploitation; multi-armed bandit; personalization; recommenders; regression; reinforcement learning; utility,Conference Paper,,Scopus,2-s2.0-33751025354
SCOPUS,"Chang E.Y., Hoi S.C.H., Wang X., Wei-Ying M., Lyu M.R.",A unified learning paradigm for large-scale personalized information management,2005,Emerging Information Technology Conference 2005,2005,,1544372,151,154,,2,10.1109/EITC.2005.1544372,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751191408&doi=10.1109%2fEITC.2005.1544372&partnerID=40&md5=9a1271e1b08b0bc998239179e6e096be,"Microsoft Research Asia, 49 Zhichun Road, Beijing, China; Electrical and Computer Engineering, University of California, Santa Barbara, United States; Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong, Hong Kong","Chang, E.Y., Electrical and Computer Engineering, University of California, Santa Barbara, United States; Hoi, S.C.H., Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong, Hong Kong; Wang, X., Microsoft Research Asia, 49 Zhichun Road, Beijing, China; Wei-Ying, M., Microsoft Research Asia, 49 Zhichun Road, Beijing, China; Lyu, M.R., Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong, Hong Kong","Statistical-learning approaches such as unsupervised learning, supervised learning, active learning, and reinforcement learning have generally been separately studied and applied to solve application problems. In this paper, we provide an overview of our newly proposed unified learning paradigm (ULP), which combines these approaches into one synergistic framework. We outline the architecture and the algorithm of ULP, and explain benefits of employing this unified learning paradigm on personalizing information management. © 2005 IEEE.",,Conference Paper,,Scopus,2-s2.0-33751191408
SCOPUS,[No author name available],"Proceedings of the 1st International Workshop on Interoperability of Web-Based Educational Systems - In Conjunction with the 14th International World Wide Web Conference, WWW 2005",2005,CEUR Workshop Proceedings,143,,,,,84,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883484696&partnerID=40&md5=ee9175932efc35ede5e676b2714edaee,,,The proceedings contain 12 papers. The topics discussed include: a model and infrastructure for federated learning content repositories; a simple query interface for interoperable learning repositories; interoperability for peer-to-peer networks: opening P2P to the rest of the world; query translation between RDF and XML: a case study in the educational domain; improving interoperability through better re-usability; on interoperability of ontologies for web-based educational systems; defining several ontologies to enhance the expressive power of queries; learner-centered accessibility for interoperable web-based educational systems; knowledge level design support for adaptive learning contents - ontological consideration on knowledge level structure of SCORM2004 contents; web-based learning in remedial course of science and technology; and intelligent agent for e-tourism: personalization travel support agent using reinforcement learning.,,Conference Review,,Scopus,2-s2.0-84883484696
SCOPUS,"Chen J.-F., Lin W.-C., Bai H.-S., Yang C.-C., Chao H.-C.",Constructing an intelligent behavior avatar in a virtual world: A self-learning model based on reinforcement,2005,"Proceedings of the 2005 IEEE International Conference on Information Reuse and Integration, IRI - 2005",2005,,1506510,421,426,,,10.1109/IRI-05.2005.1506510,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745700495&doi=10.1109%2fIRI-05.2005.1506510&partnerID=40&md5=1c47bdf85edf43669ebab206922220f7,"Department of Information Engineering, TamKang University, Taiwan; Department of Information Technology, Tak-Ming College, Taiwan","Chen, J.-F., Department of Information Engineering, TamKang University, Taiwan; Lin, W.-C., Department of Information Technology, Tak-Ming College, Taiwan; Bai, H.-S., Department of Information Engineering, TamKang University, Taiwan; Yang, C.-C., Department of Information Engineering, TamKang University, Taiwan; Chao, H.-C., Department of Information Engineering, TamKang University, Taiwan","In this paper, a novel method for personal intelligent behavior avatar (IBA) is proposed to acquire autonomous behavior based on the interactions between user and smart objects in the virtual environment. In this method, the behavior decision model and the self-learning model are integrated by Bayesian Networks and reinforcement learning. The Bayesian Networks can treat interaction experiences using statistical processes, and the sureness of decision making is represented by certainty factors using stochastic reasoning. The reinforcement learning is implemented by learning experimentation or trial and error mechanisms to improve the performance of IBA through feedback. Therefore, the IBA makes a strategic decision that is approximated and appropriate to the user through the self-learning process by reinforcement learning. Finally, the feasibility of this method is investigated by imitating user 's behavior and the results of self-learning process. The results of simulation show that the method is successful in imitating user 's behavior and improving the performance of IBA. © 2005 IEEE.",Bayesian Networks; Intelligent behavior avatar; Reinforcement learning; Virtual environment,Conference Paper,,Scopus,2-s2.0-33745700495
SCOPUS,"Zhong M., Wang M., Lu R.",Research on user's interest of web personalized information service,2005,Journal of Computational Information Systems,1,4,,959,965,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748151446&partnerID=40&md5=a62c93d05f8749b3abd44d7597fa92b5,"Department of Compute Science and Engineering, Shanghai Jiaotong University, Shanghai 200030, China; School of Information Engineering, East China Jiaotong University, Nanchang 330013, China; School of Computer Information Engineering, Jiangxi Normal University, Nanchang 330022, China","Zhong, M., Department of Compute Science and Engineering, Shanghai Jiaotong University, Shanghai 200030, China, School of Information Engineering, East China Jiaotong University, Nanchang 330013, China; Wang, M., School of Computer Information Engineering, Jiangxi Normal University, Nanchang 330022, China; Lu, R., Department of Compute Science and Engineering, Shanghai Jiaotong University, Shanghai 200030, China","An important problem in Web Personalized Information Service is to describe, measure, acquire and update a user's interest information. Aiming at the problem, this paper puts forward a new approach: Firstly, a general model for the user's interest information is constructed; Secondly, the learning agent initializes the interestingness of the user's interest topic according to his background information; Lastly, the learning agent updates the interestingness dynamically, based on his browsing actions or behaviors, and on his feedbacks by adopting Q-Learning algorithm of Reinforcement Learning. The above approach has four advantages: 1)the user need not describe and edit his profile; 2)the interestingness of the user's interest topic can be computed and quantified; 3)the agent can track the user's interest navigation and expansion dynamically and adapt to it; 4)the approach can be implemented easily.",Interestingness; Personalized information service; Q-learning; User interest model; Web,Article,,Scopus,2-s2.0-33748151446
SCOPUS,"Preda M., Popescu D.",Personalized Web recommendations: Supporting epistemic information about end-users,2005,"Proceedings - 2005 IEEE/WIC/ACM InternationalConference on Web Intelligence, WI 2005",2005,,1517935,692,695,,1,10.1109/WI.2005.115,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748869891&doi=10.1109%2fWI.2005.115&partnerID=40&md5=87549293607face91123aa8cf505297c,"University of Craiova, Department of Computer Science, Craiova, 200585; University of Craiova, Department of Automation, Craiova, 200585","Preda, M., University of Craiova, Department of Computer Science, Craiova, 200585; Popescu, D., University of Craiova, Department of Automation, Craiova, 200585",The online recommendations are a popular presence in the Web sites world due to their potential to increase the customers' satisfaction. The ability to represent epistemic information about the clients' beliefs is important to understand their needs. This paper presents a recommender system based on reinforcement learning. The system represents concepts presented on a Web site by epistemic logical programs and uses a similarity measure between programs in order to facilitate generalization. A prototype of this system and experiments are presented. © 2005 IEEE.,,Conference Paper,,Scopus,2-s2.0-33748869891
SCOPUS,"Agogino A., Tumer K.",Multi-agent reward analysis for learning in noisy domains,2005,Proceedings of the International Conference on Autonomous Agents,,,,233,240,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33644813420&partnerID=40&md5=8dd6ac100b9f6688afbab875b55b06cc,"UC Santa Cruz, NASA Ames Research Center, Mailstop 269-3, Moffett Field, CA 94035, United States; NASA Ames Research Center, Mailstop 269-4, Moffett Field, CA 94035, United States","Agogino, A., UC Santa Cruz, NASA Ames Research Center, Mailstop 269-3, Moffett Field, CA 94035, United States; Tumer, K., NASA Ames Research Center, Mailstop 269-4, Moffett Field, CA 94035, United States","In many multi-agent learning problems, it is difficult to determine, a priori, the agent reward structure that will lead to good performance. This problem is particularly pronounced in continuous, noisy domains ill-suited to simple table backup schemes commonly used in TD(λ)/Q-learning. In this paper, we present a new reward evaluation method that provides a visualization of the tradeoff between coordination among the agents and the difficulty of the learning problem each agent faces. This method is independent of the learning algorithm and is only a function of the problem domain and the agents' reward structure. We then use this reward property visualization method to determine an effective reward without performing extensive simulations. We test this method in both a static and a dynamic multi-rover learning domain where the agents have continuous state spaces and where their actions are noisy (e.g., the agents' movement decisions are not always carried out properly). Our results show that in the more difficult dynamic domain, the reward efficiency visualization method provides a two order of magnitude speedup in selecting a good reward. Most importantly it allows one to quickly create and verify rewards tailored to the observational limitations of the domain. Copyright 2005 ACM.",Multiagent Systems; Reinforcement Learning; Visualization,Conference Paper,,Scopus,2-s2.0-33644813420
SCOPUS,"Hwang K.-S., Hsu Y.-P., Hsieh H.-W., Lin H.-Y.",Hardware implementation of FAST-based reinforcement learning algorithm,2005,"Proceedings of the 2005 IEEE International Workshop on VLSI Design and Video Technology, IWVDVT 2005",,,,143,146,,5,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23844518746&partnerID=40&md5=d9d8f4f08fe1f1ca5c563113c35f8811,"Department of Electrical, National Chung-Cheng University, Chiayi 711, Taiwan; Department of Computer Science and Information Engineering, National Formosa University, Yunlin 632, Taiwan","Hwang, K.-S., Department of Electrical, National Chung-Cheng University, Chiayi 711, Taiwan; Hsu, Y.-P., Department of Computer Science and Information Engineering, National Formosa University, Yunlin 632, Taiwan; Hsieh, H.-W., Department of Electrical, National Chung-Cheng University, Chiayi 711, Taiwan; Lin, H.-Y., Department of Electrical, National Chung-Cheng University, Chiayi 711, Taiwan","A FAST-based (Flexible Adaptable-Size Topology) reinforcement learning chip is implemented in this article. Basically, the FAST is an ART-like (Adaptive Resonance Theory) mechanism. The ART is characterized as one of unsupervised learning neural network models, facilitated to solve stability-plasticity dilemma. The chip is a self organizing architecture which consists of three main structures including similarity, learning, and pruning. Dynamically adjusting the size of sensitivity regions of each neuron and adaptively pruning one of the neurons when an input pattern activates more than one neuron, the chip can preserve hardware resources (available neurons) to accommodate more categories. The clustered result by the implemented chip is then sent to an AHC (Adaptive Heuristic Critic) architecture (emulated by a personal computer) to learn to balance an inverted pendulum system which is also emulated by the personal computer for verifying the implemented architecture. © 2005 IEEE.",,Conference Paper,,Scopus,2-s2.0-23844518746
SCOPUS,[No author name available],"15th Workshop on Information Technology and Systems, WITS 2005",2005,"15th Workshop on Information Technology and Systems, WITS 2005",,,,278,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905734803&partnerID=40&md5=16f91daafd582e0cb11e83120c2cdee3,,,The proceedings contain 56 papers. The topics discussed include: a clustering based approach for facilitating semantic web service discovery; a service oriented engine for searching deep web; resource scheduling in grid computing networks to maximize business value; adaptive and personalized interfaces for mobile web; variance-based active learning for classifier induction; why does collaborative filtering work? - recommendation model selection by bipartite graph randomness analysis; building web collections for vertical markets; querying the semantic web with ginseng: a guided input natural language search engine; contextual web search based on semantic relationships; operational models of intrusion prevention systems; protecting privacy against classification attacks in data mining; and learning optimal seller strategies with intelligent agents: application of evolutionary and reinforcement learning.,,Conference Review,,Scopus,2-s2.0-84905734803
SCOPUS,"Oh D., Tan C.L.",Making better recommendations with online profiling agents,2004,Proceedings of the National Conference on Artificial Intelligence,,,,785,792,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-9444285517&partnerID=40&md5=52bdd57d2094350dc0f993039c1ff5df,"School of Computing, National University of Singapore, 3 Science Drive 2, Singapore 117543, Singapore","Oh, D., School of Computing, National University of Singapore, 3 Science Drive 2, Singapore 117543, Singapore; Tan, C.L., School of Computing, National University of Singapore, 3 Science Drive 2, Singapore 117543, Singapore","In recent years, we have witnessed the success of autonomous agents applying machine learning techniques across a wide range of applications. However, agents applying the same machine learning techniques in online applications have not been so successful. Even agent-based hybrid recommender systems that combine information filtering techniques with collaborative filtering techniques have only been applied with considerable success to simple consumer goods such as movies, books, clothing and food. Complex, adaptive autonomous agent systems that can handle complex goods such as real estate, vacation plans, insurance, mutual funds, and mortgage have yet emerged. To a large extent, the reinforcement learning methods developed to aid agents in learning have been more successfully deployed in offline applications. The inherent limitations in these methods have rendered them somewhat ineffective in online applications. In this paper, we postulate that a small amount of prior knowledge and human-provided input can dramatically speed up online learning. We will demonstrate that our agent HumanE - with its prior knowledge or ""experiences"" about the real estate domain -can effectively assist users in identifying requirements, especially unstated ones, quickly and unobtrusively.",Electronic profiling; Experience; Inference; Intelligent agents; Interactive learning; Personalization; Reinforcement learning; User preferences,Conference Paper,,Scopus,2-s2.0-9444285517
SCOPUS,"Martin K.N., Arroyo I.",AgentX: Using Reinforcement Learning to Improve the Effectiveness of Intelligent Tutoring Systems,2004,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3220,,,564,572,,17,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048874714&partnerID=40&md5=c80fd6957e1ff113fd93a7e0ef37fab6,"Department of Computer Science, University of Massachusetts, 140 Governors Drive, Amherst, MA 01003, United States","Martin, K.N., Department of Computer Science, University of Massachusetts, 140 Governors Drive, Amherst, MA 01003, United States; Arroyo, I., Department of Computer Science, University of Massachusetts, 140 Governors Drive, Amherst, MA 01003, United States","Reinforcement Learning (RL) can be used to train an agent to comply with the needs of a student using an intelligent tutoring system. In this paper, we introduce a method of increasing efficiency by way of customization of the hints provided by a tutoring system, by applying techniques from RL to gain knowledge about the usefulness of hints leading to the exclusion or introduction of other helpful hints. Students are clustered into learning levels and can influence the agents method of selecting actions in each state in their cluster of affect. In addition, students can change learning levels based on their performance within the tutoring system and continue to affect the entire student population. The RL agent, AgentX, then uses the cluster information to create one optimal policy for all students in the cluster and begin to customize the help given to the cluster based on that optimal policy. © Springer-Verlag 2004 References.",,Article,,Scopus,2-s2.0-35048874714
SCOPUS,"Lee F.-M., Li L.-H., Liu Y.-C.",Automated negotiation in multiple e-Marketplaces by using Learning Personalized Mobile Shopping Agents,2004,"Proceedings of the International Conference on Artificial Intelligence, IC-AI'04",2,,,777,783,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12744281483&partnerID=40&md5=d315d9774254985d85ee436efabd63d8,"Department of Information Management, Chaoyang University of Technology, 168, GiFeng East Road, Wufeng, Taichung County, Taiwan","Lee, F.-M., Department of Information Management, Chaoyang University of Technology, 168, GiFeng East Road, Wufeng, Taichung County, Taiwan; Li, L.-H., Department of Information Management, Chaoyang University of Technology, 168, GiFeng East Road, Wufeng, Taichung County, Taiwan; Liu, Y.-C., Department of Information Management, Chaoyang University of Technology, 168, GiFeng East Road, Wufeng, Taichung County, Taiwan","In the B2C- or C2C- based e-Marketplaces, the same merchandise is sold by different sellers in different e-Marketplaces. The trading platforms nowadays has resulted in information explosion that makes buyers unable to retrieve and analyze entire merchandise information easily and, therefore, decreases their negotiation power. Moreover, without the records of buyers ' transaction preference, it 's not easy for most agent systems to help common buyers to increase their negotiation power. In this paper, we propose the Learning Personalized Mobile Shopping Agent (LPMSA) and apply it to three e-Marketplace architectures: alliance, broker, noncooperation. The buyer can dispatch mobile agents to multiple e-Marketplaces for collecting merchandise information, negotiating with sellers, and buying merchandise from the above architectures. Furthermore, the agent can learn more about buyer's preference. As a result, the proposed architecture can not only find suitable trading partners for buyers but also help them to get better deals.",Automated Negotiation; E-Marketplace; Mobile Agent; Reinforcement Learning,Conference Paper,,Scopus,2-s2.0-12744281483
SCOPUS,"Liu N.X., Zhou X., Baras J.S.",Adaptive hierarchical resource management for satellite channel in hybrid MANET-satellite-internet network,2004,IEEE Vehicular Technology Conference,60,6,,4027,4031,,4,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17144380830&partnerID=40&md5=b13f3023df97a84484e23fb5f9fd43ba,"Institute for Systems Research, University of Maryland, College Park, MD 20742, United States","Liu, N.X., Institute for Systems Research, University of Maryland, College Park, MD 20742, United States; Zhou, X., Institute for Systems Research, University of Maryland, College Park, MD 20742, United States; Baras, J.S., Institute for Systems Research, University of Maryland, College Park, MD 20742, United States","MANETs are often deployed in an infrastructure-less or hostile region where the satellite provides the only link for the MANETs to communicate with the rest part of the world. It faces many challenges to support multiple serviced communications between MANETs and Internet through satellite. In this paper we propose an efficient resource management scheme called AHRM to dynamically allocate bandwidths among multiple MANET users and multiple priority and non-priority services sharing a multi-access satellite channel. It uses a flexible hierarchical structure to exploit the channel utility and resolve contention from two levels. A bandwidth adaptation algorithm is designed to adjust the allocation dynamically in response to traffic and link status changes. The algorithm turns out to be in line with reinforcement learning and is a customized version of it for the practical satellite network setting. Implementation issues are discussed. Simulation results are presented, showing that the scheme can guarantee fast delivery of critical messages in spite of channel contention, and significantly improve the performance of multiple services. © 2004 IEEE.",Adaptive hierarchical scheduling; Hybrid network; Resource management; Satellite,Conference Paper,,Scopus,2-s2.0-17144380830
SCOPUS,"Peng C., Vuorimaa P.",Automatic navigation among mobile DTV services,2004,ICEIS 2004 - Proceedings of the Sixth International Conference on Enterprise Information Systems,,,,140,145,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-8444233936&partnerID=40&md5=61d5a01b971e0282eec404d1fbe43bb2,"Telecom. Software Multimedia Lab., Dept. of Compter Sci. and Eng., Helsinki University of Technology, P.O. Box 5400, FIN-02015, Finland","Peng, C., Telecom. Software Multimedia Lab., Dept. of Compter Sci. and Eng., Helsinki University of Technology, P.O. Box 5400, FIN-02015, Finland; Vuorimaa, P., Telecom. Software Multimedia Lab., Dept. of Compter Sci. and Eng., Helsinki University of Technology, P.O. Box 5400, FIN-02015, Finland","Limited number of input buttons on a mobile device, such as mobile phones and PDAs, restricts people's access to digital broadcast services. In this paper, we present a reinforcement learning approach to automatically navigating among services in mobile digital television systems. Our approach uses standard Q-learning algorithm as a theory basis to predict next button for the user by learning usage patterns from interaction experiences. We did the experiment using a modified algorithm in test system. The experimental results demonstrate that the performance is good and the method is feasible and appropriate in practice.",Automatic navigation; Button prediction; Exploration; Intelligent user interface; Reinforcement learning,Conference Paper,,Scopus,2-s2.0-8444233936
SCOPUS,"Tran T., Cohen R.",Improving user satisfaction in agent-based electronic marketplaces by reputation modelling and adjustable product quality,2004,"Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS 2004",2,,,828,835,,42,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4544220420&partnerID=40&md5=d1231e95154e8761409eedfa69f28d67,"School of Computer Science, University of Waterloo, Waterloo, Ont. N2L 3G1, Canada","Tran, T., School of Computer Science, University of Waterloo, Waterloo, Ont. N2L 3G1, Canada; Cohen, R., School of Computer Science, University of Waterloo, Waterloo, Ont. N2L 3G1, Canada","In this paper, we propose a market model and learning algorithms for buying and selling agents in electronic marketplaces. We take into account the fact that multiple selling agents may offer the same good with different qualities, and that selling agents may alter the quality of their goods. We also consider the possible existence of dishonest selling agents in the market. In our approach, buying agents learn to maximize their expected value of goods using reinforcement learning. In addition, they model and exploit the reputation of selling agents to avoid interaction with the disreputable ones, and therefore to reduce the risk of purchasing low value goods. Our selling agents learn to maximize their expected profits by using reinforcement learning to adjust product prices, and also by altering product quality to provide more customized value to their goods. This paper focuses on presenting results from experiments investigating the behaviours of buying and selling agents in large-sized electronic marketplaces. Our results confirm that buying and selling agents following the proposed algorithms obtain greater satisfaction than buying and selling agents who only use reinforcement learning, with the buying agents not modelling sellers' reputation and the selling agents not adjusting product quality.",,Conference Paper,,Scopus,2-s2.0-4544220420
SCOPUS,"Dinerstein J., Egbert P.K., De Garis H., Dinerstein N.",Fast and learnable behavioral and cognitive modeling for virtual character animation,2004,Computer Animation and Virtual Worlds,15,2,,95,108,,17,10.1002/cav.8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23444439826&doi=10.1002%2fcav.8&partnerID=40&md5=ad3fb45b1cdf0ae4c85400551d24b452,"Brigham Young University, 3366 TMCB, Provo, UT 84602, United States; Computer Science Department, Brigham Young University, United States; Computer Science Department, Utah State University, United States","Dinerstein, J., Brigham Young University, 3366 TMCB, Provo, UT 84602, United States; Egbert, P.K., Computer Science Department, Brigham Young University, United States; De Garis, H., Computer Science Department, Utah State University, United States; Dinerstein, N., Computer Science Department, Utah State University, United States","Behavioral and cognitive modeling for virtual characters is a promising field. It significantly reduces the workload on the animator, allowing characters to act autonomously in a believable fashion. It also makes interactivity between humans and virtual characters more practical than ever before. In this paper we present a novel technique where an artificial neural network is used to approximate a cognitive model. This allows us to execute the model much more quickly, making cognitively empowered characters more practical for interactive applications. Through this approach, we can animate several thousand intelligent characters in real time on a PC. We also present a novel technique for how a virtual character, instead of using an explicit model supplied by the user, can automatically learn an unknown behavioral/cognitive model by itself through reinforcement learning. The ability to learn without an explicit model appears promising for helping behavioral and cognitive modeling become more broadly accepted and used in the computer graphics community, as it can further reduce the workload on the animator. Further, it provides solutions for problems that cannot easily be modeled explicitly. Copyright © 2004 John Wiley & Sons, Ltd.",Behavioral modeling; Cognitive modeling; Computer animation; Machine learning; Reinforcement learning; Synthetic characters,Article,,Scopus,2-s2.0-23444439826
SCOPUS,[No author name available],"1st International Workshop on Computational Autonomy - Potential, Risks, Solutions, AUTONOMY 2003 held at 2nd International Joint Conference on Autonomous Agents and Multi-agent Systems, AAMAS 2003",2004,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2969,,,1,273,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942809633&partnerID=40&md5=d288c9b3105fb1f25c7a05da4938ed98,,,"The proceedings contain 21 papers. The special focus in this conference is on Agents and Computational Autonomy. The topics include: Agency, learning and animal-based reinforcement learning; agent belief autonomy in open multi-agent systems; dimensions of adjustable autonomy and mixed-initiative interaction; a taxonomy of autonomy in multiagent organisation; autonomy and reasoning for natural and artificial agents; autonomy in multi-agent systems; requirements for achieving software agents autonomy and defining their responsibility; agent design from the autonomy perspective; from individual based modeling to autonomy oriented computation; adjustable autonomy challenges in personal assistant agents; dynamic imputation of agent cognition; a teamwork coordination strategy using hierarchical role relationship matching and a dialectic architecture for computational autonomy.",,Conference Review,,Scopus,2-s2.0-84942809633
SCOPUS,"Chen J., Yang Z.",A learning multi-agent system for personalized information filtering,2003,"ICICS-PCM 2003 - Proceedings of the 2003 Joint Conference of the 4th International Conference on Information, Communications and Signal Processing and 4th Pacific-Rim Conference on Multimedia",3,,1292790,1864,1868,,4,10.1109/ICICS.2003.1292790,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148857678&doi=10.1109%2fICICS.2003.1292790&partnerID=40&md5=4e15f8c3abcc879d52a869e26135da00,"Information Communication Institute of Singapore (ICIS), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore","Chen, J., Information Communication Institute of Singapore (ICIS), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore; Yang, Z., Information Communication Institute of Singapore (ICIS), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore","A multi-agent hybrid learning approach to the problem of personalized information filtering is proposed in this paper. There are four agents in the multi-agent model. The Problem is modeled as Monte Carlo reinforcement learning. Our proposed algorithm is modified Monte Carlo method combined with features of unsupervised Suffix Tree Clustering and supervised Backpropagation network. We argue that this proposed approach can precisely capture the user's interest without repeatedly asking for his/her explicit rates and converge to the user's interest quickly. A conclusion is drawn that our approach is efficient, precise and converges more quickly compared with existing approaches. A prototype system is being developed. © 2003 IEEE.",Intelligent system; Multi-agent; Multi-agent learning; Personalized information filtering; Reinforcement learning,Conference Paper,,Scopus,2-s2.0-35148857678
SCOPUS,Yuan S.-T.,A personalized and integrative comparison-shopping engine and its applications,2003,Decision Support Systems,34,2,,139,156,,38,10.1016/S0167-9236(02)00077-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037209477&doi=10.1016%2fS0167-9236%2802%2900077-5&partnerID=40&md5=37491eee2acf272198efc1cf74b290e9,"Information Management Department, Fu-Jen University, 242 Taipei, Taiwan","Yuan, S.-T., Information Management Department, Fu-Jen University, 242 Taipei, Taiwan","Agents are the catalysts for commerce on the Web today. For example, comparison-shopping agents mediate the interactions between consumers and suppliers in order to yield markets that are more efficient. However, today's shopping agents are price-dominated, unreflective of the nature of supplier/consumer differentiation or the changing course of differentiation over time. This paper aims to tackle this dilemma and advances shopping agents into a stage where both kinds of differentiation are taken into account for enhanced understanding of the realities. We call them personalized and integrative shopping agents. These agents can leverage the interactive power of the Web for a more accurate understanding of consumer's preferences. This paper then presents a comparison-shopping engine that can be easily instantiated to become personalized and integrative shopping agents. This engine comprises of a product/merchant information collector, a consumer behavior extractor, a user profile manager, and an on-line learning personalized ranking module. We have built this engine and instantiated a comparison-shopping system for collecting preliminary evaluation results. The results show that this system is quite promising in overcoming the reality challenges of comparison shopping. In order to strengthen the contributions of this engine, we also gave a fielded application of this engine for personalized travel information discovery and explained the great potentials of this engine for a variety of comparison-shopping tasks. © 2002 Elsevier Science B.V. All rights reserved.",(Multi-) agent systems; Comparison shopping; Consumer valuation models; Neural networks; Reinforcement learning,Conference Paper,,Scopus,2-s2.0-0037209477
SCOPUS,[No author name available],"13th International Conference on Inductive Logic Programming, ILP 2003",2003,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2835,,,1,401,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974530692&partnerID=40&md5=330b9d7c58da4790b6a118631b56a369,,,The proceedings contain 25 papers. The special focus in this conference is on Inductive Logic Programming. The topics include: A personal view of how best to apply ILP; agents that reason and learn; mining model trees; complexity parameters for first-order classes; a multi-relational decision tree learning algorithm; applying theory revision to the design of distributed databases; disjunctive learning with a soft-clustering method; ILP for mathematical discovery; an exhaustive matching procedure for the improvement of learning efficiency; efficient data structures for inductive logic programming; graph kernels and gaussian processes for relational reinforcement learning; on condensation of a clause; a comparative evaluation of feature set evolution strategies for multi-relational boosting; comparative evaluation of approaches to propositionalization; improved distances for structured data; induction of enzyme classes from biological databases; estimating maximum likelihood parameters for stochastic context-free graph grammars; induction of the effects of actions by monotonic methods; hybrid abductive inductive learning; query optimization in inductive logic programming by reordering literals; efficient learning of unlabeled term trees with contractible variables from positive data; relational IBL in music with a new structural similarity measure and an effective grammar-based compression algorithm for tree structured data.,,Conference Review,,Scopus,2-s2.0-84974530692
SCOPUS,"Ren F.-C., Chang C.-J., Chen Y.-S.",A Q-learning-based multi-rate transmission control scheme for RRC in WCDMA systems,2002,"IEEE International Symposium on Personal, Indoor and Mobile Radio Communications, PIMRC",3,,1045263,1422,1426,,,10.1109/PIMRC.2002.1045263,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955496140&doi=10.1109%2fPIMRC.2002.1045263&partnerID=40&md5=74417be91b63fd9cc066b06f0b2241f7,"Computer and Communications Research Laboratories, Industrial Technology Research Institute, Hsinchu, Taiwan; Department of Communication Engineering, National Chiao Tung University, Hsinchu, Taiwan","Ren, F.-C., Computer and Communications Research Laboratories, Industrial Technology Research Institute, Hsinchu, Taiwan, Department of Communication Engineering, National Chiao Tung University, Hsinchu, Taiwan; Chang, C.-J., Department of Communication Engineering, National Chiao Tung University, Hsinchu, Taiwan; Chen, Y.-S., Department of Communication Engineering, National Chiao Tung University, Hsinchu, Taiwan","In this paper, a Q-learning-based multi-rate transmission control scheme (Q-MRTC) for radio resource control (RRC) in WCDMA systems is proposed. The RRC problem is modelled as a semi-Markov decision process (SMDP). And we successfully apply a real-time reinforcement learning algorithm, named Q-learning, to accurately estimate the transmission cost for the multi-rate transmission control. For the cost function approximation, we apply the feature extraction method to map the original state space into a more compact set which represents the resultant interference profile. Simulation results show that the Q-MRTC can achieve higher system throughput and better users' satisfaction index, by an amount of 87% and 50%, respectively, than the interference-based multi-rate transmission control scheme, while keeping the QoS requirement. ©2002 IEEE.",CDMA communication system; Multi-rate transmission; Q-learning; Radio resource management,Conference Paper,,Scopus,2-s2.0-79955496140
SCOPUS,"Wu D.-H., Ye X.-Q., Gu W.-K.",Closed-loop algorithm to detect human face using color and reinforcement learning,2002,Journal of Zhejinag University: Science,3,1,,72,76,,,10.1631/jzus.2002.0072,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036050298&doi=10.1631%2fjzus.2002.0072&partnerID=40&md5=8597918e640b41e2055b3140993f6bbb,"Inst. of Info. Syst. and Elec. Eng., Zhejiang Univ., Hangzhou 310027, China","Wu, D.-H., Inst. of Info. Syst. and Elec. Eng., Zhejiang Univ., Hangzhou 310027, China; Ye, X.-Q., Inst. of Info. Syst. and Elec. Eng., Zhejiang Univ., Hangzhou 310027, China; Gu, W.-K., Inst. of Info. Syst. and Elec. Eng., Zhejiang Univ., Hangzhou 310027, China","A closed-loop algorithm to detect human face using color information and reinforcement learning is presented in this paper. By using a skin-color selector, the regions with color like that of human skin are selected as candidates for human face. In the next stage, the candidates are matched with a face model and given an evaluation of the match degree by the matching module. And if the evaluation of the match result is too low, a reinforcement learning stage will start to search the best parameters of the skin-color selector. It has been tested using many photos of various ethnic groups under various lighting conditions, such as different light source, high light and shadow. And the experiment result proved that this algorithm is robust to the varying lighting conditions and personal conditions.",Human face detection; Reinforcement learning; Skin-color selector,Article,,Scopus,2-s2.0-0036050298
SCOPUS,[No author name available],"7th Pacific Rim International Conference on Artificial Intelligence on Trends in Artificial Intelligence, PRICAI 2002",2002,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2417,,,1,621,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947998672&partnerID=40&md5=56bd6c4e6dc38f290bedc3a09e7521af,,,"The proceedings contain 88 papers. The special focus in this conference is on Invited Talks, Logic and AI Foundation. The topics include: Commercial applications of machine learning for personalized wireless portals; learning, collecting, and using ontological knowledge for NLP; hidden variables in knowledge representation; research and applications at the intersection of multimedia and artificial intelligence; integration of multimedia and art for new human-computer communications; a refinement on UNSEARCHMO; a strong relevant logic approach to removing paradoxes from deontic logic; causal propagation in an argumentation-theoretic approach; representing actions over dynamic domains; consistency of action descriptions; solving factored MDPs with large action space using algebraic decision diagrams; dynamic fuzziness; distributed reinforcement of arc-consistency; parallel execution of stochastic search procedures on reduced SAT instances; two transformations of clauses into constraints and their properties for cost-based hypothetical reasoning; hidden markov modeling for multi-agent systems; modelling PRS-like agents’ mental states; genetic algorithm and social simulation; adaptive directed acyclic graphs for multiclass classification; network optimization through learning and pruning in neuromanifold; a novel discrete incremental clustering technique for the derivation of fuzzy membership functions; application of episodic Q-learning to a multi-agent cooperative task; LC-learning; phased method for average reward reinforcement learning - preliminary results; extension of the RDR method that can adapt to environmental changes and acquire knowledge from both experts and data; case generation method for constructing an RDR knowledge base; association rules using rough set and association rule methods and change-point estimation using new minimum message length approximations.",,Conference Review,,Scopus,2-s2.0-84947998672
SCOPUS,"Zhang B.-T., Seo Y.-W.",Personalized web-document filtering using reinforcement learning,2001,Applied Artificial Intelligence,15,7,,665,685,,41,10.1080/088395101750363993,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0242322792&doi=10.1080%2f088395101750363993&partnerID=40&md5=068c1f95f84f5da8db38e9ff190d8805,"Biointelligence Lab., Sch. of Comp. Sci. and Engineering, Seoul National University, Seoul, South Korea; Biointelligence Lab., Sch. of Comp. Sci. and Engineering, Seoul National University, Seoul 151-742, South Korea","Zhang, B.-T., Biointelligence Lab., Sch. of Comp. Sci. and Engineering, Seoul National University, Seoul, South Korea, Biointelligence Lab., Sch. of Comp. Sci. and Engineering, Seoul National University, Seoul 151-742, South Korea; Seo, Y.-W., Biointelligence Lab., Sch. of Comp. Sci. and Engineering, Seoul National University, Seoul, South Korea","Document filtering is increasingly deployed in Web environments to reduce information overload of users. We formulate online information filtering as a reinforcement learning problem, i.e., TD(0). The goal is to learn user profiles that best represent information needs and thus maximize the expected value of user relevance feedback. A method is then presented that acquires reinforcement signals automatically by estimating user's implicit feedback from direct observations of browsing behaviors. This ""learning by observation"" approach is contrasted with conventional relevance feedback methods which require explicit user feedbacks. Field tests have been performed that involved 10 users reading a total of 18,750 HTML documents during 45 days. Compared to the existing document filtering techniques, the proposed learning method showed superior performance in information quality and adaptation speed to user preferences in online filtering.",,Article,,Scopus,2-s2.0-0242322792
SCOPUS,"Stefán P., Monostori L.",On the relationship between learning capability and the Boltzmann-Formula,2001,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2070,,,227,236,,2,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953822952&partnerID=40&md5=bb5fa3ea3da126332e9faf22e1a5b4f3,"Computer and Automation Research Institute, Hungarian Academy of Sciences, Kende u. 13-17, Budapest, Hungary","Stefán, P., Computer and Automation Research Institute, Hungarian Academy of Sciences, Kende u. 13-17, Budapest, Hungary; Monostori, L., Computer and Automation Research Institute, Hungarian Academy of Sciences, Kende u. 13-17, Budapest, Hungary",In this paper a combined use of reinforcement learning and simulated annealing is treated. Most of the simulated annealing methods suggest using heuristic temperature bounds as the basis of annealing. Here a theoretically established approach tailored to reinforcement learning following Softmax action selection policy will be shown. An application example of agent-based routing will also be illustrated. © Springer-Verlag Berlin Heidelberg 2001.,,Conference Paper,,Scopus,2-s2.0-77953822952
SCOPUS,[No author name available],"3rd International Workshop on Intelligent Virtual Agents, IVA 2001",2001,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2190,,,1,243,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944205759&partnerID=40&md5=c1baaa046d6a5121818321a71b00690b,,,"The proceedings contain 23 papers. The special focus in this conference is on Intelligent Virtual Agents. The topics include: Intelligent virtual agents for education and training; eye pattern analysis in intelligent virtual agents; communicating emotion in virtual environments through artificial scents; a framework for reasoning about animation systems; equipping a lifelike animated agent with a mind; intelligent virtual agent societies on the internet; virtual agent societies with the MVITAL intelligent agent system; an overview of the use of mobile agents in virtual environments; continuous presence in collaborative virtual environments; a dramatised actant model for interactive improvisational plays; describing autonomous virtual agents and avatars; a platform for real-time virtual agents with planning capabilities; an architecture for real time automatic composition of background music; the lexicon and the alphabet of gesture, gaze, and touch; extraction and reconstruction of personal characters from human movement; language evolution through collaborative reinforcement learning and a tool for animating faces of 3d agents.",,Conference Review,,Scopus,2-s2.0-84944205759
SCOPUS,[No author name available],14th Biennial Conference on Advances in Artificial Intelligence 2001,2001,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2056,,,1,364,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949920937&partnerID=40&md5=818cce3345588545e0b07044411b3228,,,"The proceedings contain 38 papers. The special focus in this conference is on Advances in Artificial Intelligence. The topics include: A case study for learning from imbalanced data sets; a holonic multi-agent infrastructure for electronic procurement; a low-scan incremental association rule maintenance method based on the apriori property; a statistical corpus-based term extractor; body-based reasoning using a feeling-based lexicon, mental imagery, and an object-oriented metaphor hierarchy; combinatorial auctions, knapsack problems, and hill-climbing search; concept-learning in the presence of between-class and within-class imbalances; constraint programming lessons learned from crossword puzzles; constraint-based vehicle assembly line sequencing; how ai can help SE; imitation and reinforcement learning in agents with heterogeneous actions; knowledge and planning in an action-based multi-agent framework; learning about constraints by reflection; learning bayesian belief network classifiers; local score computation in learning belief networks; personalized contexts in help systems; a natural language question answering system; search techniques for non-linear constraint satisfaction problems with inequalities; searching for macro operators with automatically generated heuristics; solving multiple-instance and multiple-part learning problems with decision trees and rule sets. application to the mutagenesis problem; stacking for misclassification cost performance; stratified partial-order logic programming; learning classes of actions and outcomes through interaction; user interface aspects of a translation typing system; a hybrid approach to making recommendations and its application to the movie domain; agents with genders for inventory planning in e-management; Évaluation d'un système pour le résumé automatique de documents Électroniques; on obligations, relativised obligations, and bilateral commitments and question answering using unification-based grammar.",,Conference Review,,Scopus,2-s2.0-84949920937
SCOPUS,"Seo Young-Woo, Zhang Byoung-Tak",Learning user's preferences by analyzing Web-browsing behaviors,2000,Proceedings of the International Conference on Autonomous Agents,,,,381,387,,47,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033700745&partnerID=40&md5=e804803772de377c8c621ff60bb12c35,"Seoul Natl Univ, Seoul, South Korea","Seo, Young-Woo, Seoul Natl Univ, Seoul, South Korea; Zhang, Byoung-Tak, Seoul Natl Univ, Seoul, South Korea","This paper describes a method for an information filtering agent to learn user's preferences. The proposed method observes user's reactions to the filtered documents and learns from them the profiles for the individual users. Reinforcement learning is used to adapt the most significant terms that best represent user's interests. In contrast to conventional relevance feedback methods which require explicit user feedbacks, our approach learns user preferences implicitly from direct observations of browsing behaviors during interaction. Field tests have been made which involved 10 users reading a total of 18,750 HTML documents during 45 days. The proposed method showed superior performance in personalized information filtering compared to the existing relevance feedback methods.",,Conference Paper,,Scopus,2-s2.0-0033700745
SCOPUS,"Masumitsu K., Echigo T.",Video summarization using reinforcement learning in eigenspace,2000,IEEE International Conference on Image Processing,2,,,267,270,,4,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034441177&partnerID=40&md5=3053b263e811d92376716a34dca9b8a7,"IBM Research, Tokyo Research Laboratory, 1623-14, Shimotsuruma, Yamato-shi, Kanagawa, Japan","Masumitsu, K., IBM Research, Tokyo Research Laboratory, 1623-14, Shimotsuruma, Yamato-shi, Kanagawa, Japan; Echigo, T., IBM Research, Tokyo Research Laboratory, 1623-14, Shimotsuruma, Yamato-shi, Kanagawa, Japan","In this paper, we propose video summarization using reinforcement learning. The importance score of each frame in a video is calculated from the user's actions in handling similar previous frames; if such frames were watched rather than skipped, a high score is assigned. To calculate the score, instead of using raw feature vectors extracted from images, we use feature vectors projected on eigenspace: as a result, we can deal with the features comprehensively. We also give an algorithm that uses the reinforcement learning method to create a personalized video summary. The summarization algorithm is applied to a soccer video to confirm its effectiveness.",,Conference Paper,,Scopus,2-s2.0-0034441177
SCOPUS,[No author name available],"Advances in Artificial Intelligence - International Joint Conference 7th Ibero-American Conference on AI, 15th Brazilian Symposium on AI, IBERAMIA-SBIA 2000, Proceedings",2000,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1952 LNAI,,,,,496,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650136862&partnerID=40&md5=2ab2cad3d8443444ca48e71ef0b44bdc,,,The proceedings contain 49 papers. The topics discussed include: decision-rule solutions for data mining with missing values; getting computer systems to function as team players; case-based management of software engineering experience; handling cases and the coverage in a limited quantity of memory for case-based planning systems; integrating rules and cases in learning via case explanation and paradigm shift; personals: an intelligent agent for searching web pages; ELOPES - the java embedded object production system; global and local search for scheduling job shop with parallel machines; knowledge-based interactive scheduling of multipoint batch plants; petrolia: a new algorithm for plan generation; using and evaluating adaptive agents for electronic commerce negotiation; representing operational knowledge by contextual graphics; and a new distributed reinforcement learning algorithm for multiple objective optimization problems.,,Conference Review,,Scopus,2-s2.0-78650136862
SCOPUS,"Seo Young-Woo, Zhang Byoung-Tak",Reinforcement learning agent for personalized information filtering,2000,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,248,251,,61,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033688671&partnerID=40&md5=0bbfc79c2083cfd6cde5235144850154,"Seoul Natl Univ, Seoul, South Korea","Seo, Young-Woo, Seoul Natl Univ, Seoul, South Korea; Zhang, Byoung-Tak, Seoul Natl Univ, Seoul, South Korea","This paper describes a method for learning user's interests in the Web-based personalized information filtering system called WAIR. The proposed method analyzes user's reactions to the presented documents and learns from them the profiles for the individual users. Reinforcement learning is used to adapt the term weights in the user profile so that user's preferences are best represented. In contrast to conventional relevance feedback methods which require explicit user feedbacks, our approach learns user preferences implicitly from direct observations of user behaviors during interaction. Field tests have been made which involved 7 users reading a total of 7,700 HTML documents during 4 weeks. The proposed method showed superior performance in personalized information filtering compared to the existing relevance feedback methods.",,Conference Paper,,Scopus,2-s2.0-0033688671
SCOPUS,"Kim G.H., Lee C.S.G.",Genetic reinforcement learning approach to the heterogeneous machine scheduling problem,1998,IEEE Transactions on Robotics and Automation,14,6,,879,893,,14,10.1109/70.736772,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032316233&doi=10.1109%2f70.736772&partnerID=40&md5=8424cfe580cccbfdfbc918b966362945,"IEEE, United States; School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN 47907, United States","Kim, G.H., IEEE, United States, School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN 47907, United States; Lee, C.S.G., IEEE, United States, School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN 47907, United States","This paper focuses on the development of a learning-based heuristic for scheduling heterogeneous machines. Although list scheduling methods have been widely used for a large class of scheduling problems, including the heterogeneous machine scheduling problem, they involve designing priority rules, which usually require a fair amount of insights on the characteristics of the problem to be solved. Instead of elaborate design of priority rules in a single step, we propose an iterative list scheduling process, which refines priority rules while generating a number of schedules. The proposed iterative list scheduling is formulated as a renforcement learning problem, with states and actions defined in list scheduling. Due to the large number of possible states, renforcement learning algorithms which use value functions in constructing an optimal policy may not be suitable for scheduling problems. Thus, to directly work with policies rather than the values of states, we propose genetic reinforcement learning (GRL), in which the policies of renforcement learning are encoded into the chromosomes of genetic algorithms and a near-optimal policy is searched for by genetic algorithms. A GRL-based scheduler, called evolutionary intracell scheduler (EVIS), has been developed and applied to various scheduling problems such as the heterogeneous machine scheduling, the processor scheduling, the job-shop scheduling, the flow-shop scheduling, and the open-shop scheduling problems. The proposed mode of EVIS, which has a linear order of population-fitness convergence, is verified by computer experiments. Even without fine tuning EVIS, the quality of solutions achieved by EVIS is comparable to that of problem-tailored heuristics for most of the problem instances.",Genetic renforcement learning; Genetical gorithms; Machine scheduling problem; Renforcement learning,Article,,Scopus,2-s2.0-0032316233
SCOPUS,"Thomopoulos S.C.A., Braught G.",Machine perception and intelligent control architecture for multi-robot coordination based on biological principles,1996,Proceedings of SPIE - The International Society for Optical Engineering,2905,,,64,72,,2,10.1117/12.256339,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0008675692&doi=10.1117%2f12.256339&partnerID=40&md5=993b0b18ad32cba330b76d38dcba839f,"INTELNET Inc., 200 Innovation Blvd., State College, PA 16803, United States","Thomopoulos, S.C.A., INTELNET Inc., 200 Innovation Blvd., State College, PA 16803, United States; Braught, G., INTELNET Inc., 200 Innovation Blvd., State College, PA 16803, United States","Intelligent control, inspired by biological and AI (artificial intelligence) principles, has increased the understanding of controlling complex processes without precise mathematical model of the controlled process. Through customized applications, intelligent control has demonstrated that it is a step in the right direction. However, intelligent control has yet to provide a complete solution to the problem of integrated manufacturing systems via intelligent reconfiguration of the robotics systems. The aim of this paper is to present an intelligent control architecture and design methodology based on biological principles that govern self-organization of autonomous agents. Two key structural elements of the proposed control architecture have been tested individually on key pilot applications and shown promising results. The proposed intelligent control design is inspired by observed individual and collective biological behavior in colonies of living organisms that are capable of self-organization into groups of specialized individuals capable of collectively achieving a set of prescribed or emerging objectives. The nervous and brain system in the proposed control architecture is based on reinforcement learning principles and conditioning and modeled using adaptive neurocontrollers. Mathematical control theory (e.g. optimal control, adaptive control, and neurocontrol) is used to coordinate the interactions of multiple robotics agents. ©2005 Copyright SPIE - The International Society for Optical Engineering.",,Conference Paper,,Scopus,2-s2.0-0008675692
SCOPUS,"Aiba H., Terano T.",A computational model for distributed knowledge systems with learning mechanisms,1996,Expert Systems with Applications,10,3-4 SPEC. ISS.,,417,427,,9,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029724561&partnerID=40&md5=94aef0749f79d93a223768fd0a3a5948,"Grad. School of Systems Management, University of Tsukuba, 3-29-1 Otsuka, Bunkyo-ku, Tokyo 112, Japan; Engineering Systems Department, Mitsubishi Research Institute Inc., 2-3-6 Otemachi, Chiyoda-ku, Toyko 100, Japan","Aiba, H., Grad. School of Systems Management, University of Tsukuba, 3-29-1 Otsuka, Bunkyo-ku, Tokyo 112, Japan, Engineering Systems Department, Mitsubishi Research Institute Inc., 2-3-6 Otemachi, Chiyoda-ku, Toyko 100, Japan; Terano, T., Grad. School of Systems Management, University of Tsukuba, 3-29-1 Otsuka, Bunkyo-ku, Tokyo 112, Japan","This paper addresses the issues of machine learning in distributed knowledge systems, which will consist of distributed software agents with problem solving, communication and learning functions. To develop such systems, we must analyze the roles of problem-solving and communication capabilities among knowledge systems. To facilitate the analyses, we propose a computational model: LPC. The model consists of a set of agents with (a) a knowledge base for learned concepts, (b) a knowledge base for problem solving, (c) prolog-based inference mechanisms and (d) a set of beliefs on the reliability of the other agents. Each agent can improve its own problem-solving capabilities by deductive learning from the given problems, by memory-based learning from communications between the agents and by reinforcement learning from the reliability of communications between the other agents. An experimental system of the model has been implemented in Prolog language on a Window-based personal computer. Intensive experiments have been carried out to examine the feasibility of the machine learning mechanisms of agents for problem-solving and communication capabilities. The experimental results have shown that the multiagent system improves the performance of the whole system in problem solving, when each agent has a higher learning ability or when an agent with a very high ability for problem solving joins the organization to cooperate with the other agents in problem solving. These results suggest that the proposed model is useful in analyzing the learning mechanisms applicable to distributed knowledge systems. Copyright © 1996 Elsevier Science Ltd.",,Article,,Scopus,2-s2.0-0029724561
SCOPUS,"Kim Gyoung H., Lee C.S.George",Genetic reinforcement learning for scheduling heterogeneous machines,1996,Proceedings - IEEE International Conference on Robotics and Automation,3,,,2798,2803,,3,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029706758&partnerID=40&md5=aeb35de8e2ec785baf10a559c2b28ef0,"Purdue Univ, West Lafayette, United States","Kim, Gyoung H., Purdue Univ, West Lafayette, United States; Lee, C.S.George, Purdue Univ, West Lafayette, United States","The development of a learning-based heuristic for scheduling heterogeneous machines is discussed. A GRL-based scheduler, called EVIS (EVolutionary Intracell Scheduler) is proposed. The proposed model has the linear order of population-fitness convergence. The model is verified through computer experiments. Even without fine tuning of EVIS, the quality of solutions found by EVIS is comparable to that of problem-tailored heuristics for most of the problem instances.",,Conference Paper,,Scopus,2-s2.0-0029706758
SCOPUS,Rennison Earl,Personalized galaxies of information,1995,Conference on Human Factors in Computing Systems - Proceedings,2,,,31,32,,7,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029204727&partnerID=40&md5=46e64663194ee5b069580c8532f337c5,"MIT Media Lab, Cambridge, United States","Rennison, Earl, MIT Media Lab, Cambridge, United States","The Personalized Galaxies of Information demonstration presents a new interface approach for visualizing, navigating and accessing information objects in a large body of unstructured information, such as on-line new stories, photographs and video clips available via Clarinews; electronic mail; and World Wide Web documents. The system provides mechanisms to analyze the relationships between information objects and builds a representation of the underlying structure of the entire body of information. This relational structure is used to construct a visual information space with which the user interacts to explore the contents of the information base. The system also uses a learning algorithm to adaptively customize the presentation of information to a particular user's interests. This dynamic, personalized structuring of information helps users perform directed searches while simultaneously affording general browsing in a fluid and seamless environment.",,Conference Paper,,Scopus,2-s2.0-0029204727
SCOPUS,"Maes Pattie, Kozierok Robyn",Learning interface agents,1993,Proceedings of the National Conference on Artificial Intelligence,,,,459,465,,112,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027708859&partnerID=40&md5=df7028dac1a1660a147b9a020dc8489d,"MIT Media Lab, Cambridge, United States","Maes, Pattie, MIT Media Lab, Cambridge, United States; Kozierok, Robyn, MIT Media Lab, Cambridge, United States","Interface agents are computer programs that employ Artificial Intelligence techniques in order to provide assistance to a user dealing with a particular computer application. The paper discusses an interface agent which has been modelled closely after the metaphor of a personal assistant. The agent learns how to assist the user by (i) observing the user's actions and imitating them, (ii) receiving user feedback when it takes wrong actions and (iii) being trained by the user on the basis of hypothetical examples. The paper discusses how this learning agent was implemented using memory-based learning and reinforcement learning techniques. It presents actual results from two proto-type agents built using these techniques: one for a meeting scheduling application and one for electronic mail. It argues that the machine learning approach to building interface agents is a feasible one which has several advantages over other approaches: it provides a customized and adaptive solution which is less costly and ensures better user acceptability. The paper also argues what the advantages are of the particular learning techniques used.",,Conference Paper,,Scopus,2-s2.0-0027708859
SCOPUS,"Kozierok R., Maes P.",A Learning interface agent for scheduling meetings,1993,"International Conference on Intelligent User Interfaces, Proceedings IUI",Part F127502,,,81,88,,57,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943177303&partnerID=40&md5=7e3e84b3f621b3d76bbe6069ae98fb28,"MIT Media-Lab, 20 Ames Street, Cambridge, MA, United States","Kozierok, R., MIT Media-Lab, 20 Ames Street, Cambridge, MA, United States; Maes, P., MIT Media-Lab, 20 Ames Street, Cambridge, MA, United States","This paper describes a Learning Interface Agent for a meeting scheduling application. The agent employs Machine Learning techniques to customize itself to the user's personal scheduling rules and preferences by observing the user's actions and receiving direct user-feedback. Our approach provides the user with sophisticated control over the gradual delegation of scheduling tasks to the agent, as a trust relationship is built. We report upon an experiment in which a collection of such assistants became gradually more helpful to their users through the use of memory-based and reinforcement learning. The experimental data reported upon demonstrate that the learning approach to building intelligent interface agents is a very promising one which has several advantages over more standard approaches. © 1992 ACM.",Interface agents; Learning interface agents; Machine learning; Personal assistants; Software agents,Conference Paper,,Scopus,2-s2.0-84943177303
SCOPUS,Hoberock L.L.,Personalized-Proctorial Instruction fir Dynamic Systems and Control,1972,"Journal of Dynamic Systems, Measurement and Control, Transactions of the ASME",94,2,,165,167,,1,10.1115/1.3426564,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024532643&doi=10.1115%2f1.3426564&partnerID=40&md5=c287f59b6aa7640be6e383ac9b1045db,"Mechanical Engineering Department, University of Texas at Austin, Austin, TX, United States","Hoberock, L.L., Mechanical Engineering Department, University of Texas at Austin, Austin, TX, United States",A highly promising teaching technique using reinforcement learning theory and individually paced instruction is analyzed and shown to offer several advantages in conducting dynamic systems and control courses. © 1972 ASME.,,Article,,Scopus,2-s2.0-85024532643
