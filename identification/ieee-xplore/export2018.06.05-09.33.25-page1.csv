"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication_Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Smart Lifelong Learning System Based on Q-Learning","A. A. Kardan; O. R. B. Speily","Adv. E-Learning Technol. Lab., AmirKabir Univ. of Technol., Tehran, Iran","2010 Seventh International Conference on Information Technology: New Generations","20100701","2010","","","1086","1091","The majority of current web-based learning systems are closed learning environments where courses and learning materials are fixed and the only dynamic aspect is the organization of the material that can be adapted to allow a relatively individualized learning environment. In this paper, we propose an evolving web-based learning system which can adapt itself to its learners. More specifically, the novelty with respect to the system lies in its ability to find relevant content on the web, and its ability to personalize and adapt this content based on the system's observation of its learners and the accumulated ratings given by the learners. Hence, although learners do not have direct interaction with the open Web, the system can retrieve relevant information related to them and their situated learning characteristics. Lifelong learning scenarios have particular differences in their need for personalized recommendations that make not possible reusing existing general approaches of recommender systems. The paper describes those challenges and we propose a hybrid technique based on machine learning to recognize learner preferences and predict theirs required contents with high accuracy.","","Electronic:978-1-4244-6271-1; POD:978-1-4244-6270-4","10.1109/ITNG.2010.140","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501486","Learning Promotion;Lifelong Learning;Machine Learning;Q Learning;Recommender Systems;Reinforcement Learning","Adaptive systems;Electronic learning;Information filtering;Information retrieval;Information technology;Learning systems;Least squares approximation;Machine learning;Multitasking;Recommender systems","Internet;computer aided instruction;continuing professional development;information filters;learning (artificial intelligence)","Q-Learning;Web-based learning systems;learning materials;machine learning;personalized recommendations;recommender systems;smart lifelong learning system","","0","","12","","","","12-14 April 2010","","IEEE","IEEE Conferences"
"Analysis of re-sequencing buffer overflow probability based on stochastic delay characteristics","D. Zhou; H. Li; J. Li","State Key Lab. of Integrated Service Networks, Xidian Univ., Xi'an, China","2013 IEEE 24th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)","20131125","2013","","","2490","2495","With the development of multi-interface terminals, a host can connect to the Internet simultaneously by multiple access technologies. Under multi-access technology, a multi-path transmission can obtain high throughput, increased available bandwidth and enhanced reliability. However, the multi-path transmission with multi-access technology also has the problems that the packet re-ordering is unavoidable, and the fast retransmission is unnecessarily requested. Considering the stochastically varying transmission delay, the problems above may eventually result in a degradation of throughput. As a result, in this paper, we focus on the analysis of buffer overflow probability problem which is influenced by the transmission interval. First, we utilize Reinforcement Learning method to estimate the stochastic delay of end-to-end paths. Then, we discuss problems of re-sequencing buffer occupancy distribution and the overflow probability. In this paper, we model the stochastic delay as a continuous random variable, and then, discuss its mean value and variance. Simulation result shows that the re-sequencing buffer overflow probability is influenced by the transmission intervals and the variance of stochastic delay.","2166-9570;21669570","Electronic:978-1-4673-6235-1; POD:978-1-4673-6233-7; USB:978-1-4673-6234-4","10.1109/PIMRC.2013.6666565","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6666565","Concurrent Multi-path Transmission;Re-sequencing Buffer Overflow Probability;Reinforcement Learning","Buffer storage;Delays;Estimation;Learning (artificial intelligence);Mathematical model;Sequential analysis;Stochastic processes","Internet;delays;learning (artificial intelligence);probability;random processes;stochastic processes","Internet;continuous random variable;end-to-end paths;mean value;mean variance;multiinterface terminals;multipath transmission;multiple access technology;packet re-ordering;re-sequencing buffer occupancy distribution problem;reinforcement learning method;resequencing buffer overflow probability analysis;stochastic delay characteristics;stochastic varying transmission delay;transmission interval","","1","","20","","","","8-11 Sept. 2013","","IEEE","IEEE Conferences"
"Green Wi-Fi Implementation and Management in Dense Autonomous Environments for Smart Cities","Y. Zhang; C. Jiang; J. Wang; Z. Han; J. Yuan; J. Cao","Department of Electrical Engineering, Tsinghua University, Beijing, China","IEEE Transactions on Industrial Informatics","20180404","2018","14","4","1552","1563","Advanced informatics technologies facilitate the construction of green smart cities, especially the Wi-Fi implementation and management, for rapidly increasing personal Wi-Fi devices in autonomous environments residing in nonoverlapped channels often result in low energy efficiency and severe cochannel interference. In this paper, a green Wi-Fi management framework is constructed in order to reduce the overall energy consumption through turning off a portion of access points (APs) and aggregating their users to the other active APs. A Tabu-search-assisted active AP selection algorithm is proposed to minimize the power consumption with a seamless wireless converge. For the active APs, based on our defined metric airtime cost that is integrated by the in-range interference and the hidden terminal interference, a reinforcement-learning-aided AP self-management algorithm is proposed to dynamically adjust APs' channels in the partially overlapped channel space. Extensive simulations and field experiments demonstrate that the power consumption can be reduced by about 65%, and the airtime cost of APs can be reduced by 50% compared with the typical least congestion channel search algorithm.","1551-3203;15513203","","10.1109/TII.2017.2785820","National Basic Research Program of China; National Natural Science Foundation of China; US NSF; Young Elite Scientists Sponsorship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8233146","Energy efficiency;green Wi-Fi;partially overlapped channels (POCs);self-management","Energy consumption;Green products;Informatics;Interference;Power demand;Wireless communication;Wireless fidelity","cochannel interference;computer network management;energy conservation;green computing;learning (artificial intelligence);power consumption;radiofrequency interference;search problems;smart cities;telecommunication power management;wireless LAN;wireless channels","AP self-management algorithm;Tabu-search-assisted active AP selection algorithm;access points;advanced informatics technologies;cochannel interference;defined metric airtime cost;dense autonomous environments;energy consumption reduction;green Wi-Fi management framework;green smart cities;hidden terminal interference;in-range interference;least congestion channel search algorithm;low energy efficiency;metric airtime cost;nonoverlapped channels;partially overlapped channel space;personal Wi-Fi devices;power consumption;reinforcement-learning-aided AP self-management algorithm","","","","","","","20171221","April 2018","","IEEE","IEEE Journals & Magazines"
"Smartphone Interruptibility Using Density-Weighted Uncertainty Sampling with Reinforcement Learning","R. Fisher; R. Simmons","Machine Learning Dept., Carnegie Mellon Univ., Pittsburgh, PA, USA","2011 10th International Conference on Machine Learning and Applications and Workshops","20120209","2011","1","","436","441","We present the In-Context application for smart-phones, which combines signal processing, active learning, and reinforcement learning to autonomously create a personalized model of interruptibility for incoming phone calls. We empirically evaluate the system, and show that we can obtain an average of 96.12% classification accuracy when predicting interruptibility after a week of training. In contrast to previous work, we leverage density-weighted uncertainty sampling combined with a reinforcement learning framework applied to passively collected data to achieve comparable or superior classification accuracy using many fewer queries issued to the user.","","Electronic:978-0-7695-4607-0; POD:978-1-4577-2134-2","10.1109/ICMLA.2011.128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147012","Active learning;interruptibility;mobile devices;reinforcement learning","Accuracy;Context;Data mining;Feature extraction;Support vector machines;Switches;Uncertainty","learning (artificial intelligence);smart phones","active learning;density weighted uncertainty sampling;reinforcement learning;signal processing;smartphone interruptibility","","4","","20","","","","18-21 Dec. 2011","","IEEE","IEEE Conferences"
"Table of contents","","","2014 IIAI 3rd International Conference on Advanced Applied Informatics","20141201","2014","","","v","xix","The following topics are dealt with: data mining; Japanese WordNet synonym misplacement detection; social network; recommender system; sentiment analysis; workshop-based instruction; Japanese public libraries; machine learning methods; collaborative Web presentation support system; SMS4 ultracompact hardware implementation; wireless sensor networks; personalized public transportation recommendation system; adaptive user interface; NIS-Apriori algorithm; GetRNIA software tool; rough set-based rule generation; tree-Ga bump hunting; neural network model; weighted citation network analysis; sound proofing ventilation unit; touch interaction; mutually dependent Markov decision processes; ozone treatment; dynamic query optimization; big data; learner activity recognition; IoT-security approach; nutrition-based vegetable production; farm product cultivation; polynomial time mat learning; C-deterministic regular formal graph system; article abstract key expression extraction; English text comprehension; online social games; knowledge creation; knowledge utilization; online stock trading; customer behavior analysis; project-based collaborative learning; in-field mobile game-based learning activities; e-portfolio system design; self-regulated learning ontological model; mobile augmented reality based scaffolding platform; context-aware mobile Japanese conversation learning system; English writing error classification; image processing; outside-class learning; exercise-centric teaching materials; UML modeling; online historical document reading literacy; MMORPG-based learning environment; computer courses; undergraduate education; energy management system; higher education; decentralised auction-based bandwidth allocation; wireless networked control systems; resource scheduling algorithm; embedded cloud computing; Poisson distribution; Japanese seismic activity; suspect vehicle detection; 3D network traffic visualization; Web information retrieval; agent based disaster evacua- ion assist system; electroencephalogram; random number generator; multiagent simulations; multicore environment; CPU scheduler; multithreaded processes; reserve-price biddings; real-time traffic signal control; evolutionary computation; robot-assisted rehabilitation system; hybrid automata; Batik motif classification; color-texture-based feature extraction; backpropagation; multimedia storytelling; e-tourism service; Web mining; search engine; simulation-based e-learning mobile application software; library classification training system; WebQuest learning strategy; context-aware ubiquitous English learning; support vector machine; RFID tag ownership transfer protocol; cognitive linguistics; collaborative software engineering learning; write-access reduction method; NVM-DRAM hybrid memory; garbage collection; parallel indexing scheme; lazy-updating snoop cache protocol; distributed storage system; ITS application; software engineering education; ophthalmic multimodal imaging system; injected bug classification; secure live virtual machine migration; flash memory management; genetic programming; heterogeneous databases; time series similarity search; concurrency control program generation; incremental data migration; multidatabase system; software release time decision making; analytic hierarchy process; interactive genetic algorithm; biometric intelligence; talking robots; archaeological ruin analysis; GIS; optical wireless pedestrian-support systems; visual impairment; extreme programming; Japanese e-commerce Web sites; Chinese sign language animation; hearing-impaired people mammography inspection; geographical maps; electroculogram; XML element retrieval technique; image recognition; reinforcement learning; ECU formal verification; gasoline direct injection engines; earthquake disaster simulation; smart devices for autistic children; RoboCup rescue simulation; inductive logic programming; master-slave asynchronous evolutionary hybrid algorithm; VANET routing opt","","CD-ROM:978-1-4799-4175-9; Electronic:978-1-4799-4173-5; POD:978-1-4799-1679-5","10.1109/IIAI-AAI.2014.4","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6913248","","","Big Data;DRAM chips;Internet of Things;Markov processes;Poisson distribution;Unified Modeling Language;XML;agriculture;analytic hierarchy process;archaeology;augmented reality;automata theory;backpropagation;bandwidth allocation;biometrics (access control);cache storage;citation analysis;cloud computing;computational complexity;computer animation;computer games;computer science education;concurrency control;consumer behaviour;data mining;data visualisation;distributed databases;educational courses;electro-oculography;electroencephalography;electronic commerce;emergency management;energy management systems;engines;feature extraction;flash memories;formal verification;further education;genetic algorithms;geographic information systems;groupware;handicapped aids;human computer interaction;humanoid robots;image classification;image colour analysis;image texture;inductive logic programming;intelligent tutoring systems;investment;library automation;linguistics;mammography;medical robotics;mobile computing;multi-agent systems;multi-threading;multimedia computing;multiprocessing systems;natural language processing;networked control systems;neural nets;object detection;ozonation (materials processing);patient rehabilitation;pedestrians;processor scheduling;public transport;query processing;random number generation;recommender systems;rescue robots;resource allocation;rough set theory;search engines;security of data;seismology;social networking (online);software prototyping;software tools;stock markets;storage management;support vector machines;teaching;telecommunication network routing;text analysis;time series;traffic control;traffic engineering computing;travel industry;trees (mathematics);unsupervised learning;user interfaces;vehicular ad hoc networks;ventilation;virtual machines;wireless sensor networks","3D network traffic visualization;Batik motif classification;C-deterministic regular formal graph system;CPU scheduler;Chinese sign language animation;ECU formal verification;English text comprehension;English writing error classification;GIS;GetRNIA software tool;ITS application;IoT-security approach;Japanese WordNet synonym misplacement detection;Japanese e-commerce Web sites;Japanese public libraries;Japanese seismic activity;MMORPG-based learning environment;NIS-Apriori algorithm;NVM-DRAM hybrid memory;Poisson distribution;RFID tag ownership transfer protocol;RoboCup rescue simulation;SMS4 ultracompact hardware implementation;UML modeling;VANET routing optimization;Web image sharing services;Web information retrieval;Web mining;WebQuest learning strategy;XML element retrieval technique;adaptive user interface;agent based disaster evacuation assist system;analytic hierarchy process;archaeological ruin analysis;article abstract key expression extraction;autistic children;backpropagation;big data;biometric intelligence;cognitive linguistics;collaborative Web presentation support system;collaborative software engineering learning;color-texture-based feature extraction;computer courses;concurrency control program generation;context-aware mobile Japanese conversation learning system;context-aware ubiquitous English learning;customer behavior analysis;data mining;decentralised auction-based bandwidth allocation;distributed storage system;dynamic query optimization;e-portfolio system design;e-tourism service;earthquake disaster simulation;electroencephalogram;electrooculogram;embedded cloud computing;energy management system;evolutionary computation;exercise-centric teaching materials;extreme programming;farm product cultivation;flash memory management;garbage collection;gasoline direct injection engines;genetic programming;geographical maps;hearing-impaired people mammography inspection;heterogeneous databases;higher education;hybrid automata;image processing;image recognition;in-field mobile game-based learning activities;incremental data migration;inductive logic programming;injected bug classification;interactive genetic algorithm;knowledge creation;knowledge utilization;lazy-updating snoop cache protocol;learner activity recognition;library classification training system;machine learning methods;master-slave asynchronous evolutionary hybrid algorithm;mobile augmented reality based scaffolding platform;multiagent simulations;multicore environment;multidatabase system;multimedia storytelling;multithreaded processes;mutually dependent Markov decision processes;neural network model;nutrition-based vegetable production;online historical document reading literacy;online social games;online stock trading;ophthalmic multimodal imaging system;optical wireless pedestrian-support systems;outside-class learning;ozone treatment;parallel indexing scheme;personalized public transportation recommendation system;polynomial time mat learning;project-based collaborative learning;random number generator;real-time traffic signal control;recommender system;reinforcement learning;reserve-price biddings;resource scheduling algorithm;robot-assisted rehabilitation system;rough set-based rule generation;search engine;secure live virtual machine migration;self-regulated learning ontological model;sentiment analysis;simulation-based e-learning mobile application software;social network;software engineering education;software release time decision making;sound proofing ventilation unit;support vector machine;suspect vehicle detection;talking robots;time series similarity search;touch interaction;tree-Ga bump hunting;undergraduate education;visual impairment;weighted citation network analysis;wireless networked control systems;wireless sensor networks;workshop-based instruction;write-access reduction method","","0","","","","","","Aug. 31 2014-Sept. 4 2014","","IEEE","IEEE Conferences"
"Dynamic thermal management for multimedia applications using machine learning","Y. Ge; Q. Qiu","Department of Electrical and Computer Engineering, Binghamton University, USA","2011 48th ACM/EDAC/IEEE Design Automation Conference (DAC)","20110811","2011","","","95","100","Multimedia applications are expected to form the largest portion of workload in general purpose PC and portable devices. The ever-increasing computation intensity of multimedia applications elevates the processor temperature and consequently impairs the reliability and performance of the system. In this paper, we propose to perform dynamic thermal management using reinforcement learning algorithm for multimedia applications. The proposed learning model does not need any prior knowledge of the workload information or the system thermal and power characteristics. It learns the temperature change and workload switching patterns by observing the temperature sensor and event counters on the processor, and finds the management policy that provides good performance-thermal tradeoff during the runtime. We validated our model on a Dell personal computer with Intel Core 2 processor. Experimental results show that our approach provides considerable performance improvements with marginal increase in the percentage of thermal hotspot comparing to existing workload phase detection approach.","0738-100x;0738100x","DVD:978-1-4503-0636-2; Electronic:978-1-4503-0636-2; POD:978-1-4503-0636-2","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5981924","Dynamic thermal management;multimedia application;reinforcement learning","Decoding;Heuristic algorithms;Learning;Multimedia communication;Radiation detectors;Temperature sensors;Thermal management","learning (artificial intelligence);multimedia computing;thermal management (packaging)","Dell personal computer;Intel Core 2 processor;dynamic thermal management;machine learning;multimedia applications;reinforcement learning algorithm;workload phase detection approach","","2","","17","","","","5-9 June 2011","","IEEE","IEEE Conferences"
"Learn to Coordinate with Generic Non-Stationary Opponents","Z. Kaifu","Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China, zkf03@mails.tsinghua.edu.cn","2006 5th IEEE International Conference on Cognitive Informatics","20070910","2006","1","","558","565","Learning to coordinate with non-stationary opponents is a major challenge for adaptive agents. Most previous research investigated only restricted classes of such dynamic opponents. The main contribution of this paper is twofold: (i) A class of generic non-stationary opponents is introduced. The opponents keep mixed strategies which change with less regularity. Its showed that the independent reinforcement learners (ILs), which have neither prior knowledge nor opponent models, cannot coordinate well with this type of opponent, (ii) A new exploration strategy, the DAE (detect and explore) mechanism, is tailored for the ILs in such coordination tasks. This mechanism allows the ILs dynamically detect changes in the opponents behavior and adjust their learning rate and exploration temperature. It's showed that ILs using this strategy are still able to converge in self-play and are able to coordinate well with the non-stationary opponents","","POD:1-4244-0475-4","10.1109/COGINF.2006.365546","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4216463","Coordination game;Exploration strategy;Non-stationary opponent;Reinforcement learning","Cognitive informatics;Computer science;Decision making;Game theory;Learning;Multiagent systems;Temperature;Testing","learning (artificial intelligence);multi-agent systems","adaptive agents;detect and explore mechanism;dynamic opponents;exploration strategy;generic nonstationary opponents;independent reinforcement learner","","1","","16","","","","17-19 July 2006","","IEEE","IEEE Conferences"
"Evolution of context-aware user profiles","J. Thomsen; Y. Vanrompay; Y. Berbers","Condat AG, Berlin, Germany","2009 International Conference on Ultra Modern Telecommunications & Workshops","20091204","2009","","","1","6","Context-awareness and adaptation are key issues in mobile and ubiquitous computing. Applications on mobile devices use context information to adapt themselves to changing environments. User profiles play an important role in these systems as they serve as an individualization filter in a wide range of possible context adaptation parameters. In this paper we propose a modeling approach for the evolution of context-aware user profiles. A motivating scenario, the intelligent selection of a suitable medical expert in an emergency situation, shows the need for context-aware matching of user profiles. This is achieved by a similarity matching algorithm and reinforcement learning.","2157-0221;21570221","POD:978-1-4244-3942-3","10.1109/ICUMT.2009.5345395","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5345395","","Application software;Computer science;Context modeling;Filters;Intelligent networks;Mobile computing;Multiple signal classification;Personal digital assistants;Runtime;Ubiquitous computing","learning (artificial intelligence);medical computing;mobile computing","adaptation;context-aware user profiles;context-awareness;medical expert;mobile computing;reinforcement learning;similarity matching algorithm;ubiquitous computing","","1","","21","","","","12-14 Oct. 2009","","IEEE","IEEE Conferences"
"Stochastic Optimal Relaxed Automatic Generation Control in Non-Markov Environment Based on Multi-Step <formula formulatype=""inline""> <tex Notation=""TeX"">$Q(lambda)$</tex></formula> Learning","T. Yu; B. Zhou; K. W. Chan; L. Chen; B. Yang","College of Electric Power, South China University of Technology, Guangzhou, China","IEEE Transactions on Power Systems","20110721","2011","26","3","1272","1282","This paper proposes a stochastic optimal relaxed control methodology based on reinforcement learning (RL) for solving the automatic generation control (AGC) under NERC's control performance standards (CPS). The multi-step <i>Q</i>(λ) learning algorithm is introduced to effectively tackle the long time-delay control loop for AGC thermal plants in non-Markov environment. The moving averages of CPS1/ACE are adopted as the state feedback input, and the CPS control and relaxed control objectives are formulated as multi-criteria reward function via linear weighted aggregate method. This optimal AGC strategy provides a customized platform for interactive self-learning rules to maximize the long-run discounted reward. Statistical experiments show that the RL theory based <i>Q</i>(λ) controllers can effectively enhance the robustness and dynamic performance of AGC systems, and reduce the number of pulses and pulse reversals while the CPS compliances are ensured. The novel AGC scheme also provides a convenient way of controlling the degree of CPS compliance and relaxation by online tuning relaxation factors to implement the desirable relaxed control.","0885-8950;08858950","","10.1109/TPWRS.2010.2102372","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5706397","AGC;CPS;multi-step <formula formulatype=""inline""><tex Notation=""TeX"">$Q(lambda)$</tex> </formula> learning;non-Markov environment;relaxed control;stochastic optimization","Aerospace electronics;Frequency control;Markov processes;Power grids;Power system dynamics;Standards","delays;learning (artificial intelligence);optimal control;power generation control;power system stability;robust control;state feedback;statistical analysis;stochastic processes;thermal power stations","AGC thermal plant;CPS compliance;CPS control;NERC control performance standard;RL theory based controller;interactive self learning rule;linear weighted aggregate method;long run discounted reward;multicriteria reward function;multistep Q(λ) learning algorithm;nonMarkov environment;online tuning relaxation factors;optimal AGC strategy;pulse reversal;reinforcement learning;state feedback input;statistical experiment;stochastic optimal relaxed automatic generation control;time delay control loop","","28","","31","","","20110204","Aug. 2011","","IEEE","IEEE Journals & Magazines"
"An experimental approach to robotic grasping using a connectionist architecture and generic grasping functions","M. A. Moussa; M. S. Kamel","Dept. of Syst. Design Eng., Waterloo Univ., Ont., Canada","IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)","20020806","1998","28","2","239","253","An experimental approach to robotic grasping is presented. This approach is based on developing a generic representation of grasping rules, which allows learning them from experiments between the object and the robot. A modular connectionist design arranged in subsumption layers is used to provide a mapping between sensory inputs and robot actions. Reinforcement feedback is used to select between different grasping rules and to reduce the number of failed experiments. This is particularly critical for applications in the personal service robot environment. Simulated experiments on a 15-object database show that the system is capable of learning grasping rules for each object in a finite number of experiments as well as generalizing from experiments on one object to grasping from another","1094-6977;10946977","","10.1109/5326.669561","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=669561","","Databases;Grasping;Grippers;Humans;Intelligent robots;Manipulators;Performance evaluation;Robot sensing systems;Service robots;System testing","feedback;learning systems;manipulators;neural nets","connectionist architecture;generic grasping functions;generic grasping rule representation;learning;modular connectionist design;object database;personal service robot environment;reinforcement feedback;robot actions;robotic grasping;sensory inputs;simulated experiments;subsumption layers","","17","","26","","","","May 1998","","IEEE","IEEE Journals & Magazines"
"Perturbed learning automata in potential games","G. C. Chasparis; J. S. Shamma; A. Rantzer","Department of Automatic Control, Lund University, 221 00-SE, Sweden","2011 50th IEEE Conference on Decision and Control and European Control Conference","20120301","2011","","","2453","2458","This paper presents a reinforcement learning algorithm and provides conditions for global convergence to Nash equilibria. For several reinforcement learning schemes, including the ones proposed here, excluding convergence to action profiles which are not Nash equilibria may not be trivial, unless the step-size sequence is appropriately tailored to the specifics of the game. In this paper, we sidestep these issues by introducing a new class of reinforcement learning schemes where the strategy of each agent is perturbed by a state-dependent perturbation function. Contrary to prior work on equilibrium selection in games, where perturbation functions are globally state dependent, the perturbation function here is assumed to be local, i.e., it only depends on the strategy of each agent. We provide conditions under which the strategies of the agents will converge to an arbitrarily small neighborhood of the set of Nash equilibria almost surely. We further specialize the results to a class of potential games.","0191-2216;01912216","Electronic:978-1-61284-801-3; POD:978-1-61284-800-6; USB:978-1-61284-799-3","10.1109/CDC.2011.6161294","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6161294","","Convergence;Games;Learning;Learning systems;Nash equilibrium;Sensitivity;Vectors","functions;game theory;learning (artificial intelligence);learning automata;perturbation techniques","Nash equilibria;equilibrium selection;global convergence;perturbed learning automata;potential games;reinforcement learning scheme;state-dependent perturbation function;step-size sequence","","5","","16","","","","12-15 Dec. 2011","","IEEE","IEEE Conferences"
"Integration of Semantic and Episodic Memories","A. Horzyk; J. A. Starzyk; J. Graham","Department of Automatics and Biomedical Engineering, AGH University of Science and Technology, Krakow, Poland","IEEE Transactions on Neural Networks and Learning Systems","20171116","2017","28","12","3084","3095","This paper describes the integration of semantic and episodic memory (EM) models and the benefits of such integration. Semantic memory (SM) is used as a foundation of knowledge and concept learning, and is needed for the operation of any cognitive system. EM retains personal experiences stored based on their significance-it is supported by the SM, and in return, it supports SM operations. Integrated declarative memories are critical for cognitive system development, yet very little research has been done to develop their computational models. We considered structural self-organization of both semantic and episodic memories with a symbolic representation of input events. Sequences of events are stored in EM and are used to build associations in SM. We demonstrated that integration of semantic and episodic memories improves the native operation of both types of memories. Experimental results are presented to illustrate how the two memories complement each other by improving recognition, prediction, and context-based generalization of individual memories.","2162-237X;2162237X","","10.1109/TNNLS.2017.2728203","National Science Centre of Poland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8008846","Cognitive system;episodic memory (EM);event significance;motivated and reinforcement learning;semantic memory (SM)","Biological system modeling;Computational modeling;Learning systems;Neurons;Semantics;Training","cognitive systems;generalisation (artificial intelligence);learning (artificial intelligence)","SM operations;cognitive system development;computational models;concept learning;context-based generalization;episodic memories;integrated declarative memories;reinforcement learning;semantic memories;semantic memory models","","","","","","","20170811","Dec. 2017","","IEEE","IEEE Journals & Magazines"
"Cooperative retransmissions using Markov decision process with reinforcement learning","G. N. Shirazi; P. Y. Kong; C. K. Tham","Institute for Infocomm Research, Agency for Science, Technology & Research (A&#191;STAR), 1 Fusionopolis Way, #21-01 Connexis South Tower, 138632 Singapore","2009 IEEE 20th International Symposium on Personal, Indoor and Mobile Radio Communications","20100415","2009","","","652","656","In cooperative retransmissions, nodes with better channel qualities help other nodes in retransmitting a failed packet to its intended destination. In this paper, we propose a cooperative retransmission scheme where each node makes local decision to cooperate or not to cooperate at what transmission power using a Markov decision process with reinforcement learning. With the reinforcement learning, the proposed scheme avoids solving an Markov decision process with a large number of states. Through simulations, we show that the proposed scheme is robust to collisions, is scalable with regard to the network size, and can provide significant cooperative diversity.","2166-9570;21669570","Electronic:978-1-4244-5123-4; POD:978-1-4244-5122-7","10.1109/PIMRC.2009.5450098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5450098","","Automatic repeat request;Bismuth;Dynamic programming;Learning;Poles and towers;Relays;Robustness;Throughput;Transmitters;Wireless networks","Markov processes;diversity reception;wireless channels","Markov decision process;cooperative diversity;cooperative retransmission;reinforcement learning","","0","","11","","","","13-16 Sept. 2009","","IEEE","IEEE Conferences"
"Self-optimization of capacity and coverage in LTE networks using a fuzzy reinforcement learning approach","R. Razavi; S. Klein; H. Claussen","Bell Laboratories, Alcatel-Lucent, Blanchardtown Industrial Park, Dublin 15, Republic of Ireland","21st Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications","20101217","2010","","","1865","1870","This paper introduces a solution to enable self-optimization of coverage and capacity in LTE networks through base stations' downtilt angle adjustment. The proposed method is based on fuzzy reinforcement learning techniques and operates in a fully distributed and autonomous fashion without any need for a priori information or human interventions. The solution is shown to be capable of handling extremely noisy feedback information from mobile users as well as being responsive to the changes in the environment including self-healing properties. The simulation results confirm the convergence of the solution to the global optimal settings and that the proposed scheme provides up to 20% performance improvement when compared with an existing fuzzy logic based reinforcement learning approach.","2166-9570;21669570","Electronic:978-1-4244-8016-6; POD:978-1-4244-8017-3","10.1109/PIMRC.2010.5671622","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5671622","Downtilt Adjustment;Fuzzy Logic;LTE;Reinforcement Learning;Self-x Networks;component","Fuzzy logic;Geophysical measurement techniques;Ground penetrating radar;Learning;Measurement;Mobile communication;Optimization","Long Term Evolution;feedback;fuzzy logic;learning (artificial intelligence);optimisation;telecommunication computing","LTE networks;base station;fuzzy logic;fuzzy reinforcement learning;mobile users;noisy feedback information;self-healing;self-optimization","","30","","14","","","","26-30 Sept. 2010","","IEEE","IEEE Conferences"
"Reinforcement learning system to mitigate small-cell interference through directionality","A. Paatelma; D. H. Nguyen; H. Saarnisaari; N. Kandasamy; K. R. Dandekar","CWC, University of Oulu, Finland","2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)","20180215","2017","","","1","7","Beam-steering techniques using directional antennas are expected to play an important role in wireless network capacity expansion through ubiquitous small-cell deployment. However, integrating directional antennas into the existing wireless PHY and MAC stack of small cells has been challenging due to the added protocol overhead and lack of a robust antenna beam selection technique that can adapt well to environmental changes. This paper presents the design, implementation, and evaluation of LinkPursuit, a novel learning protocol for distributed antenna state selection in directional small-cell networks. LinkPursuit relies on reconfigurable antennas and a synchronous TimeDivision Multiple Access (TDMA) MAC to achieve simultaneous directional transmission and reception. Further, the system employs a practical antenna selection protocol based on the well known adaptive pursuit algorithm from the reinforcement learning literature. We implement a realtime prototype of LinkPursuit on the WARP platform and conduct extensive experiments to evaluate its performance. The empirical results show that appropriate use of directionality in LinkPursuit can result in higher network sum rates than omnidirectional transmission under various degrees of cross-link interference.","","Electronic:978-1-5386-3531-5; POD:978-1-5386-3532-2; Paper:978-1-5386-3529-2; USB:978-1-5386-3530-8","10.1109/PIMRC.2017.8292393","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8292393","","Directive antennas;Media Access Protocol;Slot antennas;Time division multiple access;Transmitting antennas;Wireless communication","access protocols;beam steering;cellular radio;directive antennas;learning (artificial intelligence);radiofrequency interference;time division multiple access","LinkPursuit;adaptive pursuit algorithm;beam-steering techniques;directional antennas;directional small-cell networks;distributed antenna state selection;reconfigurable antennas;reinforcement learning system;robust antenna beam selection technique;small-cell interference;synchronous TimeDivision Multiple Access MAC;ubiquitous small-cell deployment;wireless network capacity expansion","","","","","","","","8-13 Oct. 2017","","IEEE","IEEE Conferences"
"Identification and evaluation of performance parameters for RE-BAR bending training simulator","B. M. Menon; P. Aswathi; S. Deepu; R. R. Bhavani","AMMACHI Labs, Amrita School of Engineering, Amritapuri, Amrita Vishwa Vidyapeetham, Amrita University, Kerala, India","2017 8th International Conference on Computing, Communication and Networking Technologies (ICCCNT)","20171214","2017","","","1","7","Construction rebars (steel concrete reinforcing bars) are used to provide structural strength and reinforcement for the concrete structure. This requires the bending and cutting of the rebar to proper size before they can be used for construction. A novel haptic based barbending simulator has been devised which enables the trainees to learn and improve the construction rebar bending skill in a safe and controlled way. With its limited assessment and reporting features, the computerised virtual training simulator proves to be effective in training. Adding the features like personalized skill tracking and predictive performance modeling holds even more potential in supporting the training program. Towards this goal, a user performance modeling needs to be done which includes an initial study on performance parameters, assessment criteria and data collection before remodeling the simulator. This paper presents a study on the performance parameters for the bar bending simulator and also evaluates its effectiveness in modeling expert and novice performances. During this study we also hypothesize the parameters that can distinguish an expert and novice performances which was validated with 2 classification techniques - Support vector machine and J48 Decision tree. While revealing the classification rules J48 algorithm provides 78.94% accuracy where as SVM provides 94.737% accuracy. The study also shows that the 2 performance parameters force applied over time and bend angle accuracy are effective to distinguish expert and novice level of expertize for the construction rebar bending skill.","","Electronic:978-1-5090-3038-5; POD:978-1-5090-3039-2","10.1109/ICCCNT.2017.8204042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8204042","Decision trees;Intelligent systems;Machine learning algorithms;Modeling;Support vector machines;Vocational training","Bars;Data models;Force;Hidden Markov models;Shape;Training;Vehicles","bending;computer based training;construction industry;decision trees;digital simulation;haptic interfaces;mechanical engineering computing;pattern classification;rebar;steel;support vector machines","RE-BAR bending training simulator;assessment criteria;bar bending simulator;bending cutting;computerised virtual training simulator;concrete structure;construction rebar bending skill;construction rebars;data collection;personalized skill tracking;predictive performance modeling;steel concrete;structural strength;training program;user performance modeling","","","","","","","","3-5 July 2017","","IEEE","IEEE Conferences"
"Towards an agent-based Approach for Service Emergence in Pervasive Computing","A. Hassnaoui; M. Bakhouya; J. Gaber","Institut National des Telecommunications, France","Advanced Int'l Conference on Telecommunications and Int'l Conference on Internet and Web Applications and Services (AICT-ICIW'06)","20060403","2006","","","15","15","Pervasive computing is a new paradigm with a goal to provide computing and communication services all the time and everywhere. In this paper, a service emergence model for the implementation of pervasive computing applications is presented. Inspired by natural immune system concepts, the model allows the emergence of ad hoc services on the fly according to dynamically changing context environments such as computing context and user context. In this model, ad hoc or composite services are represented by an organization or group of autonomous agents. Agents establish relationships based on affinities to form groups of agents to provide the composite services. Affinities are parameters that represent priorities between agents. They help to distinguish between agents that can satisfy certain conditions or criteria. Affinity adjustments are based on two level of satisfaction. The first level is a local satisfaction depending on available services offered by neighboring agents together with respect to dynamic changes in network resources. The second level is a global satisfaction determined by the user satisfaction regarding the end service provided.","","POD:0-7695-2522-9","10.1109/AICT-ICIW.2006.196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1602147","Agent-based system;Anytime/anywhere elearning;Immune system.;Pervasive computing;Reinforcement learning;Service composition and emergence;collaborative mobile learning","Autonomous agents;Computer vision;Context modeling;Context-aware services;Hospitals;Immune system;Personal digital assistants;Pervasive computing;Protocols;Ubiquitous computing","","Agent-based system;Anytime/anywhere elearning;Immune system.;Pervasive computing;Reinforcement learning;Service composition and emergence;collaborative mobile learning","","0","","14","","","","19-25 Feb. 2006","","IEEE","IEEE Conferences"
"Teaching project management using a real-world group project","L. Collingbourne; W. K. G. Seah","School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand","2015 IEEE Frontiers in Education Conference (FIE)","20151207","2015","","","1","8","It is well established that an effective pedagogy for project management requires students to get real-world experience. The challenge in providing this when teaching undergraduate engineers is the dichotomy between achieving realism and maintaining sufficient simplicity to make the course tractable. A real-world group technology project at Victoria University of Wellington (VUW) in New Zealand establishes essential non-technical attributes required by the engineering profession while covering key elements of the project management body of knowledge (PMBOK). This paper first shows how the project covers the knowledge required in project management and then presents the results of two years of data collected from students' reflection on their own learning. We have established a pleasing congruence across the years against the specific learning topics of team working skills, communication skills and personal working skills with an improvement in project management skills. A key finding emerging from our analysis is the importance of reinforcement learning and reflective learning. We show a key link between these two learning mechanisms and the project pedagogy. Further analysis shows the link between the project pedagogy and four skill areas acquired. Finally, our research has identified specific areas for us to focus on for subsequent years.","","Paper:978-1-4799-8454-1","10.1109/FIE.2015.7344301","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344301","graduate attributes;project management;real-world project","Cultural differences;Education;Monitoring;Project management;Software;Software engineering","educational courses;engineering education;further education;learning (artificial intelligence);project management;team working","New Zealand;PMBOK;VUW;Victoria University of Wellington;communication skills;dichotomy;educational course;effective pedagogy;personal working skills;project management body of knowledge;real-world group project;reflective learning;reinforcement learning;teaching project management;team working skills;undergraduate engineers teaching","","1","","26","","","","21-24 Oct. 2015","","IEEE","IEEE Conferences"
"Exploiting Reinforcement Learning to Profile Users and Personalize Web Pages","S. Ferretti; S. Mirri; C. Prandi; P. Salomoni","Dept. of Comput. Sci. & Eng., Univ. di Bologna, Bologna, Italy","2014 IEEE 38th International Computer Software and Applications Conference Workshops","20140922","2014","","","252","257","In this paper, we present a Web content adaptation system that is able to automatically adapt textual elements of Web pages, based on the user profile and preferences. The system employs Web intelligence to perform these automatic adaptations on single elements composing a Web page. In particular, a reinforcement learning algorithm, i.e. Q-learning, based on the idea of reward/punishment is utilized as the machine learning system that manages the user profile. Based on it, the user profile is updated, so that automatic adaptations can be effectively performed while surfing the Web. We created a simulation scenario to test our approach over different users with specific preferences and/or different kinds of disabilities. Simulation results confirm the viability of the proposal.","","Electronic:978-1-4799-3578-9; POD:978-1-4799-3579-6","10.1109/COMPSACW.2014.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903138","Web personalization;content adaptation;legibility;reinforcement learning;user profiling","Adaptation models;Context;Learning (artificial intelligence);Learning systems;Prototypes;Senior citizens;Web pages","Internet;learning (artificial intelligence);user interfaces","Q-learning;Web content adaptation system;Web intelligence;Web page personalization;machine learning system;reinforcement learning;simulation scenario;textual elements;user preference;user profile","","1","","24","","","","21-25 July 2014","","IEEE","IEEE Conferences"
"Smart Cable-Driven Camera Robotic Assistant","I. Rivas-Blanco; C. López-Casado; C. J. Pérez-del-Pulgar; F. García-Vacas; J. C. Fraile; V. F. Muñoz","Department of System Engineering and Automation, University of Malaga, Andaluc&#x00ED;a Tech, M&#x00E1;laga, Spain","IEEE Transactions on Human-Machine Systems","20180313","2018","48","2","183","196","This paper describes the mechanical design and a cognition system for a novel concept-of-camera robotic assistant. The system combines the advantages of intra-abdominal devices and autonomous camera navigation. The robotic assistant is composed of a magnetic intra-abdominal camera robot with two internal cable-driven degrees of freedom and an external robot that handles an external magnet. The intelligence of the robot is implemented in a cognitive architecture based on a long-term memory that stores the robot's knowledge and learning capabilities to improve the robot's behavior. The navigation strategy combines a reactive control based on instrument tracking and a proactive control based on predefined behaviors, depending on the actual state of the task. The robot's learning capabilities include a semantic customization, to adapt the camera's behavior to the surgeon's preferences, and reinforcement learning, to improve the camera navigation strategy. This paper details both the hardware implementation of the system and the software implementation in a robotic operating system architecture. The cognition system and the performance of the cable-driven mechanism have been validated with a set of in vitro experiments. Moreover, the camera robot has been evaluated through an in-vivo experiment in a pig.","2168-2291;21682291","","10.1109/THMS.2017.2767286","Spanish national projects; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8107576","Cognitive robotics;mechatronics;medical robotics;robot control;robot motion","Cameras;Navigation;Robot vision systems;Surgery;Tools","cameras;control engineering computing;learning (artificial intelligence);medical computing;medical robotics;robot programming;robot vision;surgery","autonomous camera navigation;camera navigation strategy;cognition system;cognitive architecture;concept-of-camera robotic assistant;external magnet;external robot;intra-abdominal devices;learning capabilities;long-term memory;magnetic intra-abdominal camera robot;mechanical design;reinforcement learning;robotic operating system architecture;smart cable","","","","","","","20171114","April 2018","","IEEE","IEEE Journals & Magazines"
"Multiagent reinforcement learning method with an improved ant colony system","Ruoying Sun; S. Tatsumi; Gang Zhao","Fac. of Eng., Osaka City Univ., Japan","2001 IEEE International Conference on Systems, Man and Cybernetics. e-Systems and e-Man for Cybernetics in Cyberspace (Cat.No.01CH37236)","20020806","2001","3","","1612","1617 vol.3","Multiagent reinforcement learning has gained increasing attention in recent years. The authors discuss coordination means for sharing episodes and sharing policies in the field of multiagent reinforcement learning. From the point of the view of reinforcement learning, we analyse the performance of indirect media communication among multi-agents on an ant colony system which is an efficient method that uses pheromones to solve optimization problems. Based on the above, we propose the Q-ACS method, modifying the global updating rule in ACS for learning agents to share better episodes benefited from the exploitation of accumulated knowledge. Meanwhile, taking the visited times into account, we propose T-ACS by presenting a state transition policy for learning agents to share better policies, benefiting from biased exploration. To demonstrate the coordination performance of learning agents in our methods, we conducted experiments on an optimization problem, the traveling salesman problem. Comparison of results with ACS, Q-ACS and T-ACS show that the improved methods are efficient for solving the optimization problem","1062-922X;1062922X","POD:0-7803-7087-2","10.1109/ICSMC.2001.973515","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=973515","","Ant colony optimization;Distributed computing;Explosives;Internet;Learning;Optimization methods;Personal digital assistants;Sun;Telephony;Traveling salesman problems","learning (artificial intelligence);multi-agent systems;travelling salesman problems","ACS;Q-ACS method;T-ACS;accumulated knowledge;ant colony system;biased exploration;coordination means;coordination performance;global updating rule;indirect media communication;learning agents;multiagent reinforcement learning;optimization problems;pheromones;state transition policy","","3","","18","","","","2001","07 Oct 2001-10 Oct 2001","IEEE","IEEE Conferences"
"A comparison of two algorithms for robot learning from demonstration","H. B. Suay; S. Chernova","Robotics Engineering Program, Worcester Polytechnic Institute, MA 01609, USA","2011 IEEE International Conference on Systems, Man, and Cybernetics","20111121","2011","","","2495","2500","Robot learning from demonstration focuses on algorithms that enable a robot to learn a policy from demonstrations performed by a teacher, typically a human expert. This paper presents an experimental evaluation of two learning from demonstration algorithms, Interactive Reinforcement Learning and Behavior Networks. We evaluate the performance of these algorithms using a humanoid robot and discuss the relative advantages and drawbacks of these methods with respect to learning time, number of demonstrations, ease of implementation and other metrics. Our results show that Behavior Networks rely on a greater degree of domain knowledge and programmer expertise, requiring very precise definitions for behavior pre- and post-conditions. By contrast Interactive RL requires a relatively simple implementation based only on the robot's sensor data and actions. However, Behavior Networks leverage the pre-coded knowledge to effectively reduce learning time and the required number of human interactions to learn the task.","1062-922X;1062922X","Electronic:978-1-4577-0653-0; POD:978-1-4577-0652-3","10.1109/ICSMC.2011.6084052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6084052","Learning and Adaptive Systems;Personal Robots","Actuators;Humans;Knowledge engineering;Learning;Robot sensing systems;Strontium","human-robot interaction;humanoid robots;learning by example","behavior networks;human interactions;humanoid robot;interactive reinforcement learning;robot learning from demonstration","","0","","15","","","","9-12 Oct. 2011","","IEEE","IEEE Conferences"
"Inter-cluster connection in cognitive wireless mesh networks based on intelligent network coding","X. Chen; Z. Zhao; H. Zhang; T. Jiang; D. Grace","Key Laboratory of Integrate Information Network Technology, Zhejiang University, Zheda Road 38, Hangzhou, 310027, China","2009 IEEE 20th International Symposium on Personal, Indoor and Mobile Radio Communications","20100415","2009","","","1251","1256","Cognitive wireless mesh networks have great flexibility to improve the spectrum utilization by opportunistically accessing the authorized frequency bands, within which the secondary users (SUs) should not violate the quality of service (QoS) requirement of the primary users (PUs) while transmitting. In this paper, we consider inter-cluster connection among neighboring clusters under the framework of cognitive wireless mesh networks. Corresponding to the neighboring clusters, all nodes operate in half-duplex mode; hence exchanging control message usually needs four time slots by traditional scheme, which leads to a loss in networking and spectral efficiency especially at the gateway node. A novel scheme based on network coding is proposed, which needs only two time slots. Our simulation experiments reveal the following findings: the performances of traditional inter-cluster connection and network coding based inter-cluster connection are comparable. Next, how to choose optimal signal amplification factor at the gateway node according to the wireless environment is discussed. And we present an intelligent policy based on reinforcement learning to solve the problem. Theoretical analysis and numerical results both show the policy can achieve optimal throughput for the SUs in the long run.","2166-9570;21669570","Electronic:978-1-4244-5123-4; POD:978-1-4244-5122-7","10.1109/PIMRC.2009.5449924","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5449924","CogMesh;cluster;cognitive radio;cognitive wireless mesh networks;network coding;reinforcement learning;relaying","Chromium;Cognitive radio;Frequency;Intelligent networks;Laboratories;Learning;Network coding;Quality of service;Relays;Wireless mesh networks","cognitive radio;network coding;wireless mesh networks","QoS;cognitive wireless mesh networks;gateway node;half-duplex mode;intelligent network coding;intelligent policy;intercluster connection;optimal signal amplification factor;primary users;quality of service;reinforcement learning;secondary users;spectrum utilization","","3","","15","","","","13-16 Sept. 2009","","IEEE","IEEE Conferences"
"Adaptive hierarchical resource management for satellite channel in hybrid MANET-satellite-Internet network","N. X. Liu; Xiaoming Zhou; J. S. Baras","Inst. for Syst. Res., Maryland Univ., College Park, MD, USA","IEEE 60th Vehicular Technology Conference, 2004. VTC2004-Fall. 2004","20050425","2004","6","","4027","4031 Vol. 6","MANETs are often deployed in an infrastructure-less or hostile region where the satellite provides the only link for the MANETs to communicate with the rest of the world. It faces many challenges to support multiple serviced communications between MANETs and Internet through satellite. In this paper, we propose an efficient resource management scheme called AHRM to dynamically allocate bandwidths among multiple MANET users and multiple priority and non-priority services sharing a multi-access satellite channel. It uses a flexible hierarchical structure to exploit the channel utility and resolve contention from two levels. A bandwidth adaptation algorithm is designed to adjust the allocation dynamically in response to traffic and link status changes. The algorithm turns out to be in line with reinforcement learning and is a customized version of it for the practical satellite network setting. Implementation issues are discussed. Simulation results are presented, showing that the scheme can guarantee fast delivery of critical messages in spite of channel contention, and significantly improve the performance of multiple services.","1090-3038;10903038","POD:0-7803-8521-7","10.1109/VETECF.2004.1404834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1404834","","Algorithm design and analysis;Artificial satellites;Bandwidth;Delay;Educational institutions;Intelligent networks;Mobile ad hoc networks;Resource management;Telecommunication traffic;Web and internet services","IntServ networks;Internet;ad hoc networks;channel allocation;learning (artificial intelligence);mobile satellite communication;multi-access systems","AHRM;adaptive hierarchical resource management;adaptive hierarchical scheduling;bandwidth adaptation;contention resolution;dynamic bandwidth allocation;hybrid MANET-satellite-Internet network;link status;mobile wireless ad hoc network;multiaccess satellite channel;multiple priority services;multiple service communications;reinforcement learning;traffic status","","3","","12","","","","26-29 Sept. 2004","","IEEE","IEEE Conferences"
"New software for learner-centered circuits instruction","M. D. Ciletti","Dept. of Electr. & Comput. Eng., Colorado Univ., Colorado Springs, CO, USA","Frontiers in Education Conference, 1998. FIE '98. 28th Annual","20020806","1998","3","","1100 vol.3","","Summary form only given. Today's classroom and technologies offer solutions to the challenges that face students and instructors in circuits. Software tools now support several areas of the curriculum (e.g. circuits and electromagnetic fields), and many students learn to use them early in their studies. The availability of powerful personal computers linked to classroom video projection systems creates an opportunity for faculty to broaden the scope of their instruction on-the-fly, with a high level of audience interaction and exploration. With software tools, examples can be explored freely, and students can address ""what-if"" questions immediately. Students gain valuable reinforcement for their understanding of abstract concepts by seeing physical, practical effects on the screen, under their control. Here, the authors discuss such software for learner-centered circuits instruction.","0190-5848;01905848","POD:0-7803-4762-5","10.1109/FIE.1998.738572","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=738572","","Circuit analysis;Circuit theory;Electrical engineering education;Fourier series;Fourier transforms;Linear circuits;Software tools;Springs;Time domain analysis;User-generated content","computer aided instruction;educational courses;electronic engineering computing;electronic engineering education;microcomputer applications","classroom video projection systems;curriculum;instructors;learner-centered circuits instruction software;personal computers;software tools;students","","0","","","","","","4-7 Nov. 1998","04 Nov 1998-07 Nov 1998","IEEE","IEEE Conferences"
"Context-aware unified routing for VANETs based on virtual clustering","Y. Ji; C. Wu; T. Yoshinaga","National Institute of Informatics, Tokyo, Japan","2016 IEEE 27th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)","20161222","2016","","","1","6","We propose a context-aware routing protocol for vehicular ad hoc networks (VANETs). Two types of context information is considered in this paper specifically communication type (broadcast or unicast) and packet size. The proposed protocol constructs route based on virtual clustering which only exchanges beacon messages in one-hop neighborhood area. The packets are forwarded by the cluster heads, and the last 2-hop route is optimized by using a reinforcement learning algorithm which can attain good performance with low overhead. The advantage of the proposed protocol is shown by using computer simulations.","","Electronic:978-1-5090-3254-9; POD:978-1-5090-3255-6","10.1109/PIMRC.2016.7794599","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7794599","","Conferences;Context;Payloads;Routing protocols;Unicast;Vehicles","learning (artificial intelligence);mobile computing;pattern clustering;routing protocols;telecommunication computing;vehicular ad hoc networks","VANET;computer simulation;context-aware unified routing protocol;one-hop neighborhood area;packet size;reinforcement learning algorithm;vehicular ad hoc network;virtual clustering","","","","","","","","4-8 Sept. 2016","","IEEE","IEEE Conferences"
"Customised pearlmutter propagation: A hardware architecture for trust region policy optimisation","S. Shao; W. Luk","Department of Computing, Imperial College London","2017 27th International Conference on Field Programmable Logic and Applications (FPL)","20171005","2017","","","1","6","Reinforcement Learning (RL) is an area of machine learning in which an agent interacts with the environment by making sequential decisions. The agent receives reward from the environment to find an optimal policy that maximises the reward. Trust Region Policy Optimisation (TRPO) is a recent policy optimisation algorithm that achieves superior results in various RL benchmarks, but is computationally expensive. This paper proposes Customised Pearlmutter Propagation (CPP), a novel hardware architecture that accelerates TRPO on FPGA. We use the Pearlmutter Algorithm to address the key computational bottleneck of TRPO in a hardware efficient manner, avoiding symbolic differentiation with change of variables. Experimental evaluation using robotic locomotion benchmarks demonstrates that the proposed CPP architecture implemented on Stratix-V FPGA can achieve up to 20 times speed-up against 6-threaded Keras deep learning library with Theano backend running on a Core i7-5930K CPU.","","Electronic:978-9-0903-0428-1; POD:978-1-5386-2040-3","10.23919/FPL.2017.8056789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8056789","","Acceleration;Benchmark testing;Computer architecture;Field programmable gate arrays;Hardware;Machine learning;Optimization","computer architecture;field programmable gate arrays;learning (artificial intelligence);multi-agent systems;optimisation","CPP architecture;Customised Pearlmutter Propagation;Customised pearlmutter propagation;Pearlmutter Algorithm;Stratix-V FPGA;TRPO;hardware architecture;machine learning;reinforcement learning;robotic locomotion benchmarks;sequential decision making;trust region policy optimisation","","","","","","","","4-8 Sept. 2017","","IEEE","IEEE Conferences"
"Adaptive learning based directional MAC protocol for millimeter wave (mmWave) wireless networks","P. Tiwari; D. K. Meena; L. S. Pillutla","Dhirubhai Ambani Institute of Information and Communication Technology (DA-IICT), Gandhinagar - 382007, Gujarat, India","2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)","20180215","2017","","","1","5","Directional antennas are used to counter the increased path loss at millimeter wave (mmWave) frequencies (beyond 30 GHz). Usage of directional antennas imply lack of channel sensing on the part of various network nodes. Consequently coordination among various network nodes becomes critical for efficient network operation. In this paper we propose the adaptive learning directional medium access control (AL-DMAC) protocol based on reinforcement learning, to facilitate implicit coordination among various nodes. Our simulation results demonstrate that the AL-DMAC protocol along with a suitably chosen neighbor discovery mechanism yields higher network throughput than both the naive directional slotted ALOHA (DSA) protocol and the memory guided directional medium access control (MD-MAC) protocol. Further the AL-DMAC protocol and the MD-MAC protocol have comparable performance in terms of fairness, measured in terms of the Jain's fairness index.","","Electronic:978-1-5386-3531-5; POD:978-1-5386-3532-2; Paper:978-1-5386-3529-2; USB:978-1-5386-3530-8","10.1109/PIMRC.2017.8292296","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8292296","MAC protocols and reinforcement learning;directional antennas;mmWave","Adaptive learning;Directional antennas;Media Access Protocol;Millimeter wave communication;Millimeter wave technology;Throughput","access protocols;directive antennas;learning (artificial intelligence);millimetre wave communication;telecommunication computing;wireless channels","AL-DMAC protocol;MD-MAC protocol;adaptive learning directional medium access control;channel sensing;directional antennas;directional slotted ALOHA protocol;frequency 30.0 GHz;millimeter wave wireless networks;reinforcement learning","","","","","","","","8-13 Oct. 2017","","IEEE","IEEE Conferences"
"A learning strategy for paging in mobile environments","I. Koukoutsidis; P. Demestichas; M. Theologou","Sch. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Greece","2003 5th European Personal Mobile Communications Conference (Conf. Publ. No. 492)","20041101","2003","","","585","590","The essence of designing a good paging strategy is to incorporate user mobility characteristics in a predictive mechanism that reduces the average paging cost with as little computational effort as possible. We introduce a novel paging scheme based on the concept of reinforcement learning. Learning endows the paging mechanism with the predictive power necessary to determine a mobile terminal's position, without having to extract a location probability distribution for each specific user. The proposed algorithm is compared against a heuristic randomized learning strategy akin to reinforcement learning, that we invented for this purpose and performs better than the case where no learning is used at all. It is shown that if the user normally moves only among a fraction of cells in the location area, significant savings can be achieved over the randomized strategy, without excessive time to train the network.","0537-9989;05379989","Paper:0-85296-753-5","10.1049/cp:20030322","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1350260","","","cellular radio;learning (artificial intelligence);paging communication;software agents;telecommunication computing","heuristic randomized learning strategy;intelligent agent;learning strategy;location probability distribution;mobile environments;predictive mechanism;reinforcement learning;terminal paging;user mobility characteristics","","0","","","","","","22-25 April 2003","","IET","IET Conferences"
"Personalizing robot behavior for interruption in social human-robot interaction","Y. S. Chiang; T. S. Chu; C. D. Lim; T. Y. Wu; S. H. Tseng; L. C. Fu","Department of Computer Science and Information Engineering, National Taiwan University, Taipei 10617, Taiwan","2014 IEEE International Workshop on Advanced Robotics and its Social Impacts","20150126","2014","","","44","49","People engaging in an activity usually has individual tolerance to be interrupted [1], [2]. Humans subconsciously adapt their behaviors to draw other one's attention and to get into a conversation based on their historical experiences, but robots often fail to be aware of humans' feeling and thus interrupt their users repeatedly. To endow service robots with such socially acceptable ability, we propose an online human-aware interactive learning framework in this paper, under which the robot personalizes its behaviors according to both observed user's attention and its conjecture about user's awareness of itself. To this purpose, the correlation between the robot's theory of awareness, user's attention and robot behavior are explored through reinforcement learning techniques. The conducted experiment shows that the robot can personalize its interruption strategy, and the optimal policies converged for at least 26 episodes.","2162-7568;21627568","Electronic:978-1-4799-6968-5; POD:978-1-4799-6969-2; USB:978-1-4799-6967-8","10.1109/ARSO.2014.7020978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7020978","","Face;Hidden Markov models;Interrupters;Learning (artificial intelligence);Markov processes;Robot sensing systems","human-robot interaction;learning (artificial intelligence);service robots;social sciences","interruption strategy;online human-aware interactive learning framework;reinforcement learning techniques;robot behavior personalization;robot theory of awareness;service robots;social human-robot interaction;user attention;user awareness","","0","","23","","","","11-13 Sept. 2014","","IEEE","IEEE Conferences"
"Supervisory output prediction for bilinear systems by reinforcement learning","G. C. Chasparis; T. Natschläger","Software Competence Center Hagenberg GmbH, Austria","IET Control Theory & Applications","20170608","2017","11","10","1514","1521","Online output prediction is an indispensable part of any model predictive control implementation. For several application scenarios, operating conditions may change quite often, while designing the data collection process may not be an option. To this end, this study introduces a supervisory output prediction scheme, tailored specifically for input-output stable bilinear systems, that intends on automating the process of selecting the most appropriate prediction model during runtime. The selection process is based upon a reinforcement-learning scheme, where prediction models are selected according to their prior prediction performance. An additional selection process is concerned with appropriately partitioning the control-inputs' domain also to allow for switched-system approximations of the original bilinear dynamics. The authors show analytically that the proposed scheme converges (in probability) to the best model and partition. They also demonstrate these properties through simulations of temperature prediction in residential buildings.","1751-8644;17518644","","10.1049/iet-cta.2016.1400","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7942299","","","bilinear systems;buildings (structures);digital simulation;learning (artificial intelligence);predictive control;switching systems (control);temperature control","control-input domain;data collection process;input-output stable bilinear systems;model predictive control implementation;online output prediction;original bilinear dynamics;prior prediction performance;reinforcement learning;residential buildings;supervisory output prediction;switched-system approximations;temperature prediction simulation","","","","","","","","6 23 2017","","IET","IET Journals & Magazines"
"A reinforcement self-learning model on an intelligent behavior avatar in a virtual world","Jui-Fa Chen; Wei-Chuan Lin; Hua-Sheng Bai; Hsiao-Chuan Chao","Dept. of Inf. Eng., Tamkang Univ., Tamsui, Taiwan","IEEE International Conference on Sensor Networks, Ubiquitous, and Trustworthy Computing (SUTC'06)","20060619","2006","1","","7 pp.","","In this paper, a novel method for personal intelligent behavior avatar (IBA) is proposed to acquire autonomous behavior based on the interactions between user and smart objects in the virtual environment. In this method, the behavior decision model and the self-learning model are integrated by Bayesian networks and reinforcement learning. The Bayesian networks can treat interaction experiences using statistical processes, and the sureness of decision making is represented by certainty factors using stochastic reasoning. The reinforcement learning is implemented by learning experimentation or trial and error mechanisms to improve the performance of IBA through feedback. Therefore, the IBA makes a strategic decision that is approximated and appropriate to the user through the self-learning process by reinforcement learning. Finally, the feasibility of this method is investigated by imitating user's behavior and the results of self-learning process. The results of simulation show that the method is successful in imitating user's behavior and improving the performance of IBA","","POD:0-7695-2553-9","10.1109/SUTC.2006.1636185","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1636185","","Artificial intelligence;Avatars;Bayesian methods;Chaos;Decision making;Educational institutions;Information technology;Intelligent agent;Learning;Virtual environment","approximation theory;avatars;belief networks;decision making;heuristic programming;statistical analysis;unsupervised learning","Bayesian network;IBA;approximation theory;behavior decision model;learning experimentation;personal intelligent behavior avatar;reinforcement self-learning model;statistical process;stochastic reasoning;strategic decision making","","0","","11","","","","5-7 June 2006","","IEEE","IEEE Conferences"
"Reward Sensitivity of ACC as an Intermediate Phenotype between DRD4-521T and Substance Misuse","T. E. Baker; T. Stockwell; G. Barnes; R. Haesevoets; C. B. Holroyd","1University of Victoria","Journal of Cognitive Neuroscience","20160204","2016","28","3","460","471","<para>The development and expression of the midbrain dopamine system is determined in part by genetic factors that vary across individuals such that dopamine-related genes are partly responsible for addiction vulnerability. However, a complete account of how dopamine-related genes predispose individuals to drug addiction remains to be developed. Adopting an intermediate phenotype approach, we investigated whether reward-related electrophysiological activity of ACC—a cortical region said to utilize dopamine reward signals to learn the value of extended, context-specific sequences of goal-directed behaviors—mediates the influence of multiple dopamine-related functional polymorphisms over substance use. We used structural equation modeling to examine whether two related electrophysiological phenomena associated with the control and reinforcement learning functions of ACC—theta power and the reward positivity—mediated the relationship between the degree of substance misuse and genetic polymorphisms that regulate dopamine processing in frontal cortex. Substance use data were collected from 812 undergraduate students. One hundred ninety-six returned on a subsequent day to participate in an electrophysiological experiment and to provide saliva samples for DNA analysis. We found that these electrophysiological signals mediated a relationship between the DRD4-521T dopamine receptor genotype and substance misuse. Our results provide a theoretical framework that bridges the gap between genes and behavior in drug addiction and illustrate how future interventions might be individually tailored for specific genetic and neurocognitive profiles.</para>","0898-929X;0898929X","","10.1162/jocn_a_00905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395964","","","","","","","","","","","","March 2016","","MIT Press","MIT Press Journals"
"Emotion-driven learning agent for setting rich presence in mobile telephony","S. Saha; R. Quazi","The Royal Institute of Technology (KTH), Sweden and University of Dhaka, Bangladesh","2008 11th International Conference on Computer and Information Technology","20090321","2008","","","121","126","Presence or personal status information is going to be an integral part of human life in the near future. With the possibility of personalizing user preferences in a fine grained way, mobile presence will appeal to most users. Among all the advantages, one of the most annoying problems is to set the presence status manually each time. This paper discusses the development of an intelligent agent based presence client that will learn and make decisions on behalf of the user about his or her presence status. The decision is emotion driven and the learning depends on real world experience. The proposed system utilizes a neural network (NN) based emotion-driven agent to learn user preferences. As a NN learning algorithm, two approaches based on Differential Evolution and Reinforcement have been proposed, of which either one can be used. Rich presence status is set using a scripting language named Call Processing Language; and SIP is used for publishing the presence to others.","","CD-ROM:978-1-4244-2136-7; POD:978-1-4244-2135-0","10.1109/ICCITECHN.2008.4803023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4803023","ANN;CPL;DE;Presence;SIP;context aware;learning agent;neural network;reinforcement","Artificial neural networks;Context awareness;Humans;Information technology;Intelligent agent;Mobile computing;Mobile radio mobility management;Neural networks;Publishing;Telephony","authoring languages;learning (artificial intelligence);mobile computing;neural nets;software agents;telephony","call processing language;differential evolution;emotion-driven agent;emotion-driven learning agent;intelligent agent;mobile telephony;neural network;personalizing user preferences;reinforcement;rich presence;scripting language","","0","","21","","","","24-27 Dec. 2008","","IEEE","IEEE Conferences"
"Reference signal power control for load balancing in downlink LTE-A self-organizing networks","C. Ma; R. Yin; G. Yu; J. Zhang","Institute of information and communication engineering, Zhejiang University, Hangzhou, China","2012 IEEE 23rd International Symposium on Personal, Indoor and Mobile Radio Communications - (PIMRC)","20121129","2012","","","460","464","Self-organizing network (SON) is considered as a driving technology for the deployment of next generation radio access networks. This paper addresses the problem of load balancing (LB) for multi-hop cellular network (MCN) with fixed relays such as LTE-A network in the context of SON. The designed SON algorithm, namely RSPC-RL, is based on two ideas: relay node reference signal power control (RSPC) and multi-agent reinforcement learning (RL). In the proposed RSPC-RL algorithm, the relay node is modeled as an agent that learns an optimal policy of reference signal power control from its interaction with environment to balance the load distribution of the network through dynamically changing its coverage area. Numerical results show the significant performance gain brought about by the proposed algorithm RSPC-RL.","2166-9570;21669570","Electronic:978-1-4673-2569-1; POD:978-1-4673-2566-0; USB:978-1-4673-2568-4","10.1109/PIMRC.2012.6362829","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6362829","","Algorithm design and analysis;Heuristic algorithms;Learning;Load management;Power control;Relays;Throughput","Long Term Evolution;cellular radio;control engineering computing;learning (artificial intelligence);mobile computing;multi-agent systems;next generation networks;power control;radio access networks;resource allocation;telecommunication control","MCN;RSPC-RL;RSPC-RL algorithm;SON;downlink LTE-A self-organizing networks;load balancing;load distribution;multiagent reinforcement learning;multihop cellular network;next generation radio access networks;relay node reference signal power control","","7","","18","","","","9-12 Sept. 2012","","IEEE","IEEE Conferences"
"Optimal medication dosing from suboptimal clinical examples: A deep reinforcement learning approach","S. Nemati; M. M. Ghassemi; G. D. Clifford","Dept. of Biomedical Informatics, Emory University, Atlanta, GA 30322","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","2978","2981","Misdosing medications with sensitive therapeutic windows, such as heparin, can place patients at unnecessary risk, increase length of hospital stay, and lead to wasted hospital resources. In this work, we present a clinician-in-the-loop sequential decision making framework, which provides an individualized dosing policy adapted to each patient's evolving clinical phenotype. We employed retrospective data from the publicly available MIMIC II intensive care unit database, and developed a deep reinforcement learning algorithm that learns an optimal heparin dosing policy from sample dosing trails and their associated outcomes in large electronic medical records. Using separate training and testing datasets, our model was observed to be effective in proposing heparin doses that resulted in better expected outcomes than the clinical guidelines. Our results demonstrate that a sequential modeling approach, learned from retrospective data, could potentially be used at the bedside to derive individualized patient dosing policies.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7591355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7591355","","Cost function;Drugs;Feature extraction;Hospitals;Learning (artificial intelligence);Time series analysis;Training","decision making;drugs;electronic health records;learning (artificial intelligence);patient treatment","MIMIC II intensive care unit database;clinical guidelines;clinical phenotype;clinician-in-the-loop sequential decision making framework;deep reinforcement learning algorithm;electronic medical records;hospital stay length;individualized dosing policy;individualized patient dosing policies;optimal heparin dosing policy;optimal medication dosing;retrospective data;sample dosing trails;sensitive therapeutic windows;sequential modeling approach;suboptimal clinical examples;testing datasets;training datasets","Algorithms;Databases, Factual;Dose-Response Relationship, Drug;Heparin;Humans;Learning;Length of Stay;Markov Chains;Reinforcement (Psychology);Retrospective Studies","1","1","","","","","16-20 Aug. 2016","","IEEE","IEEE Conferences"
"Reinforcement learning approach towards effective content recommendation in MOOC environments","V. R. Raghuveer; B. K. Tripathy; T. Singh; S. Khanna","SCSE, VIT University, Vellore, Tamilnadu, India","2014 IEEE International Conference on MOOC, Innovation and Technology in Education (MITE)","20150126","2014","","","285","289","Understanding the Learner requirements is an important aspect of any learning environment as it helps to recommend the LOs in a more personalized manner. With the growing demand for MOOCs offered by coursera, edx, etc. the learner information plays a vital role in understanding the extent to which the learners can gain out of such courses. The Learning Management Systems (LMS) across the web uses the explicit (rating, performance, etc.) and implicit feedback (LOs used) obtained through interaction with the learners to derive such information. As the requirements of the learners varies with the individual's interest and learning background, a common approach for recommending LOs may not cater the needs of all the learners. To overcome this issue, this paper proposes reinforcement learning based algorithm to analyze the learner information (derived from both implicit and explicit feedback) and generate the knowledge on the learner's requirements and capabilities inside a specific learning context. The reinforcement learning system (RILS) implemented as a part of this work utilizes the knowledge thus generated in order to recommend the appropriate LOs for the learners. The results have highlighted that the knowledge derived from the learning information analysis proved to effective in generating personalized recommendation policies that can cater the context specific requirements of the learners.","","Electronic:978-1-4799-6876-3; POD:978-1-4799-6877-0","10.1109/MITE.2014.7020289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7020289","LO recommendation;MOOC;Reinforcement Learning;learning context;learning experience","Collaboration;Conferences;Context;Educational institutions;Electronic learning;Technological innovation","Internet;computer aided instruction;educational courses;human computer interaction;information analysis;learning (artificial intelligence);recommender systems","LMS;MOOC environments;RILS;Web;content recommendation;explicit feedback;generating personalized recommendation policies;implicit feedback;knowledge utilization;learning information analysis;learning management systems;massive open online course;reinforcement learning system","","1","","14","","","","19-20 Dec. 2014","","IEEE","IEEE Conferences"
"Towards a virtual personal assistant based on a user-defined portfolio of multi-domain vocal applications","T. Ekeinhor-Komi; J. L. Bouraoui; R. Laroche; F. Lefèvre","Orange Labs, France","2016 IEEE Spoken Language Technology Workshop (SLT)","20170209","2016","","","106","113","This paper proposes a novel approach to defining and simulating a new generation of virtual personal assistants as multi-application multi-domain distributed dialogue systems. The first contribution is the assistant architecture, composed of independent third-party applications handled by a Dispatcher. In this view, applications are black-boxes responding with a self-scored answer to user requests. Next, the Dispatcher distributes the current request to the most relevant application, based on these scores and the context (history of interaction etc.), and conveys its answer to the user. To address variations in the user-defined portfolio of applications, the second contribution, a stochastic model automates the online optimisation of the Dispatcher's behaviour. To evaluate the learnability of the Dispatcher's policy, several parametrisations of the user and application simulators are enabled, in such a way that they cover variations of realistic situations. Results confirm in all considered configurations of interest, that reinforcement learning can learn adapted strategies.","","Electronic:978-1-5090-4903-5; POD:978-1-5090-4904-2; USB:978-1-5090-4902-8","10.1109/SLT.2016.7846252","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846252","dialogue strategy;multi-application spoken dialogue systems;multi-domain;reinforcement learning","Gold;History;Learning (artificial intelligence);Meteorology;Portfolios;Semantics;Symmetric matrices","computational linguistics;human computer interaction;interactive systems;learning (artificial intelligence);optimisation;stochastic processes","Dispatcher policy learnability evaluation;application simulator parametrisation;black-boxes;multiapplication multidomain distributed dialogue systems;multidomain vocal applications;online optimisation;reinforcement learning;stochastic model;third-party applications;user parametrisation;user-defined portfolio;virtual personal assistant architecture","","","","","","","","13-16 Dec. 2016","","IEEE","IEEE Conferences"
"A learning-based control architecture for an assistive robot providing social engagement during cognitively stimulating activities","J. Chan; G. Nejat","Autonomous Systems and Biomechatronics Laboratory in the Department of Mechanical and Industrial Engineering at the University of Toronto, 5 King's College Road, ON, M5S 3G8 Canada","2011 IEEE International Conference on Robotics and Automation","20110818","2011","","","3928","3933","Recent studies have shown that sustained engagement in cognitively stimulating activities has had positive effects on the cognitive functioning of humans. The objective of our work is to develop an intelligent socially assistive robot that can engage individuals in person-centered cognitively stimulating activities. In this paper, we present the design of a novel learning-based control architecture that enables the robot to act as a social motivator by providing assistance, encouragement and celebration during the course of an activity. A hierarchical reinforcement learning (HRL) approach is used to provide the robot with the ability to: (i) learn appropriate assistive behaviors based on the structure of the activity and (ii) personalize the interaction based on the person's affective state during the activity. Preliminary experiments show that the proposed learning-based control architecture is effective in determining the optimal assistive behaviors of the robot during a memory game interaction.","1050-4729;10504729","CD:978-1-61284-380-3; Electronic:978-1-61284-385-8; Paper:978-1-61284-386-5","10.1109/ICRA.2011.5980426","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5980426","","Games;Heart rate;Humans;Robot sensing systems;Speech recognition;Training","cognition;intelligent robots;learning (artificial intelligence);service robots;social sciences","cognitively stimulating activity;hierarchical reinforcement learning;human cognitive function;intelligent socially assistive robot;learning-based control architecture;memory game interaction","","4","","18","","","","9-13 May 2011","","IEEE","IEEE Conferences"
"Angle of arrival estimation in dynamic indoor THz channels with Bayesian filter and reinforcement learning","B. Peng; Q. Jiao; T. Kürner","Technische Universit&#x00E4;t Braunschweig Schleinitzstra&#x00DF;e 22, 38106 Braunschweig, Germany","2016 24th European Signal Processing Conference (EUSIPCO)","20161201","2016","","","1975","1979","This paper presents a novel algorithm to estimate the Angle of Arrival (AoA) in a dynamic indoor Terahertz channel. In a realistic application, the user equipment is often moved by the user during the data transmission and the AoA must be estimated periodically, such that the adaptive directional antenna can be adjusted to realize a high antenna gain. The Bayesian filter is applied to exploit continuity and smoothness of the channel dynamics for the AoA estimation. Reinforcement learning is introduced to adapt the prior transition probabilities between system states, in order to fit the variation of application scenarios and personal habits. The algorithm is validated using the ray launching channel simulator and realistic human movement models.","","Electronic:978-0-9928-6265-7; POD:978-1-5090-1891-8","10.1109/EUSIPCO.2016.7760594","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7760594","Bayesian filter;Terahertz communication;angle of arrival estimation;dynamic channel;reinforcement learning","Azimuth;Bayes methods;Directive antennas;Estimation;Gain;Learning (artificial intelligence)","belief networks;directive antennas;filtering theory;learning (artificial intelligence);probability","AoA estimation;Bayesian filter;adaptive directional antenna;angle of arrival estimation;dynamic indoor THz channels;high antenna gain;prior transition probabilities;ray launching channel simulator;reinforcement learning","","1","","","","","","Aug. 29 2016-Sept. 2 2016","","IEEE","IEEE Conferences"
"Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453)","","","Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453)","20040107","2003","3","","","","The following topics are dealt with: tactile sensing; aerial vehicles; legged robots; motion and path planning; learning systems; simultaneous localization and mapping; visual tracking; cooperative sensing; outdoor vehicles; biped walking; collision avoidance; reinforcement learning; visual servoing; sensor applications; underwater robots; legged locomotion; learning control; sensor-based planning; computational intelligence; mobile robot localization; robot vision; Internet robots; humanoid robots; fuzzy and neural control; sensing for mobile platforms; biologically inspired robots; trajectory planning; architecture and programming; vision-based monitoring; 3D sensing; cellular and modular robots; planning algorithms; mobiligence; multi-robot control; intelligent environment; sensor fusion; micro and nano robotic systems; task allocation; actuator systems; multi-robot systems; manufacturing systems; mechanism design; integrated MEMS sensors and actuators; force-responsive mechatronics in industry; medical robots and haptics; service robots; dexterous hands; sensing and navigation; telerobotics; personal robots; rescue and security robots; spaced robots; human/robot cooperation; compliant motion control; robot assisted surgery; grasping; human-robot interaction; intelligent robots; and virtual reality.","","POD:0-7803-7860-1","10.1109/IROS.2003.1249176","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1249176","","Fuzzy control;Learning control systems;Mechatronics;Motion control;Multisensor systems;Neurocontrollers;Robots;Tactile sensors;Tracking","collision avoidance;control engineering computing;fuzzy control;learning systems;mechatronics;motion control;neurocontrollers;robots;sensor fusion;tactile sensors;tracking","Internet robots;MEMS sensors;actuator systems;aerial vehicles;biped walking;collision avoidance;compliant motion control;computational intelligence;cooperative sensing;force-responsive mechatronics;fuzzy control;human-robot interaction;humanoid robots;intelligent robots;learning control;learning systems;legged locomotion;legged robots;manufacturing systems;medical robots;microrobotic systems;mobile robot localization;motion planning;multirobot control;nanorobotic systems;neural control;outdoor vehicles;path planning;reinforcement learning;robot assisted surgery;robot vision;sensor fusion;sensor-based planning;simultaneous localization;simultaneous mapping;spaced robots;tactile sensing;task allocation;telerobotics;trajectory planning;underwater robots;vision-based monitoring;visual servoing;visual tracking","","0","","","","","","27-31 Oct. 2003","","IEEE","IEEE Conferences"
"Recipe tuning by reinforcement learning in the SandS ecosystem","B. Fernandez-Gauna; M. Graña","Computational Intelligence Group, University of the Basque Country, UPV/EHU, San Sebastian, Spain","2014 6th International Conference on Computational Aspects of Social Networks","20141013","2014","","","55","60","The Social and Smart (SandS) project ecosystem is compounded of household appliance users sharing recipes for the used of appliances, an intermediate control layer, and an intelligent social layer which aims to optimize the appliance recipes maximizing user satisfaction. We consider two aspects of the social intelligence, the innovation producing new recipes for unkown user tasks, and the adaptation to personalize the recipe to an individual user on the basis of his/her specific feedback. The second aspect is proposed to be dealt with by Reinforcement Learning approach, thus user feedback becomes the system reward. In this paper we discuss such an architecture based on the actor-critic approach, providing some experimental results on synthetic datasets that demonstrate the feasibility of the approach, previous to real life implementations.","","Electronic:978-1-4799-5940-2; POD:978-1-4799-5941-9","10.1109/CASoN.2014.6920422","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6920422","Reinforcement Learning;Social computing;Social networks;subconscious social intelligence","Biological system modeling;Computational modeling;Computer architecture;Robots;Service-oriented architecture","domestic appliances;learning (artificial intelligence);social sciences computing;user interfaces","SandS ecosystem;actor-critic approach;household appliance;intelligent social layer;recipe tuning;reinforcement learning;social and smart project ecosystem;social intelligence;user satisfaction","","0","","15","","","","July 30 2014-Aug. 1 2014","","IEEE","IEEE Conferences"
"Distributed energy cooperation for energy harvesting nodes using reinforcement learning","W. T. Lin; I. W. Lai; C. H. Lee","Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan","2015 IEEE 26th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)","20151203","2015","","","1584","1588","Wireless communication with nodes capable of harvesting energy emerges as a new technology challenge. In this paper, we investigate the problem of utilizing energy cooperation among energy-harvesting transmitters to maximize the data rate performance. We consider a general framework which can be applied to either cellular networks with base station energy cooperation through wired power grid or sensor networks with transmitting node energy cooperation through wireless power transfer. We model this energy cooperation problem as an infinite horizon Markov decision process (MDP), which can be optimally solved by the value iteration algorithm. Since the optimal value iteration algorithm has high complexity and requires non-causal information, we propose a distributed algorithm by using reinforcement learning and splitting the MDP into several small MDPs, each associated with a transmitter. Simulation results demonstrate the effectiveness of the proposed distributed energy cooperation algorithm.","","Electronic:978-1-4673-6782-0; POD:978-1-4673-6783-7","10.1109/PIMRC.2015.7343551","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7343551","","Batteries;Energy exchange;Power grids;Radio transmitters;Receivers;Wireless communication","Markov processes;cellular radio;computational complexity;energy harvesting;iterative methods;learning (artificial intelligence);power grids;radio transmitters;telecommunication computing;telecommunication power management","MDP;base station energy cooperation;cellular network;distributed energy cooperation;energy harvesting transmitter;infinite horizon Markov decision process;iteration algorithm;reinforcement learning;sensor network;wired power grid;wireless communication;wireless power transfer","","","","16","","","","Aug. 30 2015-Sept. 2 2015","","IEEE","IEEE Conferences"
"Improving management of Anemia in End Stage Renal Disease using Reinforcement Learning","A. E. Gaweda","Department of Medicine, Division of Nephrology, University of Louisville, USA","2009 International Joint Conference on Neural Networks","20090731","2009","","","953","958","We present a reinforcement learning approach to elicit individualized dose adjustment policies for patients suffering anemia due to end stage renal disease. Our goal is to achieve stable steady-state anemia management in patients with exhibiting different levels of treatment response. The approach uses Q-learning with parsimonious parametric representation of the state-action value function. We show that this approach achieves stability even in highly responsive patients.","2161-4393;21614393","POD:978-1-4244-3548-7","10.1109/IJCNN.2009.5179004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5179004","","Automatic control;Cardiac disease;Conference management;Function approximation;Humans;Learning;Medical treatment;Neural networks;Protocols;Steady-state","diseases;kidney;learning (artificial intelligence);medical computing;patient treatment","Q-learning;end stage renal disease;individualized dose adjustment policy;parsimonious parametric representation;patient treatment;reinforcement learning;stable steady-state anemia management;state-action value function","","1","","14","","","","14-19 June 2009","","IEEE","IEEE Conferences"
"Model Learning and Knowledge Sharing for a Multiagent System With Dyna-Q Learning","K. S. Hwang; W. C. Jiang; Y. J. Chen","Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan","IEEE Transactions on Cybernetics","20150413","2015","45","5","978","990","In a multiagent system, if agents' experiences could be accessible and assessed between peers for environmental modeling, they can alleviate the burden of exploration for unvisited states or unseen situations so as to accelerate the learning process. Since how to build up an effective and accurate model within a limited time is an important issue, especially for complex environments, this paper introduces a model-based reinforcement learning method based on a tree structure to achieve efficient modeling and less memory consumption. The proposed algorithm tailored a Dyna-Q architecture to multiagent systems by means of a tree structure for modeling. The tree-model built from real experiences is used to generate virtual experiences such that the elapsed time in learning could be reduced. As well, this model is suitable for knowledge sharing. This paper is inspired by the concept of knowledge sharing methods in multiagent systems where an agent could construct a global model from scattered local models held by individual agents. Consequently, it can increase modeling accuracy so as to provide valid simulated experiences for indirect learning at the early stage of learning. To simplify the sharing process, the proposed method applies resampling techniques to grafting partial branches of trees containing required and useful experiences disseminated from experienced peers, instead of merging the whole trees. The simulation results demonstrate that the proposed sharing method can achieve the objectives of sample efficiency and learning acceleration in multiagent cooperation applications.","2168-2267;21682267","","10.1109/TCYB.2014.2341582","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6871355","Decision tree;Dyna-Q;model sharing;multiagent system","Decision trees;Mathematical model;Multi-agent systems;Planning;Stochastic processes;Support vector machine classification;Vectors","","","","3","","20","","","20140805","May 2015","","IEEE","IEEE Journals & Magazines"
"Neural networks for incremental dimensionality reduced reinforcement learning","W. Curran; R. Pocius; W. D. Smart","Oregon State University, Corvallis, United States","2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","20171214","2017","","","1559","1565","State-of-the-art personal robots must perform complex manipulation tasks to be viable in assistive scenarios. However, many of these robots, like the PR2, use manipulators with high degrees-of-freedom. The complexity of these robots lead to large dimensional state spaces, which are difficult to fully explore. Our previous work introduced the IDRRL algorithm, which compresses the learning space by transforming a high-dimensional learning space onto a lower-dimensional manifold while preserving expressivity. In this work we formally prove that IDRRL maintains PAC-MDP guarantees. We then improve upon our previous formulation of IDRRL by introducing cascading autoencoders (CAE) for dimensionality reduction, producing the new algorithm IDRRL-CAE. We demonstrate the improvement of this extension over our previous formulation, IDRRL-PCA, in the Mountain Car and Swimmers domains.","","Electronic:978-1-5386-2682-5; POD:978-1-5386-2683-2; USB:978-1-5386-2681-8","10.1109/IROS.2017.8205962","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8205962","","Algorithm design and analysis;Computational complexity;Learning (artificial intelligence);Principal component analysis;Robots","learning (artificial intelligence);manipulators;neurocontrollers;principal component analysis","IDRRL algorithm;IDRRL-CAE;IDRRL-PCA;PAC-MDP;PR2;cascading autoencoders;dimensionality reduction;high-dimensional learning space;incremental dimensionality;lower-dimensional manifold;manipulation tasks;manipulators;neural networks;personal robots;reinforcement learning","","","","","","","","24-28 Sept. 2017","","IEEE","IEEE Conferences"
"<formula><tex>$mathsf{Hap-SliceR}$</tex></formula>: A Radio Resource Slicing Framework for 5G Networks With Haptic Communications","A. Aijaz","Telecommunications Research Laboratory, Toshiba Research Europe, Ltd., Bristol, BS1 4ND, U.K.&#x00A0;(e-mail: adnan.aijaz@toshiba-trel.com).","IEEE Systems Journal","","2017","Early Access","Early Access","1","12","It is expected that the emerging 5G networks will not only support diverse use cases, but also enable unprecedented applications such as haptic communications. Therefore, network slicing will provide the required design flexibility. Radio resource slicing would be an indispensable component of any network slicing solution. This paper proposes <formula><tex>$mathsf{Hap-SliceR}$</tex></formula>, which is a novel radio resource slicing framework for 5G networks with haptic communications. First, <formula><tex>$mathsf{Hap-SliceR}$</tex></formula> derives a network-wide radio resource slicing strategy for 5G networks. The optimal slicing strategy, which is based on a reinforcement learning approach, allocates radio resources to different slices while accounting for the dynamics and utility requirements of different slices. Second, <formula><tex>$mathsf{Hap-SliceR}$</tex></formula> provides customization of radio resources for haptic communications over 5G networks. The radio resource allocation requirements of haptic communications have been translated into a unique radio resource allocation problem. A low-complexity heuristic algorithm has been developed for resource allocation. Finally, a comprehensive performance evaluation of <formula><tex>$mathsf{Hap-SliceR}$</tex></formula> has been conducted based on a recently proposed 5G air-interface design.","1932-8184;19328184","","10.1109/JSYST.2017.2647970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7831356","5G;LTE-A;haptic communications;radio resource allocation;radio resource slicing;virtualization","5G mobile communication;Base stations;Dynamic scheduling;Haptic interfaces;Resource virtualization;Wireless communication","","","","","","","","","20170124","","","IEEE","IEEE Early Access Articles"
"Contextual multi-armed bandit algorithms for personalized learning action selection","I. Manickam; A. S. Lan; R. G. Baraniuk","Rice University, United States of America","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20170619","2017","","","6344","6348","Optimizing the selection of learning resources and practice questions to address each individual student's needs has the potential to improve students' learning efficiency. In this paper, we study the problem of selecting a personalized learning action for each student (e.g. watching a lecture video, working on a practice question, etc.), based on their prior performance, in order to maximize their learning outcome. We formulate this problem using the contextual multi-armed bandits framework, where students' prior concept knowledge states (estimated from their responses to questions in previous assessments) correspond to contexts, the personalized learning actions correspond to arms, and their performance on future assessments correspond to rewards. We propose three new Bayesian policies to select personalized learning actions for students that each exhibits advantages over prior work, and experimentally validate them using real-world datasets.","","Electronic:978-1-5090-4117-6; POD:978-1-5090-4118-3; USB:978-1-5090-4116-9","10.1109/ICASSP.2017.7953377","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7953377","contextual bandits;personalized learning","Approximation algorithms;Bayes methods;Context;Programmable logic arrays;Random variables;Schedules;Uncertainty","educational institutions","Bayesian policies;contextual multi-armed bandit algorithms;learning resources;personalized learning action selection;prior concept knowledge states;student learning efficiency","","","","","","","","5-9 March 2017","","IEEE","IEEE Conferences"
"User modeling with limited data: Application to stakeholder-driven watershed design","S. Mukhopadhyay; V. B. Singh; M. Babbar-Sebens","Computer & Information Science, Indiana University Purdue University Indianapolis, USA","2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20141204","2014","","","3855","3860","We have developed a web-based, interactive, watershed planning system called WRESTORE (Watershed Restoration Using Spatio-Temporal Optimization of Resources) (http://wrestore.iupui.edu) that allows stake-holder communities to participate in a democratic, collaborative form of optimization process for designing best management practices (BMPs) on their landscape, while also optimizing based on subjective, qualitative landowners' criteria beyond the usual socio-economic, physical, and ecological criteria. This system utilizes multiple advanced computational approaches including the SWAT (Soil and Water Assessment Tool) hydrologic model for watershed simulations, interactive genetic algorithms and reinforcement-based machine learning algorithms for search and optimization, and deep learning artificial neural networks for user modeling, within an encompassing human-computer interaction framework. A substantial user study of the WRESTORE system was conducted recently involving multiple real stakeholders varying from consultants, government officials, watershed alliance members, etc., with the objective of gaining insight about WRESTORE'S usability and utility. In particular focus was the user modeling component that develops a computational model of a user's preferences and criteria, based on real-time user-provided ratings for a subset of possible designs (similar to the idea of user profiling commonly done for human-computer interaction systems). The user model constructed based on the real user's personalized feedbacks can then be used to influence the automated search and optimization for BMP alternatives in WRESTORE. In this paper, we describe the methods developed for user modeling for interactive optimization, and the experimental set-up as well as results with real user studies. These results clearly demonstrate that development of user models for such personalized, interactive optimization is both feasible and valuable for developing community-based computa- ional water sustainability solutions.","1062-922X;1062922X","Electronic:978-1-4799-3840-7; POD:978-1-4799-3841-4; USB:978-1-4799-3839-1","10.1109/SMC.2014.6974532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6974532","decision support system;interactive optimization;machine learning;sustainability design;user modeling","Adaptation models;Artificial neural networks;Computational modeling;Data models;Mathematical model;Optimization","Internet;environmental science computing;human computer interaction;hydrology;interactive systems;learning (artificial intelligence);neural nets;socio-economic effects;water resources","BMP;SWAT hydrologic model;WRESTORE;Web-based interactive watershed planning system;best management practices;community-based computational water sustainability solutions;deep learning artificial neural networks;ecological criteria;human-computer interaction framework;interactive genetic algorithms;limited data;physical criteria;reinforcement-based machine learning algorithms;socio-economic criteria;soil and water assessment tool;stakeholder-driven watershed design;user modeling;watershed restoration using spatio-temporal optimization of resources","","0","","21","","","","5-8 Oct. 2014","","IEEE","IEEE Conferences"
"Learning motor skills with non-rigid materials by reinforcement learning","D. Shinohara; T. Matsubara; M. Kidode","Graduate School of Information Science, Nara Institute of Science and Technology, Japan","2011 IEEE International Conference on Robotics and Biomimetics","20120412","2011","","","2676","2681","This paper focuses on learning motor skills for anthropomorphic robots which must interact with non-rigid materials to perform tasks, such as wearing clothes, turning socks inside out, and bandaging. To learn such a motor skill, the task to be performed needs to be quantitatively defined using not only the state of the robot, but also the state of the non-rigid material. However, the non-rigid material is generally represented in a high dimensional configuration space (e.g., [1]) and obtaining such information in a real environment is difficult. In this paper we propose a novel learning framework for learning motor skills interacting with non-rigid materials by reinforcement learning that avoids these difficulties. Our learning framework focuses on the topological relationship between the configuration of the robot and the non-rigid material based on the consideration that most details of the material (e.g., wrinkles) are not important for performing the motor tasks. This focus allows us to define the task performance and provide reward signals based on a low-dimensional variable and to measure task performance in a real environment using reliable sensors. We constructed an experimental setting with an anthropomorphic dual-arm robot and a tailor-made T-shirt for the robot. To demonstrate the feasibility of the proposed method, we applied the method to have the robot perform the motor task of putting on the T-shirt. As a result of our learning framework, through trial and error the robot was able to acquire sequential movements that performed the goal of putting both arms into the corresponding sleeves of the T-shirt.","","DVD:978-1-4577-2137-3; Electronic:978-1-4577-2138-0; POD:978-1-4577-2136-6","10.1109/ROBIO.2011.6181709","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6181709","","Joints;Manipulators;Materials;Robot kinematics;Topology;Trajectory","humanoid robots;learning (artificial intelligence);motion control","anthropomorphic robot;dual-arm robot;high dimensional configuration space;low-dimensional variable;motor skill;nonrigid material;reinforcement learning;sequential movement;tailor-made T-shirt;topological relationship","","2","","18","","","","7-11 Dec. 2011","","IEEE","IEEE Conferences"
"Genetic reinforcement learning for scheduling heterogeneous machines","G. H. Kim; C. S. G. Lee","Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA","Proceedings of IEEE International Conference on Robotics and Automation","20020806","1996","3","","2798","2803 vol.3","Concerns the development of a learning-based heuristic for scheduling heterogeneous machines. List scheduling methods are flexible enough to be used for a large class of problems, including the heterogeneous machine problem. However, designing a priority rule requires insight into the characteristics of the problem. We propose the iterative list scheduling, which refines priority rules while generating a number of schedules. We also show that the iterative list scheduling can be formulated as a reinforcement learning problem, defining states and actions. Due to the large number of possible states, reinforcement learning algorithms which use value functions in constructing an optimal policy may not be suitable for scheduling problems. Encoding the policies of reinforcement learning into genetic algorithms leads to the genetic reinforcement learning (GRL), which directly works with the policies rather than the values of states. A GRL-based scheduler, EVIS (Evolutionary Intracell Scheduler), has been applied to problems such as the heterogeneous machine scheduling, the job-shop scheduling, the flow-shop scheduling, and the open-shop scheduling problems. The proposed model of EVIS, which has the linear order of population-fitness convergence, was verified with computer experiments. Even without fine tuning of EVIS, the quality of solutions found by EVIS was comparable to that of problem-tailored heuristics for most of the problem instances","1050-4729;10504729","POD:0-7803-2988-0","10.1109/ROBOT.1996.506586","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=506586","","Biological cells;Encoding;Genetic algorithms;Genetic engineering;Iterative algorithms;Job shop scheduling;Learning;Processor scheduling;Scheduling algorithm;Space exploration","genetic algorithms;heuristic programming;iterative methods;learning (artificial intelligence);production control","EVIS;Evolutionary Intracell Scheduler;genetic reinforcement learning;heterogeneous machine scheduling;iterative list scheduling;learning-based heuristic;linear-order population-fitness convergence;optimal policy;reinforcement learning problem","","3","","16","","","","22-28 Apr 1996","22 Apr 1996-28 Apr 1996","IEEE","IEEE Conferences"
"Learning capabilities for improving automatic transmission control","L. Fournier","Dept. of Comput. Sci., Stanford Univ., CA, USA","Intelligent Vehicles '94 Symposium, Proceedings of the","20020806","1994","","","455","460","We analyzed the gear-box position selection (GPS) problem on automatic transmission (AT) and proposed an algorithm, based on learning control, to improve vehicle behavior and driver satisfaction. Our approach guarantees optimization of vehicle performance and adaptation to the driver's style with road condition sensitivity. This improvement has been achieved by combining three knowledge acquisition sources: embedded dynamic models of powertrain, inductive inspection of driver actions and AT designer expertise; and by adding learning capabilities in order to significantly increase the system autonomy. Technically, GPS raises the following four problems which this paper addresses: (1) To achieve vehicle performance optimization of multiple antagonistic criteria, locally and globally over time, we considered a parametric disciminant function depending on an evaluation of the driver satisfaction and so called driver-style-state functions, as a reward for the system, and applied a reinforcement learning algorithm, derived from Q-learning method and combined with a mechanism to escape local optima. (2) Learning directly from the driver is performed when he selects AT ratio in manual mode. (3) Each driver's personal style is represented by a Glass creation/selection mechanism. (4) GPS raises a few singularities which are addressed by a set of restriction rules derived from AT control expertise.","","POD:0-7803-2135-9","10.1109/IVS.1994.639561","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=639561","","Algorithm design and analysis;Automatic control;Global Positioning System;Inspection;Knowledge acquisition;Mechanical power transmission;Power system modeling;Road vehicles;Vehicle driving;Vehicle dynamics","intelligent control;learning (artificial intelligence);learning systems;road vehicles","Glass creation/selection mechanism;Q-learning method;automatic transmission control;driver satisfaction;driver-style-state functions;embedded dynamic models;gear-box position selection;inductive inspection;knowledge acquisition;learning control;multiple antagonistic criteria;parametric disciminant function;performance optimization;powertrain;reinforcement learning algorithm;road condition sensitivity;singularities;vehicle behavior","","1","1","5","","","","24-26 Oct. 1994","","IEEE","IEEE Conferences"
"Computer-aided process control laboratory systems","K. M. Yusof; T. K. Liong; A. K. Baderon","Dept. of Chem. Eng., Univ. Teknologi Malaysia, Malaysia","Proceedings IEEE 1st International Conference on Multi Media Engineering Education","20020806","1994","","","276","280","This paper presents the use of personal computers in the process control laboratory at the Department of Chemical Engineering, Universiti Teknologi Malaysia. A description of four control systems interfaced to computers-a flow control system, a level control system, a heated tank control system and a fermenter/batch reactor control system are given. Being versatile controllers and data loggers, the computers also provide a friendly and appealing environment for students to perform experiments on the systems. The different characteristic of each system provides a variety of hands-on experience which incorporates the application of process control and reinforcement of the theories learnt in class. Finally, a sample experiment is included to illustrate the application and experience gained by students","","POD:0-7803-1963-X","10.1109/MMEE.1994.383204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=383204","","Application software;Chemical engineering;Computer interfaces;Control systems;Inductors;Laboratories;Level control;Microcomputers;Process control;Temperature control","computer aided instruction;control engineering computing;control engineering education;flow control;level control;process control","computer-aided process control laboratory systems;data loggers;fermenter/batch reactor control system;flow control system;heated tank control system;level control system;versatile controllers","","0","","","","","","6-8 Jul 1994","06 Jul 1994-08 Jul 1994","IEEE","IEEE Conferences"
"Multi-objective reinforcement learning algorithm and its application in drive system","Zhang Huajun; Zhao Jin; Wang Rui; Ma Tan","Department of Control Science and Engineering, Huazhong University of Science and Technology, China","2008 34th Annual Conference of IEEE Industrial Electronics","20090123","2008","","","274","279","Generally, reinforcement learning (RL) is used to design neurocontroller for control system with single objective. When facing multi-objective system, it is necessary to design the neurocontroller according to the personal preference. This paper proposed a multi-objective reinforcement learning algorithm (MORLA) to design neurocontroller with the personal preference. It transformed the multi-objective into synthetical objective and applied parallel genetic algorithm (PGA) to evolve the neurocontroller according to the synthetical objective. To establish the synthetical objective, the objective weight which represents the personal preference is calculated by solving the constrained optimization problem (COP) at the end of each generation. The COP requires not only the biggest variance of the synthetical objective in the population, but also requires the weight to fit the designerpsilas preference. After acquiring the weights, the PGA can select the elitists from the population according to the designerpsilas preference and design a satisfying neurocontroller by evolutionary operations. At last, the MORLA is used to design neurocontroller for a speed-controlled induction motor drive with indirect vector control. This paper designed several neurocontrollers with different personal preferences for the drive system. The simulation results show the feasibility and validity of the MORLA.","1553-572X;1553572X","CD-ROM:978-1-4244-1766-7; POD:978-1-4244-1767-4","10.1109/IECON.2008.4757965","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4757965","","Algorithm design and analysis;Constraint optimization;Control systems;Convergence;Design engineering;Design optimization;Electronics packaging;Genetic algorithms;Learning;Neurocontrollers","control system synthesis;genetic algorithms;induction motor drives;learning (artificial intelligence);machine control;neurocontrollers;velocity control","MORLA;constrained optimization problem;control system;drive system;indirect vector control;multiobjective reinforcement learning algorithm;neurocontroller;parallel genetic algorithm;speed-controlled induction motor drive","","1","","32","","","","10-13 Nov. 2008","","IEEE","IEEE Conferences"
"Visual summary of egocentric photostreams by representative keyframes","M. Bolaños; R. Mestre; E. Talavera; X. Giró-i-Nieto; P. Radeva","Universitat de Barcelona, Catalonia/Spain","2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)","20150730","2015","","","1","6","Building a visual summary from an egocentric photostream captured by a lifelogging wearable camera is of high interest for different applications (e.g. memory reinforcement). In this paper, we propose a new summarization method based on keyframes selection that uses visual features extracted by means of a convolutional neural network. Our method applies an unsupervised clustering for dividing the photostreams into events, and finally extracts the most relevant keyframe for each event. We assess the results by applying a blind-taste test on a group of 20 people who assessed the quality of the summaries.","","Electronic:978-1-4799-7079-7; POD:978-1-4799-7080-3","10.1109/ICMEW.2015.7169863","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169863","egocentric;keyframes;lifelogging;summarization","Cameras;Feature extraction;Image segmentation;Indexes;Motion segmentation;Videos;Visualization","cameras;convolution;feature extraction;neural nets;pattern clustering;personal computing;unsupervised learning","blind-taste test;convolutional neural network;egocentric photostreams;keyframe selection;lifelogging wearable camera;memory reinforcement;representative keyframes;summarization method;unsupervised clustering;visual feature extraction;visual summary","","2","","17","","","","June 29 2015-July 3 2015","","IEEE","IEEE Conferences"
"Illustration about a Metacognition-based learning detection system conceived to improve web-based self-regulated learning","J. Liang","Institute of Educational Technology, Institute of Education, Tsinghua University, Beijing, China","2011 International Conference on E-Business and E-Government (ICEE)","20110616","2011","","","1","4","Many learners have found that it is difficult to complete their web-based learning plan. Once on the Internet, they can't help browsing more interesting Web pages instead of continuing to do their learning tasks. This situation we called Information Trek. To solve this problem, this study proposes an learning detection system which can discover whether the contents of a web page a student viewing is about learning or not. If a student is detected to be in the state of viewing the non-learning pages, then the alert reinforcement window will be shown. If the attentive time in learning has been reached, then encouraging reinforcement feedback is given. We must consider adequately about personalization given the different levels of Metacognition. In this system, preferring the method to let learner go back to the learning state themselves, we design some functions to guide learners regulate themselves based on the essential process of online self-regulated leaning integrating Metacognitive process.","","Electronic:978-1-4244-8694-6; POD:978-1-4244-8691-5","10.1109/ICEBEG.2011.5886854","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5886854","learning detection system;metacognitive strategies;metacogniton;web-based self-regulated learning","Artificial neural networks;Conferences;Educational technology;Filtering;Helium;Text categorization;Web pages","","","","0","","10","","","","6-8 May 2011","","IEEE","IEEE Conferences"
"Adaptive joint call admission control and access network selection for multimedia wireless systems","E. Alexandri; G. Martinez; D. Zeghlache","INT, Motorola Labs., Paris, France","The 5th International Symposium on Wireless Personal Multimedia Communications","20021216","2002","3","","1390","1394 vol.3","Third generation wireless networks and beyond will solicit the cooperation of heterogeneous access networks, so as to provide multimedia traffic to different classes of users, with varying quality requisites over regions and time zones. We address the problem of how to partition the traffic demand efficiently onto the underlying radio access networks. The design objective is a resource allocation strategy, which provides a maximal resource utilization across all access networks. At the same time, the allocation should respect quality levels related to handover dropping performance; these levels can be predefined per service and per region. We propose a solution based on reinforcement learning, which runs independently at each of the cells of every access system, and report results. In the case where network revenue does not depend solely on resource utilization, but on parameters such as the type of service and/or the service duration, the method is readily extensible to include these factors.","1347-6890;13476890","POD:0-7803-7442-8","10.1109/WPMC.2002.1088408","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1088408","","Adaptive control;Call admission control;Communication system traffic control;Learning;Multimedia systems;Programmable control;Radio access networks;Resource management;Telecommunication traffic;Wireless networks","3G mobile communication;cellular radio;learning (artificial intelligence);multimedia communication;optimisation;quality of service;radio access networks;resource allocation;telecommunication congestion control;telecommunication traffic","adaptive access network selection;adaptive call admission control;handover dropping;multimedia traffic;network revenue;quality levels;radio access networks;reinforcement learning;resource allocation strategy;resource utilization optimization;third generation wireless networks","","7","","5","","","","27-30 Oct. 2002","","IEEE","IEEE Conferences"
"Framework for control and deep reinforcement learning in traffic","C. Wu; K. Parvate; N. Kheterpal; L. Dickstein; A. Mehta; E. Vinitsky; A. M. Bayen","UC Berkeley, Electrical Engineering and Computer Science","2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)","20180315","2017","","","1","8","Recent advances in deep reinforcement learning (RL) offer an opportunity to revisit complex traffic control problems at the level of vehicle dynamics, with the aim of learning locally optimal policies (with respect to the policy parameterization) for a variety of objectives such as matching a target velocity or minimizing fuel consumption. In this article, we present a framework called CISTAR (Customized Interface for SUMO, TraCI, and RLLab) that integrates the widely used traffic simulator SUMO with a standard deep reinforcement learning library RLLab. We create an interface allowing for easy customization of SUMO, allowing users to easily implement new controllers, heterogeneous experiments, and user-defined cost functions that depend on arbitrary state variables. We demonstrate the usage of CISTAR with several benchmark control and RL examples.","","Electronic:978-1-5386-1526-3; POD:978-1-5386-1527-0; USB:978-1-5386-1525-6","10.1109/ITSC.2017.8317694","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8317694","Simulation;control;deep reinforcement learning;vehicle dynamics","Acceleration;Automobiles;Learning (artificial intelligence);Libraries;Machine learning;Roads;Vehicle dynamics","control engineering computing;learning (artificial intelligence);optimisation;road traffic control;road vehicles;traffic engineering computing;vehicle dynamics","Customized Interface;SUMO;benchmark control;fuel consumption minimization;locally optimal policies;policy parameterization;standard deep reinforcement learning library RLLab;traffic control;vehicle dynamics","","","","","","","","16-19 Oct. 2017","","IEEE","IEEE Conferences"
"Towards a learning framework for dancing robots","I. S. Tholley; Q. Meng; P. W. H. Chung","Computer Science Department and Research School of Informatics, Loughborough University, UK","2009 IEEE International Conference on Control and Automation","20100208","2009","","","1581","1586","How can we make robots learn how to dance? How do humans learn to dance? An emerging culture of dancing robots is becoming more prominent in the research community with more emphasis on how we can show of our own creativity rather than allowing the robots to develop their own cognitive and psychological behaviours to the music being played. There are many different types of music and indeed, many different robots and many ways, in which they can dance to music however, much of the work carried out in this field concern limiting robots to dance in particular ways to a specific music and no adaptive behaviour implemented in them to be able to respond intuitively to music in general. We propose in this paper, a way in which such a problem can begin to be looked into, by introducing fundamental things that should be learnt that are necessary for dancing. We programmed a virtual robot to learn to dance to the beat as well as recognise the downbeat of any time-signature and tailor its movements to the loudness of music, using the Sarsa and the Sarsa(Â¿) algorithms from reinforcement learning as the learning framework. Experimental results show that it is possible to make robots learn to dance to these fundamental rhythmic features of music.","1948-3449;19483449","POD:978-1-4244-4706-0","10.1109/ICCA.2009.5410324","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5410324","","Automatic control;Cognitive robotics;Computer science;Humans;Learning;Psychology;Rhythm;Robotics and automation;Robots;Timing","learning systems;robots","Sarsa algorithms;dancing robots;reinforcement learning;time-signature;virtual robot","","2","","20","","","","9-11 Dec. 2009","","IEEE","IEEE Conferences"
"Automated Ingestion Detection for a Health Monitoring System","W. P. Walker; D. K. Bhatia","Embedded and Adaptive Computing Group, the University of Texas at Dallas, Richardson, TX, USA","IEEE Journal of Biomedical and Health Informatics","20140303","2014","18","2","682","692","Obesity is a global epidemic that imposes a financial burden and increased risk for a myriad of chronic diseases. Presented here is an overview of a prototype automated ingestion detection (AID) process implemented in a health monitoring system (HMS). The automated detection of ingestion supports personal record keeping which is essential during obesity management. Personal record keeping allows the care provider to monitor the therapeutic progress of a patient. The AID-HMS determines the levels of ingestion activity from sounds captured by an external throat microphone. Features are extracted from the sound recording and presented to machine learning classifiers, where a simple voting procedure is employed to determine instances of ingestion. Using a dataset acquired from seven individuals consisting of consumption of liquid and solid, speech, and miscellaneous sounds, > 94% of ingestion sounds are correctly identified with false positive rates around 9% based on 10-fold cross validation. The detected levels of ingestion activity are transmitted and stored on a remote web server, where information is displayed through a web application operating in a web browser. This information allows remote users (health provider) determine meal lengths and levels of ingestion activity during the meal. The AID-HMS also provides a basis for automated reinforcement for the patient.","2168-2194;21682194","","10.1109/JBHI.2013.2279193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6594833","Health monitoring;obesity management;patient empowerment;patient monitoring","Detectors;Feature extraction;Liquids;Microphones;Monitoring;Solids","Internet;data acquisition;diseases;epidemics;feature extraction;learning (artificial intelligence);medical signal processing;microphones;online front-ends;patient care;patient monitoring;pattern classification;signal classification;speech processing","AID-HMS;Web application;Web browser;automated ingestion detection;chronic diseases;dataset acquisition;external throat microphone;false positive rates;feature extraction;health monitoring system;ingestion activity;machine learning classifiers;miscellaneous sounds;myriad;obesity management;patient care;patient monitoring;personal record;prototype automated ingestion detection;remote Web server;sound recording;speech;therapeutic progress","0","4","","65","","","20130909","March 2014","","IEEE","IEEE Journals & Magazines"
"Imitation Learning for Dynamic VFI Control in Large-Scale Manycore Systems","R. G. Kim; W. Choi; Z. Chen; J. R. Doppa; P. P. Pande; D. Marculescu; R. Marculescu","Carnegie Mellon University, Pittsburgh, PA, USA","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","20170823","2017","25","9","2458","2471","Manycore chips are widely employed in high-performance computing and large-scale data analysis. However, the design of high-performance manycore chips is dominated by power and thermal constraints. In this respect, voltage-frequency island (VFI) is a promising design paradigm to create scalable energy-efficient platforms. By dynamically tailoring the voltage and frequency of each island, we can further improve the energy savings within given performance constraints. Inspired by the recent success of imitation learning (IL) in many application domains and its significant advantages over reinforcement learning (RL), we propose the first architecture-independent IL-based methodology for dynamic VFI (DVFI) control in manycore systems. Due to its popularity in the EDA community, we consider an RL-based DVFI control methodology as a strong baseline. Our experimental results demonstrate that IL is able to obtain higher quality policies than RL (on average, 5% less energy with the same level of performance) with significantly less computation time and hardware area overheads (3.1X and 8.8X, respectively).","1063-8210;10638210","","10.1109/TVLSI.2017.2700726","10.13039/100000001 - U.S. National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7934357","Dynamic voltage and frequency scaling (DVFS);low power;machine learning (ML);manycore systems;voltage-frequency islands (VFIs)","Control systems;Energy consumption;Energy dissipation;Hardware;Resource management;Scalability;Very large scale integration","energy conservation;learning (artificial intelligence);microprocessor chips;multiprocessing systems;parallel processing;performance evaluation;power aware computing","DVFI control;EDA community;RL-based DVFI control;architecture-independent IL-based methodology;dynamic VFI control;dynamic voltage and frequency scaling;energy savings;high-performance computing;high-performance manycore chips;imitation learning;large-scale data analysis;large-scale manycore systems;performance constraints;power constraints;reinforcement learning;scalable energy-efficient platforms;thermal constraints;voltage-frequency island","","1","","","","","20170526","Sept. 2017","","IEEE","IEEE Journals & Magazines"
"A game-theoretic framework with reinforcement learning for multinode cooperation in wireless networks","M. W. Baidas","Dept. of Electr. Eng., Kuwait Univ., Kuwait City, Kuwait","2013 IEEE 24th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)","20131125","2013","","","981","986","In this paper, a game-theoretic framework based on the iterated prisoner's dilemma (IPD) is proposed to model the repeated dynamic interactions of multiple source nodes when communicating with multiple destinations in ad-hoc wireless networks. In such networks where nodes are autonomous, selfish, and not familiar with other nodes' strategies, fully cooperative behaviors cannot be assumed. Thus, a Q-learning algorithm is proposed to allow network nodes to adapt to and play the IPD game against opponents with a variety of known and unknown strategies. Simulation results illustrate that the proposed Q-learning algorithm allows network nodes to play optimally and achieve their maximum expected return values.","2166-9570;21669570","Electronic:978-1-4673-6235-1; POD:978-1-4673-6233-7; USB:978-1-4673-6234-4","10.1109/PIMRC.2013.6666280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6666280","Amplify-and-forward (AF);Q-learning;cooperation;game-theory;prisoner's dilemma;reinforcement learning","Broadcasting;Convergence;Games;Learning (artificial intelligence);Silicon;Thin film transistors;Wireless networks","ad hoc networks;cooperative communication;game theory;learning (artificial intelligence);telecommunication computing","IPD;IPD game;Q-learning algorithm;ad-hoc wireless networks;fully cooperative behaviors;game-theoretic framework;iterated prisoner dilemma;multinode cooperation;multiple source nodes;network nodes;reinforcement learning","","2","","21","","","","8-11 Sept. 2013","","IEEE","IEEE Conferences"
"Application of reinforcement learning to admission control in CDMA network","B. Makarevitch","Commun. Lab., Helsinki Univ. of Technol., Espoo, Finland","11th IEEE International Symposium on Personal Indoor and Mobile Radio Communications. PIMRC 2000. Proceedings (Cat. No.00TH8525)","20020806","2000","2","","1353","1357 vol.2","The paper describes an admission control algorithm for the CDMA networks which is able to adapt to the operating environment. The algorithm is based on the principle of reinforcement learning and it achieves near-optimal performance for various radio propagation conditions and network operator's objectives. The performance evaluation results for different state space alternatives and algorithm parameters are presented and compared with the conventional admission control based on the power thresholds","","POD:0-7803-6463-5","10.1109/PIMRC.2000.881639","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=881639","","Admission control;Base stations;Communication system control;Degradation;Downlink;Intelligent networks;Interference;Learning;Multiaccess communication;Paper technology","Markov processes;cellular radio;code division multiple access;learning (artificial intelligence);multiuser channels;radio networks;telecommunication congestion control","CDMA network;Markov decision process;admission control algorithm;algorithm parameters;cellular radio;near-optimal performance;network operator objectives;performance evaluation results;power thresholds;radio propagation conditions;reinforcement learning;state space alternatives","","3","1","4","","","","2000","18 Sep 2000-21 Sep 2000","IEEE","IEEE Conferences"
"Reinforcement learning approach to dynamic activation of base station resources in wireless networks","P. Y. Kong; D. Panaitopol","Khalifa Univ. of Sci., Technol. & Res. (KUSTAR), Abu Dhabi, United Arab Emirates","2013 IEEE 24th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)","20131125","2013","","","3264","3268","Recently, the issue of energy efficiency in wireless networks has attracted much research attention due to the growing concern on global warming and operator's profitability. We focus on energy efficiency of base stations because they account for 80% of total energy consumed in a wireless network. In this paper, we intend to reduce energy consumption of a base station by dynamically activating and deactivating the modular resources at the base station depending on the instantaneous network traffic. We propose an online reinforcement learning algorithm that will continuously adapt to the changing network traffic in deciding which action to take to maximize energy saving. As an online algorithm, the proposed scheme does not require a separate training phase and can be deployed immediately. Simulation results have confirmed that the proposed algorithm can achieve more than 50% energy saving without compromising network service quality which is measured in terms of user blocking probability.","2166-9570;21669570","Electronic:978-1-4673-6235-1; POD:978-1-4673-6233-7; USB:978-1-4673-6234-4","10.1109/PIMRC.2013.6666710","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6666710","Green wireless networks;energy efficient base station;online Q-Learning;reinforcement learning","Base stations;Dynamic scheduling;Energy consumption;Heuristic algorithms;Learning (artificial intelligence);Q-factor;Wireless networks","learning (artificial intelligence);probability;radio networks;telecommunication computing;telecommunication traffic","base station resource dynamic activation;blocking probability;energy efficiency;energy saving;global warming;network traffic;operator profitability;reinforcement learning approach;wireless networks","","4","","10","","","","8-11 Sept. 2013","","IEEE","IEEE Conferences"
"Path planning with user route preference - A reward surface approximation approach using orthogonal Legendre polynomials","A. R. Srinivasan; S. Chakraborty","Department of Mechanical Aerospace and Biomedical Engineering, University of Tennessee, Knoxville, 37996 USA","2016 IEEE International Conference on Automation Science and Engineering (CASE)","20161117","2016","","","1100","1105","As self driving cars become more ubiquitous, users would look for natural ways of informing the car AI about their personal choice of routes. This choice is not always dictated by straightforward logic such as shortest distance or shortest time, and can be influenced by hidden factors, such as comfort and familiarity. This paper presents a path learning algorithm for such applications, where from limited positive demonstrations, an autonomous agent learns the user's path preference and honors that choice in its route planning, but has the capability to adopt alternate routes, if the original choice(s) become impractical. The learning problem is modeled as a Markov decision process. The states (way-points) and actions (to move from one way-point to another) are pre-defined according to the existing network of paths between the origin and destination and the user's demonstration is assumed to be a sample of the preferred path. The underlying reward function which captures the essence of the demonstration is computed using an inverse reinforcement learning algorithm and from that the entire path mirroring the expert's demonstration is extracted. To alleviate the problem of state space explosion when dealing with a large state space, the reward function is approximated using a set of orthogonal polynomial basis functions with a fixed number of coefficients regardless of the size of the state space. A six fold reduction in total learning time is achieved compared to using simple basis functions, that has dimensionality equal to the number of distinct states.","","Electronic:978-1-5090-2409-4; POD:978-1-5090-2410-0; USB:978-1-5090-2408-7","10.1109/COASE.2016.7743527","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7743527","","Automobiles;Databases;Learning (artificial intelligence);Markov processes;Path planning;Planning;Real-time systems","Markov processes;learning (artificial intelligence);mobile robots;path planning;state-space methods","Markov decision process;autonomous agent learns;dimensionality;inverse reinforcement learning algorithm;large state space;orthogonal Legendre polynomials;orthogonal polynomial basis functions;path planning;reward function;reward surface approximation;route planning;self driving cars;six fold reduction;state space explosion;user path preference;user route preference","","","","","","","","21-25 Aug. 2016","","IEEE","IEEE Conferences"
"An auction-based approach to spectrum allocation using multi-agent reinforcement learning","N. Abji; A. Leon-Garcia","Dept. of Electrical and Computer Engineering, University of Toronto, Italy","21st Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications","20101217","2010","","","2233","2238","We present an auction-based approach to spectrum management in a multi-operator context. Service providers compete for customers in real-time through live auctions. To automate the bidding process we implement a multi-agent reinforcement learning solution. We study the effect of real-time competition between service providers by considering the cases where there is a single provider and multiple providers. Furthermore, we demonstrate how users of varying types, based on application-type and willingness to pay, can be accommodated. We utilize a low-complexity bid-proportional allocation mechanism which ensures fairness. Our simulation results show that when there is a single provider, revenue can be maximized by artificially limiting supply and creating contention. However, when there are multiple providers from which the customers can dynamically choose, there is no longer an incentive to restrict supply due to the direct competition between service providers.","2166-9570;21669570","Electronic:978-1-4244-8016-6; POD:978-1-4244-8017-3","10.1109/PIMRC.2010.5671682","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5671682","","Land mobile radio","learning (artificial intelligence);multi-agent systems;radio spectrum management;telecommunication computing","auction-based approach;bidding process;low-complexity bid-proportional allocation mechanism;multiagent reinforcement learning;service providers;spectrum allocation;spectrum management","","4","","10","","","","26-30 Sept. 2010","","IEEE","IEEE Conferences"
"Risk-sensitivity through multi-objective reinforcement learning","K. Van Moffaert; T. Brys; A. Nowé","Department of Computer Science, Vrije Universiteit Brussel, Brussels, Belgium","2015 IEEE Congress on Evolutionary Computation (CEC)","20150914","2015","","","1746","1753","Usually in reinforcement learning, the goal of the agent is to maximize the expected return. However, in practical applications, algorithms that solely focus on maximizing the mean return could be inappropriate as they do not account for the variability of their solutions. Thereby, a variability measure could be included to accommodate for a risk-sensitive setting, i.e. where the system engineer can explicitly define the tolerated level of variance. Our approach is based on multi-objectivization where a standard single-objective environment is extended with one (or more) additional objectives. More precisely, we augment the standard feedback signal of an environment with an additional objective that defines the variance of the solution. We highlight that our algorithm, named risk-sensitive Pareto Q-learning, is (1) specifically tailored to learn a set of Pareto non-dominated policies that trade-off these two objectives. Additionally (2), the algorithm can also retrieve every policy that has been learned throughout the state-action space. This in contrast to standard risk-sensitive approaches where only a single trade-off between mean and variance is learned at a time.","1089-778X;1089778X","Electronic:978-1-4799-7492-4; POD:978-1-4799-7493-1","10.1109/CEC.2015.7257098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7257098","","","feedback;learning (artificial intelligence);risk analysis;sensitivity analysis","multiobjective reinforcement learning;nondominated policies;risk-sensitive Pareto Q-learning;standard feedback signal;standard single-objective environment;state-action space;variability measure","","1","","27","","","","25-28 May 2015","","IEEE","IEEE Conferences"
"An MARL-Based Distributed Learning Scheme for Capturing User Preferences in a Smart Environment","J. Lim; H. Son; D. Lee; D. Lee","Sch. of Comput., KAIST, Daejeon, South Korea","2017 IEEE International Conference on Services Computing (SCC)","20170914","2017","","","132","139","Providing a personalized service to a user in a smart environment has been one of the key goals in the area of pervasive computing. The proliferation of individually developed smart devices in the name of Internet of Things opens up a possibility of providing personalized services to a user in an autonomous and distributed manner. As a user's task often involves services supported by multiple devices, capturing a device-specific service preference is not enough to maximize a user's comfort. In this paper, we propose a distributed learning scheme for capturing multiple device service preferences in a smart environment. We exploit multi-agent reinforcement learning (MARL) method where each smart device acts as a reinforcement learning agent to incrementally and cooperatively capture a user specific preference of a task. Experiments confirm that smart devices with the proposed scheme are able to capture multiple device service preferences from a small number of interactions with a user and an environment. Also, the proposed transfer learning method improves learning performance for a new task.","","Electronic:978-1-5386-2005-2; POD:978-1-5386-2006-9; USB:978-1-5386-2004-5","10.1109/SCC.2017.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8034977","context aware;distributed learning;personalization;smart device;user preference","Brightness;Learning (artificial intelligence);Ontologies;Performance evaluation;Servers;Smart devices;TV","learning (artificial intelligence);multi-agent systems;ubiquitous computing","Internet of Things;MARL;autonomous distributed manner;capturing user preferences;device-specific service preference;distributed learning scheme;individually developed smart devices;multiagent reinforcement learning method;multiple device service preferences;personalized service;pervasive computing;reinforcement learning agent;smart environment;transfer learning method;user specific preference","","","","","","","","25-30 June 2017","","IEEE","IEEE Conferences"
"Dynamic Class of Service mapping for Quality of Experience control in future networks","F. D. Priscoli; L. Fogliati; A. Palo; A. Pietrabissa","","WTC 2014; World Telecommunications Congress 2014","20140624","2014","","","1","6","The present work introduces a novel approach to cope with some key limitations of the present communication networks. In particular, the need for effectively managing heterogeneous resources over heterogeneous networks while guaranteeing personalized Quality of Experience (QoE) requirements to the applications, claims for a full cognitive approach which is realized through the introduction of an appropriate Future Internet architecture. The paper, taking this architecture in mind, introduces the innovative concept of dynamic association between applications and Classes of Services, performed by means of proper Reinforcement Learning algorithms. The resulting procedure guarantees QoE personalization, requires low processing capabilities and entails limited signalling overhead.","","Paper:978-3-8007-3602-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6840010","","","","","","0","","","","","","1-3 June 2014","","VDE","VDE Conferences"
"A Fast Interactive Search System for Healthcare Services","M. Daltayanni; C. Wang; R. Akella","Technol. & Inf. Manage., Univ. of California Santa Cruz, Santa Cruz, CA, USA","2012 Annual SRII Global Conference","20120924","2012","","","525","534","In this paper we describe the design, development, and evaluation of a general human-machine interaction search system, and its potential and use in the context of a collaboration project with SAP and Saffron. The objective of a specialized version of the system is to provide medical and healthcare information services to users via interactive search for personalized patient needs. Patients usually have questions regarding healthcare, including those which concern illness symptoms, duration and types of treatment, possible drug effects, and more. Authorized personnel would often be ideal in responding to such needs; however they could potentially be very expensive, and not easy to support and maintain. If patients could have access to information at their home, by means of i-phone or online access, this could save time, doctor office visit expenses, as well as valuable and restricted medical time. What is more, information concerning other anonymized and similar patient cases provides knowledge and perspective on a wide range of patient issues. From the doctors' perspective, they typically need to spend time on differential analysis about new patient cases: study symptoms, research possible causes, rank results by emergency priority and treat them accordingly. A search system that would direct a doctor (or patient/user) to similar patient cases would save significant amount of manual search time. The powerful new feature of this system is the storage and mining of past patient cases knowledge, to create metadata to be used in the subsequent retrieval of relevant documents. Finally, the interactive search system would speed up identification of rare cases; for instance, symptoms that do not appear commonly in past cases may require special treatment or expert referral. We build a model which dynamically learns medical needs of interacting MDs and patients. The model works on free or unstructured text, allowing disambiguation of vague words and flexibility in descri- ing medical needs. In addition, both experts with an advanced knowledge of medical terminology, and beginning users using basic medical terms, can achieve high search relevance. Furthermore, our approach obviates the need for the assignment of tags or labels, such as treatment, symptoms, causes, to documents, to respond effectively to user queries. In particular, we build a temporal difference algorithm to predict user's information needs by incorporating both current and predicted knowledge into learning the user profile. Our source of information about the user consists of submitted queries and feedback on the returned results. We tested our system on publicly available medical data (OhsuMed TREC dataset 2002) and we achieved a significant improvement in retrieval accuracy, compared to the literature. We provide quantitative results as well as demonstration screenshots which illustrate a) the value of interaction (user time spent with system versus results accuracy), b) the value of using medical terminology understanding, when compared with simple general words, and c) the value of allowing the maximum number of feedback submissions to vary.","2166-0778;21660778","Electronic:978-1-5090-5643-9; POD:978-1-4673-2318-5; USB:978-0-7695-4770-1","10.1109/SRII.2012.65","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6311035","healthcare information services;interactive retrieval;medical data retrieval;reinforcement learning;temporal difference","Diseases;Information services;Medical diagnostic imaging;Senior citizens;Terminology;Unified modeling language","data mining;health care;human computer interaction;information needs;information retrieval;interactive systems;medical information systems;text analysis","SAP;Saffron;basic medical terms;collaboration project;data mining;data storage;differential analysis;document retrieval;fast interactive search system;free or unstructured text;healthcare information services;human-machine interaction search system;information needs;medical information services;medical terminology;metadata;patient cases knowledge;patient issues;personalized patient needs;publicly available medical data;rare cases;submitted queries;temporal difference algorithm;unstructured text","","0","","25","","","","24-27 July 2012","","IEEE","IEEE Conferences"
"Incorporating prior knowledge into Q-learning for drug delivery individualization","A. E. Gaweda; M. K. Muezzinoglu; G. R. Aronoff; A. A. Jacobs; J. M. Zurada; M. E. Brier","Louisville Univ., KY, USA","Fourth International Conference on Machine Learning and Applications (ICMLA'05)","20060320","2005","","","6 pp.","","Individualization of drug delivery in treatment of chronic ailments is a challenge to the physician. Variability of response across patient population requires tailoring the dosing strategies to individual's needs. We have previously demonstrated the potential of reinforcement learning methods to support the physician in the management of anemia. In this paper, we propose the incorporation of prior knowledge into the learning mechanism to further improve the outcomes of the treatment.","","POD:0-7695-2495-8","10.1109/ICMLA.2005.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607452","","Animals;Bones;Decision making;Drug delivery;Humans;Jacobian matrices;Learning systems;Production;Protocols;Red blood cells","diseases;drug delivery systems;drugs;learning (artificial intelligence);medical computing","Q-learning;anemia;chronic ailment treatment;drug delivery individualization;reinforcement learning","","2","","8","","","","15-17 Dec. 2005","","IEEE","IEEE Conferences"
"Robot self-preservation and adaptation to user preferences in game play, a preliminary study","Á. Castro-González; F. Amirabdollahian; D. Polani; M. Malfaz; M. A. Salichs","RoboticsLab at the Carlos III University of Madrid, 28911, Legan&#x00E9;s, Madrid, Spain","2011 IEEE International Conference on Robotics and Biomimetics","20120412","2011","","","2491","2498","It is expected that in a near future, personal robots will be endowed with enough autonomy to function and live in an individual's home. This is while commercial robots are designed with default configuration and factory settings which may often be different to an individual's operating preferences. This paper presents how reinforcement learning is applied and utilised towards personalisation of a robot's behaviour. Two-level reinforcement learning has been implemented: first level is in charge of energy autonomy, i.e. how to survive, and second level is involved in adapting robot's behaviour to user's preferences. In both levels Q-learning algorithm has been applied. First level actions have been learnt in a simulated environment and then the results have been transferred to the real robot. Second level has been fully implemented in the real robot and learnt by human-robot interaction. Finally, experiments showing the performance of the system are presented.","","DVD:978-1-4577-2137-3; Electronic:978-1-4577-2138-0; POD:978-1-4577-2136-6","10.1109/ROBIO.2011.6181679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6181679","","Batteries;Games;Humans;Learning;Machine learning;Machine learning algorithms;Robots","human-robot interaction;learning (artificial intelligence)","Q-learning algorithm;commercial robots;game play;human-robot interaction;personal robots;reinforcement learning;robot behaviour personalisation;robot self-preservation;user preferences adaptation","","2","","28","","","","7-11 Dec. 2011","","IEEE","IEEE Conferences"
"A 55nm time-domain mixed-signal neuromorphic accelerator with stochastic synapses and embedded reinforcement learning for autonomous micro-robots","A. Amravati; S. B. Nasir; S. Thangadurai; I. Yoon; A. Raychowdhury","Georgia Institute of Technology, Atlanta, GA","2018 IEEE International Solid - State Circuits Conference - (ISSCC)","20180312","2018","","","124","126","Even as rapid advances are being made in the areas of deep neural networks (DNNs) and convolutional neural networks (CNNs) with most hardware demonstrations geared towards inference in vision-based platforms [1-5], we recognize that true autonomy in intelligent agents will only emerge when such bio-mimetic systems can perform continuous learning through interactions with the environment. Reinforcement learning (RL) presents one such computational paradigm inspired by behaviorist psychology, where autonomous agents take actions in an environment to maximize a notion of cumulative reward. This concept is deeply rooted in the human brain where dopamine mediated neurotransmitters (in the cortex, striatum and thalamus of the brain) have been shown to encourage reward-motivated behavior in all our social interactions (Fig. 7.4.1). In this paper, we present a 690μW (V<sub>CC</sub>=1.2V) neuromorphic accelerator fabricated in 55nm CMOS, which: (1) inherits unique properties of stochastic neural networks, (2) leverages recent advances in Q-learning as an implementation of RL, and (3) demonstrates energy-efficient time-domain mixed-signal (TD-MS) circuit architectures, to provide autonomy to a mobile, self-driving micro-robot at the edge of the cloud, with possible applications in disaster relief, reconnaissance and personal robotics.","","Electronic:978-1-5090-4940-0; POD:978-1-5386-2227-8","10.1109/ISSCC.2018.8310215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8310215","","Biological neural networks;Neuromorphics;Neurons;Semiconductor device measurement;Sensors;Stochastic processes;Synapses","biomimetics;brain;feedforward neural nets;learning (artificial intelligence);microrobots;mobile robots;neurocontrollers;robot vision","55nm time-domain mixed-signal neuromorphic accelerator;CMOS;RL;autonomous agents;behaviorist psychology;bio-mimetic systems;computational paradigm;convolutional neural networks;cumulative reward;deep neural networks;dopamine mediated neurotransmitters;human brain;intelligent agents;mobile self-driving microrobot;personal robotics;reinforcement learning;reward-motivated behavior;social interactions;stochastic neural networks;stochastic synapses;striatum;thalamus","","","","","","","","11-15 Feb. 2018","","IEEE","IEEE Conferences"
"Learning from demonstration: Teaching a myoelectric prosthesis with an intact limb via reinforcement learning","G. Vasan; P. M. Pilarski","Department of Computing Science and the Department of Medicine, University of Alberta, Edmonton, AB T6G 2E1, Canada","2017 International Conference on Rehabilitation Robotics (ICORR)","20170814","2017","","","1457","1464","Prosthetic arms should restore and extend the capabilities of someone with an amputation. They should move naturally and be able to perform elegant, coordinated movements that approximate those of a biological arm. Despite these objectives, the control of modern-day prostheses is often nonintuitive and taxing. Existing devices and control approaches do not yet give users the ability to effect highly synergistic movements during their daily-life control of a prosthetic device. As a step towards improving the control of prosthetic arms and hands, we introduce an intuitive approach to training a prosthetic control system that helps a user achieve hard-to-engineer control behaviours. Specifically, we present an actor-critic reinforcement learning method that for the first time promises to allow someone with an amputation to use their non-amputated arm to teach their prosthetic arm how to move through a wide range of coordinated motions and grasp patterns. We evaluate our method during the myoelectric control of a multi-joint robot arm by non-amputee users, and demonstrate that by using our approach a user can train their arm to perform simultaneous gestures and movements in all three degrees of freedom in the robot's hand and wrist based only on information sampled from the robot and the user's above-elbow myoelectric signals. Our results indicate that this learning-from-demonstration paradigm may be well suited to use by both patients and clinicians with minimal technical knowledge, as it allows a user to personalize the control of his or her prosthesis without having to know the underlying mechanics of the prosthetic limb. These preliminary results also suggest that our approach may extend in a straightforward way to next-generation prostheses with precise finger and wrist control, such that these devices may someday allow users to perform fluid and intuitive movements like playing the piano, catching a ball, and comfortably shaking hands.","","Electronic:978-1-5386-2296-4; POD:978-1-5386-2297-1; USB:978-1-5386-2295-7","10.1109/ICORR.2017.8009453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8009453","","Elbow;Manipulators;Muscles;Prosthetics;Training;Wrist","electromyography;gait analysis;medical robotics;medical signal processing;prosthetics;signal sampling;student experiments","actor-critic reinforcement learning method;amputation;biological arm;control approaches;coordinated motions;daily-life control;effect highly synergistic movements;elegant coordinated movements;fluid movements;grasp patterns;hard-to-engineer control behaviours;information sampling;intact limb;intuitive movements;learning-from-demonstration paradigm;minimal technical knowledge;modern-day prostheses;multijoint robot arm;myoelectric prosthesis teaching;next-generation prostheses;nonamputated arm;prosthetic arms;prosthetic control system;prosthetic device;prosthetic limb;reinforcement learning;robot hand;three degrees-of-freedom;user above-elbow myoelectric signals;wrist","","","","","","","","17-20 July 2017","","IEEE","IEEE Conferences"
"Active learning for personalizing treatment","K. Deng; J. Pineau; S. Murphy","Department of Statistics, University of Michigan, USA","2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)","20110728","2011","","","32","39","The personalization of treatment via genetic biomarkers and other risk categories has drawn increasing interest among clinical researchers and scientists. A major challenge here is to construct individualized treatment rules (ITR), which recommend the best treatment for each of the different categories of individuals. In general, ITRs can be constructed using data from clinical trials, however these are generally very costly to run. In order to reduce the cost of learning an ITR, we explore active learning techniques designed to carefully decide whom to recruit, and which treatment to assign, throughout the online conduct of the clinical trial. As an initial investigation, we focus on simple ITRs that utilize a small number of subpopulation categories to personalize treatment. To minimize the maximal uncertainty regarding the treatment effects for each subpopulation, we propose the use of a minimax bandit model and provide an active learning policy for solving it. We evaluate our active learning policy using simulated data and data modeled after a clinical trial involving treatments for depressed individuals. We contrast this policy with other plausible active learning policies. The techniques presented in the paper may be generalized to tackle problems of efficient exploration in other domains.","2325-1824;23251824","Electronic:978-1-4244-9888-8; POD:978-1-4244-9887-1","10.1109/ADPRL.2011.5967348","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967348","","Clinical trials;Learning systems;Loss measurement;Machine learning;Recruitment;Resource management;Uncertainty","learning (artificial intelligence);medical computing;minimax techniques;patient treatment","active learning;clinical research;genetic biomarkers;individualized treatment rules;minimax bandit model;risk category;treatment personalization","","1","","32","","","","11-15 April 2011","","IEEE","IEEE Conferences"
"Coordinating SON instances: Reinforcement learning with distributed value function","O. Iacoboaiea; B. Sayrac; S. Ben Jemaa; P. Bianchi","Orange Labs, 38-40 rue du General Leclerc 92130, Issy les Moulineaux, France","2014 IEEE 25th Annual International Symposium on Personal, Indoor, and Mobile Radio Communication (PIMRC)","20150629","2014","","","1642","1646","With the emergence of Self-Organizing Network (SON) functions network operators are faced with a practical problem: coordination of SON instances. The SON functions are usually designed in a standalone manner, i.e. they do not take into account the possibility that other instances of the same or different SON functions may be running in the network. This creates the risk of conflicts and network instability. Therefore a SON COordinator (SONCO) is needed. In this paper we design an operator centric SONCO that sees the SON instances as black-boxes, i.e. it does not know the algorithm inside the SON functions. Our aim is to improve the network stability (i.e. number of parameter changes) for SON instances of the same SON function. We employ Reinforcement Learning (RL) in order to profit from the information on the past SONCO decisions. We simplify the expression of the action-value function and we use state aggregation to further reduce the required state space, making it scale linearly with the number of coordinated cells. We provide a study case with the Mobility Load Balancing (MLB) function independently instantiated on every cell. The results show that the proposed SONCO improves the network stability.","2166-9570;21669570","Electronic:978-1-4799-4912-0; POD:978-1-4799-4911-3","10.1109/PIMRC.2014.7136431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7136431","Coordination;LTE;MLB;SON;SON instances;reinforcement learning;state aggregation","Algorithm design and analysis;Conferences;Heuristic algorithms;Kernel;Learning (artificial intelligence);Markov processes;Optimization","Long Term Evolution;cellular radio;learning (artificial intelligence);resource allocation;telecommunication computing","LTE;MLB;RL;SON instance coordination;SONCO operator centric design;action-value function;black-boxes;mobility load balancing coordinated cells;network stability;reinforcement learning;self-organizing network functions;state aggregation;state space","","2","","16","","","","2-5 Sept. 2014","","IEEE","IEEE Conferences"
"Offline and online adaptation in prosocial games","K. C. Apostolakis; K. Stefanidis; A. Psaltis; K. Dimitropoulos; P. Daras","Information Technologies Institute - ITI, Centre for Research and Technology Hellas - CERTH, Thessaloniki, Greece","2017 9th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games)","20171005","2017","","","201","208","Personalization and maintenance of high levels of engagement still remain two of the main challenges in the design of serious games. Towards this end, in this paper we propose a novel adaptation approach for both online and offline adaptation in prosocial games. In this paper, we describe the implementation of an artificial intelligence driven adaptation manager, whose purpose is to direct players towards game content the players are most likely to enjoy (measured in their engagement responses). As a consequence, we demonstrate how the adaptation manager can be used to increase the chances of players attaining the game's specific prosocial learning objectives. Each mechanism (offline and online) processes different information about the player and concerns different types of factors affecting engagement and prosocial behavior. More specifically, the online mechanism maintains a player engagement profile for game elements related to the provision of Corrective Feedback and Positive Reinforcement, in order to adapt existing game content in real time. On the other hand, off-line adaptation matches players to game scenarios according to the players' prosocial ability and the game scenarios' ranking. The efficiency of the proposed adaptation manger as a tool for enhancing students' prosocial skills development is demonstrated through a small scale experiment, under real-conditions in a school environment, using the prosocial game of Path of Trust.","","Electronic:978-1-5090-5812-9; POD:978-1-5386-1203-3; USB:978-1-5090-5811-2","10.1109/VS-GAMES.2017.8056602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8056602","adaptation;engagement;prosociality;serious games","","artificial intelligence;computer aided instruction;serious games (computing)","Path of Trust;artificial intelligence driven adaptation manager;corrective feedback;direct players;engagement responses;existing game content;game elements;game scenarios;maintenance;off-line adaptation;offline adaptation;online adaptation;online mechanism;player engagement profile;positive reinforcement;prosocial behavior;prosocial games;prosocial skills development;school environment;serious games;students","","","","","","","","6-8 Sept. 2017","","IEEE","IEEE Conferences"
"Genetic reinforcement learning approach to the heterogeneous machine scheduling problem","Gyoung Hwan Kim; C. S. G. Lee","Dept. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA","IEEE Transactions on Robotics and Automation","20020806","1998","14","6","879","893","Focuses on the development of a learning-based heuristic for scheduling heterogeneous machines. Although list scheduling methods have been widely used for a large class of scheduling problems, including the heterogeneous machine scheduling problem, they involve designing priority rules, which usually require a fair amount of insights on the characteristics of the problem to be solved. Instead of elaborate design of priority rules in a single step, we propose an iterative list scheduling process, which refines priority rules while generating a number of schedules. The proposed iterative list scheduling is formulated as a reinforcement learning problem, with states and actions defined in list scheduling. Due to the large number of possible states, reinforcement learning algorithms which use value functions in constructing an optimal policy may not be suitable for scheduling problems. Thus, to directly work with policies rather than the values of states, we propose genetic reinforcement learning (GRL), in which the policies of reinforcement learning are encoded into the chromosomes of genetic algorithms and a near-optimal policy is searched for by genetic algorithms. A GRL-based scheduler, called evolutionary intracell scheduler (EVIS), has been developed and applied to various scheduling problems such as the heterogeneous machine scheduling, the processor scheduling, the job-shop scheduling, the flow-shop scheduling, and the open-shop scheduling problems. The proposed model of EVIS, which has a linear order of population-fitness convergence, is verified by computer experiments. Even without fine tuning EVIS, the quality of solutions achieved by EVIS is comparable to that of problem-tailored heuristics for most of the problem instances","1042-296X;1042296X","","10.1109/70.736772","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=736772","","Approximation algorithms;Biological cells;Cost function;Genetic algorithms;Helium;Iterative algorithms;Job shop scheduling;Learning;Processor scheduling;Scheduling algorithm","genetic algorithms;iterative methods;learning (artificial intelligence);minimisation;production control;scheduling","evolutionary intracell scheduler;flow-shop scheduling;genetic reinforcement learning;heterogeneous machine scheduling problem;iterative list scheduling process;job-shop scheduling;learning-based heuristic;near-optimal policy;open-shop scheduling problems","","6","","67","","","","Dec 1998","","IEEE","IEEE Journals & Magazines"
"Self-learning system for personalized e-learning","V. Pant; S. Bhasin; S. Jain","Dept. of Computer Science & Engineering, Graphic Era University, Dehradun, India","2017 International Conference on Emerging Trends in Computing and Communication Technologies (ICETCCT)","20180205","2017","","","1","6","Knowledge is a basic requirement in the era of globalization. The act of acquiring knowledge is learning and with the advent of technology, learning has become easier. E-Learning is a popular learning method which uses the web or other technologies to promote learning. A very traditional learning method is discussed and some new methods with the integration of modern technology have been discussed. Although technologies like artificial intelligence, cloud computing, ontology, fuzzy logic etc. have been used in learning systems they are still less automated, not personalized and do not pose the capability to self-learn. A method is proposed to increase the automation and self-learning in the e-learning management systems. Further, the personalization of learning for the user so that every type of learner can learn from the type and level of the content the learner seems fit is the main aim. The proposed system is feasible, economical and scalable which makes it suitable for every level.","","CD:978-1-5386-1146-3; Electronic:978-1-5386-1147-0; POD:978-1-5386-1148-7","10.1109/ICETCCT.2017.8280344","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8280344","Machine learning;e-learning system;learning management system;reinforcement learning","Crawlers;Electronic learning;Learning management systems;Semantics;Task analysis;Videos","cloud computing;computer aided instruction;fuzzy logic;ontologies (artificial intelligence)","artificial intelligence;cloud computing;e-learning management systems;fuzzy logic;learning systems;ontology;personalized e-learning;self-learning system","","","","","","","","17-18 Nov. 2017","","IEEE","IEEE Conferences"
"Towards scalable and privacy preserving commercial content dissemination in social wireless networks","F. Hajiaghajani; S. Biswas","Electrical and Computer Engineering, Michigan State University, East Lansing, USA","2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)","20180215","2017","","","1","7","This paper proposes a Q-learning based Device-to-Device multicast routing framework for Social Wireless Networks. The goal of the proposed Scalable Q-learning based Gain-aware Routing (SQGR) content dissemination algorithm is to maximize a predefined economic gain for commercial content generators. This economic gain is defined as the revenue from delivery of a coupon minus the forwarding cost associated with that delivery. SQGR, with its embedding learning abilities, is expected to be robust in dynamic mobility environments. It also preserves scalability and privacy since it does not require storage of per-individual consuming interest and interaction profiles within the network. Using the DTN simulator software ONE, we evaluate functional validity and compare gain performance of SQGR with few existing protocols under various commercial, network and protocol parameters.","","Electronic:978-1-5386-3531-5; POD:978-1-5386-3532-2; Paper:978-1-5386-3529-2; USB:978-1-5386-3530-8","10.1109/PIMRC.2017.8292416","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8292416","Economic Gain of Dissemination;Q-Learning;Reinforcement Learning;Social Wireless Networks","Biological system modeling;Economics;Peer-to-peer computing;Privacy;Probabilistic logic;Routing;Scalability","data privacy;delay tolerant networks;learning (artificial intelligence);mobile computing;mobile radio;multicast communication;routing protocols","Device-to-Device multicast;Gain-aware Routing content dissemination algorithm;SQGR;Scalable Q-learning;commercial content dissemination;dynamic mobility environments;social wireless networks","","","","","","","","8-13 Oct. 2017","","IEEE","IEEE Conferences"
"A Q-learning-based multi-rate transmission control scheme for RRC in WCDMA systems","Fang-Ching Ren; Chung-Ju Chang; Yih-Shen Chen","","The 13th IEEE International Symposium on Personal, Indoor and Mobile Radio Communications","20021210","2002","3","","1422","1426 vol.3","A Q-learning-based multirate transmission control scheme (Q-MRTC) for radio resource control (RRC) in WCDMA systems is proposed. The RRC problem is modelled as a semi-Markov decision process (SMDP). We successfully apply a real-time reinforcement learning algorithm, named Q-learning, to accurately estimate the transmission cost for the multi-rate transmission control. For the cost function approximation, we apply the feature extraction method to map the original state space into a more compact set which represents the resultant interference profile. Simulation results show that the Q-MRTC can achieve higher system throughput and better users' satisfaction index, by an amount of 87% and 50%, respectively, than the interference-based multi-rate transmission control scheme, while keeping the QoS requirement.","","POD:0-7803-7589-0","10.1109/PIMRC.2002.1045263","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1045263","","Control systems;Cost function;Feature extraction;Function approximation;Interference;Learning;Multiaccess communication;Radio control;State-space methods;Throughput","Markov processes;broadband networks;code division multiple access;costing;feature extraction;function approximation;learning (artificial intelligence);multiuser channels;radio networks;radiofrequency interference;telecommunication control","Q-learning-based multirate transmission control;QoS;RRC;WCDMA systems;feature extraction method;function approximation;interference profile;real-time reinforcement learning algorithm;semi-Markov decision process;simulation results;state space mapping;system throughput;transmission cost estimation;users satisfaction index","","0","","12","","","","15-18 Sept. 2002","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Dynamic Treatment Regimes on Medical Registry Data","Y. Liu; B. Logan; N. Liu; Z. Xu; J. Tang; Y. Wang","Div. of Biostat., Med. Coll. of Wisconsin, Milwaukee, WI, USA","2017 IEEE International Conference on Healthcare Informatics (ICHI)","20170914","2017","","","380","385","In this paper, we propose the first deep reinforce-ment learning framework to estimate the optimal Dynamic Treat-ment Regimes from observational medical data. This framework is more flexible and adaptive for high dimensional action and state spaces than existing reinforcement learning methods to model real life complexity in heterogeneous disease progression and treatment choices, with the goal to provide doctor and patients the data-driven personalized decision recommendations. The proposed deep reinforcement learning framework contains a supervised learning step to predict the most possible expert actions; and a deep reinforcement learning step to estimate the long term value function of Dynamic Treatment Regimes. We motivated and implemented the proposed framework on a data set from the Center for International Bone Marrow Transplant Research (CIBMTR) registry database, focusing on the sequence of prevention and treatments for acute and chronic graft versus host disease. We showed results of the initial implementation that demonstrates promising accuracy in predicting human expert decisions and initial implementation for the reinforcement learning step.","","Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3","10.1109/ICHI.2017.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031178","","Biomedical imaging;Decision making;Diseases;Games;Learning (artificial intelligence);Machine learning","bone;data analysis;diseases;learning (artificial intelligence);patient treatment","CIBMTR;Dynamic Treatment Regimes;International Bone Marrow Transplant Research registry database;chronic graft versus host disease;deep reinforcement learning framework;deep reinforcement learning step;heterogeneous disease progression;medical registry data;observational medical data;personalized decision recommendations;reinforcement learning framework;supervised learning step","","","","","","","","23-26 Aug. 2017","","IEEE","IEEE Conferences"
"Consistent Goal-Directed User Model for Realisitc Man-Machine Task-Oriented Spoken Dialogue Simulation","O. Pietquin","&#201;cole Sup&#233;rieure d'&#201;lectricit&#233; - SUPELEC, Metz Campus - STS Team, 2 rue &#201;douard Belin - F-57070 Metz - FRANCE. email: olivier.pietquin@supelec.fr","2006 IEEE International Conference on Multimedia and Expo","20061226","2006","","","425","428","Because of the great variability of factors to take into account, designing a spoken dialogue system is still a tailoring task. Rapid design and reusability of previous work is made very difficult. For these reasons, the application of machine learning methods to dialogue strategy optimization has become a leading subject of researches this last decade. Yet, techniques such as reinforcement learning are very demanding in training data while obtaining a substantial amount of data in the particular case of spoken dialogues is time-consuming and therefore expansive. In order to expand existing data sets, dialogue simulation techniques are becoming a standard solution. In this paper we describe a user modeling technique for realistic simulation of man-machine goal-directed spoken dialogues. This model, based on a stochastic description of man-machine communication, unlike previously proposed models, is consistent along the interaction according to its history and a predefined user goal","1945-7871;19457871","CD-ROM:1-4244-0367-7; POD:1-4244-0366-7","10.1109/ICME.2006.262563","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4036627","","Acoustic noise;Automatic speech recognition;History;Learning systems;Man machine systems;Optimization methods;Sociotechnical systems;Speech processing;Stochastic processes;Training data","interactive systems;learning (artificial intelligence);man-machine systems;optimisation;speech processing;stochastic processes;user modelling","machine learning method;optimization;realistic man-machine simulation;spoken dialogue system;stochastic description;task-oriented dialogue simulation technique;user modeling technique","","4","","14","","","","9-12 July 2006","","IEEE","IEEE Conferences"
"A tailored Q- Learning for routing in wireless sensor networks","V. K. Sharma; S. S. P. Shukla; V. Singh","Dept. of Comput. Sci. & Eng., Jaypee Polytech. & Training Centre, Rewa, India","2012 2nd IEEE International Conference on Parallel, Distributed and Grid Computing","20130207","2012","","","663","668","Wireless sensor networks (WSNs) have major importance in distributed sensing applications. The important concern in the intend of wireless sensor networks is battery consumption which usually rely on non-renewable sources of energy. In this paper we have proposed a tailored Q-Learning algorithm for routing scheme in wireless sensor network. Our primary goal is to make an efficient routing algorithm with help of modified Q-Learning approach to minimize the energy consumption utilized by sensor nodes. This approach is a modified version of existing Q-Learning method for WSN that leads to the convergence problem.","","Electronic:978-1-4673-2925-5; POD:978-1-4673-2922-4","10.1109/PDGC.2012.6449899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6449899","Convergence Problem;Q-Learning;Reinforcement learning;WSN Flooding Routing Protocol","Artificial neural networks;Lead;Wireless sensor networks","learning (artificial intelligence);telecommunication computing;telecommunication network routing;wireless sensor networks","Q- learning;Q-learning algorithm;WSN;battery consumption;distributed sensing applications;energy consumption;modified Q-learning approach;nonrenewable energy sources;routing algorithm;routing scheme;sensor nodes;wireless sensor network routing","","1","","13","","","","6-8 Dec. 2012","","IEEE","IEEE Conferences"
"A Service Recommendation Using Reinforcement Learning for Network-based Robots in Ubiquitous Computing Environments","A. Moon; T. Kang; H. Kim; H. Kim","Electronice and Telecommunication Research Institute, 161 Gajeong-dong, Yuseong-gu, Daejeon, 305-350, Korea. Email: akmoon@etri.re.kr","RO-MAN 2007 - The 16th IEEE International Symposium on Robot and Human Interactive Communication","20080116","2007","","","821","826","Ubiquitous robotic companion (URC ) is a new concept for the network-based robot platform which can enable to be following its master wherever or whenever he/she be in order to provide necessary services. The robot platforms in present normally interest in providing services through the direct interaction in responding to the user's demands. On the other hand, URC services are required to be provided by the means of recognizing the circumstances and taking a user's preference into account. In this paper, we propose a service recommendation scheme for URC robots. The proposed service recommendation, developed based on the reinforcement learning, can be used to provide personalized services by learning users' preferences or tasks through the interaction with users. Using simulation for rapid testing, we evaluate of the proposed scheme under a variety of user modeling types and discount factors.","1944-9445;19449445","CD-ROM:978-1-4244-1635-6; POD:978-1-4244-1634-9","10.1109/ROMAN.2007.4415198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4415198","","Control systems;Human robot interaction;Intelligent robots;Inventory management;Machine learning;Moon;Robot control;Robot sensing systems;Service robots;Ubiquitous computing","computer networks;control engineering computing;learning (artificial intelligence);man-machine systems;robots;ubiquitous computing;user modelling","human-robot interaction;network-based robot;network-based robot platform;rapid testing;reinforcement learning;service recommendation scheme;ubiquitous computing environment;ubiquitous robotic companion;user modeling","","2","","15","","","","26-29 Aug. 2007","","IEEE","IEEE Conferences"
"A task-oriented service personalization scheme for smart environments using reinforcement learning","B. Tegelund; H. Son; D. Lee","School of Computer Science, KAIST, Daejeon, S. Korea","2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops)","20160421","2016","","","1","6","Users want IoT environments to provide them with personalized support. These environments therefore need to be able to learn user preferences, such as what temperature the room should be or which lights should be turned on. We propose a novel system that separates the tasks of learning a user's preferences and realizing them within the environment. The system is able to capture user preferences by reinterpreting the problem as an optimization problem and applying inverse reinforcement learning to it. The system is shown to be able to accurately extract simple preferences given a small number of user demonstrations. These preferences are then realized by actuates devices running reinforcement learning-based agents to provide an environment consistent with the learnt preferences, also in situations not included in any user demonstration.","","Electronic:978-1-5090-1941-0; POD:978-1-5090-1942-7","10.1109/PERCOMW.2016.7457110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457110","","Brightness;Context;Context modeling;Learning (artificial intelligence);Motion pictures;Performance evaluation;Sensors","Internet of Things;ambient intelligence;learning (artificial intelligence);multi-agent systems","IoT environments;personalized support;reinforcement learning-based agents;smart environments;task-oriented service personalization scheme;user preference learning","","2","","10","","","","14-18 March 2016","","IEEE","IEEE Conferences"
"Adaptive Learning Based on Exercises Fitness Degree","A. M. Mirea; M. C. Preda","","2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology","20091009","2009","3","","215","218","The paper considers the e-learning systems that provide personalized content to their users and that permanently adapt to the evolution of the users during their learning stages. Such systems help the students to consolidate their knowledge faster than other methods. The main contribution is the proposal of a mathematical model of an adaptive learning system with the mentioned characteristics. The model involves a multi step process where, at each stage, the performances of the student are measured and the system is adapting accordingly.","","POD:978-0-7695-3801-3","10.1109/WI-IAT.2009.266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5284965","adaptive control;adaptive learning environments;learning systems;personalized learning;reinforcement learning","Adaptive systems;Computer science;Electronic learning;Intelligent agent;Learning systems;Mathematical model;Paper technology;Performance evaluation;Proposals;Testing","","","","0","","5","","","","15-18 Sept. 2009","","IEEE","IEEE Conferences"
"Budgeted Learning for Developing Personalized Treatment","K. Deng; R. Greiner; S. Murphy","Dept. of Stat., Univ. of Michigan, Ann Arbor, MI, USA","2014 13th International Conference on Machine Learning and Applications","20150209","2014","","","7","14","There is increased interest in using patient-specific information to personalize treatment. Personalized treatment decision rules can be learned using data from standard clinical trials, but such trials are very costly to run. This paper explores the use of budgeted learning techniques to design more efficient clinical trials, by effectively determining which type of patients to recruit, at each time, throughout the duration of the trial. We propose a Bayesian bandit model and discuss the computational challenges and issues pertaining to this approach. We compare our budgeted learning algorithm, which approximately minimizes the Bayes risk, using both simulated data and data modeled after a clinical trial for treating depressed individuals, with other plausible algorithms. We show that our budgeted learning algorithm demonstrated excellent performance across a wide variety of situations.","","Electronic:978-1-4799-7415-3; POD:978-1-4799-7416-0; USB:978-1-4799-7414-6","10.1109/ICMLA.2014.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7033084","Active Learning;Bayesian;Budgeted Learning;Personalized Treatment;Reinforcement Learning","Algorithm design and analysis;Approximation algorithms;Approximation methods;Bayes methods;Clinical trials;Fasteners;Resource management","Bayes methods;learning (artificial intelligence);medical information systems;minimisation;patient treatment","Bayes risk;Bayesian bandit model;budgeted learning;clinical trial;patient-specific information;personalized treatment decision rule","","0","","24","","","","3-6 Dec. 2014","","IEEE","IEEE Conferences"
"A dialogue game framework with personalized training using reinforcement learning for computer-assisted language learning","P. h. Su; Y. B. Wang; T. h. Yu; L. s. Lee","Graduate Institute of Communication Engineering, National Taiwan University, Taiwan","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","20131021","2013","","","8213","8217","We propose a framework for computer-assisted language learning as a pedagogical dialogue game. The goal is to offer personalized learning sentences on-line for each individual learner considering the learner's learning status, in order to strike a balance between more practice on poorly-pronounced units and complete practice on the whole set of pronunciation units. This objective is achieved using a Markov decision process (MDP) trained with reinforcement learning using simulated learners generated from real learner data. Preliminary experimental results on a subset of the example dialogue script show the effectiveness of the framework.","1520-6149;15206149","Electronic:978-1-4799-0356-6; POD:978-1-4799-0357-3; USB:978-1-4799-0355-9","10.1109/ICASSP.2013.6639266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6639266","Computer-Assisted Language Learning;Dialogue Game;Markov Decision Process;Reinforcement Learning","Educational institutions;Games;Hidden Markov models;Learning (artificial intelligence);Markov processes;Speech;Training","computer aided instruction;computer games;learning (artificial intelligence);natural languages;speech recognition","Markov decision process;computer assisted language learning;dialogue game framework;pedagogical dialogue game;personalized sentence learning;personalized training;reinforcement learning;simulated learner","","6","","35","","","","26-31 May 2013","","IEEE","IEEE Conferences"
"Impedance Control of Robot Manipulator in Contact Task Using Machine Learning","B. Kim; S. Park","Department of Mechanical Engineering, Korea University, Seoul, Korea, Tel : +82-2-3290-3868; E-mail: biomimetic@korea.ac.kr","2006 SICE-ICASE International Joint Conference","20070226","2006","","","2590","2594","In performing contact tasks using robot manipulators, force control is essential. One approach is to select appropriate stiffness ellipse at the endpoint of the manipulator, where stiffness ellipse is a geometrical shape of force element represented in the principal axis of task space. In this study, we introduce a novel method to tailor stiffness ellipse required to perform contact tasks by using associative search network. Using appropriate performance indexes in the open-door task experiment, we acquired stiffness ellipse trajectory which optimizes dynamic movement of manipulator. Derived stiffness ellipse (or impedance in general) through learning process can be used for the similar task of learning process","","CD-ROM:89-950038-5-5; POD:89-950038-4-7","10.1109/SICE.2006.314795","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4108082","Associative Search Network;Contact task;impedance control;reinforcement learning;stiffness ellipse","Force control;Humans;Impedance;Machine learning;Manipulators;Mechanical engineering;Orbital robotics;Performance analysis;Robot control;Shape","force control;learning (artificial intelligence);manipulators","associative search network;force control;impedance control;machine learning;robot manipulator;stiffness ellipse","","2","","14","","","","18-21 Oct. 2006","","IEEE","IEEE Conferences"
"Implementation of a learning fuzzy controller","S. Shenoi; K. Ashenayi; M. Timmerman","Center for Intelligent Syst., Tulsa Univ., OK, USA","IEEE Control Systems","20020806","1995","15","3","73","80","This article describes our efforts at designing and implementing a practical learning fuzzy controller using inexpensive hardware. The controller engages basic control concepts and system-independent learning rules to enable it to adapt in real time to unknown plants even when it starts with a vacuous initial control policy. The controller remains dormant when the plant is operating satisfactorily and autonomously initiates online adaptation in real time when adverse performance is observed. The Intel-8031-based hardware implementation is geared for extensibility, robustness, and fault tolerance. Limited plant-dependent information is incorporated to tailor the hardware to applications. The design produces learning rates exceeding 200 reinforcements per second. The controller thus is able to learn to control unknown plants in real time even while it is controlling them. Physical experiments indicate that the learning fuzzy controller can rapidly and effectively deal with variations in plant characteristics, compensate for wear and tear, and handle disturbances and noise.<<ETX>>","1066-033X;1066033X","","10.1109/37.387620","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=387620","","Control systems;Delay;Error correction;Fault tolerance;Fuzzy control;Hardware;Real time systems;Robustness;Shape control;Table lookup","control system synthesis;fuzzy control;learning systems;microcontrollers;robust control","Intel-8031-based hardware;basic control concepts;extensibility;fault tolerance;learning fuzzy controller;limited plant-dependent information;robustness;system-independent learning rules;vacuous initial control policy","","11","","24","","","","June 1995","","IEEE","IEEE Journals & Magazines"
"Reinforcement learning for game personalization on edge devices","A. Bodas; B. Upadhyay; C. Nadiger; S. Abdelhak","Intel Corporation","2018 International Conference on Information and Computer Technologies (ICICT)","20180510","2018","","","119","122","Good progress has been shown recently in the area of active learning, specifically, Reinforcement learning (RL). In this paper, the authors show how RL can be used to personalize games based on user-interaction with the game. The work uses Deep Q network models (DQN) and the open source framework OpenAI to build an RL model that is able to optimize the gamer's engagement level in a game. The authors define an example quantitative measure of gamer engagement and incorporate that into the DQN learning reward function. The gamer experience optimization is empirically demonstrated using a game of Pong. Simulation testing and analysis of results indicate adapted RL models increase engagement reward values, thus enhancing gamer experience. The contribution of this paper is twofold: (1) using RL, it paves the path for wider adaptation to user-behavior, starting with gaming, and (2) it shows analysis and feasibility of an RL algorithm on an edge device (Personal Computer) in real-time.","","Electronic:978-1-5386-5384-5; POD:978-1-5386-5385-2; Paper:978-1-5386-5382-1; USB:978-1-5386-5383-8","10.1109/INFOCT.2018.8356853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8356853","artificial intelligence;computer games;edge computing;game personalization;reinforcement learning","Adaptation models;Buildings;Games;Learning (artificial intelligence);Mathematical model;Training","computer games;learning (artificial intelligence);mobile computing","DQN learning reward function;RL algorithm;RL model;active learning;deep Q network models;edge device;engagement reward values;game personalization;gamer engagement;gamer experience;open source framework OpenAI;personal computer;reinforcement learning","","","","","","","","23-25 March 2018","","IEEE","IEEE Conferences"
"Learning to dribble on a real robot by success and failure","M. Riedmiller; R. Hafner; S. Lange; M. Lauer","Dept. of Mathematics and Informatics, Institute of Computer Science and Institute of Cognitive Science, University of Osnabr&#252;ck, Germany","2008 IEEE International Conference on Robotics and Automation","20080613","2008","","","2207","2208","Learning directly on real world systems such as autonomous robots is a challenging task, especially if the training signal is given only in terms of success or failure (reinforcement learning). However, if successful, the controller has the advantage of being tailored exactly to the system it eventually has to control. Here we describe, how a neural network based RL controller learns the challenging task of ball dribbling directly on our middle-size robot. The learned behaviour was actively used throughout the RoboCup world championship tournament 2007 in Atlanta, where we won the first place. This constitutes another important step within our Brainstormers project. The goal of this project is to develop an intelligent control architecture for a soccer playing robot, that is able to learn more and more complex behaviours from scratch.","1050-4729;10504729","POD:978-1-4244-1646-2","10.1109/ROBOT.2008.4543536","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4543536","","Biological neural networks;Cognitive robotics;Control systems;Informatics;Intelligent robots;Learning;Mathematics;Robotics and automation;System testing;USA Councils","control engineering computing;learning (artificial intelligence);learning systems;mobile robots;multi-robot systems;neurocontrollers","Brainstormers project;RoboCup;intelligent control;middle-size robot;neural network;reinforcement learning;robot learning;soccer playing robot","","2","","7","","","","19-23 May 2008","","IEEE","IEEE Conferences"
"A Neural Network Model of Multisensory Representation of Peripersonal Space: Effect of tool use","M. Ursino; M. Zavaglia; E. Magosso; A. Serino; G. di Pellegrino","Department of Electronics, Computer Science and Systems, University of Bologna, Bologna, Italy","2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","20071022","2007","","","2735","2739","This work describes an original neural network to simulate representation of the peripersonal space around one hand, in basal conditions and after training with a tool used to reach the far space. The model is composed of two unimodal areas (visual and tactile) connected to a third bimodal area (visual-tactile). Neurons in the bimodal area integrate visual and tactile information and are activated only when a stimulus falls inside the peripersonal space. Moreover, the model assumes that synapses linking unimodal to bimodal neurons can be reinforced by an Hebbian rule during training, but this reinforcement is also under the influence of attention mechanisms. Results show that the peripersonal space, which includes just a small visual space around the hand in normal conditions, becomes elongated in the direction of the tool after training. This expansion of the peripersonal space depends on an expansion of the visual receptive field of bimodal neurons, due to a reinforcement of visual synapses, which were just latent before training. The model may be of value to analyze the neural mechanisms responsible for representing and plastically shaping peripersonal space, and in perspective, for interpretation of psychophysical data on patients with brain damage.","1094-687X;1094687X","CD-ROM:978-1-4244-0788-0; POD:978-1-4244-0787-3","10.1109/IEMBS.2007.4352894","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4352894","","Animals;Biological neural networks;Computer science;Computer simulation;Humans;Neural networks;Neurons;Plastics;Psychology;Radio frequency","Hebbian learning;biology computing;neural nets;neurophysiology;touch (physiological);vision","Hebbian rule;attention mechanism;bimodal neurons;bimodal visual-tactile perception;hand peripersonal space;neural network model;peripersonal space multisensory representation;peripersonal space plastic shaping;synapses;unimodal neurons;unimodal tactile perception;unimodal visual perception;visual receptive field expansion;visual synapse reinforcement","Animals;Hand;Haplorhini;Humans;Neural Networks (Computer);Neurons, Afferent;Personal Space;Physical Stimulation;Synapses;Tool Use Behavior;Touch;Visual Perception","2","","19","","","","22-26 Aug. 2007","","IEEE","IEEE Conferences"
"Distributed Q-learning based dynamic spectrum access in high capacity density cognitive cellular systems using secondary LTE spectrum sharing","N. Morozs; D. Grace; T. Clarke","Department of Electronics, University of York, Heslington, YO10 5DD, United Kingdom","2014 International Symposium on Wireless Personal Multimedia Communications (WPMC)","20150122","2014","","","462","467","In this paper a distributed Q-learning based dynamic spectrum access (DSA) algorithm is applied to a cognitive cellular system designed for providing ultra high capacity density with only secondary access to an LTE channel. Large scale simulations of a stadium temporary event scenario show that the distributed Q-learning based DSA scheme provides robust quality of service (QoS) and extremely high system throughput densities to the users of the stadium network, whilst successfully coexisting with a primary network of macro eNodeBs on the same LTE channel. It is also shown that incorporating spectrum awareness or spectrum sensing based admission control into the DSA algorithm in this scenario does not improve its performance. Therefore, distributed Q-learning based DSA is a viable and easily implementable solution for facilitating secondary LTE spectrum sharing in high capacity density cognitive cellular systems.","1347-6890;13476890","Electronic:978-9860-3-3407-4","10.1109/WPMC.2014.7014863","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014863","Cognitive Cellular Systems;Dynamic Spectrum Access;Reinforcement Learning;Small Cells;Spectrum Sharing","Cognitive radio;Data models;Interference;Quality of service;Sensors;Throughput","Long Term Evolution;cellular radio;channel allocation;cognitive radio;learning (artificial intelligence);quality of service;radio spectrum management;signal detection;telecommunication computing;telecommunication congestion control","LTE channel secondary access;admission control;distributed Q-learning;dynamic spectrum access;high capacity density cognitive cellular systems;macro eNodeBs;quality of service;secondary LTE spectrum sharing;spectrum awareness;spectrum sensing","","4","","23","","","","7-10 Sept. 2014","","IEEE","IEEE Conferences"
"Learning rate control for downlink shared channel in WCDMA","B. Makarevitch","","14th IEEE Proceedings on Personal, Indoor and Mobile Radio Communications, 2003. PIMRC 2003.","20040114","2003","3","","2919","2922 vol.3","The paper considers bit rate control algorithms for the downlink shared channel in WCDMA (wideband code division multiple access) network. The goal of the rate control is to maximise the data throughput while satisfying quality constraints for the voice users. A learning algorithm, based on Reinforcement learning, is proposed. The algorithm facilitates frame acknowledgments to make decisions on the rate change. The control channel power is used as the state variable and a probabilistic decision policy is applied. Performance of the algorithm is evaluated and compared with performance of a simple algorithm without learning. Data throughput and voice quality cost function are used as performance measures. The learning algorithm achieves higher throughput and lower voice quality cost than the reference algorithm.","","POD:0-7803-7822-9","10.1109/PIMRC.2003.1259284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1259284","","Control systems;Cost function;Downlink;Intelligent networks;Interference;Learning;Multiaccess communication;Propagation losses;Throughput;Wideband","broadband networks;code division multiple access;learning (artificial intelligence);probability;telecommunication channels;telecommunication links","Reinforcement learning;WCDMA;bit rate control algorithm;downlink shared channel;learning algorithm;probabilistic decision policy;state variable;voice user;wideband code division multiple access","","1","","5","","","","7-10 Sept. 2003","","IEEE","IEEE Conferences"
"Design of semi-decentralized control laws for distributed-air-jet micromanipulators by reinforcement learning","L. Matignon; G. J. Laurent; N. Le Fort-Piat","FEMTOST/UFC-ENSMM-UTBM-CNRS, Universit&#233; de Franche-Comt&#233;, Besan&#231;on, France","2009 IEEE/RSJ International Conference on Intelligent Robots and Systems","20091215","2009","","","3277","3283","Recently, a great deal of interest has been developed in learning in multi-agent systems to achieve decentralized control. Machine learning is a popular approach to find controllers that are tailored exactly to the system without any prior model. In this paper, we propose a semi-decentralized reinforcement learning control approach in order to position and convey an object on a contact-free MEMS-based distributed-manipulation system. The experimental results validate the semi-decentralized reinforcement learning method as a way to design control laws for such distributed systems.","2153-0858;21530858","POD:978-1-4244-3803-7","10.1109/IROS.2009.5353902","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5353902","","Actuators;Control systems;Distributed control;Electrodes;Machine learning;Micromanipulators;Microvalves;Multiagent systems;Open loop systems;Sorting","distributed control;jets;learning (artificial intelligence);micromanipulators;micromechanical devices;multi-agent systems","MEMS based distributed manipulation system;distributed air jet micromanipulators;distributed systems;machine learning;multi agent systems;reinforcement learning;semi decentralized control laws;semi decentralized reinforcement learning control","","1","","21","","","","10-15 Oct. 2009","","IEEE","IEEE Conferences"
"Reinforcement Learning for Active Queue Management in Mobile All-IP Networks","N. Vucevic; J. Perez-Romero; O. Sallent; R. Agusti","Dept. TSC, Universitat Polit&#232;cnica de Catalunya (UPC), Barcelona, Spain","2007 IEEE 18th International Symposium on Personal, Indoor and Mobile Radio Communications","20071204","2007","","","1","5","In future all-IP based wireless networks, like the envisaged in the long term evolution (LTE) architectures for future systems, network providers will have to deal with large traffic volumes with different QoS requirements. In order to increase exploitation of network resources wisely, intelligent adaptive solutions for class based traffic regulation are needed. In particular, active queue management (AQM) is regarded as one of these solutions to provide low queuing delay and high throughput to flows by smart packet discarding. In this paper, we propose a novel AQM solution for future all-IP networks based on a reinforcement learning scheme that allows controlling both the queuing delay and the packet loss of the different service classes. The proposed approach is evaluated through simulations and compared against other algorithms used in the literature, like the random early detection (RED) and the drop from tail (DFT), confirming the benefits of the proposed algorithm.","2166-9570;21669570","CD-ROM:978-1-4244-1144-3; POD:978-1-4244-1143-6","10.1109/PIMRC.2007.4394713","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4394713","","Bandwidth;Delay;Diffserv networks;Intelligent networks;Learning;Mobile communication;Quality of service;Tail;Telecommunication traffic;Traffic control","IP networks;learning (artificial intelligence);mobile computing;mobility management (mobile radio);queueing theory;telecommunication traffic","AQM;QoS;active queue management;all-IP based wireless networks;class based traffic regulation;drop from tail;intelligent adaptive solutions;mobile all-IP networks;network resources exploitation;queuing delay;random early detection;reinforcement learning","","2","","15","","","","3-7 Sept. 2007","","IEEE","IEEE Conferences"
"Emergency Navigation in Confined Spaces Using Dynamic Grouping","H. Bi; O. J. Akinwande; E. Gelenbe","Dept. of Electr. & Electron. Eng., Imperial Coll. London, London, UK","2015 9th International Conference on Next Generation Mobile Applications, Services and Technologies","20160107","2015","","","120","125","The performance of Emergency Management Systems (EMS) in confined spaces is highly dependent on the decision algorithm employed for the safe navigation of the evacuees to the available exits. In the algorithm proposed in this paper, we have considered evacuees under two groups, based on their age and physical condition, and we tailor two routing metrics, one for each group, in finding suitable paths for the evacuees. A dynamic grouping mechanism that can adjust an evacuee's group, and therefore routing metric, according to its on-going health condition is employed during the evacuation. To implement the routing metrics, we have used the Cognitive Packet Network (CPN) with random neural networks (RNN) and reinforcement learning. The CPN is an adaptive routing protocol that is loop-free at all times and easily handles multiple quality of service (QoS) metrics. Simulation results show that allowing the navigation system to be sensitive to the on-going health conditions and mobility of the evacuees, using our proposed dynamic grouping, can achieve higher survival rates.","","CD-ROM:978-1-4799-8659-0; Electronic:978-1-4799-8660-6; POD:978-1-4799-8661-3","10.1109/NGMAST.2015.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7373229","Cognitive Packet Network;Dynamic Grouping;Emergency navigation;QoS driven protocol","Buildings;Hazards;Heuristic algorithms;Measurement;Navigation;Quality of service;Routing","cognitive radio;emergency management;learning (artificial intelligence);navigation;neural nets;packet radio networks;quality of service;routing protocols;safety","CPN;EMS;QoS metrics;RNN;adaptive routing protocol;cognitive packet network;confined spaces;decision algorithm;dynamic grouping mechanism;emergency management systems;emergency navigation;health conditions;navigation safety;navigation system;physical condition;quality of service metrics;random neural networks;reinforcement learning;routing metrics","","","","22","","","","9-11 Sept. 2015","","IEEE","IEEE Conferences"
"MPRL: Multiple-Periodic Reinforcement Learning for difficulty adjustment in rehabilitation games","Y. A. Sekhavat","Faculty of Multimedia, Tabriz Islamic Art University, Iran","2017 IEEE 5th International Conference on Serious Games and Applications for Health (SeGAH)","20170608","2017","","","1","7","Generally, the difficulty level of a therapeutic game is regulated manually by a therapist. However, home-based rehabilitation games require a technique for automatic difficulty adjustment. This paper proposes a personalized difficulty adjustment technique for a rehabilitation game that automatically regulates difficulty settings based on a patient's skills in real-time. To this end, ideas from reinforcement learning are used to dynamically adjust the difficulty of a game. We show that difficulty adjustment is a multiple-objective problem, in which some objectives might be evaluated at different periods. To address this problem, we propose and use Multiple-Periodic Reinforcement Learning (MPRL) that makes it possible to evaluate different objectives of difficulty adjustment in separate periods. The results of experiments show that MPRL outperforms traditional Multiple-Objective Reinforcement Learning (MORL) in terms of user satisfaction parameters as well as improving the movement skills of patients.","","Electronic:978-1-5090-5482-4; POD:978-1-5090-5483-1","10.1109/SeGAH.2017.7939260","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7939260","","Biological cells;Silicon","learning (artificial intelligence);medical computing;patient rehabilitation;serious games (computing)","home-based rehabilitation games;multiple-objective problem;multiple-periodic reinforcement learning;patient movement skills;therapeutic game","","","","","","","","2-4 April 2017","","IEEE","IEEE Conferences"
