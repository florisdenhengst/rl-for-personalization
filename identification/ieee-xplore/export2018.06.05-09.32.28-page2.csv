"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication_Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Personalized tuning of a reinforcement learning control algorithm for glucose regulation","E. Daskalaki; P. Diem; S. G. Mougiakakou","Diabetes Technology Research Group, ARTORG Center for Biomedical Engineering Research, University of Bern, 3010, Switzerland","2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20130926","2013","","","3487","3490","Artificial pancreas is in the forefront of research towards the automatic insulin infusion for patients with type 1 diabetes. Due to the high inter- and intra-variability of the diabetic population, the need for personalized approaches has been raised. This study presents an adaptive, patient-specific control strategy for glucose regulation based on reinforcement learning and more specifically on the Actor-Critic (AC) learning approach. The control algorithm provides daily updates of the basal rate and insulin-to-carbohydrate (IC) ratio in order to optimize glucose regulation. A method for the automatic and personalized initialization of the control algorithm is designed based on the estimation of the transfer entropy (TE) between insulin and glucose signals. The algorithm has been evaluated in silico in adults, adolescents and children for 10 days. Three scenarios of initialization to i) zero values, ii) random values and iii) TE-based values have been comparatively assessed. The results have shown that when the TE-based initialization is used, the algorithm achieves faster learning with 98%, 90% and 73% in the A+B zones of the Control Variability Grid Analysis for adults, adolescents and children respectively after five days compared to 95%, 78%, 41% for random initialization and 93%, 88%, 41% for zero initial values. Furthermore, in the case of children, the daily Low Blood Glucose Index reduces much faster when the TE-based tuning is applied. The results imply that automatic and personalized tuning based on TE reduces the learning period and improves the overall performance of the AC algorithm.","1094-687X;1094687X","Electronic:978-1-4577-0216-7; POD:978-1-4577-0215-0; USB:978-1-4577-0214-3","10.1109/EMBC.2013.6610293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6610293","","Algorithm design and analysis;Diabetes;Insulin;Integrated circuits;Learning (artificial intelligence);Sugar;Vectors","blood;entropy;learning (artificial intelligence);medical computing;paediatrics;sugar","Actor-Critic learning approach;TE-based initialization;adolescents;artificial pancreas;automatic insulin infusion;basal rate;control variability grid analysis;glucose regulation;glucose signals;insulin signals;insulin-to-carbohydrate ratio;low-blood glucose index;patient-specific control strategy;personalized tuning;reinforcement learning control algorithm;transfer entropy;type 1 diabetes","Adolescent;Adult;Algorithms;Blood Glucose;Child;Humans;Insulin;Pancreas, Artificial;Precision Medicine;Signal Processing, Computer-Assisted","3","","27","","","","3-7 July 2013","","IEEE","IEEE Conferences"
"A Three-Layered Mutually Reinforced Model for Personalized Citation Recommendation","X. Cai; J. Han; W. Li; R. Zhang; S. Pan; L. Yang","School of Automation, Northwestern Polytechnical University, Xi'an 710072, China.","IEEE Transactions on Neural Networks and Learning Systems","","2018","Early Access","Early Access","1","12","Fast-growing scientific papers pose the problem of rapidly and accurately finding a list of reference papers for a given manuscript. Citation recommendation is an indispensable technique to overcome this obstacle. In this paper, we propose a citation recommendation approach via mutual reinforcement on a three-layered graph, in which each paper, author or venue is represented as a vertex in the paper layer, author layer, and venue layer, respectively. For personalized recommendation, we initiate the random walk separately for each query researcher. However, this has a high computational complexity due to the large graph size. To solve this problem, we apply a three-layered interactive clustering approach to cluster related vertices in the graph. Personalized citation recommendations are then made on the subgraph, generated by the clusters associated with each researcher's needs. When evaluated on the ACL anthology network, DBLP, and CiteSeer ML data sets, the performance of our proposed model-based citation recommendation approach is comparable with that of other state-of-the-art citation recommendation approaches. The results also demonstrate that the personalized recommendation approach is more effective than the nonpersonalized recommendation approach.","2162-237X;2162237X","","10.1109/TNNLS.2018.2817245","China Postdoctoral Science Foundation; National Natural Science Foundation of China; Research Grants Council of Hong Kong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8337085","Mutually reinforced model;personalized citation recommendation;three-layered interactive clustering.","Clustering algorithms;Computational complexity;Context modeling;Data models;Hybrid power systems;Indexes;Learning systems","","","","","","","","","20180412","","","IEEE","IEEE Early Access Articles"
"Evaluation of trust in robots: A cognitive approach","B. Kumar; A. D. Dubey","Department of CS/IS SMCS-CEST, Fiji National University, Fiji","2017 International Conference on Computer Communication and Informatics (ICCCI)","20171123","2017","","","1","6","The study of Human-Robot interaction faces one of the biggest challenges in measuring the trustworthiness of the robots. The enhancement and the augmentation of the human capabilities using the human robot integration are dependent on the reliability and dependability of the robots. These factors become more significant when the participation of the robot is the human robot integration is active and the cohesion between humans and robots is high. In order to measure the trust and other cognitive parameters of the robot, we have designed trust model in this research paper. This paper evaluates the trust of a customized robot while performing a task using three different algorithms. The algorithms used for the path planning task in this paper are simple artificial neural network; reinforcement based artificial neural network and Situation-Operator Model. The trust model proposed in this paper has been simulated using the results obtained while the robot performed its tasks using the three algorithms. The results show that the trust of the robot increases with each learning cycle thereby indicating that the training of the robot enhances the trust parameter of the robot.","","CD:978-1-4673-8853-5; Electronic:978-1-4673-8855-9; POD:978-1-4673-8856-6","10.1109/ICCCI.2017.8117701","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8117701","Capability;Cognitive;Human Computer Interaction;Robot;Trust Model","Artificial neural networks;Indexes;Informatics;Path planning;Robots","human-robot interaction;learning (artificial intelligence);man-machine systems;neural nets;path planning;robots","Situation-Operator Model;customized robot;human capabilities;human robot integration;robot increases;robot trustworthiness;trust model;trust parameter","","","","","","","","5-7 Jan. 2017","","IEEE","IEEE Conferences"
"Recurrent neural-network training by a learning automaton approach for trajectory learning and control system design","M. K. Sudareshan; T. A. Condarcure","Dept. of Electr. & Comput. Eng., Arizona Univ., Tucson, AZ, USA","IEEE Transactions on Neural Networks","20020806","1998","9","3","354","368","We present a training approach using concepts from the theory of stochastic learning automata that eliminates the need for computation of gradients. This approach also offers the flexibility of tailoring a number of specific training algorithms based on the selection of linear and nonlinear reinforcement rules for updating automaton action probabilities. The training efficiency is demonstrated by application to two complex temporal learning scenarios, viz, learning of time-dependent continuous trajectories and feedback controller designs for continuous dynamical plants. For the first problem, it is shown that training algorithms can be tailored following the present approach for a recurrent neural net to learn to generate a benchmark circular trajectory more accurately than possible with existing gradient-based training procedures. For the second problem, it is shown that recurrent neural-network-based feedback controllers can be trained for different control objectives","1045-9227;10459227","","10.1109/72.668879","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=668879","","Adaptive control;Application software;Automatic control;Backpropagation;Computational complexity;Control systems;Learning automata;Microcomputers;Recurrent neural networks;Stochastic processes","control system synthesis;learning automata;learning systems;neurocontrollers;recurrent neural nets","control system design;feedback control;learning systems;recurrent neural-network;reinforcement rules;stochastic learning automaton;temporal learning;trajectory learning","","22","","45","","","","May 1998","","IEEE","IEEE Journals & Magazines"
"A distributed reinforcement learning approach to maximize resource utilization and control handover dropping in multimedia wireless networks","E. Alexandri; G. Martinez; D. Zeghlache","Motorola Labs, Gif-sur-Yvette, France","The 13th IEEE International Symposium on Personal, Indoor and Mobile Radio Communications","20021210","2002","5","","2249","2253 vol.5","A new scheme to maximize resource utilization in a cellular network while respecting constraints on handover dropping probability is proposed and analyzed. The constraints are set for each traffic class separately and have to be respected by the network independently of the area in a localized manner. The problem is formulated as a Markov Decision Process (MDP) and solved by making use of the model-free simulation-based Q-learning algorithm that runs at each cell. Integration of the handover limit in the model is achieved by observing which of the new call arrivals, at a particular state of the system, are mostly responsible for violation of the handover dropping limit. Through trial and error, the algorithm proceeds to the statistical elimination of new admissions in the system, those causing excessive dropping. Results obtained via the proposed Reinforcement Learning (RL) based approach are compared with a resource allocation that takes into consideration heterogeneous and unevenly distributed traffic over the geographical area under consideration. For the scenarios examined, comparable results and performance are observed with an advantage for RL in blocking and utilization.","","POD:0-7803-7589-0","10.1109/PIMRC.2002.1046544","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1046544","","Bandwidth;Communication system traffic control;Intelligent networks;Land mobile radio cellular systems;Learning;Quality of service;Resource management;Telecommunication traffic;Traffic control;Wireless networks","Markov processes;cellular radio;learning (artificial intelligence);multimedia communication;probability","Markov decision process;call arrivals;cellular network;distributed reinforcement learning approach;handover dropping;handover dropping probability;handover limit;model-free simulation-based Q-learning algorithm;multimedia wireless networks;resource utilization;statistical elimination;traffic class;unevenly distributed traffic","","3","","4","","","","15-18 Sept. 2002","","IEEE","IEEE Conferences"
"A Location Management Scheme with Reinforcement Learning for PCS Networks","Y. h. Zhu; G. Xiao; Z. y. Li; F. Zhu","College of Information Engineering, Zhejiang University of Technology, Hangzhou, China 310032, phone: +86-571-88320163; fax: +86-571-88320163; e-mail: yhzhu@zjut.edu.cn","2007 IEEE International Conference on Networking, Sensing and Control","20070625","2007","","","224","227","Mobility management, consisting of location management and handoff management, is a challenge for personal communication services (PCS) networks. Location management mainly involves two operations: location update and paging. A two-stage paging strategy with reinforcement learning is proposed. Under this scheme, each cell in a location area is given a preference. Besides, a small real number is used to decide whether to choose and page cells with higher preferences or with lower ones during the first paging stage so that the scheme can quickly adapt to the moving pattern of mobiles. Simulation results show that the proposed paging strategy can reduce the cost as much as 25%, compared with the paging strategy of the basic location management scheme used in the existing PCS networks.","","CD-ROM:1-4244-1076-2; POD:1-4244-1075-4","10.1109/ICNSC.2007.372781","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4238994","Personal communication service (PCS) networks;location management;mobility management","Communication switching;Conference management;Databases;Learning;Mobile communication;Mobile radio mobility management;Paging strategies;Personal communication networks;Personal digital assistants;Wireless sensor networks","learning (artificial intelligence);mobile computing;mobility management (mobile radio);personal communication networks","handoff management;location management scheme;mobility management;paging strategy;personal communication services;reinforcement learning","","0","","22","","","","15-17 April 2007","","IEEE","IEEE Conferences"
"Learning to Ground in Spoken Dialogue Systems","O. Pietquin","&#201;cole Sup&#233;rieure d'&#201;lectricit&#233; - SUP&#201;LEC, Metz Campus - IMS Team, 2 rue &#201;douard Belin - F-57070 Metz - FRANCE. e-mail: olivier.pietquin@supelec.fr","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","20070604","2007","4","","IV-165","IV-168","Machine learning methods such as reinforcement learning applied to dialogue strategy optimization has become a leading subject of researches since the mid 90's. Indeed, the great variability of factors to take into account makes the design of a spoken dialogue system a tailoring task and reusability of previous work is very difficult. Yet, techniques such as reinforcement learning are very demanding in training data while obtaining a substantial amount of data in the particular case of spoken dialogues is time-consuming and therefore expansive. In order to expand existing data sets, dialogue simulation techniques are becoming a standard solution. In this paper, we present a user model for realistic spoken dialogue simulation and a method for using this model so as to simulate the grounding process. This allows including grounding subdialogues as actions in the reinforcement learning process and learning adapted strategy.","1520-6149;15206149","CD-ROM:1-4244-0728-1; POD:1-4244-0727-3","10.1109/ICASSP.2007.367189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218063","Speech Communication;Unsupervised Learning;User Modelling","Automatic speech recognition;Grounding;Learning systems;Machine learning;Man machine systems;Optimization methods;Space exploration;Speech processing;Speech synthesis;Stochastic processes","interactive systems;speech-based user interfaces;unsupervised learning","dialogue simulation techniques;grounding process;realistic spoken dialogue simulation;reinforcement learning;spoken dialogue systems","","2","","15","","","","15-20 April 2007","","IEEE","IEEE Conferences"
"Design and Evaluation of a Self-Learning HTTP Adaptive Video Streaming Client","M. Claeys; S. Latre; J. Famaey; F. De Turck","Dept. of Inf. Tech., Ghent Univ., Ghent, Belgium","IEEE Communications Letters","20140416","2014","18","4","716","719","HTTP Adaptive Streaming (HAS) is becoming the de facto standard for Over-The-Top (OTT)-based video streaming services such as YouTube and Netflix. By splitting a video into multiple segments of a couple of seconds and encoding each of these at multiple quality levels, HAS allows a video client to dynamically adapt the requested quality during the playout to react to network changes. However, state-of-the-art quality selection heuristics are deterministic and tailored to specific network configurations. Therefore, they are unable to cope with a vast range of highly dynamic network settings. In this letter, a novel Reinforcement Learning (RL)-based HAS client is presented and evaluated. The self-learning HAS client dynamically adapts its behaviour by interacting with the environment to optimize the Quality of Experience (QoE), the quality as perceived by the end-user. The proposed client has been thoroughly evaluated using a network-based simulator and is shown to outperform traditional HAS clients by up to 13% in a mobile network environment.","1089-7798;10897798","","10.1109/LCOMM.2014.020414.132649","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6746772","Streaming media;intelligent agent;learning systems;quality of service","Adaptive systems;Bandwidth;Bit rate;Convergence;Standards;Streaming media;Video sequences","hypermedia;learning (artificial intelligence);quality of experience;transport protocols;video streaming","OTT media streaming;QoE;mobile network environment;network-based simulator;novel reinforcement learning based HAS client;over-the-top based video streaming services;quality of experience;self-learning HAS client;self-learning HTTP adaptive video streaming client","","27","","11","","","20140221","April 2014","","IEEE","IEEE Journals & Magazines"
"Self-organized femto-to-macro interference coordination with partial information","A. Galindo-Serrano; L. Giupponi","Centre Tecnol. de Telecomunicacions de Catalunya (CTTC), Castelldefels, Spain","2013 IEEE 24th International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC Workshops)","20140109","2013","","","111","116","In this paper we propose a self-organized method for Intercell Interference Coordination (ICIC) between femto and macro layers. We consider the challenging situation where femtocells are completely autonomous, i.e. they do not receive feedback from the macro network. The absence of a macro to femto interface is compliant with 3GPP Releases 10. We propose a distributed learning approach, based on Reinforcement Learning (RL), for environments characterized by partial information, due to the lack of communication between femtos and macros. The theory behind this approach is funded in the Partially Observable Markov Decision Process (POMDP). The POMDP requires to construct a set of beliefs about the environment. These beliefs are constructed following the spatial Interpolation theory, which allows to estimate the interference perceived by the macrousers. Simulation results show that, through the proposed methodology, femtocells can autonomously learn a transmission power policy to manage the aggregated interference at macrousers. Performances are compared to the complete information situation, which is compliant with the status of Release 11.","","Electronic:978-1-4799-0122-7; POD:978-1-4799-0121-0; USB:978-1-4799-0120-3","10.1109/PIMRCW.2013.6707847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6707847","Femtocell network;POMDP;interference management;multiagent system;spatial interpolation","Estimation;Femtocells;Interference;Interpolation;Macrocell networks;OFDM;Signal to noise ratio","Markov processes;femtocellular radio;learning (artificial intelligence);telecommunication computing","3GPP Releases 10;POMDP;RL;distributed learning approach;femtocell;intercell interference coordination;macrouser;partially observable Markov decision process;reinforcement learning;self-organized femto-to-macro interference coordination;spatial interpolation theory;transmission power policy","","0","1","16","","","","8-9 Sept. 2013","","IEEE","IEEE Conferences"
"Extraction of a reward expectation signal from cortical units following ballistic movements generated by a brain machine interface","D. McNiel; M. Akanda; A. Tarigoppula; P. Y. Chhatbar; J. Francis","Dept. of Phys. & Pharm., State University of New York Downstate Med. Center, Brooklyn, NY","2014 IEEE Signal Processing in Medicine and Biology Symposium (SPMB)","20150108","2014","","","1","1","Movement decoding algorithms used in today's brain-machine interface (BMI) technologies require movement-related neural activity in large quantities as training data to decode with sufficient accuracy the intended movements of the user. Because of physical disability the end users of BMI systems may be unable to readily provide such training data. Moreover, variability in the neural control of movements across patients with disability may result in individually unique training data. These issues limit the generalizability of movement decoding algorithms across BMI users. One potential method of circumventing this generalizability limitation and individualizing BMI technology is the use of reinforcement learning, a group of techniques that require minimal feedback in order to find solutions to an arbitrary problem. One promising means of providing feedback to a reinforcement learning-based BMI is via a neural reward signal found in multiple cortical and subcortical areas. Particularly attractive is the idea of parallel extraction of both the movement control signal and the reward signal from the same electrode array. We examined the neural signal underlying the expectation of reward depending on the probability of successfully reaching a target given the initial ballistic movement generated by a BMI. The real-time extraction of such signal could be used to determine if the user expects a movement generated by a BMI to succeed or fail. This information could then be used to update the control architecture of the BMI to generate an output more in line with the user's intention.","","Electronic:978-1-4799-8184-7; POD:978-1-4799-8185-4","10.1109/SPMB.2014.7002960","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7002960","","Accuracy;Biomedical engineering;Decoding;Educational institutions;Nervous system;Neurosurgery;Training data","biomechanics;biomedical electrodes;brain;brain-computer interfaces;decoding;feature extraction;feedback;learning (artificial intelligence);medical control systems;motion control;neurophysiology","arbitrary problem;brain-machine interface;cortical units;electrode array;generalizability limitation;individualizing BMI technology;individually unique training data;initial ballistic movement;intended user movements;minimal feedback;movement decoding algorithms;movement-related neural activity;multiple cortical areas;neural control variability;parallel extraction;physical disability;real-time extraction;reinforcement learning;reinforcement learning-based BMI;reward expectation signal extraction;subcortical areas;training data decoding","","0","","","","","","13-13 Dec. 2014","","IEEE","IEEE Conferences"
"Towards efficient, personalized anesthesia using continuous reinforcement learning for propofol infusion control","C. Lowery; A. A. Faisal","Dept. of Comput., Imperial Coll. London, London, UK","2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)","20140102","2013","","","1414","1417","We demonstrate the use of reinforcement learning algorithms for efficient and personalized control of patients' depth of general anesthesia during surgical procedures - an important aspect for Neurotechnology. We used the continuous actor-critic learning automaton technique, which was trained and tested in silico using published patient data, physiological simulation and the bispectral index (BIS) of patient EEG. Our two-stage technique learns first a generic effective control strategy based on average patient data (factory stage) and can then fine-tune itself to individual patients (personalization stage). The results showed that the reinforcement learner as compared to a bang-bang controller reduced the dose of the anesthetic agent administered by 9.4% and kept the patient closer to the target state, as measured by RMSE (4.90 compared to 8.47). It also kept the BIS error within a narrow, clinically acceptable range 93.9% of the time. Moreover, the policy was trained using only 50 simulated operations. Being able to learn a control strategy this quickly indicates that the reinforcement learner could also adapt regularly to a patient's changing responses throughout a live operation and facilitate the task of anesthesiologists by prompting them with recommended actions.","1948-3546;19483546","CD-ROM:978-1-4673-1968-3; Electronic:978-1-4673-1969-0; POD:978-1-4673-1967-6","10.1109/NER.2013.6696208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6696208","","Algorithm design and analysis;Anesthesia;Brain modeling;Indexes;Learning (artificial intelligence);Monitoring;Surgery","bang-bang control;drug delivery systems;drugs;electroencephalography;learning (artificial intelligence);medical control systems;neurophysiology;surgery","BIS error;EEG;RMSE;anesthesiology;anesthetic agent dose reduction;bang-bang controller;bispectral index;continuous actor-critic learning automaton technique;continuous reinforcement learning algorithm;control fine tuning;depth of general anesthesia control;efficient personalized anesthesia control;generic effective control strategy learning;neurotechnology;physiological simulation;propofol infusion control;surgical procedure","","1","1","20","","","","6-8 Nov. 2013","","IEEE","IEEE Conferences"
"A learning multi-agent system for personalized information filtering","Junhua Chen; Zhonghua Yang","Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore, Singapore","Fourth International Conference on Information, Communications and Signal Processing, 2003 and the Fourth Pacific Rim Conference on Multimedia. Proceedings of the 2003 Joint","20040504","2003","3","","1864","1868 vol.3","A multi-agent hybrid learning approach to the problem of personalized information filtering is proposed in this paper. There are four agents in the multi-agent model. The problem is modeled as Monte Carlo reinforcement learning. Our proposed algorithm is modified Monte Carlo method combined with features of unsupervised suffix tree clustering and supervised backpropagation network. We argue that this proposed approach could precisely capture the user's interest without repeatedly asking for his/her explicit rates and converge to the user's interest quickly. A conclusion is drawn that our approach is efficient, precise and converges more quickly compared with existing approaches. A prototype system is being developed.","","POD:0-7803-8185-8","10.1109/ICICS.2003.1292790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1292790","","Backpropagation algorithms;Clustering algorithms;Contacts;Information filtering;Information filters;Information retrieval;Monte Carlo methods;Multiagent systems;Search engines;Supervised learning","Internet;Monte Carlo methods;backpropagation;information filters;multi-agent systems","Monte Carlo reinforcement learning;backpropagation network;learning multiagent system;personalized information filtering;suffix tree clustering","","1","","9","","","","15-18 Dec. 2003","","IEEE","IEEE Conferences"
"A Novel Scheduling Algorithm for Video Traffic in High-Rate WPANs","S. Moradi; A. H. Mohsenian Rad; V. W. S. Wong","Univ. of British Columbia, Vancouver","IEEE GLOBECOM 2007 - IEEE Global Telecommunications Conference","20071226","2007","","","742","747","The emerging high-rate wireless personal area network (WPAN) technology is capable of supporting high-speed and high-quality real-time multimedia applications. In particular, MPEG-4 video streams are deemed to be a widespread traffic type. However, in the current IEEE 802.15.3 standard for media access control (MAC) of high-rate WPANs, the implementation details of some key issues such as scheduling and quality of service (QoS) provisioning have not been addressed. In this paper, we first propose a mathematical model for the optimal scheduling scheme for MPEG-4 flows in high-rate WPANs. We also propose an RL scheduler based on the reinforcement learning (RL) technique. Simulation results show that our proposed RL scheduler achieves nearly optimal performance and performs better than F-SRPT (Mangharam et al., 2004), EDD+SRPT (Torok et al., 2005), and PAP (Kim and Cho, 2005) scheduling algorithms in terms of a lower decoding failure rate.","1930-529X;1930529X","CD-ROM:978-1-4244-1043-9; POD:978-1-4244-1042-2","10.1109/GLOCOM.2007.144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4411054","","Communication system traffic control;MPEG 4 Standard;Mathematical model;Media Access Protocol;Optimal scheduling;Quality of service;Scheduling algorithm;Streaming media;Traffic control;Wireless personal area networks","personal area networks;scheduling;telecommunication traffic;video coding;video streaming;wireless sensor networks","MPEG-4 video streams;RL scheduler;high-rate wireless personal area network;real-time multimedia;reinforcement learning;scheduling algorithm;video traffic","","3","","22","","","","26-30 Nov. 2007","","IEEE","IEEE Conferences"
"An Effective Approach to Continuous User Authentication for Touch Screen Smart Devices","A. B. Buduru; S. S. Yau","Inf. Assurance Center, Sch. of Comput., Inf., & Decision Syst. Eng., Arizona State Univ., Tempe, AZ, USA","2015 IEEE International Conference on Software Quality, Reliability and Security","20150924","2015","","","219","226","Due to the rapid increase in the use of personal smart devices, more sensitive data is stored and viewed on these smart devices. This trend makes it easier for attackers to access confidential data by physically compromising (including stealing) these smart devices. Currently, most personal smart devices employ one of the one-time user authentication schemes, such as four-to-six digits, fingerprint or pattern-based schemes. These authentication schemes are often not good enough for securing personal smart devices because the attackers can easily extract all the confidential data from the smart device by breaking such schemes, or by keeping the authenticated session open on a physically compromised smart device. In addition, existing re-authentication or continuous authentication techniques for protecting personal smart devices use centralized architecture and require servers at a centralized location to train and update the learning model used for continuous authentication, which impose additional communication overhead. In this paper, an approach is presented to generating and updating the authentication model on the user's smart device with user's gestures, instead of a centralized server. There are two major advantages in this approach. One is that this approach continuously learns and authenticates finger gestures of the user in the background without requiring the user to provide specific gesture inputs. The other major advantage is to have better authentication accuracy by treating uninterrupted user finger gestures over a short time interval as a single gesture for continuous user authentication.","","Electronic:978-1-4673-7989-2; POD:978-1-4673-7990-8","10.1109/QRS.2015.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272936","Touch-screen smart devices;adaptive continuous user authentication;and user re-authentication;reinforcement learning","Accuracy;Authentication;Context;Fingers;Object recognition;Performance evaluation","authorisation;computer crime;gesture recognition;message authentication;smart phones;touch sensitive screens","attackers;authentication accuracy;authentication model;centralized server;confidential data access;continuous user authentication;finger gestures authentication;personal smart devices;sensitive data;touch screen smart devices;uninterrupted user finger gestures","","2","","19","","","","3-5 Aug. 2015","","IEEE","IEEE Conferences"
"Learning social relations for culture aware interaction","P. Patompak; S. Jeong; I. Nilkhamhang; N. Y. Chong","School of Information Science, Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Nomi, Ishikawa 923-1292, Japan","2017 14th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)","20170727","2017","","","26","31","Each person has their private physical and/or psychological area where they do not want to share with others during social interactions. This area gives them comfort about interactions and its size usually depends on various factors such as culture, personal traits, and acquaintanceship. This issue may also arise in case of human-robot interaction, especially when the robot is required to generate a socially competent interaction strategy toward people they are interacting with. Here, we propose a new robot exploration strategy to socially interact with people by considering the social relationship between the robot and each person. To that end, two definitions of interaction area are made: (1) Acceptable area allowed to be shared with other people and robots, and (2) Private area where a human does not want to be interfered by others. Based on these definitions, the robot can optimize the path to maximize the frequency/degree of visiting the acceptable area of each person and to minimize the frequency/degree of trespassing into the private area of them at the same time in an iterative way. In this paper, the social force model (SFM) of each person, based on the potential field concept, is designed by a fuzzy inference system and its parameter is optimized by the reinforcement learning model during interactions. We have shown that the proposed model can generate a suitable SFM of each person, which was quite similar to a ground truth model, allowing to plan a path to simultaneously optimize the two factors of interaction area, respectively.","","Electronic:978-1-5090-3056-9; POD:978-1-5090-3057-6","10.1109/URAI.2017.7992879","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7992879","","Adaptation models;Force;Human-robot interaction;Iron;Mathematical model;Navigation;Robots","fuzzy reasoning;human-robot interaction;learning (artificial intelligence);social aspects of automation","SFM;culture aware interaction;fuzzy inference system;human-robot interaction;potential field concept;reinforcement learning model;robot exploration strategy;social force model;social interactions;social relations;socially competent interaction strategy","","","","","","","","June 28 2017-July 1 2017","","IEEE","IEEE Conferences"
"Joint radio resource management for LTE-UMTS coexistence scenarios","N. Vučević; J. Pérez-Romero; O. Sallent; R. Agustí","Dept. TSC, Universitat Polit&#232;cnica de Catalunya (UPC), c/ Jordi Girona 1-3, 08034, Barcelona, Spain","2009 IEEE 20th International Symposium on Personal, Indoor and Mobile Radio Communications","20100415","2009","","","12","16","Constantly increasing demand for throughput and quality in wireless communication systems leads to continuous research of wise radio resource management, because of the scarce availability of frequency bands and the consequent capacity limitations. In addition, technology evolution is addressed towards spectral efficient techniques that can offer higher data rates. This is the case of OFDMA (Orthogonal Frequency-Division Multiple Access), introduced by 3GPP as the technology for future Long Term Evolution (LTE). However, given the current penetration of legacy technologies such as UMTS (Universal Mobile Telecommunications System), operators will have to deal with the coexistence of multiple Radio Access Technologies (RATs), so that the exploitation of the complementarities between technologies through Joint Radio Resource Management (JRRM) mechanisms will be needed. In this paper we propose a novel dynamic JRRM algorithm for LTE-UMTS coexistence scenarios. The proposed mechanism is based on Reinforcement Learning (RL) which is considered to be a good candidate to achieve cognition in future reconfigurable networks. The proposed solution implements autonomous RL agents in each base station which decide on the allocation of the most suitable RAT to each user. We give a detailed description of the solution and analyze the behavior under various load conditions. We also demonstrate the capability of the algorithm to adjust in dynamic scenarios.","2166-9570;21669570","Electronic:978-1-4244-5123-4; POD:978-1-4244-5122-7","10.1109/PIMRC.2009.5450181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5450181","LTE;WCDMA;joint radio resource management;reinforcement learning","3G mobile communication;Availability;Cognition;Heuristic algorithms;Learning;Long Term Evolution;Rats;Resource management;Throughput;Wireless communication","3G mobile communication;OFDM modulation;frequency division multiple access","3GPP;LTE-UMTS coexistence;OFDMA;joint radio resource management;multiple radio access technology;orthogonal frequency-division multiple access;reinforcement learning;spectral efficient technique;universal mobile telecommunications system;wireless communication system","","3","1","18","","","","13-16 Sept. 2009","","IEEE","IEEE Conferences"
"A Comparative Study of Parallel Reinforcement Learning Methods with a PC Cluster System","M. Kushida; K. Takahashi; H. Ueda; T. Miyahara","Hiroshima City University, Japan","2006 IEEE/WIC/ACM International Conference on Intelligent Agent Technology","20070108","2006","","","416","419","This paper presents a comparative study of three parallel implementation models for reinforcement learning. Two of them utilize Q-learning, and the other one utilizes fuzzy Q-learning for agent learning. In order to evaluate performance and validity of the three method, a PC (personal computer) cluster system consisting of 16 PCs connected via Gigabit ethernet has been built. For communications to deliver data among PCs, MPI (Message Passing Interface) is employed. Experimental results are compared with one another to show the performance and characteristics of the three methods.","","POD:0-7695-2748-5","10.1109/IAT.2006.3","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4052954","","Computer simulation;Ethernet networks;Learning systems;Master-slave;Message passing;Personal communication networks;Process control","fuzzy set theory;learning (artificial intelligence);message passing;parallel algorithms;workstation clusters","PC cluster system;agent learning;fuzzy Q-learning;gigabit Ethernet;message passing interface;parallel implementation model;parallel reinforcement learning","","2","","7","","","","18-22 Dec. 2006","","IEEE","IEEE Conferences"
"Comparing generic parameter controllers for EAs","G. Karafotias; M. Hoogendoorn; B. Weel","Computational Intelligence Group, VU University Amsterdam, The Netherlands","2014 IEEE Symposium on Foundations of Computational Intelligence (FOCI)","20150115","2014","","","46","53","Parameter controllers for Evolutionary Algorithms (EAs) deal with adjusting parameter values during an evolutionary run. Many ad hoc approaches have been presented for parameter control, but few generic parameter controllers exist and, additionally, no comparisons or in depth analyses of these generic controllers are available in literature. This paper presents an extensive comparison of such generic parameter control methods, including a number of novel controllers based on reinforcement learning which are introduced here. We conducted experiments with different EAs and test problems in an one-off setting, i.e. relatively long runs with controllers used out-of-the-box with no tailoring to the problem at hand. Results reveal several interesting insights regarding the effectiveness of parameter control, the niche applications/EAs, the effect of continuous treatment of parameters and the influence of noise and randomness on control.","","Electronic:978-1-4799-4491-0; POD:978-1-4799-4490-3","10.1109/FOCI.2014.7007806","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7007806","","Aerospace electronics;Computational intelligence;Estimation;Interpolation;Learning (artificial intelligence);Phase change random access memory;Silicon","control engineering computing;evolutionary computation;learning (artificial intelligence)","ad hoc approaches;depth analyses;evolutionary algorithms;generic parameter control methods;niche applications;reinforcement learning","","0","","24","","","","9-12 Dec. 2014","","IEEE","IEEE Conferences"
"Latent state models of primary user behavior for opportunistic spectrum access","J. Pajarinen; J. Peltonen; M. A. Uusitalo; A. Hottinen","Department of Information and Computer Science, Helsinki University of Technology, P.O. Box 5400, FI-02015 TKK, Finland","2009 IEEE 20th International Symposium on Personal, Indoor and Mobile Radio Communications","20100415","2009","","","1267","1271","Opportunistic spectrum access, where cognitive radio devices detect available unused radio channels and exploit them for communication, avoiding collisions with existing users of the channels, is a central topic of research for future wireless communication. When each device has limited resources to sense which channels are available, the task becomes a reinforcement learning problem that has been studied with partially observable Markov decision processes (POMDPs). However, current POMDP solutions are based on simplistic representations where channels are simply on/off (transmitting or idle). We show that more complicated Markov models where on/off states are part of complicated behavior of the channel owner (primary user) yield better POMDPs achieving more successful transmissions and less collisions.","2166-9570;21669570","Electronic:978-1-4244-5123-4; POD:978-1-4244-5122-7","10.1109/PIMRC.2009.5450037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5450037","","Chromium;Cognitive radio;Computer science;Interference;Internet telephony;Learning;Road accidents;Telecommunication traffic;Wireless communication;Wireless sensor networks","Markov processes;cognitive radio;learning (artificial intelligence);wireless channels","cognitive radio devices;latent state models;opportunistic spectrum access;partially observable Markov decision processes;primary user behavior;radio channels;reinforcement learning;wireless communication","","3","4","16","","","","13-16 Sept. 2009","","IEEE","IEEE Conferences"
"Spectrum markets for service provider spectrum trading with reinforcement learning","N. Abji; A. Leon-Garcia","University of Toronto, Canada","2011 IEEE 22nd International Symposium on Personal, Indoor and Mobile Radio Communications","20120126","2011","","","650","655","We present an auction-based spectrum market approach to service provider spectrum trading. Service providers buy and sell spectrum amongst one another in a spectrum market and simultaneously compete for customers from a common pool. Multi-agent reinforcement learning solutions are applied in both customer nodes and service providers to dynamically manage participation in the market. We outline four possible regulatory scenarios with varying degrees of flexibility and competition. Simulations demonstrate that the allocation of spectrum is efficient and fair. Customers and service providers of varying size are shown to benefit from this approach while the system spectrum efficiency is also significantly improved.","2166-9570;21669570","Electronic:978-1-4577-1348-4; POD:978-1-4577-1346-0; USB:978-1-4577-1347-7","10.1109/PIMRC.2011.6140043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140043","","Base stations;FCC;Learning;Radio spectrum management;Real time systems;Resource management;Wireless communication","cognitive radio;multi-agent systems;telecommunication services","customer providers;multiagent reinforcement learning solutions;reinforcement learning;service provider spectrum trading","","2","","11","","","","11-14 Sept. 2011","","IEEE","IEEE Conferences"
"Planning and learning algorithms for routing in Disruption-Tolerant Networks","M. O. Stehr; C. Talcott","SRI International, Computer Science Laboratory, Menlo Park, California 94025, USA","MILCOM 2008 - 2008 IEEE Military Communications Conference","20090119","2008","","","1","8","We give an overview of algorithms that we have been developing in the DARPA disruption-tolerant networking program, which aims at improving communication in networks with intermittent and episodic connectivity. Thanks to the use of network caching, this can be accomplished without the need for a simultaneous end-to-end path that is required by traditional Internet and mobile ad-hoc network (MANET) protocols. We employ a disciplined two-level approach that clearly distinguishes the dissemination of application content from the dissemination of network-related knowledge, each of which can be supported by different algorithms. Specifically, we present probabilisitc reflection, a single-message protocol enabling the dissemination of knowledge in strongly disrupted networks. For content dissemination, we present two approaches, namely a symbolic planning algorithm that exploits partially predictable temporal behavior, and a distributed and disruption-tolerant reinforcement learning algorithm that takes into account feedback about past performance.","2155-7578;21557578","POD:978-1-4244-2676-8","10.1109/MILCOM.2008.4753336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4753336","","Ad hoc networks;Costs;Disruption tolerant networking;IP networks;Mobile ad hoc networks;Personal digital assistants;Protocols;Routing;Satellites;Unmanned aerial vehicles","learning (artificial intelligence);protocols;telecommunication computing;telecommunication network planning;telecommunication network routing","DARPA;disruption-tolerant networks;probabilisitc reflection;reinforcement learning algorithm;routing;single-message protocol;symbolic planning algorithm","","10","","21","","","","16-19 Nov. 2008","","IEEE","IEEE Conferences"
"Personalized automatic image annotation based on reinforcement learning","Yabo Ni; Miao Zheng; Jiajun Bu; Chun Chen; Dazhou Wang","Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science, Zhejiang University, Hangzhou 310027, China","2013 IEEE International Conference on Multimedia and Expo (ICME)","20130926","2013","","","1","6","With the rapidly increasing number of personal image collections on the web, it is of great importance to annotate these user-uploaded images in personalized manner. But personalized image annotation is largely ignored by the mainstream of image annotation research. In this paper, we focus on personalizing the automatic image annotation by proposing a general framework which jointly exploits the generic content-based image annotation, personal image tagging history and the content of personal history images. In our framework, two sets of candidate annotations are extracted for each image based on content-based annotation and personal image tagging history. Considering that the user's interest may not stay the same, when exploiting the personal image tagging history, we also take the content of personal history images into account to avoid the noise. To get the final annotations, we propose an unsupervised algorithm based on reinforcement learning to combine the above two candidate annotation sets. Encouraging results show that the proposed framework is effective and promising for personalizing automatic image annotation.","1945-7871;19457871","Electronic:978-1-4799-0015-2; POD:978-1-4799-0014-5; USB:978-1-4799-0013-8","10.1109/ICME.2013.6607456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607456","Automatic image annotation;Personal image tagging history;Personalization;Reinforcement learning","History;Learning (artificial intelligence);Noise;Semantics;Tagging;Unsupervised learning;Vocabulary","Internet;image classification;image retrieval;learning (artificial intelligence)","Web;generic content-based image annotation;personal image collections;personal image tagging history;personalized automatic image annotation;reinforcement learning;user-uploaded images","","0","","14","","","","15-19 July 2013","","IEEE","IEEE Conferences"
"Context-aware reinforcement learning-based mobile cloud computing for telemonitoring","X. Wang; W. Wang; Z. Jin","Department of Electrical and Computer Engineering, Binghamton University, State University of New York, Binghamton, NY, 13902 USA","2018 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI)","20180409","2018","","","426","429","Mobile cloud computing (MCC) has been extensively studied to provide pervasive healthcare services in a more affordable manner. Through offloading computation-intensive tasks from mobile to cloud, a significant portion of energy can be saved to extend the mobile battery life, which is critical to maintaining continuous and uninterrupted healthcare services. However, given the ever-changing clinical severity, personal demands, and environmental conditions, it is essential to explore context-aware approach capable of dynamically determining the optimal task offloading strategies and algorithmic settings, with the goal of achieving a balanced trade-off among energy efficiency, diagnostic accuracy, and processing latency. To this aim, we propose a model-free reinforcement learning based task scheduling approach to adapt to the changing requirements.","","Electronic:978-1-5386-2405-0; POD:978-1-5386-2406-7","10.1109/BHI.2018.8333459","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8333459","","Batteries;Cloud computing;Electrocardiography;Hidden Markov models;Medical services;Monitoring;Task analysis","cloud computing;health care;learning (artificial intelligence);mobile computing;scheduling","affordable manner;context-aware approach capable;context-aware reinforcement learning;continuous healthcare services;mobile battery life;mobile cloud;model-free reinforcement learning;offloading computation-intensive tasks;optimal task offloading strategies;pervasive healthcare services;task scheduling approach;uninterrupted healthcare services","","","","","","","","4-7 March 2018","","IEEE","IEEE Conferences"
"Automatically Learning User Preferences for Personalized Service Composition","Y. Zhao; S. Wang; Y. Zou; J. Ng; T. Ng","Queen's Univ., Kingston, ON, Canada","2017 IEEE International Conference on Web Services (ICWS)","20170911","2017","","","776","783","With the rapid growth of the web services technologies, users often leverage various web services to perform their daily activities, such as on-line shopping. Due to the massive amount of web services available, a user faces numerous choices to meet their personal preferences when selecting the desired services from the web services with the similar functionality. Therefore, it becomes tedious and cumbersome tasks for users to discover and compose services. To reduce user's cognitive burden, it is critical to support automated service composition and make efficient recommendation of personalized services to achieve user's overall goals. However, existing approaches only offer users with limited options designed for the interest of a group of users without considering individual users' interests. To allow users to compose personalized services without much manual specification, we propose a machine learning approach that applies a learning-to-rank algorithm, RankBoost, to automatically learn user preferences and the prioritization of the preferences from users' historical data. Moreover, our approach uses the multi-objective reinforcement learning (MORL) algorithm to make trade-offs among user preferences and recommends a collection of services to achieve the highest objective. We conduct an empirical study to evaluate our approach by collecting the historical data from 12 subjects. The results demonstrate that our approach outperforms the two well-established baseline approaches by 100%-200% in terms of precision on recommending services.","","Electronic:978-1-5386-0752-7; POD:978-1-5386-0753-4","10.1109/ICWS.2017.93","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029835","","Data mining;Feature extraction;History;Learning (artificial intelligence);Machine learning algorithms;Time-frequency analysis;Web services","Internet;Web services;learning (artificial intelligence);recommender systems;retail data processing","MORL algorithm;RankBoost;Web service technologies;automated service composition;automatic user preference learning;learning-to-rank algorithm;machine learning approach;multiobjective reinforcement learning algorithm;online shopping;personalized service composition;service recommendation","","","","","","","","25-30 June 2017","","IEEE","IEEE Conferences"
"Hello! Mr. Sage","O. E. Lancaster","","IEEE Transactions on Education","20071112","1976","19","2","54","58","Mr. Sage, an ideal Professor, can motivate, evoke involvement, and supply rewards and reinforcement, for each individual student all within the same class period. Although his minimum standards require mastery of concepts at all levels of learning, from simple memory to creativity, he does not expect the same performance of all students. His grades are directly related to the breadth of subject matter and depth of mastery. He personalizes his instruction within the classroom and in homework assignments by a range in the difficulty of illustration, thereby challenging each student to his capacity. He uses tests and examinations as learning experiences. To achieve this he combines the excellent feature of the conventional system and the current views on personalized system of instruction.","0018-9359;00189359","","10.1109/TE.1976.4321037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4321037","","Engineering education;Eyes;History;Testing;Vehicle dynamics;Vehicles","","","","0","","15","","","","May 1976","","IEEE","IEEE Journals & Magazines"
"Introduce the ""Flipped Classroom"" into the Basic Medical Experimental Teaching by the Medical Virtual Simulation Platform","G. Zhenying; B. Huiling; W. Guoying","Sch. of Med., Henan Univ., Kaifeng, China","2015 7th International Conference on Information Technology in Medicine and Education (ITME)","20160310","2015","","","484","487","Based on drawing ""flipped classroom"" experience of practitioners domestic and foreign, together with the full use of modern information tools and the reinforcement of the construction of discipline support system, we build the medical virtual simulation platform. On this basis, we perform reasonable instructional design and good application of ""flipped classroom"" in medical experimental teaching. These efforts adapt the learning requirements of students at different levels, and can enhance students' self-learning ability. These efforts can also train students' creative and independent thinking ability, achieve personalized learning, and improve the quality of medical students' experimental teaching. Construction of medical virtual simulation platform in the school of medicine not only provides a new method for basic medical experimental teaching to enable introduction of ""flipped classroom"" into basic medical experimental teaching, but also promotes implementation of information education and improves the teaching quality of professional disciplines.","","CD-ROM:978-1-4673-8301-1; Electronic:978-1-4673-8302-8; POD:978-1-4673-8303-5","10.1109/ITME.2015.172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429195","""flipped classroom"";basic medicine;experimental teaching","Adaptation models;Computational modeling;Computers;Education;Online services;Solid modeling;Videos","computer aided instruction;educational institutions;human factors;learning (artificial intelligence);medical computing;teaching","""Flipped Classroom"";information education;medical experimental teaching;medical virtual simulation platform;modern information tool","","","","9","","","","13-15 Nov. 2015","","IEEE","IEEE Conferences"
"Work in progress — Tools and technology to implement a students personal laboratory","M. Walters","Academic Product Manager, National Instruments","2011 Frontiers in Education Conference (FIE)","20120202","2011","","","F1G-1","F1G-2","Today's students want to solve problems and experience engineering regardless of where they are - in lecture, in the laboratory, or the dorm room. Professors want to provide a hands-on learning experience to empower students who want to tinker, experiment, and explore concepts while improving the comprehension through reinforcement. Student access to affordable, low-cost technology enables educators to address limitations in the laboratory, including access to equipment, time on task, and cost. With a portable laboratory, a student can learn concepts in their preferred environments and provides a supplement to the traditional lecture and laboratory based courses.","0190-5848;01905848","Electronic:978-1-61284-469-5; POD:978-1-61284-468-8; USB:978-1-61284-467-1","10.1109/FIE.2011.6143112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143112","hands-on;myDAQ;student-laboratory;student-ownership","Data acquisition;Educational institutions;Hardware;Instruments;Laboratories;Nickel;Signal generators","data acquisition;educational technology;engineering education;laboratory techniques;portable instruments;virtual instrumentation","equipment access;hands-on learning experience;low-cost educational technology;myDAQ;portable laboratory;students personal laboratory;work in progress","","1","","3","","","","12-15 Oct. 2011","","IEEE","IEEE Conferences"
"Implications of decentralized Q-learning resource allocation in wireless networks","F. Wilhelmi; B. Bellalta; C. Cano; A. Jonsson","Wireless Networking (WN-UPF), Universitat Pompeu Fabra, Barcelona, Spain","2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)","20180215","2017","","","1","5","Reinforcement Learning is gaining attention by the wireless networking community due to its potential to learn good-performing configurations only from the observed results. In this work we propose a stateless variation of Q-learning, which we apply to exploit spatial reuse in a wireless network. In particular, we allow networks to modify both their transmission power and the channel used solely based on the experienced throughput. We concentrate in a completely decentralized scenario in which no information about neighbouring nodes is available to the learners. Our results show that although the algorithm is able to find the best-performing actions to enhance aggregate throughput, there is high variability in the throughput experienced by the individual networks. We identify the cause of this variability as the adversarial setting of our setup, in which the most played actions provide intermittent good/poor performance depending on the neighbouring decisions. We also evaluate the effect of the intrinsic learning parameters of the algorithm on this variability.","","Electronic:978-1-5386-3531-5; POD:978-1-5386-3532-2; Paper:978-1-5386-3529-2; USB:978-1-5386-3530-8","10.1109/PIMRC.2017.8292321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8292321","","Aggregates;Interference;Meters;Resource management;Signal to noise ratio;Throughput;Wireless networks","learning (artificial intelligence);radio networks;resource allocation","Reinforcement Learning;decentralized Q-learning resource allocation;wireless network;wireless networking community","","","","","","","","8-13 Oct. 2017","","IEEE","IEEE Conferences"
"Dynamic power control in Wireless Body Area Networks using reinforcement learning with approximation","R. Kazemi; R. Vesilo; E. Dutkiewicz; Ren Liu","Department of Electronic Engineering, Macquarie University, Sydney, Australia","2011 IEEE 22nd International Symposium on Personal, Indoor and Mobile Radio Communications","20120126","2011","","","2203","2208","A Wireless Body Area Network (WBAN) is made up of multiple tiny physiological sensors implanted in/on the human body with each sensor equipped with a wireless transceiver that communicates to a coordinator in a star topology. Energy is the scarcest resource in WBANs. Power control mechanisms to achieve a certain level of utility while using as little power for transmission as possible can play an important role in reducing energy consumption in such very energy-constrained networks. In this paper, we propose a novel power controller to mitigate internetwork interference in WBANs and increase the maximum achievable throughput with the minimum energy consumption. The proposed power controller employs reinforcement learning with approximation to learn from the environment and improve its performance. We compare the performance of the proposed controller to two other power controllers, one based on game theory and the other one based on fuzzy logic. Simulation results show that compared to the other two approaches, RLPC provides a substantial saving in energy consumption per bit, with a substantial increase in network lifetime.","2166-9570;21669570","Electronic:978-1-4577-1348-4; POD:978-1-4577-1346-0; USB:978-1-4577-1347-7","10.1109/PIMRC.2011.6139908","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139908","Dynamic Power Control;Fuzzy Logic;Game Theory;Interference;Reinforcement Learning;WBAN","Approximation methods;Energy consumption;Interference;Power control;Sensors;Throughput;Wireless sensor networks","approximation theory;body area networks;control engineering computing;fuzzy control;game theory;interference suppression;learning (artificial intelligence);power control;radio transceivers;telecommunication control;telecommunication network reliability","WBAN;approximation;dynamic power control;energy consumption reduction;energy-constrained networks;fuzzy logic;game theory;interference mitigation;network lifetime;physiological sensors;reinforcement learning;wireless body area networks;wireless transceiver","","6","","17","","","","11-14 Sept. 2011","","IEEE","IEEE Conferences"
"User centered and context dependent personalization through experiential transcoding","S. Ferretti; S. Mirri; C. Prandi; P. Salomoni","Department of Computer Science and Engineering University of Bologna Bologna, Italy","2014 IEEE 11th Consumer Communications and Networking Conference (CCNC)","20141103","2014","","","486","491","The pervasive presence of devices exploited to use and deliver entertainment text-based content can make reading easier to get but more difficult to enjoy, in particular for people with reading-related disabilities. The main solutions that allow overcoming some of the difficulties experienced by users with specific and special needs are based on content adaptation and user profiling. This paper presents a system that aims to improve content legibility by exploiting experiential transcoding techniques. The system we propose tracks users' behaviors so as to provide a tailored adaptation of textual content, in order to fit the needs of a specific user on the several different devices she/he actually uses.","2331-9852;23319852","Electronic:978-1-4799-2355-7; POD:978-1-4799-2357-1","10.1109/CCNC.2014.6940520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6940520","content adaptation;device capabilities;legibility;reinforcement learning;user profiling","Adaptation models;Context;Entertainment industry;Multimedia communication;Prototypes;Transcoding;Web pages","Web sites;assisted living;human factors;text analysis;transcoding;ubiquitous computing","content adaptation;content legibility improvement;context dependent personalization;entertainment text-based content;transcoding techniques;user centered personalization;user profiling","","4","","23","","","","10-13 Jan. 2014","","IEEE","IEEE Conferences"
"Concept maps and learning objects","L. I. Navarro; M. M. Such; D. M. Martin; C. P. Sancho; P. P. Peco","Dept. of Comput. Sci., Agrarian Univ. of Havana, Cuba","Fifth IEEE International Conference on Advanced Learning Technologies (ICALT'05)","20050919","2005","","","263","265","Concept maps constitute one of the tools frequently used in learning management as they offer the possibility to personalize learning, share knowledge and reinforce learning to learn skills. At the same time, many initiatives or standards are being developed rapidly to make the contents in different learning management systems and learning environments compatible. This paper states the need to combine the technique of Concept Maps with initiatives that package contents developed by IMS to produce more portable and powerful content. A model to create tools for learning management is proposed.","2161-3761;21613761","POD:0-7695-2338-2","10.1109/ICALT.2005.90","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1508670","","Computer science;Content management;Education;Environmental management;Knowledge management;Packaging;Power system management;Power system modeling;Psychology;Standards development","computer aided instruction;educational administrative data processing","concept maps;knowledge sharing;learning environments;learning management systems;learning objects;learning personalization;learning reinforcement;skills learning","","1","","8","","","","5-8 July 2005","","IEEE","IEEE Conferences"
"GongBroker: A Broker Model for Power Trading in Smart Grid Markets","X. Wang; M. Zhang; F. Ren; T. Ito","Sch. of Comput. & Inf. Technol., Univ. of Wollongong, Wollongong, NSW, Australia","2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)","20160204","2015","2","","21","24","The Smart Grid market is dynamic and complex, and brokers are widely introduced to better manage this market. This paper proposes an intelligent broker model - GongBroker, with smart trading strategies to cope with the dynamics and complexity. GongBroker first predicts the short-term demands of various consumers, and then buys energy from the wholesale market through auctions, and sells energy to various consumers in the retail market. In order to predict the customer demand, a data-driven method is proposed. All the consumers are hierarchically clustered according to their historical energy consumptions, and different prediction methods are tailored for different customer clusters to predict one-day-ahead hourly energy demand. Based on the predicted demand, the GongBroker employs a Markov Decision Process for the one-day-ahead auction in the wholesale market. To compete with other brokers, GongBroker uses independent reinforcement learning processes to optimize prices for different types of consumers. Experimental results demonstrate that GongBroker not only is competitive in making profit, but also keeps a good supply-demand balance.","","Electronic:978-1-4673-9618-9; POD:978-1-4673-9619-6","10.1109/WI-IAT.2015.108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7397309","Broker Model;Data-driven;Reinforcement Learning;Smart Grid Market","Adaptation models;Electronic mail;Energy consumption;Mathematical model;Prediction algorithms;Prediction methods;Smart grids","Markov processes;demand side management;learning (artificial intelligence);power markets;smart power grids;supply and demand","GongBroker;Markov decision process;consumers short-term demands;data-driven method;day-ahead hourly energy demand;independent reinforcement learning processes;intelligent broker model;power trading;prediction methods;smart grid markets;smart trading strategies;supply-demand balance","","1","","6","","","","6-9 Dec. 2015","","IEEE","IEEE Conferences"
"Attention-Aware Face Hallucination via Deep Reinforcement Learning","Q. Cao; L. Lin; Y. Shi; X. Liang; G. Li","Sch. of Data & Comput. Sci., Sun Yat-sen Univ., Guangzhou, China","2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20171109","2017","","","1656","1664","Face hallucination is a domain-specific super-resolution problem with the goal to generate high-resolution (HR) faces from low-resolution (LR) input images. In contrast to existing methods that often learn a single patch-to-patch mapping from LR to HR images and are regardless of the contextual interdependency between patches, we propose a novel Attention-aware Face Hallucination (Attention-FH) framework which resorts to deep reinforcement learning for sequentially discovering attended patches and then performing the facial part enhancement by fully exploiting the global interdependency of the image. Specifically, in each time step, the recurrent policy network is proposed to dynamically specify a new attended region by incorporating what happened in the past. The state (i.e., face hallucination result for the whole image) can thus be exploited and updated by the local enhancement network on the selected region. The Attention-FH approach jointly learns the recurrent policy network and local enhancement network through maximizing the long-term reward that reflects the hallucination performance over the whole image. Therefore, our proposed Attention-FH is capable of adaptively personalizing an optimal searching path for each face image according to its own characteristic. Extensive experiments show our approach significantly surpasses the state-of-the-arts on in-the-wild faces with large pose and illumination variations. The state (i.e., face hallucination result for the whole image) can thus be exploited and updated by the local enhancement network on the selected region. The Attention-FH approach jointly learns the recurrent policy network and local enhancement network through maximizing the long-term reward that reflects the hallucination performance over the whole image. Therefore, our proposed Attention-FH is capable of adaptively personalizing an optimal searching path for each face image according to its own characteristic. Extensive experiments show our appro- ch significantly surpasses the state-of-the-arts on in-the-wild faces with large pose and illumination variations.","1063-6919;10636919","Electronic:978-1-5386-0457-1; POD:978-1-5386-0458-8","10.1109/CVPR.2017.180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8099663","","Computer vision;Face;History;Image resolution;Learning (artificial intelligence);Neural networks","face recognition;image enhancement;image resolution;learning (artificial intelligence)","attention-FH approach;attention-aware face hallucination;deep reinforcement learning;domain-specific super-resolution problem;facial part enhancement;hallucination performance;high-resolution face generation;in-the-wild faces;local enhancement network;low-resolution input images;optimal searching path;patch-to-patch mapping;recurrent policy network","","1","","","","","","21-26 July 2017","","IEEE","IEEE Conferences"
"Data-driven inverse learning of passenger preferences in urban public transits","G. Wu; Y. Ding; Y. Li; J. Luo; F. Zhang; J. Fu","Worcester Polytechnic Institute (WPI), 100 Institute Road, Worcester, MA 01609, USA","2017 IEEE 56th Annual Conference on Decision and Control (CDC)","20180122","2017","","","5068","5073","Urban public transit planning is crucial in reducing traffic congestion and enabling green transportation. However, there is no systematic way to integrate passengers' personal preferences in planning public transit routes and schedules so as to achieve high occupancy rates and efficiency gain of ride-sharing. In this paper, we take the first step tp exact passengers' preferences in planning from history public transit data. We propose a data-driven method to construct a Markov decision process model that characterizes the process of passengers making sequential public transit choices, in bus routes, subway lines, and transfer stops/stations. Using the model, we integrate softmax policy iteration into maximum entropy inverse reinforcement learning to infer the passenger's reward function from observed trajectory data. The inferred reward function will enable an urban planner to predict passengers' route planning decisions given some proposed transit plans, for example, opening a new bus route or subway line. Finally, we demonstrate the correctness and accuracy of our modeling and inference methods in a large-scale (three months) passenger-level public transit trajectory data from Shenzhen, China. Our method contributes to smart transportation design and human-centric urban planning.","","Electronic:978-1-5090-2873-3; POD:978-1-5090-2874-0; USB:978-1-5090-2872-6","10.1109/CDC.2017.8264410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8264410","","Data models;Entropy;Markov processes;Planning;Public transportation;Roads;Trajectory","Markov processes;data analysis;entropy;learning (artificial intelligence);town and country planning;traffic engineering computing;transportation","China;Markov decision process model;Shenzhen;bus route;bus routes;data-driven inverse learning;data-driven method;enabling green transportation;green transportation;high occupancy rates;history public transit data;human-centric urban planning;inference methods;inferred reward function;large-scale passenger-level public transit trajectory data;maximum entropy inverse reinforcement;observed trajectory data;public transit routes;sequential public transit choices;smart transportation design;traffic congestion;transit plans;urban planner;urban public transit planning","","","","","","","","12-15 Dec. 2017","","IEEE","IEEE Conferences"
"Personalized Course Sequence Recommendations","J. Xu; T. Xing; M. van der Schaar","Department of Electrical and Computer Engineering, University of Miami, Coral Gables, FL, USA","IEEE Transactions on Signal Processing","20160824","2016","64","20","5340","5352","Given the variability in student learning, it is becoming increasingly important to tailor courses as well as course sequences to student needs. This paper presents a systematic methodology for offering personalized course sequence recommendations to students. First, a forward-search backward-induction algorithm is developed that can optimally select course sequences to decrease the time required for a student to graduate. The algorithm accounts for prerequisite requirements (typically present in higher level education) and course availability. Second, using the tools of multiarmed bandits, an algorithm is developed that can optimally recommend a course sequence that both reduces the time to graduate while also increasing the overall GPA of the student. The algorithm dynamically learns how students with different contextual backgrounds perform for given course sequences and, then, recommends an optimal course sequence for new students. Using real-world student data from the UCLA Mechanical and Aerospace Engineering Department, we illustrate how the proposed algorithms outperform other methods that do not include student contextual information when making course sequence recommendations.","1053-587X;1053587X","","10.1109/TSP.2016.2595495","10.13039/100000001 - National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7524023","Personalized education;contextual bandits;course sequence recommendation;dynamic programming","Adaptive systems;Aerospace engineering;Complexity theory;Education;Electronic mail;Heuristic algorithms;Signal processing algorithms","aerospace computing;computer aided instruction;educational courses;engineering education;inference mechanisms;mechanical engineering computing;recommender systems","GPA;UCLA Mechanical and Aerospace Engineering Department;contextual backgrounds;course availability;forward-search backward-induction algorithm;multiarmed bandit tool;personalized course sequence recommendations;prerequisite requirements;student learning;systematic methodology","","2","","","","","20160727","Oct.15, 15 2016","","IEEE","IEEE Journals & Magazines"
"Policy search for learning robot control using sparse data","B. Bischoff; D. Nguyen-Tuong; H. van Hoof; A. McHutchon; C. E. Rasmussen; A. Knoll; J. Peters; M. P. Deisenroth","Cognitive Systems, Bosch Corporate Research, Germany","2014 IEEE International Conference on Robotics and Automation (ICRA)","20140929","2014","","","3882","3887","In many complex robot applications, such as grasping and manipulation, it is difficult to program desired task solutions beforehand, as robots are within an uncertain and dynamic environment. In such cases, learning tasks from experience can be a useful alternative. To obtain a sound learning and generalization performance, machine learning, especially, reinforcement learning, usually requires sufficient data. However, in cases where only little data is available for learning, due to system constraints and practical issues, reinforcement learning can act suboptimally. In this paper, we investigate how model-based reinforcement learning, in particular the probabilistic inference for learning control method (Pilco), can be tailored to cope with the case of sparse data to speed up learning. The basic idea is to include further prior knowledge into the learning process. As Pilco is built on the probabilistic Gaussian processes framework, additional system knowledge can be incorporated by defining appropriate prior distributions, e.g. a linear mean Gaussian prior. The resulting Pilco formulation remains in closed form and analytically tractable. The proposed approach is evaluated in simulation as well as on a physical robot, the Festo Robotino XT. For the robot evaluation, we employ the approach for learning an object pick-up task. The results show that by including prior knowledge, policy learning can be sped up in presence of sparse data.","1050-4729;10504729","Electronic:978-1-4799-3685-4; POD:978-1-4799-3686-1; USB:978-1-4799-3684-7","10.1109/ICRA.2014.6907422","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6907422","","Computational modeling;Data models;Grasping;Heuristic algorithms;Pneumatic systems;Robots;Valves","Gaussian distribution;Gaussian processes;learning (artificial intelligence);learning systems;probability;robots","Festo Robotino XT;PILCO;dynamic environment;generalization performance;grasping;learning robot control;linear mean Gaussian prior distribution;machine learning;manipulation;model-based reinforcement learning;object pick-up task;physical robot;policy learning;policy search;probabilistic Gaussian processes framework;probabilistic inference for learning control method;sound learning;sparse data;system constraints;uncertain environment","","3","","14","","","","May 31 2014-June 7 2014","","IEEE","IEEE Conferences"
"Teaching mentoring program for the application of active methodologies and ICT tools","N. E. Salazar","Educational Innovation Deparment, Tecsup, Trujillo, Per&#x00FA;","2017 IEEE Frontiers in Education Conference (FIE)","20171214","2017","","","1","6","This research is characterized by the effectiveness of the Teaching Mentoring Program for the application of active methodologies and ICT tools in Tecsup Norte teachers, Trujillo, Peru. A goal was set for the three locations of Tecsup Norte (Trujillo, City), Tecsup Centro (Lima, City), Tecsup Sur (Arequipa, City), to achieve that in 2016, 80%, apply at least one active methodology per semester and make use of ICT tools. The result for Tecsup Norte after completing the first semester was that 28% of teachers used an active methodology and 16% used ICT tools. At the beginning of the 2016 II semester, the Teaching Mentoring Program was designed, consisting of five stages: 1. first contact and interview with the teacher, 2. personalized training, 3. programming and revision of class material, 4. accompaniment in classroom and evaluation by rubrics, and 5. feedforward and recognition. For this, psychological strategies such as rapport, positive reinforcement and behavioral modeling were adapted, as well as the tool of coaching feedforward and evaluation rubrics were designed for the application of active methodologies. To guarantee the effectiveness of the program was a team of two specialists in active methodologies and ICT tools. Work schedule and control of man hours was approached using Gantt diagram. The results of the implementation of the program are relevant, since 95% of the teachers of the semester 2016 II applied active methodologies and ICT tools; in comparison to the result of the 2016 I semester. Tecsup Norte becomes the first site that exceeds the established goal of 80% in application of active methodologies and ICT tools, through the Teaching Mentoring program, the other venues that did not apply the Teaching Mentoring Program were: Tecsup Centro 62% and Tecsup Sur 50%. This educational innovation contributes a program for the training, accompaniment and evaluation of the teaching performance through active methodologies, such as: Flipped le- rning, case-based learning, problem-based learning, guided-learning project and the use of ICT tools.","","Electronic:978-1-5090-5920-1; POD:978-1-5090-4920-2; USB:978-1-5090-5919-5","10.1109/FIE.2017.8190607","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8190607","ICT tools;Mentoring Teaching;active methodologies;evaluation rubrics;feedforward;personalized training","Google;Interviews;Mentoring;Technological innovation;Tools;Urban areas","computer aided instruction;teaching","Arequipa City;Gantt diagram;ICT tools;Lima City;Peru;Teaching Mentoring Program;Teaching Mentoring program;Teaching mentoring program;Tecsup Centro;Tecsup Norte teachers;Tecsup Sur;Trujillo","","","","","","","","18-21 Oct. 2017","","IEEE","IEEE Conferences"
"Q-learning based power control algorithm for D2D communication","S. Nie; Z. Fan; M. Zhao; X. Gu; L. Zhang","Key Lab of Universal Wireless Communication, Beijing University of Posts and Telecommunications, Beijing, China 100876","2016 IEEE 27th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)","20161222","2016","","","1","6","In this paper, reinforcement learning (RL) based power control algorithm in underlay D2D communication is studied. The approach we use regards D2D communication as a multi-agents system, and power control is achieved by maximizing system capacity while maintaining the requirement of quality of service(QoS) from cellular users. We propose two RL based power control methods for D2D users, i.e., team-Q learning and distributed-Q learning. The former is a centralized method in which only one Q-value table needs to be maintained, while the latter enables D2D users to learn independently and reduces the complexity of Q-value table. Simulation results show the difference of the two Q-learning algorithm in terms of convergence and reward function. In addition, it is shown that through our distributed-Q learning, D2D users not only are able to learn their power in a self-organized way, but also achieve better system performance than that using traditional method in LTE(Long Term Evolution).","","Electronic:978-1-5090-3254-9; POD:978-1-5090-3255-6","10.1109/PIMRC.2016.7794793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7794793","D2D communication;Q-learning;Reinforcement learning;multi-agent system;power control","Algorithm design and analysis;Device-to-device communication;Interference;Power control;Quality of service;Radio transmitters;Signal to noise ratio","Long Term Evolution;learning (artificial intelligence);multi-agent systems;power control;quality of service;telecommunication control","LTE;Long Term Evolution;Q-learning based power control algorithm;QoS;cellular users;distributed-Q learning;multiagents system;quality of service;reinforcement learning;system capacity;underlay D2D communication","","","","","","","","4-8 Sept. 2016","","IEEE","IEEE Conferences"
"MOSAIC for Multiple-Reward Environments","N. Sugimoto; M. Haruno; K. Doya; M. Kawato","Center for Information and Neural Networks, National Institute of Information and Communications Technology, Kyoto 619-0288, Japan, and Department of Brain Robot Interface, Brain Information Communication Research Laboratory Group, ATR, Kyoto 619-0288, Japan xsugi@nict.go.jp","Neural Computation","20140519","2012","24","3","577","606","Reinforcement learning (RL) can provide a basic framework for autonomous robots to learn to control and maximize future cumulative rewards in complex environments. To achieve high performance, RL controllers must consider the complex external dynamics for movements and task (reward function) and optimize control commands. For example, a robot playing tennis and squash needs to cope with the different dynamics of a tennis or squash racket and such dynamic environmental factors as the wind. In addition, this robot has to tailor its tactics simultaneously under the rules of either game. This double complexity of the external dynamics and reward function sometimes becomes more complex when both the multiple dynamics and multiple reward functions switch implicitly, as in the situation of a real (multi-agent) game of tennis where one player cannot observe the intention of her opponents or her partner. The robot must consider its opponent's and its partner's unobservable behavioral goals (reward function). In this article, we address how an RL agent should be designed to handle such double complexity of dynamics and reward. We have previously proposed modular selection and identification for control (MOSAIC) to cope with nonstationary dynamics where appropriate controllers are selected and learned among many candidates based on the error of its paired dynamics predictor: the forward model. Here we extend this framework for RL and propose MOSAIC-MR architecture. It resembles MOSAIC in spirit and selects and learns an appropriate RL controller based on the RL controller's TD error using the errors of the dynamics (the forward model) and the reward predictors. Furthermore, unlike other MOSAIC variants for RL, RL controllers are not a priori paired with the fixed predictors of dynamics and rewards. The simulation results demonstrate that MOSAIC-MR outperforms other counterparts because of this flexible association ability among RL controllers, forward models, and reward pr- dictors.","0899-7667;08997667","","10.1162/NECO_a_00246","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6797532","","","","","","2","","","","","","March 2012","","MIT Press","MIT Press Journals"
"A Recursive Dialogue Game for Personalized Computer-Aided Pronunciation Training","P. h. Su; C. h. Wu; L. s. Lee","Department of Engineering, University of Cambridge, Cambridge, United Kingdom","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20150114","2015","23","1","127","141","Learning languages in addition to the native language is very important for all people in the globalized world today, and computer-aided pronunciation training (CAPT) is attractive since the software can be used anywhere at any time, and repeated as many times as desired. In this paper, we introduce the immersive interaction scenario offered by spoken dialogues to CAPT by proposing a recursive dialogue game to make CAPT personalized. A number of tree-structured sub-dialogues are linked sequentially and recursively as the script for the game. The system policy at each dialogue turn is to select in real-time along the dialogue the best training sentence for each specific individual learner within the dialogue script, considering the learner's learning status and the future possible dialogue paths in the script, such that the learner can have the scores for all pronunciation units considered reaching a predefined standard in a minimum number of turns. The purpose here is that those pronunciation units poorly produced by the specific learner can be offered with more practice opportunities in the future sentences along the dialogue, which enables the learner to improve the pronunciation without having to repeat the same training sentences many times. This makes the learning process for each learner completely personalized. The dialogue policy is modeled by Markov decision process (MDP) with high-dimensional continuous state space, and trained with fitted value iteration using a huge number of simulated learners. These simulated leaners have the behavior similar to real learners, and were generated from a corpus of real learner data. The experiments demonstrated very promising results and a real cloud-based system is also successfully implemented.","2329-9290;23299290","","10.1109/TASLP.2014.2375572","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6971195","Computer-aided pronunciation training (CAPT);Markov decision process;computer-assisted language learning;dialogue game;reinforcement learning","Computers;Games;Markov processes;Software;Speech;Speech processing;Training","Markov processes;cloud computing;computer based training;computer games;interactive systems;linguistics;real-time systems","MDP;Markov decision process;dialogue paths;dialogue policy;dialogue script;fitted value iteration;game script;high-dimensional continuous state space;immersive interaction scenario;language learning;learner learning status;native language;personalized CAPT;personalized computer-aided pronunciation training;pronunciation improvement;pronunciation unit scores;real cloud-based system;real learners;recursive dialogue game;sequentially-recursively linked tree-structured subdialogues;simulated leaners;spoken dialogues;system policy;training sentence","","2","","52","","","20141202","Jan. 2015","","IEEE","IEEE Journals & Magazines"
"Video Annotation Through Search and Graph Reinforcement Mining","E. Moxley; T. Mei; B. S. Manjunath","University of California at Santa Barbara, California, U.S.A.","IEEE Transactions on Multimedia","20100315","2010","12","3","184","193","Unlimited vocabulary annotation of multimedia documents remains elusive despite progress solving the problem in the case of a small, fixed lexicon. Taking advantage of the repetitive nature of modern information and online media databases with independent annotation instances, we present an approach to automatically annotate multimedia documents that uses mining techniques to discover new annotations from similar documents and to filter existing incorrect annotations. The annotation set is not limited to words that have training data or for which models have been created. It is limited only by the words in the collective annotation vocabulary of all the database documents. A graph reinforcement method driven by a particular modality (e.g., visual) is used to determine the contribution of a similar document to the annotation target. The graph supplies possible annotations of a different modality (e.g., text) that can be mined for annotations of the target. Experiments are performed using videos crawled from YouTube. A customized precision-recall metric shows that the annotations obtained using the proposed method are superior to those originally existing for the document. These extended, filtered tags are also superior to a state-of-the-art semi-supervised technique for graph reinforcement learning on the initial user-supplied annotations.","1520-9210;15209210","","10.1109/TMM.2010.2041101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5398917","Data mining;graph theory;video annotation;video content analysis","","data mining;document handling;learning (artificial intelligence);video retrieval","annotations discovery;collective annotation vocabulary;customized precision recall metric;database documents;graph reinforcement mining;multimedia documents annotation;online media database;video annotation","","28","","36","","","20100126","April 2010","","IEEE","IEEE Journals & Magazines"
"Performance improvements in schools with Adaptive Learning and Assessment","R. Raman; P. Nedungadi","School of Engineering, Amrita Vishwa Vidyapeetham, India","2010 4th International Conference on Distance Learning and Education","20101021","2010","","","10","14","This paper presents Amrita Learning, a web-based, multimedia-enabled, Adaptive Assessment and Learning System for schools. Computer-based adaptive assessments aim to use an optimal and individualized assessment path to determine the knowledge level of students. The new goal for adaptive assessment is based on educational outcomes, which describe what learners must be able to do as a result of items studied. Assessment based on outcomes creates the initial roadmap for the educational model, ensuring that students are not learning items that are already mastered. Learners and instructors can accurately determine their areas of strengths and weaknesses, and use this to determine future instruction. This paper explains the underlying principles used in the initial adaptive assessment followed by evaluation that is closely interwoven with learning. An expert module continuously adjusts the content and method of presentation based on the sequence of learner's recent responses and prior knowledge. The system maintains and updates both the individual learner profile and group profiles. Amrita Learning, targeted to school students, is built upon the principles of spiral learning with mixed presentation from multiple skill areas, thus providing continuous reinforcement in all skill-areas. The proposed competency model has been pilot tested in both city and rural area schools. In the majority of cases where students used it consistently, there were quantifiable improvements in learning levels and performance in schools. Summaries of the results and recommendations are included in this paper.","2169-1428;21691428","Electronic:978-1-4244-8752-3; POD:978-1-4244-8751-6","10.1109/ICDLE.2010.5606052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5606052","Adaptive Learning;Assessment;Continuous Evaluation;Flash Animations;ICT;Intelligent Tutoring Systems Spiral Learning;Interactive;Mastery Learning;Mixed Presentation;individualised instruction;special needs","Adaptation model;Browsers;Equations;Fires;Geometry;Mathematical model;XML","Internet;computer aided instruction;educational institutions;learning (artificial intelligence);multimedia systems","Amrita learning;Schools;Web based multimedia enabled adaptive assessment;adaptive learning;city area school;learner response;performance improvement;rural area school;spiral learning;student knowledge level","","1","","10","","","","3-5 Oct. 2010","","IEEE","IEEE Conferences"
"Hardware implementation of FAST-based reinforcement learning algorithm","Kao-Shing Hwang; Yuan-Pao Hsu; His-Wen Hsieh; Hsin-Yi Lin","Dept. of Electr., Nat. Chung-Cheng Univ., Taiwan","Proceedings of 2005 IEEE International Workshop on VLSI Design and Video Technology, 2005.","20050906","2005","","","435","438","A FAST-based (flexible adaptable-size topology) reinforcement learning chip is implemented in this article. Basically, the FAST is an ART-like (adaptive resonance theory) mechanism. The ART is characterized as one of unsupervised learning neural network models, facilitated to solve stability-plasticity dilemma. The chip is a self organizing architecture that consists of three main structures including similarity, learning, and pruning. Dynamically adjusting the size of sensitivity regions of each neuron and adaptively pruning one of the neurons when an input pattern activates more than one neuron, the chip can preserve hardware resources (available neurons) to accommodate more categories. The clustered result by the implemented chip is then sent to an AHC (adaptive heuristic critic) architecture (emulated by a personal computer) to learn to balance an inverted pendulum system, which is also emulated by the personal computer for verifying the implemented architecture.","","POD:0-7803-9005-9","10.1109/IWVDVT.2005.1504643","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1504643","","Computer architecture;Hardware;Microcomputers;Network topology;Neural networks;Neurons;Resonance;Stability;Subspace constraints;Unsupervised learning","field programmable gate arrays;learning (artificial intelligence);microcomputers;neural net architecture;pendulums;self-adjusting systems","adaptive heuristic critic architecture;adaptive resonance theory;flexible adaptable-size topology;hardware implementation;inverted pendulum system;reinforcement learning algorithm;self organizing architecture","","4","","5","","","","28-30 May 2005","","IEEE","IEEE Conferences"
"Video summarization using reinforcement learning in eigenspace","K. Masumitsu; T. Echigo","IBM Tokyo Res. Lab., Kanagawa, Japan","Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101)","20020806","2000","2","","267","270 vol.2","We propose video summarization using reinforcement learning. The importance score of each frame in a video is calculated from the user's actions in handling similar previous frames; if such frames were watched rather than skipped, a high score is assigned. To calculate the score, instead of using raw feature vectors extracted from images, we use feature vectors projected on eigenspace: as a result, we can deal with the features comprehensively. We also give an algorithm that uses the reinforcement learning method to create a personalized video summary. The summarization algorithm is applied to a soccer video to confirm its effectiveness.","1522-4880;15224880","POD:0-7803-6297-7","10.1109/ICIP.2000.899351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=899351","","Data mining;Feature extraction;Information retrieval;Joining processes;Laboratories;Layout;Learning;Multimedia communication;TV broadcasting;Watches","feature extraction;image sequences;learning (artificial intelligence);video signal processing","algorithm;eigenspace;feature vectors extraction;personalized video summary;reinforcement learning;soccer video;summarization algorithm;video frame;video summarization","","4","3","7","","","","10-13 Sept. 2000","10 Sep 2000-13 Sep 2000","IEEE","IEEE Conferences"
"Anti-lock braking systems data-driven control using Q-learning","M. B. Radac; R. E. Precup; R. C. Roman","Department of Automation and Applied Informatics, Politehnica University of Timisoara, Timisoara, Romania","2017 IEEE 26th International Symposium on Industrial Electronics (ISIE)","20170807","2017","","","418","423","A model-free tire slip control solution for a fast, highly nonlinear Anti-lock Braking System (ABS) is proposed in this work via a reinforcement Q-learning optimal control approach. The solution is tailored around a batch neural fitted scheme using two neural networks to approximate the value function and the controller, respectively. The transition samples are collected from the process through interaction by online exploiting the current iteration controller (or policy) under an ε-greedy exploration strategy. The ABS process fits this type of learning-by-interaction since it does not need an initial stabilizing controller. The validation case studies carried out on a real laboratory setup reveal that high control system performance can be achieved after several tens of interaction episodes with the controlled process. Insightful comments on the observed control behavior in a set of real-time experiments are offered along with performance comparisons with several other controllers.","","Electronic:978-1-5090-1412-5; POD:978-1-5090-1413-2","10.1109/ISIE.2017.8001283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8001283","Actor critic control;Anti-lock Braking System;Q-learning;model-free control;neural networks","","automotive components;braking;greedy algorithms;learning (artificial intelligence);motion control;neurocontrollers;optimal control","ε-greedy exploration strategy;anti-lock braking system;batch neural fitted scheme;data-driven control;iteration controller;learning-by-interaction;model-free tire slip control;neural networks;nonlinear ABS;optimal control;reinforcement Q-learning;value function","","","","","","","","19-21 June 2017","","IEEE","IEEE Conferences"
"A Reinforcement Learning Approach to Emotion-based Automatic Playlist Generation","C. Y. Chi; R. T. H. Tsai; J. Y. Lai; J. Y. j. Hsu","Dept. of CSIE, Nat. Taiwan Univ., Taipei, Taiwan","2010 International Conference on Technologies and Applications of Artificial Intelligence","20110120","2010","","","60","65","A novel trend emerged in music exploration is to organize and search songs according to their emotions. However, research on automatic playlist generation (APG) primarily focuses on metadata and audio similarity. Mainstream solutions view APG as a static problem. This paper argues that the APG problem is better modeled as a continuous optimization problem, and proposes an adaptive preference model for personalized APG based on emotions. The main idea is to collect a user's behavior in music playing, e.g., rating, skipping and replaying, as immediate feedback in learning the user's preferences for music emotion within a playlist. Reinforcement learning is adopted to learn the user's current preferences, which are used to generate personalized playlists. Learning parameters are tuned by simulation of two hypothetical users. A two-month user study is conducted to evaluate the APG solutions. The results show that the proposed approach reduces the Miss Ratio by 10% in comparison with the baseline approach.","2376-6816;23766816","Electronic:978-0-7695-4253-9; POD:978-1-4244-8668-7","10.1109/TAAI.2010.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5695433","automatic playlist generation;reinforcement learning;song emotion","","learning (artificial intelligence);music;optimisation;user interfaces","adaptive preference model;continuous optimization problem;emotion-based automatic playlist generation;music emotion;reinforcement learning;user preference","","1","","20","","","","18-20 Nov. 2010","","IEEE","IEEE Conferences"
"Dynamic Game Difficulty Scaling Using Adaptive Behavior-Based AI","C. H. Tan; K. C. Tan; A. Tay","Institute for Infocomm Research, Agency for Science Technology and Research (A*STAR), Singapore, Singapore","IEEE Transactions on Computational Intelligence and AI in Games","20111212","2011","3","4","289","301","Games are played by a wide variety of audiences. Different individuals will play with different gaming styles and employ different strategic approaches. This often involves interacting with nonplayer characters that are controlled by the game AI. From a developer's standpoint, it is important to design a game AI that is able to satisfy the variety of players that will interact with the game. Thus, an adaptive game AI that can scale the difficulty of the game according to the proficiency of the player has greater potential to customize a personalized and entertaining game experience compared to a static game AI. In particular, dynamic game difficulty scaling refers to the use of an adaptive game AI that performs game adaptations in real time during the game session. This paper presents two adaptive algorithms that use ideas from reinforcement learning and evolutionary computation to improve player satisfaction by scaling the difficulty of the game AI while the game is being played. The effects of varying the learning and mutation rates are examined and a general rule of thumb for the parameters is proposed. The proposed algorithms are demonstrated to be capable of matching its opponents in terms of mean scores and winning percentages. Both algorithms are able to generalize well to a variety of opponents.","1943-068X;1943068X","","10.1109/TCIAIG.2011.2158434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783334","Artificial intelligence;behavior based;car racing simulation;game AI;player satisfaction;real-time adaptation","Adaptation model;Artificial intelligence;Games;Humans;Pixel;Real time systems;Vehicles","computer games;evolutionary computation;learning (artificial intelligence)","adaptive behavior-based artificial intelligence;dynamic game difficulty scaling;evolutionary computation;game adaptation;gaming styles;mutation rates;reinforcement learning","","15","1","37","","","20110602","Dec. 2011","","IEEE","IEEE Journals & Magazines"
"Towards a meta motion planner B: algorithm and applications","A. Adam; E. Rivlin; I. Shimshoni","Dept. of Math., Technion-Israel Inst. of Technol., Haifa, Israel","Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164)","20060418","2001","1","","291","298 vol.1","In a companion paper (see Proceedings of ICRA 2001) we developed a framework for rating or comparing navigation packages. For a given environment a navigation package consists of a motion planner and a sensor to be used during navigation. The ability to rate or measure a navigation package is important in order to address issues like sensor customization for an environment and choice of a motion planner in an environment. In this paper we present the algorithm which we use in order to rate a given navigation package. Under the framework which was presented previously, a partially observable Markov decision process is defined. The algorithm searches for an optimal policy to be employed in this decision process. We briefly review the problem and framework, develop the algorithm and present experimental results.","1050-4729;10504729","POD:0-7803-6576-3","10.1109/ROBOT.2001.932568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=932568","","Application software;Computer science;Industrial engineering;Mathematics;Motion measurement;Navigation;Packaging;Paper technology;Path planning;Robot sensing systems","Markov processes;computerised navigation;decision theory;learning (artificial intelligence);path planning;robots;search problems","Markov decision process;meta motion planner;navigation;reinforcement learning;robots;search problem;sensor customization","","2","","7","","","","2001","","IEEE","IEEE Conferences"
"Increasing Retrieval Quality in Conversational Recommenders","M. S. Llorente; S. E. Guerrero","Universitat de Barcelona, Barcelona","IEEE Transactions on Knowledge and Data Engineering","20120817","2012","24","10","1876","1888","A major task of research in conversational recommender systems is personalization. Critiquing is a common and powerful form of feedback, where a user can express her feature preferences by applying a series of directional critiques over the recommendations instead of providing specific preference values. Incremental Critiquing (IC) is a conversational recommender system that uses critiquing as a feedback to efficiently personalize products. The expectation is that in each cycle the system retrieves the products that best satisfy the user's soft product preferences from a minimal information input. In this paper, we present a novel technique that increases retrieval quality based on a combination of compatibility and similarity scores. Under the hypothesis that a user learns during the recommendation process, we propose two novel exponential Reinforcement Learning (RL) approaches for compatibility that take into account both the instant at which the user makes a critique and the number of satisfied critiques. Moreover, we consider that the impact of features on the similarity differs according to the preferences manifested by the user. We propose a Global Weighting (GW) approach that uses a common weight for nearest cases in order to focus on groups of relevant products. We show that our methodology significantly improves recommendation efficiency in four data sets of different sizes in terms of session length in comparison with state-of-the-art approaches. Moreover, our recommender shows higher robustness against noisy user data when compared to classical approaches.","1041-4347;10414347","","10.1109/TKDE.2011.116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5871618","Conversational recommender systems;case-based reasoning;critiquing elicitation;personalization.","Cognition;Current measurement;Learning systems;Monte Carlo methods;Recommender systems;Space exploration","","","","3","","38","","","20110609","Oct. 2012","","IEEE","IEEE Journals & Magazines"
"Using Contextual Learning to Improve Diagnostic Accuracy: Application in Breast Cancer Screening","L. Song; W. Hsu; J. Xu; M. van der Schaar","Department of Electrical Engineering, University of California, Los Angeles, Los Angeles, CA, USA","IEEE Journal of Biomedical and Health Informatics","20170520","2016","20","3","902","914","Clinicians need to routinely make management decisions about patients who are at risk for a disease such as breast cancer. This paper presents a novel clinical decision support tool that is capable of helping physicians make diagnostic decisions. We apply this support system to improve the specificity of breast cancer screening and diagnosis. The system utilizes clinical context (e.g., demographics, medical history) to minimize the false positive rates while avoiding false negatives. An online contextual learning algorithm is used to update the diagnostic strategy presented to the physicians over time. We analytically evaluate the diagnostic performance loss of the proposed algorithm, in which the true patient distribution is not known and needs to be learned, as compared with the optimal strategy where all information is assumed known, and prove that the false positive rate of the proposed learning algorithm asymptotically converges to the optimum. In addition, our algorithm also has the important merit that it can provide individualized confidence estimates about the accuracy of the diagnosis recommendation. Moreover, the relevancy of contextual features is assessed, enabling the approach to identify specific contextual features that provide the most value of information in reducing diagnostic errors. Experiments were conducted using patient data collected at a large academic medical center. Our proposed approach outperforms the current clinical practice by 36% in terms of false positive rate given a 2% false negative rate.","2168-2194;21682194","","10.1109/JBHI.2015.2414934","10.13039/100000181 - U.S. Air Force Office of Scientific Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7064753","Breast cancer;computer-aided diagnosis system;contextual learning;multiarmed bandit (MAB);online learning","Biopsy;Breast cancer;Clustering algorithms;Context;Imaging","cancer;learning (artificial intelligence);medical diagnostic computing;patient diagnosis","breast cancer screening;clinical decision support tool;contextual features;diagnostic accuracy;disease;false positive rates;online contextual learning algorithm","Aged;Algorithms;Breast Neoplasms;Diagnosis, Computer-Assisted;Early Detection of Cancer;Electronic Health Records;Female;Humans;Mammography;Middle Aged;Signal Processing, Computer-Assisted","2","","42","","","20150320","May 2016","","IEEE","IEEE Journals & Magazines"
"Customized learning algorithms for episodic tasks with acyclic state spaces","T. Bountourelis; S. Reveliotis","School of Industrial & Systems Engineering, Georgia Institute of Technology, USA","2009 IEEE International Conference on Automation Science and Engineering","20090909","2009","","","627","634","The work presented in this paper provides a practical, customized learning algorithm for reinforcement learning tasks that evolve episodically over acyclic state spaces. The presented results are motivated by the optimal disassembly planning (ODP) problem described in, and they complement and enhance some earlier developments on this problem that were presented in. In particular, the proposed algorithm is shown to be a substantial improvement of the original algorithm developed in, in terms of, both, the involved computational effort and the attained performance, where the latter is measured by the accumulated reward. The new algorithm also leads to a robust performance gain over the typical Q-learning implementations for the considered problem context.","2161-8070;21618070","POD:978-1-4244-4578-3","10.1109/COASE.2009.5234189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5234189","","Aerospace industry;Algorithm design and analysis;Automation;Convergence;Data mining;Learning;Q factor;Space technology;State-space methods;Systems engineering and theory","learning (artificial intelligence)","Q-learning implementation;acyclic state space;customized learning algorithm;episodic task;optimal disassembly planning;reinforcement learning;robust performance gain","","0","","18","","","","22-25 Aug. 2009","","IEEE","IEEE Conferences"
"Multi-objective reinforcement learning algorithm and its improved convergency method","Z. Jin; Z. Huajun","Department of Control Science and Engineering, Huazhong University of Science and Technology, China","2011 6th IEEE Conference on Industrial Electronics and Applications","20110804","2011","","","2438","2445","This paper proposes a multi-objective reinforcement learning algorithm (MORLA) and uses simultaneous perturbation stochastic approximation (SPSA) to improve the convergence of it. Usually, reinforcement learning (RL) is used to design neurocontroller for control system with single objective. When facing multi-objective system, it is necessary to design the neurocontroller according to the personal preference. The MORLA can transform the multi-objective into synthetical objective and applies parallel genetic algorithm (PGA) to evolve the neurocontroller according to the synthetical objective. To establish the synthetical objective, the objective weight which represents the personal preference is calculated by solving the constrained optimization problem (COP) at the end of each generation. The COP requires not only the biggest variance of the synthetical objective in the population, but also requires the weight to fit the designer's preference. After acquiring the weights, the PGA can select the elitists from the population according to the designer's preference and design a satisfying neurocontroller by evolutionary operations. In addition, although GA has good global search ability, it descends slowly at local area. This paper applies SPSA algorithm to search optimal solution when GA is vibrating at local area. The SPSA converges fast by efficient gradient approximation that relies on measurements of the objective function. The hybrid algorithm accelerates the learning speed of reinforcement learning. At last, the MORLA is used to design neurocontroller for a speed-controlled induction motor drive with indirect vector control. With different personal preferences for the drive system, the simulation results show the feasibility and validity of the MORLA.","2156-2318;21562318","Electronic:978-1-4244-8756-1; POD:978-1-4244-8754-7","10.1109/ICIEA.2011.5976002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976002","SPSA;multi-objective reinforcement learning;speed-controlled","Algorithm design and analysis;Approximation methods;Control systems;Convergence;Genetic algorithms;Learning;Neurocontrollers","genetic algorithms;learning (artificial intelligence);neurocontrollers","constrained optimization problem;indirect vector control;multiobjective reinforcement learning;multiobjective system;neurocontroller;objective function;parallel genetic algorithm;simultaneous perturbation stochastic approximation;speed-controlled induction motor drive","","0","","47","","","","21-23 June 2011","","IEEE","IEEE Conferences"
"Knowledge sharing approaches for distributed agents system","K. S. Hwang; C. W. Hsieh; W. C. Jiang","Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan","2017 International Conference on Advanced Robotics and Intelligent Systems (ARIS)","20180222","2017","","","59","61","Considering situations in a multi-agent system, if there are tremendous number of agents sharing knowledge with each other, it is complicated activities hard to be solved. This thesis proposed a method that all agents just connect with a server to alleviate the complexity of the experiences exchange activities. The server collects learning knowledge loaded from all the agents, merges the knowledge, and shares the knowledge to all agents which lack akin experiences. The agents utilized the proposed Pheromone Mechanism in Ant Colony Algorithm to evaluate whether an experience is worthy to upload to the server. Meanwhile, to deal with the problem of massive data processing, this thesis used the open source software, Apache Hadoop, along with the MapReduce programming model. The agents can take shared experiences integrated with their own knowledge to achieve knowledge sharing and increase the efficiency significantly. The proposed approach in this thesis was implemented by a homemade server and personal computers.","","Electronic:978-1-5386-2419-7; POD:978-1-5386-2420-3; USB:978-1-5386-2418-0","10.1109/ARIS.2017.8297184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8297184","Ant colony algorithm;Distributed computing;Knowledge merging;Knowledge sharing;Reinforcement learning","Computational modeling;Data models;Multi-agent systems;Programming;Servers;Training;Trajectory","ant colony optimisation;data handling;learning (artificial intelligence);multi-agent systems;parallel programming;public domain software;software agents","Ant Colony Algorithm;Apache Hadoop;MapReduce programming model;Pheromone Mechanism;distributed agents system;experience exchange;experience sharing;homemade server;knowledge merging;knowledge sharing;learning knowledge;massive data processing;multiagent system;open source software","","","","","","","","6-8 Sept. 2017","","IEEE","IEEE Conferences"
"Constructing an intelligent behavior avatar in a virtual world: a self-learning model based on reinforcement","Jui-Fa Chen; Wei-Chuan Lin; Hua-Sheng Bai; Chia-Che Yang; Hsiao-Chuan Chao","Dept. of Inf. Eng., Tamkang Univ., Tamsui, Taiwan","IRI -2005 IEEE International Conference on Information Reuse and Integration, Conf, 2005.","20050912","2005","","","421","426","In this paper, a novel method for personal intelligent behavior avatar (IBA) is proposed to acquire autonomous behavior based on the interactions between user and smart objects in the virtual environment. In this method, the behavior decision model and the self-learning model are integrated by Bayesian networks and reinforcement learning. The Bayesian networks can treat interaction experiences using statistical processes, and the sureness of decision making is represented by certainty factors using stochastic reasoning. The reinforcement learning is implemented by learning experimentation or trial and error mechanisms to improve the performance of IBA through feedback. Therefore, the IBA makes a strategic decision that is approximated and appropriate to the user through the self-learning process by reinforcement learning. Finally, the feasibility of this method is investigated by imitating user's behavior and the results of self-learning process. The results of simulation show that the method is successful in imitating user's behavior and improving the performance of IBA.","","POD:0-7803-9093-8","10.1109/IRI-05.2005.1506510","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1506510","","Artificial intelligence;Avatars;Bayesian methods;Chaos;Decision making;Educational institutions;Information technology;Intelligent agent;Learning;Virtual environment","avatars;belief networks;decision making;human computer interaction;inference mechanisms;stochastic processes;unsupervised learning","Bayesian networks;behavior decision model;decision making;personal intelligent behavior avatar;reinforcement learning;self-learning model;statistical process;stochastic reasoning;user behavior;virtual world","","1","","9","","","","15-17 Aug. 2005","","IEEE","IEEE Conferences"
"Learning to give route directions from human demonstrations","S. Oßwald; H. Kretzschmar; W. Burgard; C. Stachniss","Department of Computer Science, University of Freiburg, 79110 Freiburg, Germany","2014 IEEE International Conference on Robotics and Automation (ICRA)","20140929","2014","","","3303","3308","For several applications, robots and other computer systems must provide route descriptions to humans. These descriptions should be natural and intuitive for the human users. In this paper, we present an algorithm that learns how to provide good route descriptions from a corpus of human-written directions. Using inverse reinforcement learning, our algorithm learns how to select the information for the description depending on the context of the route segment. The algorithm then uses the learned policy to generate directions that imitate the style of the descriptions provided by humans, thus taking into account personal as well as cultural preferences and special requirements of the particular user group providing the learning demonstrations. We evaluate our approach in a user study and show that the directions generated by our policy sound similar to human-given directions and substantially more natural than directions provided by commercial web services.","1050-4729;10504729","Electronic:978-1-4799-3685-4; POD:978-1-4799-3686-1; USB:978-1-4799-3684-7","10.1109/ICRA.2014.6907334","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6907334","","Computers;Context;Cultural differences;Entropy;Learning (artificial intelligence);Measurement;Web services","control engineering computing;learning (artificial intelligence);mobile robots;path planning","computer systems;cultural preferences;human demonstrations;human-given directions;human-written directions;inverse reinforcement learning;learning demonstrations;personal preferences;robots;route descriptions;route directions;route segment","","3","","25","","","","May 31 2014-June 7 2014","","IEEE","IEEE Conferences"
"Dynamic Information Retrieval Modeling","G. H. Yang; M. Sloan; J. Wang","","Dynamic Information Retrieval Modeling","20160706","2016","","","","","<p> Big data and human-computer information retrieval (HCIR) are changing IR. They capture the dynamic changes in the data and dynamic interactions of users with IR systems. A dynamic system is one which changes or adapts over time or a sequence of events. Many modern IR systems and data exhibit these characteristics which are largely ignored by conventional techniques. What is missing is an ability for the model to change over time and be responsive to stimulus. Documents, relevance, users and tasks all exhibit dynamic behavior that is captured in data sets typically collected over long time spans and models need to respond to these changes. Additionally, the size of modern datasets enforces limits on the amount of learning a system can achieve. Further to this, advances in IR interface, personalization and ad display demand models that can react to users in real time and in an intelligent, contextual way. </p> <p>In this book we provide a comprehensive and up to-date introduction to Dynamic Information Retrieval Modeling, the statistical modeling of IR systems that can adapt to change. We define <i>dynamics</i>, what it means within the context of IR and highlight examples of problems where dynamics play an important role. We cover techniques ranging from classic relevance feedback to the latest applications of partially observable Markov decision processes (POMDPs) and a handful of useful algorithms and tools for solving IR problems incorporating dynamics. </p> <p>The theoretical component is based around the Markov Decision Process (MDP), a mathematical framework taken from the field of Artificial Intelligence (AI) that enables us to construct models that change according to sequential inputs. We define the framework and the algorithms commonly used to optimize over it and generalize it to the case where the inputs aren't reliable. We explore the topic of reinforcement learning more broadly and introduce nother tool known as a Multi-Armed Bandit which is useful for cases where exploring model parameters is beneficial. Following this we introduce theories and algorithms which can be used to incorporate dynamics into an IR model before presenting an array of state-of-the-art research that already does, such as in the areas of session search and online advertising. </p> <p>Change is at the heart of modern Information Retrieval systems and this book will help equip the reader with the tools and knowledge needed to understand <i>Dynamic Information Retrieval Modeling</i>. </p>","","97816270552","10.2200/S00718ED1V01Y201605ICR049","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=7503460.pdf&bkn=7503459&pdfType=book","Markov decision process;dynamic information retrieval;information retrieval;information retrieval evaluation;information retrieval models;recommender systems;reinforcement learning","","","","","","","","2016","","","","","Morgan & Claypool","Morgan and Claypool eBooks"
"Expert Level Control of Ramp Metering Based on Multi-Task Deep Reinforcement Learning","F. Belletti; D. Haziza; G. Gomes; A. M. Bayen","Computer Science Deptartment, University of California at Berkeley, Berkeley, CA, USA","IEEE Transactions on Intelligent Transportation Systems","20180328","2018","19","4","1198","1207","This paper shows how the recent breakthroughs in reinforcement learning (RL) that have enabled robots to learn to play arcade video games, walk, or assemble colored bricks, can be used to perform other tasks that are currently at the core of engineering cyberphysical systems. We present the first use of RL for the control of systems modeled by discretized non-linear partial differential equations (PDEs) and devise a novel algorithm to use non-parametric control techniques for large multi-agent systems. Cyberphysical systems (e.g., hydraulic channels, transportation systems, the energy grid, and electromagnetic systems) are commonly modeled by PDEs, which historically have been a reliable way to enable engineering applications in these domains. However, it is known that the control of these PDE models is notoriously difficult. We show how neural network-based RL enables the control of discretized PDEs whose parameters are unknown, random, and time-varying. We introduce an algorithm of mutual weight regularization (MWR), which alleviates the curse of dimensionality of multi-agent control schemes by sharing experience between agents while giving each agent the opportunity to specialize its action policy so as to tailor it to the local parameters of the part of the system it is located in. A discretized PDE, such as the scalar Lighthill-Whitham-Richards PDE can indeed be considered as a macroscopic freeway traffic simulator and which presents the most salient challenges for learning to control large cyberphysical system with multiple agents. We consider two different discretization procedures and show the opportunities offered by applying deep reinforcement for continuous control on both. Using a neural RL PDE controller on a traffic flow simulation based on a Godunov discretization of the San Francisco Bay Bridge, we are able to achieve precise adaptive metering without model calibration thereby beating the state of the art in traffic metering. Furthermore, with the m- re accurate BeATS simulator, we manage to achieve a control performance on par with ALINEA, a state-of-the-art parametric control scheme, and show how using MWR improves the learning procedure.","1524-9050;15249050","","10.1109/TITS.2017.2725912","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8011495","Deep learning;continuous control;deep reinforcement learning;macroscopic traffic models;partial differential equations;reinforcement learning;transportation systems","Biological system modeling;Control systems;Cyber-physical systems;Learning (artificial intelligence);Mathematical model;Neural networks;Transportation","computer games;digital simulation;learning (artificial intelligence);multi-agent systems;partial differential equations;road traffic;road traffic control;traffic control;traffic engineering computing","Godunov discretization;MWR;PDE models;arcade video games;assemble colored bricks;continuous control;control performance;cyberphysical system;different discretization procedures;discretized PDE;electromagnetic systems;engineering applications;engineering cyberphysical systems;expert level control;hydraulic channels;learning procedure;macroscopic freeway traffic simulator;model calibration;multiagent control schemes;multiagent systems;multiple agents;multitask deep reinforcement learning;mutual weight regularization;neural RL PDE controller;nonlinear partial differential equations;nonparametric control techniques;parametric control scheme;precise adaptive metering;ramp metering;scalar Lighthill-Whitham-Richards PDE;traffic metering;transportation systems","","","","","","","20170816","April 2018","","IEEE","IEEE Journals & Magazines"
"Reinforcement learning-based cooperative sensing in cognitive radio ad hoc networks","B. F. Lo; I. F. Akyildiz","Broadband Wireless Networking Laboratory, School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, 30332, USA","21st Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications","20101217","2010","","","2244","2249","In cognitive radio networks, spectrum sensing is a fundamental function for detecting the presence of primary users in licensed frequency bands. Due to multipath fading and shadowing, the performance of detection may be considerably compromised. To improve the detection probability, cooperative sensing is an effective approach for secondary users to cooperate and combat channel impairments. This approach, however, incurs overhead such as sensing delay for reporting local decisions and the increase of control traffic in the network. In this paper, a reinforcement learning-based cooperative sensing method is proposed to address the cooperation overhead problem. By using the proposed cooperative sensing model, the secondary user learns to (i) find the optimal set of cooperating neighbors with minimum control traffic, (ii) minimize the overall cooperative sensing delay, (iii) select independent users for cooperation under correlated shadowing, and (iv) improve the energy efficiency for cooperative sensing. The simulation results show that the proposed reinforcement learning-based cooperative sensing method reduces the overhead of cooperative sensing while effectively improving the detection performance to combat correlated shadowing.","2166-9570;21669570","Electronic:978-1-4244-8016-6; POD:978-1-4244-8017-3","10.1109/PIMRC.2010.5671686","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5671686","","Correlation;Delay;Fading;Learning;Markov processes;Sensors;Shadow mapping","ad hoc networks;cognitive radio;cooperative systems;learning (artificial intelligence);telecommunication computing","ad hoc networks;cognitive radio networks;control traffic;cooperative sensing delay;detection performance;multipath fading;reinforcement learning-based cooperative sensing;reinforcement learning-based cooperative sensing method;shadowing;spectrum sensing","","11","","12","","","","26-30 Sept. 2010","","IEEE","IEEE Conferences"
"A learning model for personalized adaptive cruise control","X. Chen; Y. Zhai; C. Lu; J. Gong; G. Wang","Intelligent Vehicle Research Center, Beijing Institute of Technology, Beijing 100081 China","2017 IEEE Intelligent Vehicles Symposium (IV)","20170731","2017","","","379","384","This paper develops a learning model for personalized adaptive cruise control that can learn from human demonstration online and mimic a human driver's driving strategies in the dynamic traffic environment. Under the framework of the proposed model, reinforcement learning is used to capture the human-desired driving strategy, and the proportion-integration-differentiation controller is adopted to convert the learning strategy to low-level control commands. The performance of the learning model is tested in the simulation environment built in a driving simulator using PreScan. Experimental results show that the learning model can duplicate human driving strategies with acceptable errors. Moreover, compared with the traditional adaptive cruise control, the proposed model can provide better driving comfort and smoothness in the dynamic situation.","","Electronic:978-1-5090-4804-5; POD:978-1-5090-4805-2; USB:978-1-5090-4803-8","10.1109/IVS.2017.7995748","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7995748","","Acceleration;Adaptation models;Cruise control;Data models;Learning (artificial intelligence);Vehicle dynamics;Vehicles","adaptive control;intelligent transportation systems;learning (artificial intelligence);three-term control","PreScan;driving simulator;human demonstration;human driver;human-desired driving strategy;intelligent driving systems;learning model;personalized adaptive cruise control;proportion-integration-differentiation controller;reinforcement learning","","","","","","","","11-14 June 2017","","IEEE","IEEE Conferences"
"Past Makes Future: Role of pFC in Prediction","J. M. Fuster; S. L. Bressler","1University of California, Los Angeles","Journal of Cognitive Neuroscience","20150305","2015","27","4","639","654","<para>The pFC enables the essential human capacities for predicting future events and preadapting to them. These capacities rest on both the structure and dynamics of the human pFC. Structurally, pFC, together with posterior association cortex, is at the highest hierarchical level of cortical organization, harboring neural networks that represent complex goal-directed actions. Dynamically, pFC is at the highest level of the perception–action cycle, the circular processing loop through the cortex that interfaces the organism with the environment in the pursuit of goals. In its predictive and preadaptive roles, pFC supports cognitive functions that are critical for the temporal organization of future behavior, including planning, attentional set, working memory, decision-making, and error monitoring. These functions have a common future perspective and are dynamically intertwined in goal-directed action. They all utilize the same neural infrastructure: a vast array of widely distributed, overlapping, and interactive cortical networks of personal memory and semantic knowledge, named cognits, which are formed by synaptic reinforcement in learning and memory acquisition. From this cortex-wide reservoir of memory and knowledge, pFC generates purposeful, goal-directed actions that are preadapted to predicted future events.</para>","0898-929X;0898929X","","10.1162/jocn_a_00746","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7052276","","","","","","0","","","","","","April 2015","","MIT Press","MIT Press Journals"
"Autonomous resource allocation for dense LTE networks: A Multi Armed Bandit formulation","A. Feki; V. Capdevielle","Alcatel-Lucent Bell Labs, France","2011 IEEE 22nd International Symposium on Personal, Indoor and Mobile Radio Communications","20120126","2011","","","66","70","Resource allocation is an important prerequisite for the effective deployment of Pico Cells (PCs). This topic becomes even more challenging in the case of heterogeneous networks, where autonomous interference management mechanisms are necessary. In this article, we propose a resource sharing method inspired from the reinforcement learning theory and particularly the methods used to solve the Multi Armed Bandit (MAB) problem. The main goal resides in giving the ability for each cell to make its decision autonomously while dynamically taking into account the resource occupation of each surrounding cell. We set up the global framework for the MAB based resource allocation strategies in the case of total frequency overlapping PCs. The performances of the proposed method are evaluated in the case of Long Term Evolution (LTE) Pico Cells deployment and compared to static allocation schemes. The results demonstrate the efficiency of our method.","2166-9570;21669570","Electronic:978-1-4577-1348-4; POD:978-1-4577-1346-0; USB:978-1-4577-1347-7","10.1109/PIMRC.2011.6140047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140047","Inter-Cell Interference (ICI);LTE;MAB;Pico Cell (PC);Reinforcement Learning theory;Resource allocation","Indexes;Interference;OFDM;Resource management;Signal to noise ratio;Throughput;Time frequency analysis","Long Term Evolution;interference suppression;learning (artificial intelligence);picocellular radio;telecommunication computing;telecommunication network management","Long Term Evolution;MAB based resource allocation;autonomous interference management mechanisms;autonomous resource allocation;dense LTE networks;heterogeneous networks;multiarmed bandit formulation;pico cells;reinforcement learning theory;resource sharing method","","6","1","12","","","","11-14 Sept. 2011","","IEEE","IEEE Conferences"
"A unified learning paradigm for large-scale personalized information management","E. Y. Chang; S. C. H. Hop; Xinjing Wang; Wei-Ying Max; M. R. Lyu","Electr. & Comput. Eng., California Univ., Santa Barbara, CA, USA","Conference, Emerging Information Technology 2005.","20051205","2005","","","4 pp.","","Statistical-learning approaches such as unsupervised learning, supervised learning, active learning, and reinforcement learning have generally been separately studied and applied to solve application problems. In this paper, we provide an overview of our newly proposed unified learning paradigm (ULP), which combines these approaches into one synergistic framework. We outline the architecture and the algorithm of ULP, and explain benefits of employing this unified learning paradigm on personalizing information management.","","POD:0-7803-9328-7","10.1109/EITC.2005.1544372","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544372","","Clustering algorithms;Convergence;Humans;Information management;Kernel;Large-scale systems;Machine learning;Stability;Supervised learning;Unsupervised learning","information management;learning (artificial intelligence)","large-scale personalized information management;statistical-learning approach;unified learning paradigm","","1","","9","","","","15-16 Aug. 2005","","IEEE","IEEE Conferences"
"QoS-Oriented Wireless Routing for Smart Meter Data Collection: Stochastic Learning on Graph","Y. Cao; D. Duan; X. Cheng; L. Yang; J. Wei","Dept. of Electron. & Inf. Eng., Huazhong Univ. of Sci. & Technol., Wuhan, China","IEEE Transactions on Wireless Communications","20140808","2014","13","8","4470","4482","To ensure resilient and reliable meter data collection that is essential for the smart grid operation, we propose a QoS-oriented wireless routing scheme. Specifically tailored for the heterogeneity of the meter data traffic in the smart grid, we first design a novel utility function that not only jointly accounts for system throughput and transmission latency, but also allows for flexible tradeoff between the two with a strict transmission latency constraint, as desired by various smart meter applications. Then, we model the interactions among smart meter data concentrators as a mixed-strategy network formation game. To avoid potential information exchange which is not always practical in meter data collection scenario, a stochastic reinforcement learning algorithm with only private and incomplete information is proposed to solve the network formation problem. Such a problem formulation, together with our proposed stochastic learning algorithm on graph, results in a steady probabilistic route. Both contributions are novel and unique in comparison with existing work on this topic. Another distinct feature of our approach is its capability of effectively maintaining the QoS of smart meter data collection, even when the network is under fault or attack, as verified by simulations.","1536-1276;15361276","","10.1109/TWC.2014.2314121","Science Foundation for the Youth Scholar of Ministry of Education of China; 10.13039/100000001 - National Science Foundation; 10.13039/501100001809 - National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6779690","Network formation;smart grid;stochastic learning;wireless routing","Data collection;Games;Quality of service;Routing;Smart grids;Throughput;Wireless communication","graph theory;learning (artificial intelligence);power engineering computing;quality of service;smart meters;stochastic processes;telecommunication network routing;wireless channels","QoS-oriented wireless routing;mixed-strategy network formation game;network formation problem;smart grid operation;smart meter data collection;stochastic learning;stochastic reinforcement learning algorithm;transmission latency constraint;wireless channels","","5","","40","","","20140327","Aug. 2014","","IEEE","IEEE Journals & Magazines"
"Towards a General Supporting Framework for Self-Adaptive Software Systems","L. Wang; Y. Gao; C. Cao; L. Wang","State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China","2012 IEEE 36th Annual Computer Software and Applications Conference Workshops","20121110","2012","","","158","163","When confronted with their internal and environmental dynamics, various software systems increasingly require self-adaptation capabilities. The vision above requires the self-adaptation approach to tackle these challenges and some software systems have their own specific implementations. However, in this paper a general supporting framework is proposed for software systems to self-adapt with running environmental dynamics and meanwhile fulfill various user requirements. Three key issues are covered in the framework: 1) the overall control architecture, which adopts the double closed-loop style and respectively includes the self-adaptation loop and the self-learning loop; 2) a general descriptive language, which is a application-independent and unified language to represent self-adaptation knowledge about target systems; 3) three implementation mechanisms, including forward reasoning, planning and reinforcement learning using feedback, which are supported by the above descriptive language and executed at runtime in different modules. Finally, one scenario of on-demand services of massive data mining tasks is selected and the case study demonstrates how the framework is customized as required and how the approach works.","","Electronic:978-0-7695-4758-9; POD:978-1-4673-2714-5","10.1109/COMPSACW.2012.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341568","Double Closed-loop Control Arthitecture;General Descriptive Language;Hierarchical Task Network;Rete Algorithm;Self-Adaptive Supporting Framework;Self-Adaptive System","Adaptation models;Cognition;Computer architecture;Learning;Monitoring;Planning;Software systems","data mining;inference mechanisms;software engineering;unsupervised learning","data mining;environmental dynamics;forward reasoning;general descriptive language;general supporting framework;on-demand service;overall control architecture;reinforcement learning;self-adaptation knowledge;self-adaptation loop;self-adaptive software system;self-learning loop","","2","","18","","","","16-20 July 2012","","IEEE","IEEE Conferences"
"Personalized Web recommendations: supporting epistemic information about end-users","M. Preda; D. Popescu","Dept. of Comput. Sci., Craiova Univ., Romania","The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05)","20051017","2005","","","692","695","The online recommendations are a popular presence in the Web sites world due to their potential to increase the customers' satisfaction. The ability to represent epistemic information about the clients' beliefs is important to understand their needs. This paper presents a recommender system based on reinforcement learning. The system represents concepts presented on a Web site by epistemic logical programs and uses a similarity measure between programs in order to facilitate generalization. A prototype of this system and experiments are presented.","","POD:0-7695-2415-X","10.1109/WI.2005.115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1517935","","Automation;Computer science;Feedback;Function approximation;Humans;Learning;Logic;Ontologies;Prototypes;Recommender systems","Web sites;customer satisfaction;information filters;learning (artificial intelligence);logic programming","Web site;customer satisfaction;end-user;epistemic logical program;online recommendation;personalized Web recommendation;program similarity measure;reinforcement learning","","0","","8","","","","19-22 Sept. 2005","","IEEE","IEEE Conferences"
"Self-configuring Switched Multi-Element Antenna system for interference mitigation in femtocell networks","R. Razavi; H. Claussen","Bell Laboratories, Alcatel-Lucent, Blanchardtown Industrial Park, Dublin 15, Republic of Ireland","2011 IEEE 22nd International Symposium on Personal, Indoor and Mobile Radio Communications","20120126","2011","","","237","242","This paper introduces a Switched Multi-Element Antenna (SMEA) solution for interference mitigation in femtocell networks. While the main objective is to protect the femtocell users against uplink interference, the downlink interference from femtocell base stations to the other users is simultaneously reduced as a by-product of this technique. A tailored form of reinforcement learning is used to allow for self-configuration of the femtocell base station and to adaptively select the optimal antenna configuration in a time varying environment. Compared to the traditional Omni-directional antenna systems, the results show an average of 2.5dB gain in uplink direction in terms of reduced transmission power and approximately 1dB of gain in the downlink channel.","2166-9570;21669570","Electronic:978-1-4577-1348-4; POD:978-1-4577-1346-0; USB:978-1-4577-1347-7","10.1109/PIMRC.2011.6139947","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139947","Femtocell networks;Interference management;Multi-element antenna;Q-learning;Reinforcement learning;Self-configuration;WCDMA Femtocells","Antenna measurements;Base stations;Interference;Macrocell networks;Signal to noise ratio;Transmitting antennas","antenna arrays;femtocellular radio;radiofrequency interference","SMEA;femtocell networks;interference mitigation;omnidirectional antenna systems;reinforcement learning;self-configuring switched multielement antenna system;transmission power reduction","","3","","19","","","","11-14 Sept. 2011","","IEEE","IEEE Conferences"
"A unified control framework of HVAC system for thermal and acoustic comforts in office building","Y. Zhao; Q. Zhao; L. Xia; Z. Cheng; F. Wang; F. Song","Dept. of Autom., Tsinghua Univ., Beijing, China","2013 IEEE International Conference on Automation Science and Engineering (CASE)","20131107","2013","","","416","421","Intelligent building system attracts more and more attention in both academic and industrial communities. Learning human comfort requirements and incorporating it into building control system is one of the important issues. In the traditional HVAC control system, the thermal comfort and the acoustic comfort are often conflicted and we lack of a scheme to trade off them well. In this paper, we propose a unified control framework based on reinforcement learning to balance the multiple dimension comforts, including the thermal and acoustic comforts. We utilize the user's complaints in thermal and acoustic sensations as feedback and combine the current environment and devices information to learn the personalized optimal control policy using online Q-learning. The challenge caused by the complaints is coped with an incorporated perception estimation scheme in the Q-learning reward design. Both simulation results and the field experimental results demonstrate the effectiveness of the algorithm, especially in the adaptivity to the individual tradeoff between thermal and acoustic comfort.","2161-8070;21618070","Electronic:978-1-4799-1515-6; POD:978-1-4799-1513-2; USB:978-1-4799-1514-9","10.1109/CoASE.2013.6653964","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6653964","","Acoustics;Buildings;Estimation;Humidity;Noise;Temperature sensors;Upper bound","HVAC;architectural acoustics;building management systems;control engineering computing;learning (artificial intelligence);optimal control","HVAC system;Q-learning reward design;acoustic comforts;acoustic sensations;office building;online Q-learning;perception estimation scheme;personalized optimal control policy;reinforcement learning;thermal comforts;thermal sensations;unified control framework","","1","","17","","","","17-20 Aug. 2013","","IEEE","IEEE Conferences"
"Learning Bayesian networks probabilities from longitudinal data","R. Bellazzi; A. Riva","Lab. di Inf. Medica, Pavia Univ., Italy","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans","20020806","1998","28","5","629","636","Many real applications of Bayesian networks (BN) concern problems in which several observations are collected over time on a certain number of similar plants. This situation is typical of the context of medical monitoring, in which several measurements of the relevant physiological quantities are available over time on a population of patients under treatment, and the conditional probabilities that describe the model are usually obtained from the available data through a suitable learning algorithm. In situations with small data sets for each plant, it is useful to reinforce the parameter estimation process of the BN by taking into account the observations obtained from other similar plants. On the other hand, a desirable feature to be preserved is the ability to learn individualized conditional probability tables, rather than pooling together all the available data. In this work we apply a Bayesian hierarchical model able to preserve individual parameterization, and, at the same time, to allow the conditionals of each plant to borrow strength from all the experience contained in the data-base. A testing example and an application in the context of diabetes monitoring will be shown","1083-4427;10834427","","10.1109/3468.709608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=709608","","Bayesian methods;Biomedical monitoring;Condition monitoring;Context modeling;Diabetes;Medical treatment;Parameter estimation;Patient monitoring;Testing;Time measurement","Bayes methods;learning (artificial intelligence);observers;parameter estimation;patient diagnosis;probability","BN;Bayesian hierarchical model;Bayesian networks probability learning;conditional probabilities;diabetes monitoring;individualized conditional probability tables;longitudinal data;medical monitoring;parameter estimation process reinforcement","","2","","19","","","","Sep 1998","","IEEE","IEEE Journals & Magazines"
"Utilization of machine learning methods for assembling, training and understanding autonomous robots","P. Hartono","Department of Mechanics and Information Technology, Chukyo University, Toyota, Japan","2011 4th International Conference on Human System Interactions, HSI 2011","20110704","2011","","","398","402","For decades human society has been supported by the proliferation of complex artifacts such as electronic appliances, personal vehicles and mass transportation systems, electrical and communications grids, and in the past few decades, Internet. In the very near future, robots will play increasingly important roles in our daily life. The increase in complexity of the tasks and sometimes physical forms or morphologies of the artifacts consequently requires complex assembling and controlling procedures of them, which soon will be unmanageable by the traditional manufacturing process. The aim of this paper is to give a brief review on the potentials of the non-traditional assembling of complex artifacts, which in this study is symbolized by the creation of autonomous robots. Methods in self-assembling modular robots, real time learning of autonomous robots and a method for giving the comprehensive understanding, albeit intuitively, to human will be explained through some physical experiments.","2158-2246;21582246","Electronic:978-1-4244-9639-6; Electronic:978-1-4244-9640-2; POD:978-1-4244-9638-9","10.1109/HSI.2011.5937399","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5937399","Autonomous Robots;Modular Robots;Perceptual Understanding;Reinforcement Learning;Self Assembling;Self-Organizing Map","Learning systems;Morphology;Robot kinematics;Robot sensing systems;Topology;Training","learning (artificial intelligence);mobile robots;robotic assembly","autonomous robots assembling;autonomous robots training;autonomous robots undertanding;machine learning methods;selfassembling modular robots","","2","","15","","","","19-21 May 2011","","IEEE","IEEE Conferences"
"Reducing Delay during Vertical Handover","N. Bagdure; B. Ambudkar","Dept. of Electron. Eng., Pad. Dr. D.Y.P.I.E.T., Pune, India","2015 International Conference on Computing Communication Control and Automation","20150716","2015","","","200","204","The next generation of wireless networks will provide heterogeneous wireless technologies to a mobile user, in which they can move using terminals with multiple access interfaces and non-real-time or real-time services. The most important issue in such environment is the Always Best Connected (ABC) concept allowing the best connectivity to applications anywhere at any time. With the growing of communication world lot of research is carried out in the field of wireless networks specifically next generation networks. The integration and interoperability of various wireless networks will lead to number of challenges. One of the challenges is the handovers across various/heterogeneous wireless networks as well as reduction of call drop probability. Number of researches has proposed solutions for this challenge. This paper proposes a solution to this problem for the improvement of end-to-end Quality of Service supporting high data rates, real time transmission over wide area and enabling users to specify their personal preferences using Markov Decision Process (MDP). The proposed method uses delay as its basic parameter to select a network during handovers. The selection of the network amongst the existing wireless network considers the ongoing application of the user during the handover. Through simulations we show that our algorithm reduces call drop probability and satisfies user's requirements.","","Electronic:978-1-4799-6892-3; POD:978-1-4799-6893-0","10.1109/ICCUBEA.2015.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155834","Markov Decision process;Reinforcement Learning;Reward;Transition Probability;Vertical Handover","Bandwidth;Delays;Handover;Mobile communication;Quality of service;Wireless networks","delays;mobility management (mobile radio);next generation networks;probability;quality of service","call drop probability reduction;delay reduction;end-to-end quality of service;handover;heterogeneous wireless networks;mobile user;next generation networks;wireless network integration;wireless network interoperability","","0","","13","","","","26-27 Feb. 2015","","IEEE","IEEE Conferences"
"Design considerations of reinforcement learning power controllers in Wireless Body Area Networks","R. Kazemi; R. Vesilo; E. Dutkiewicz; R. P. Liu","Department of Engineering, Macquarie University, Sydney, Australia","2012 IEEE 23rd International Symposium on Personal, Indoor and Mobile Radio Communications - (PIMRC)","20121129","2012","","","2030","2036","A Wireless Body Area Network (WBAN) comprises a number of tiny devices implanted in/on the body that sample physiological signals of the human body and send them to a coordinator node for medical or other purposes. As these miniature devices run on built-in batteries, energy is the most valuable resource in WBANs. This makes signal interference between neighboring WBANs a serious threat because it causes energy waste in these systems. To mitigate this internetwork interference, we propose a dynamic power control mechanism in WBANs which employs reinforcement learning (RL) to learn from experience and improve its performance. This paper presents guidelines in designing efficient RL power controllers in WBANs and provides an analysis of the effect of the reward function, discount factor, learning rate and eligibility trace parameter where the main performance criteria used are convergence and solution optimality in terms of throughput and energy consumption per bit.","2166-9570;21669570","Electronic:978-1-4673-2569-1; POD:978-1-4673-2566-0; USB:978-1-4673-2568-4","10.1109/PIMRC.2012.6362688","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6362688","Game;Interference;Power Control;Reinforcement Learning;WBAN;convergence;multi agent;optimality","Convergence;Energy consumption;Games;Interference;Power control;Signal to noise ratio;Wireless communication","biomedical communication;body area networks;control engineering computing;control system synthesis;interference suppression;learning (artificial intelligence);medical computing;medical control systems;power control;prosthetics","RL;WBAN;eligibility trace parameter;energy consumption;energy waste;human body implantation;internetwork interference mitigation;medical coordinator node;physiological signal;power control mechanism;reinforcement learning;signal interference;wireless body area network","","1","","14","","","","9-12 Sept. 2012","","IEEE","IEEE Conferences"
"Learning algorithms For intelligent agents based e-learning system","N. Pandey; R. K. Tyagi; S. Sahu; A. Dwivedi","Department of Computer Science & Engineering, Ajay Kumar Garg Engineering College, Ghaziabad. India","2013 3rd IEEE International Advance Computing Conference (IACC)","20130513","2013","","","1034","1039","Requirements are the basic entity which makes the project to be successfully implemented. In the development of the software, theses requirements must be measured accurately and correctly. The requirements of the end users may be changed with respect to different individual personality. As different requirements are given by different customers, all of them cannot be processed by single software system. In such a case, it is essential to make the changes in the software systems automatically which fulfills all the requirements of the user. Such condition is met by the systems with intelligent agents. In this paper, algorithms of intelligent agents adviser agent, personalization agent and content managing agent are proposed to automatically gather the requirements of the students and fulfill them. The algorithms are structured using reinforcement learning. Theses algorithm makes the agents to sense the needs of the students and evolve in the course of their operation. These intelligent agents are applied after the deployment of the e-learning software. All the algorithms have been implemented successfully.","","Electronic:978-1-4673-4529-3; POD:978-1-4673-4527-9","10.1109/IAdCC.2013.6514369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514369","E-Learning;Intelligent Agent;Reinforcement Learnin;Requirement Engineering","Databases;Electronic learning;Intelligent agents;Negative feedback;Software algorithms;Standards","computer aided instruction;learning (artificial intelligence);multi-agent systems","adviser agent;content managing agent;e-learning system;electronic learning;intelligent agent;learning algorithm;personalization agent;reinforcement learning;software development;student requirement;user requirement","","1","","8","","","","22-23 Feb. 2013","","IEEE","IEEE Conferences"
"Augmenting practical cross-layer MAC schedulers via offline reinforcement learning","F. Pianese; P. J. Danielsen","Nokia Bell Labs","2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)","20180215","2017","","","1","6","An automated offline design process for optimized cross-layer schedulers can produce augmented scheduling algorithms tailored to a target deployment scenario. We discuss the application of ODDS, a reinforcement learning technique we introduced, for augmenting LTE MAC scheduler algorithms of practical significance. ODDS observes the correlation between the value of a utility function and the cross-layer state parameters seen by an instrumented baseline scheduler, determining the best actions via an offline Monte Carlo exploration of the problem space. The result of the ODDS process is a compact definition of a scheduling policy that has been optimized for a target scenario and utility function. In this paper we instrument a production scheduler definition to evaluate the potential of augmented schedulers in practical use, and experiment with awareness to application traffic properties by using a multi-class utility function, yielding scheduling policies that behave differently depending on the features of individual traffic classes.","","Electronic:978-1-5386-3531-5; POD:978-1-5386-3532-2; Paper:978-1-5386-3529-2; USB:978-1-5386-3530-8","10.1109/PIMRC.2017.8292409","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8292409","","Algorithm design and analysis;Measurement;Optimization;Production;Scheduling algorithms;Throughput;Training","Long Term Evolution;Monte Carlo methods;access protocols;learning (artificial intelligence);telecommunication scheduling;telecommunication traffic","LTE MAC scheduler algorithms;ODDS process;augmented schedulers;augmented scheduling algorithms;automated offline design process;cross-layer state parameters;instrumented baseline scheduler;multiclass utility function;offline Monte Carlo exploration;offline reinforcement learning;optimized cross-layer schedulers;practical cross-layer MAC schedulers;production scheduler definition;reinforcement learning technique;scheduling policy;target deployment scenario","","","","","","","","8-13 Oct. 2017","","IEEE","IEEE Conferences"
"Reinforcement learning for resource provisioning in the vehicular cloud","M. A. Salahuddin; A. Al-Fuqaha; M. Guizani","Universite du Quebec a Montreal","IEEE Wireless Communications","20160826","2016","23","4","128","135","This article presents a concise view of vehicular clouds that incorporates various vehicular cloud models that have been proposed to date. Essentially, they all extend the traditional cloud and its utility computing functionalities across the entities in the vehicular ad hoc network. These entities include fixed roadside units, onboard units embedded in the vehicle, and personal smart devices of drivers and passengers. Cumulatively, these entities yield abundant processing, storage, sensing, and communication resources. However, vehicular clouds require novel resource provisioning techniques that can address the intrinsic challenges of dynamic demands for the resources and stringent QoS requirements. In this article, we show the benefits of reinforcement-learning-based techniques for resource provisioning in the vehicular cloud. The learning techniques can perceive long-term benefits and are ideal for minimizing the overhead of resource provisioning for vehicular clouds.","1536-1284;15361284","","10.1109/MWC.2016.7553036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7553036","","Cloud computing;Dynamic scheduling;Intelligent vehicles;Quality of service;Resource management;Software as a service;Vehicle dynamics","cloud computing;learning (artificial intelligence);minimisation;quality of service;resource allocation;traffic engineering computing;vehicular ad hoc networks","QoS requirements;communication resources;dynamic resource demands;fixed roadside units;onboard units;overhead minimization;personal smart devices;processing resources;reinforcement learning;resource provisioning;sensing resources;storage resources;utility computing functionalities;vehicular ad hoc network;vehicular cloud models","","1","","","","","","August 2016","","IEEE","IEEE Journals & Magazines"
