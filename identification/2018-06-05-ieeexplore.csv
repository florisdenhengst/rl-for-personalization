Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Publication_Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,Funding Information,PDF Link,Author Keywords,IEEE Terms,INSPEC Controlled Terms,INSPEC Non-Controlled Terms,Mesh_Terms,Article Citation Count,Patent Citation Count,Reference Count,Copyright Year,License,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier
Smart Lifelong Learning System Based on Q-Learning,A. A. Kardan; O. R. B. Speily,"Adv. E-Learning Technol. Lab., AmirKabir Univ. of Technol., Tehran, Iran",2010 Seventh International Conference on Information Technology: New Generations,20100701,2010,,,1086,1091,"The majority of current web-based learning systems are closed learning environments where courses and learning materials are fixed and the only dynamic aspect is the organization of the material that can be adapted to allow a relatively individualized learning environment. In this paper, we propose an evolving web-based learning system which can adapt itself to its learners. More specifically, the novelty with respect to the system lies in its ability to find relevant content on the web, and its ability to personalize and adapt this content based on the system's observation of its learners and the accumulated ratings given by the learners. Hence, although learners do not have direct interaction with the open Web, the system can retrieve relevant information related to them and their situated learning characteristics. Lifelong learning scenarios have particular differences in their need for personalized recommendations that make not possible reusing existing general approaches of recommender systems. The paper describes those challenges and we propose a hybrid technique based on machine learning to recognize learner preferences and predict theirs required contents with high accuracy.",,Electronic:978-1-4244-6271-1; POD:978-1-4244-6270-4,10.1109/ITNG.2010.140,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501486,Learning Promotion;Lifelong Learning;Machine Learning;Q Learning;Recommender Systems;Reinforcement Learning,Adaptive systems;Electronic learning;Information filtering;Information retrieval;Information technology;Learning systems;Least squares approximation;Machine learning;Multitasking;Recommender systems,Internet;computer aided instruction;continuing professional development;information filters;learning (artificial intelligence),Q-Learning;Web-based learning systems;learning materials;machine learning;personalized recommendations;recommender systems;smart lifelong learning system,,0,,12,,,,12-14 April 2010,,IEEE,IEEE Conferences
Analysis of re-sequencing buffer overflow probability based on stochastic delay characteristics,D. Zhou; H. Li; J. Li,"State Key Lab. of Integrated Service Networks, Xidian Univ., Xi'an, China","2013 IEEE 24th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",20131125,2013,,,2490,2495,"With the development of multi-interface terminals, a host can connect to the Internet simultaneously by multiple access technologies. Under multi-access technology, a multi-path transmission can obtain high throughput, increased available bandwidth and enhanced reliability. However, the multi-path transmission with multi-access technology also has the problems that the packet re-ordering is unavoidable, and the fast retransmission is unnecessarily requested. Considering the stochastically varying transmission delay, the problems above may eventually result in a degradation of throughput. As a result, in this paper, we focus on the analysis of buffer overflow probability problem which is influenced by the transmission interval. First, we utilize Reinforcement Learning method to estimate the stochastic delay of end-to-end paths. Then, we discuss problems of re-sequencing buffer occupancy distribution and the overflow probability. In this paper, we model the stochastic delay as a continuous random variable, and then, discuss its mean value and variance. Simulation result shows that the re-sequencing buffer overflow probability is influenced by the transmission intervals and the variance of stochastic delay.",2166-9570;21669570,Electronic:978-1-4673-6235-1; POD:978-1-4673-6233-7; USB:978-1-4673-6234-4,10.1109/PIMRC.2013.6666565,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6666565,Concurrent Multi-path Transmission;Re-sequencing Buffer Overflow Probability;Reinforcement Learning,Buffer storage;Delays;Estimation;Learning (artificial intelligence);Mathematical model;Sequential analysis;Stochastic processes,Internet;delays;learning (artificial intelligence);probability;random processes;stochastic processes,Internet;continuous random variable;end-to-end paths;mean value;mean variance;multiinterface terminals;multipath transmission;multiple access technology;packet re-ordering;re-sequencing buffer occupancy distribution problem;reinforcement learning method;resequencing buffer overflow probability analysis;stochastic delay characteristics;stochastic varying transmission delay;transmission interval,,1,,20,,,,8-11 Sept. 2013,,IEEE,IEEE Conferences
Green Wi-Fi Implementation and Management in Dense Autonomous Environments for Smart Cities,Y. Zhang; C. Jiang; J. Wang; Z. Han; J. Yuan; J. Cao,"Department of Electrical Engineering, Tsinghua University, Beijing, China",IEEE Transactions on Industrial Informatics,20180404,2018,14,4,1552,1563,"Advanced informatics technologies facilitate the construction of green smart cities, especially the Wi-Fi implementation and management, for rapidly increasing personal Wi-Fi devices in autonomous environments residing in nonoverlapped channels often result in low energy efficiency and severe cochannel interference. In this paper, a green Wi-Fi management framework is constructed in order to reduce the overall energy consumption through turning off a portion of access points (APs) and aggregating their users to the other active APs. A Tabu-search-assisted active AP selection algorithm is proposed to minimize the power consumption with a seamless wireless converge. For the active APs, based on our defined metric airtime cost that is integrated by the in-range interference and the hidden terminal interference, a reinforcement-learning-aided AP self-management algorithm is proposed to dynamically adjust APs' channels in the partially overlapped channel space. Extensive simulations and field experiments demonstrate that the power consumption can be reduced by about 65%, and the airtime cost of APs can be reduced by 50% compared with the typical least congestion channel search algorithm.",1551-3203;15513203,,10.1109/TII.2017.2785820,National Basic Research Program of China; National Natural Science Foundation of China; US NSF; Young Elite Scientists Sponsorship; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8233146,Energy efficiency;green Wi-Fi;partially overlapped channels (POCs);self-management,Energy consumption;Green products;Informatics;Interference;Power demand;Wireless communication;Wireless fidelity,cochannel interference;computer network management;energy conservation;green computing;learning (artificial intelligence);power consumption;radiofrequency interference;search problems;smart cities;telecommunication power management;wireless LAN;wireless channels,AP self-management algorithm;Tabu-search-assisted active AP selection algorithm;access points;advanced informatics technologies;cochannel interference;defined metric airtime cost;dense autonomous environments;energy consumption reduction;green Wi-Fi management framework;green smart cities;hidden terminal interference;in-range interference;least congestion channel search algorithm;low energy efficiency;metric airtime cost;nonoverlapped channels;partially overlapped channel space;personal Wi-Fi devices;power consumption;reinforcement-learning-aided AP self-management algorithm,,,,,,,20171221,April 2018,,IEEE,IEEE Journals & Magazines
Smartphone Interruptibility Using Density-Weighted Uncertainty Sampling with Reinforcement Learning,R. Fisher; R. Simmons,"Machine Learning Dept., Carnegie Mellon Univ., Pittsburgh, PA, USA",2011 10th International Conference on Machine Learning and Applications and Workshops,20120209,2011,1,,436,441,"We present the In-Context application for smart-phones, which combines signal processing, active learning, and reinforcement learning to autonomously create a personalized model of interruptibility for incoming phone calls. We empirically evaluate the system, and show that we can obtain an average of 96.12% classification accuracy when predicting interruptibility after a week of training. In contrast to previous work, we leverage density-weighted uncertainty sampling combined with a reinforcement learning framework applied to passively collected data to achieve comparable or superior classification accuracy using many fewer queries issued to the user.",,Electronic:978-0-7695-4607-0; POD:978-1-4577-2134-2,10.1109/ICMLA.2011.128,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147012,Active learning;interruptibility;mobile devices;reinforcement learning,Accuracy;Context;Data mining;Feature extraction;Support vector machines;Switches;Uncertainty,learning (artificial intelligence);smart phones,active learning;density weighted uncertainty sampling;reinforcement learning;signal processing;smartphone interruptibility,,4,,20,,,,18-21 Dec. 2011,,IEEE,IEEE Conferences
Table of contents,,,2014 IIAI 3rd International Conference on Advanced Applied Informatics,20141201,2014,,,v,xix,The following topics are dealt with: data mining; Japanese WordNet synonym misplacement detection; social network; recommender system; sentiment analysis; workshop-based instruction; Japanese public libraries; machine learning methods; collaborative Web presentation support system; SMS4 ultracompact hardware implementation; wireless sensor networks; personalized public transportation recommendation system; adaptive user interface; NIS-Apriori algorithm; GetRNIA software tool; rough set-based rule generation; tree-Ga bump hunting; neural network model; weighted citation network analysis; sound proofing ventilation unit; touch interaction; mutually dependent Markov decision processes; ozone treatment; dynamic query optimization; big data; learner activity recognition; IoT-security approach; nutrition-based vegetable production; farm product cultivation; polynomial time mat learning; C-deterministic regular formal graph system; article abstract key expression extraction; English text comprehension; online social games; knowledge creation; knowledge utilization; online stock trading; customer behavior analysis; project-based collaborative learning; in-field mobile game-based learning activities; e-portfolio system design; self-regulated learning ontological model; mobile augmented reality based scaffolding platform; context-aware mobile Japanese conversation learning system; English writing error classification; image processing; outside-class learning; exercise-centric teaching materials; UML modeling; online historical document reading literacy; MMORPG-based learning environment; computer courses; undergraduate education; energy management system; higher education; decentralised auction-based bandwidth allocation; wireless networked control systems; resource scheduling algorithm; embedded cloud computing; Poisson distribution; Japanese seismic activity; suspect vehicle detection; 3D network traffic visualization; Web information retrieval; agent based disaster evacua- ion assist system; electroencephalogram; random number generator; multiagent simulations; multicore environment; CPU scheduler; multithreaded processes; reserve-price biddings; real-time traffic signal control; evolutionary computation; robot-assisted rehabilitation system; hybrid automata; Batik motif classification; color-texture-based feature extraction; backpropagation; multimedia storytelling; e-tourism service; Web mining; search engine; simulation-based e-learning mobile application software; library classification training system; WebQuest learning strategy; context-aware ubiquitous English learning; support vector machine; RFID tag ownership transfer protocol; cognitive linguistics; collaborative software engineering learning; write-access reduction method; NVM-DRAM hybrid memory; garbage collection; parallel indexing scheme; lazy-updating snoop cache protocol; distributed storage system; ITS application; software engineering education; ophthalmic multimodal imaging system; injected bug classification; secure live virtual machine migration; flash memory management; genetic programming; heterogeneous databases; time series similarity search; concurrency control program generation; incremental data migration; multidatabase system; software release time decision making; analytic hierarchy process; interactive genetic algorithm; biometric intelligence; talking robots; archaeological ruin analysis; GIS; optical wireless pedestrian-support systems; visual impairment; extreme programming; Japanese e-commerce Web sites; Chinese sign language animation; hearing-impaired people mammography inspection; geographical maps; electroculogram; XML element retrieval technique; image recognition; reinforcement learning; ECU formal verification; gasoline direct injection engines; earthquake disaster simulation; smart devices for autistic children; RoboCup rescue simulation; inductive logic programming; master-slave asynchronous evolutionary hybrid algorithm; VANET routing opt,,CD-ROM:978-1-4799-4175-9; Electronic:978-1-4799-4173-5; POD:978-1-4799-1679-5,10.1109/IIAI-AAI.2014.4,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6913248,,,Big Data;DRAM chips;Internet of Things;Markov processes;Poisson distribution;Unified Modeling Language;XML;agriculture;analytic hierarchy process;archaeology;augmented reality;automata theory;backpropagation;bandwidth allocation;biometrics (access control);cache storage;citation analysis;cloud computing;computational complexity;computer animation;computer games;computer science education;concurrency control;consumer behaviour;data mining;data visualisation;distributed databases;educational courses;electro-oculography;electroencephalography;electronic commerce;emergency management;energy management systems;engines;feature extraction;flash memories;formal verification;further education;genetic algorithms;geographic information systems;groupware;handicapped aids;human computer interaction;humanoid robots;image classification;image colour analysis;image texture;inductive logic programming;intelligent tutoring systems;investment;library automation;linguistics;mammography;medical robotics;mobile computing;multi-agent systems;multi-threading;multimedia computing;multiprocessing systems;natural language processing;networked control systems;neural nets;object detection;ozonation (materials processing);patient rehabilitation;pedestrians;processor scheduling;public transport;query processing;random number generation;recommender systems;rescue robots;resource allocation;rough set theory;search engines;security of data;seismology;social networking (online);software prototyping;software tools;stock markets;storage management;support vector machines;teaching;telecommunication network routing;text analysis;time series;traffic control;traffic engineering computing;travel industry;trees (mathematics);unsupervised learning;user interfaces;vehicular ad hoc networks;ventilation;virtual machines;wireless sensor networks,3D network traffic visualization;Batik motif classification;C-deterministic regular formal graph system;CPU scheduler;Chinese sign language animation;ECU formal verification;English text comprehension;English writing error classification;GIS;GetRNIA software tool;ITS application;IoT-security approach;Japanese WordNet synonym misplacement detection;Japanese e-commerce Web sites;Japanese public libraries;Japanese seismic activity;MMORPG-based learning environment;NIS-Apriori algorithm;NVM-DRAM hybrid memory;Poisson distribution;RFID tag ownership transfer protocol;RoboCup rescue simulation;SMS4 ultracompact hardware implementation;UML modeling;VANET routing optimization;Web image sharing services;Web information retrieval;Web mining;WebQuest learning strategy;XML element retrieval technique;adaptive user interface;agent based disaster evacuation assist system;analytic hierarchy process;archaeological ruin analysis;article abstract key expression extraction;autistic children;backpropagation;big data;biometric intelligence;cognitive linguistics;collaborative Web presentation support system;collaborative software engineering learning;color-texture-based feature extraction;computer courses;concurrency control program generation;context-aware mobile Japanese conversation learning system;context-aware ubiquitous English learning;customer behavior analysis;data mining;decentralised auction-based bandwidth allocation;distributed storage system;dynamic query optimization;e-portfolio system design;e-tourism service;earthquake disaster simulation;electroencephalogram;electrooculogram;embedded cloud computing;energy management system;evolutionary computation;exercise-centric teaching materials;extreme programming;farm product cultivation;flash memory management;garbage collection;gasoline direct injection engines;genetic programming;geographical maps;hearing-impaired people mammography inspection;heterogeneous databases;higher education;hybrid automata;image processing;image recognition;in-field mobile game-based learning activities;incremental data migration;inductive logic programming;injected bug classification;interactive genetic algorithm;knowledge creation;knowledge utilization;lazy-updating snoop cache protocol;learner activity recognition;library classification training system;machine learning methods;master-slave asynchronous evolutionary hybrid algorithm;mobile augmented reality based scaffolding platform;multiagent simulations;multicore environment;multidatabase system;multimedia storytelling;multithreaded processes;mutually dependent Markov decision processes;neural network model;nutrition-based vegetable production;online historical document reading literacy;online social games;online stock trading;ophthalmic multimodal imaging system;optical wireless pedestrian-support systems;outside-class learning;ozone treatment;parallel indexing scheme;personalized public transportation recommendation system;polynomial time mat learning;project-based collaborative learning;random number generator;real-time traffic signal control;recommender system;reinforcement learning;reserve-price biddings;resource scheduling algorithm;robot-assisted rehabilitation system;rough set-based rule generation;search engine;secure live virtual machine migration;self-regulated learning ontological model;sentiment analysis;simulation-based e-learning mobile application software;social network;software engineering education;software release time decision making;sound proofing ventilation unit;support vector machine;suspect vehicle detection;talking robots;time series similarity search;touch interaction;tree-Ga bump hunting;undergraduate education;visual impairment;weighted citation network analysis;wireless networked control systems;wireless sensor networks;workshop-based instruction;write-access reduction method,,0,,,,,,Aug. 31 2014-Sept. 4 2014,,IEEE,IEEE Conferences
Dynamic thermal management for multimedia applications using machine learning,Y. Ge; Q. Qiu,"Department of Electrical and Computer Engineering, Binghamton University, USA",2011 48th ACM/EDAC/IEEE Design Automation Conference (DAC),20110811,2011,,,95,100,"Multimedia applications are expected to form the largest portion of workload in general purpose PC and portable devices. The ever-increasing computation intensity of multimedia applications elevates the processor temperature and consequently impairs the reliability and performance of the system. In this paper, we propose to perform dynamic thermal management using reinforcement learning algorithm for multimedia applications. The proposed learning model does not need any prior knowledge of the workload information or the system thermal and power characteristics. It learns the temperature change and workload switching patterns by observing the temperature sensor and event counters on the processor, and finds the management policy that provides good performance-thermal tradeoff during the runtime. We validated our model on a Dell personal computer with Intel Core 2 processor. Experimental results show that our approach provides considerable performance improvements with marginal increase in the percentage of thermal hotspot comparing to existing workload phase detection approach.",0738-100x;0738100x,DVD:978-1-4503-0636-2; Electronic:978-1-4503-0636-2; POD:978-1-4503-0636-2,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5981924,Dynamic thermal management;multimedia application;reinforcement learning,Decoding;Heuristic algorithms;Learning;Multimedia communication;Radiation detectors;Temperature sensors;Thermal management,learning (artificial intelligence);multimedia computing;thermal management (packaging),Dell personal computer;Intel Core 2 processor;dynamic thermal management;machine learning;multimedia applications;reinforcement learning algorithm;workload phase detection approach,,2,,17,,,,5-9 June 2011,,IEEE,IEEE Conferences
Learn to Coordinate with Generic Non-Stationary Opponents,Z. Kaifu,"Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China, zkf03@mails.tsinghua.edu.cn",2006 5th IEEE International Conference on Cognitive Informatics,20070910,2006,1,,558,565,"Learning to coordinate with non-stationary opponents is a major challenge for adaptive agents. Most previous research investigated only restricted classes of such dynamic opponents. The main contribution of this paper is twofold: (i) A class of generic non-stationary opponents is introduced. The opponents keep mixed strategies which change with less regularity. Its showed that the independent reinforcement learners (ILs), which have neither prior knowledge nor opponent models, cannot coordinate well with this type of opponent, (ii) A new exploration strategy, the DAE (detect and explore) mechanism, is tailored for the ILs in such coordination tasks. This mechanism allows the ILs dynamically detect changes in the opponents behavior and adjust their learning rate and exploration temperature. It's showed that ILs using this strategy are still able to converge in self-play and are able to coordinate well with the non-stationary opponents",,POD:1-4244-0475-4,10.1109/COGINF.2006.365546,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4216463,Coordination game;Exploration strategy;Non-stationary opponent;Reinforcement learning,Cognitive informatics;Computer science;Decision making;Game theory;Learning;Multiagent systems;Temperature;Testing,learning (artificial intelligence);multi-agent systems,adaptive agents;detect and explore mechanism;dynamic opponents;exploration strategy;generic nonstationary opponents;independent reinforcement learner,,1,,16,,,,17-19 July 2006,,IEEE,IEEE Conferences
Evolution of context-aware user profiles,J. Thomsen; Y. Vanrompay; Y. Berbers,"Condat AG, Berlin, Germany",2009 International Conference on Ultra Modern Telecommunications & Workshops,20091204,2009,,,1,6,"Context-awareness and adaptation are key issues in mobile and ubiquitous computing. Applications on mobile devices use context information to adapt themselves to changing environments. User profiles play an important role in these systems as they serve as an individualization filter in a wide range of possible context adaptation parameters. In this paper we propose a modeling approach for the evolution of context-aware user profiles. A motivating scenario, the intelligent selection of a suitable medical expert in an emergency situation, shows the need for context-aware matching of user profiles. This is achieved by a similarity matching algorithm and reinforcement learning.",2157-0221;21570221,POD:978-1-4244-3942-3,10.1109/ICUMT.2009.5345395,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5345395,,Application software;Computer science;Context modeling;Filters;Intelligent networks;Mobile computing;Multiple signal classification;Personal digital assistants;Runtime;Ubiquitous computing,learning (artificial intelligence);medical computing;mobile computing,adaptation;context-aware user profiles;context-awareness;medical expert;mobile computing;reinforcement learning;similarity matching algorithm;ubiquitous computing,,1,,21,,,,12-14 Oct. 2009,,IEEE,IEEE Conferences
"Stochastic Optimal Relaxed Automatic Generation Control in Non-Markov Environment Based on Multi-Step <formula formulatype=""inline""> <tex Notation=""TeX"">$Q(lambda)$</tex></formula> Learning",T. Yu; B. Zhou; K. W. Chan; L. Chen; B. Yang,"College of Electric Power, South China University of Technology, Guangzhou, China",IEEE Transactions on Power Systems,20110721,2011,26,3,1272,1282,"This paper proposes a stochastic optimal relaxed control methodology based on reinforcement learning (RL) for solving the automatic generation control (AGC) under NERC's control performance standards (CPS). The multi-step <i>Q</i>(λ) learning algorithm is introduced to effectively tackle the long time-delay control loop for AGC thermal plants in non-Markov environment. The moving averages of CPS1/ACE are adopted as the state feedback input, and the CPS control and relaxed control objectives are formulated as multi-criteria reward function via linear weighted aggregate method. This optimal AGC strategy provides a customized platform for interactive self-learning rules to maximize the long-run discounted reward. Statistical experiments show that the RL theory based <i>Q</i>(λ) controllers can effectively enhance the robustness and dynamic performance of AGC systems, and reduce the number of pulses and pulse reversals while the CPS compliances are ensured. The novel AGC scheme also provides a convenient way of controlling the degree of CPS compliance and relaxation by online tuning relaxation factors to implement the desirable relaxed control.",0885-8950;08858950,,10.1109/TPWRS.2010.2102372,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5706397,"AGC;CPS;multi-step <formula formulatype=""inline""><tex Notation=""TeX"">$Q(lambda)$</tex> </formula> learning;non-Markov environment;relaxed control;stochastic optimization",Aerospace electronics;Frequency control;Markov processes;Power grids;Power system dynamics;Standards,delays;learning (artificial intelligence);optimal control;power generation control;power system stability;robust control;state feedback;statistical analysis;stochastic processes;thermal power stations,AGC thermal plant;CPS compliance;CPS control;NERC control performance standard;RL theory based controller;interactive self learning rule;linear weighted aggregate method;long run discounted reward;multicriteria reward function;multistep Q(λ) learning algorithm;nonMarkov environment;online tuning relaxation factors;optimal AGC strategy;pulse reversal;reinforcement learning;state feedback input;statistical experiment;stochastic optimal relaxed automatic generation control;time delay control loop,,28,,31,,,20110204,Aug. 2011,,IEEE,IEEE Journals & Magazines
An experimental approach to robotic grasping using a connectionist architecture and generic grasping functions,M. A. Moussa; M. S. Kamel,"Dept. of Syst. Design Eng., Waterloo Univ., Ont., Canada","IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",20020806,1998,28,2,239,253,"An experimental approach to robotic grasping is presented. This approach is based on developing a generic representation of grasping rules, which allows learning them from experiments between the object and the robot. A modular connectionist design arranged in subsumption layers is used to provide a mapping between sensory inputs and robot actions. Reinforcement feedback is used to select between different grasping rules and to reduce the number of failed experiments. This is particularly critical for applications in the personal service robot environment. Simulated experiments on a 15-object database show that the system is capable of learning grasping rules for each object in a finite number of experiments as well as generalizing from experiments on one object to grasping from another",1094-6977;10946977,,10.1109/5326.669561,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=669561,,Databases;Grasping;Grippers;Humans;Intelligent robots;Manipulators;Performance evaluation;Robot sensing systems;Service robots;System testing,feedback;learning systems;manipulators;neural nets,connectionist architecture;generic grasping functions;generic grasping rule representation;learning;modular connectionist design;object database;personal service robot environment;reinforcement feedback;robot actions;robotic grasping;sensory inputs;simulated experiments;subsumption layers,,17,,26,,,,May 1998,,IEEE,IEEE Journals & Magazines
Perturbed learning automata in potential games,G. C. Chasparis; J. S. Shamma; A. Rantzer,"Department of Automatic Control, Lund University, 221 00-SE, Sweden",2011 50th IEEE Conference on Decision and Control and European Control Conference,20120301,2011,,,2453,2458,"This paper presents a reinforcement learning algorithm and provides conditions for global convergence to Nash equilibria. For several reinforcement learning schemes, including the ones proposed here, excluding convergence to action profiles which are not Nash equilibria may not be trivial, unless the step-size sequence is appropriately tailored to the specifics of the game. In this paper, we sidestep these issues by introducing a new class of reinforcement learning schemes where the strategy of each agent is perturbed by a state-dependent perturbation function. Contrary to prior work on equilibrium selection in games, where perturbation functions are globally state dependent, the perturbation function here is assumed to be local, i.e., it only depends on the strategy of each agent. We provide conditions under which the strategies of the agents will converge to an arbitrarily small neighborhood of the set of Nash equilibria almost surely. We further specialize the results to a class of potential games.",0191-2216;01912216,Electronic:978-1-61284-801-3; POD:978-1-61284-800-6; USB:978-1-61284-799-3,10.1109/CDC.2011.6161294,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6161294,,Convergence;Games;Learning;Learning systems;Nash equilibrium;Sensitivity;Vectors,functions;game theory;learning (artificial intelligence);learning automata;perturbation techniques,Nash equilibria;equilibrium selection;global convergence;perturbed learning automata;potential games;reinforcement learning scheme;state-dependent perturbation function;step-size sequence,,5,,16,,,,12-15 Dec. 2011,,IEEE,IEEE Conferences
Integration of Semantic and Episodic Memories,A. Horzyk; J. A. Starzyk; J. Graham,"Department of Automatics and Biomedical Engineering, AGH University of Science and Technology, Krakow, Poland",IEEE Transactions on Neural Networks and Learning Systems,20171116,2017,28,12,3084,3095,"This paper describes the integration of semantic and episodic memory (EM) models and the benefits of such integration. Semantic memory (SM) is used as a foundation of knowledge and concept learning, and is needed for the operation of any cognitive system. EM retains personal experiences stored based on their significance-it is supported by the SM, and in return, it supports SM operations. Integrated declarative memories are critical for cognitive system development, yet very little research has been done to develop their computational models. We considered structural self-organization of both semantic and episodic memories with a symbolic representation of input events. Sequences of events are stored in EM and are used to build associations in SM. We demonstrated that integration of semantic and episodic memories improves the native operation of both types of memories. Experimental results are presented to illustrate how the two memories complement each other by improving recognition, prediction, and context-based generalization of individual memories.",2162-237X;2162237X,,10.1109/TNNLS.2017.2728203,National Science Centre of Poland; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8008846,Cognitive system;episodic memory (EM);event significance;motivated and reinforcement learning;semantic memory (SM),Biological system modeling;Computational modeling;Learning systems;Neurons;Semantics;Training,cognitive systems;generalisation (artificial intelligence);learning (artificial intelligence),SM operations;cognitive system development;computational models;concept learning;context-based generalization;episodic memories;integrated declarative memories;reinforcement learning;semantic memories;semantic memory models,,,,,,,20170811,Dec. 2017,,IEEE,IEEE Journals & Magazines
Cooperative retransmissions using Markov decision process with reinforcement learning,G. N. Shirazi; P. Y. Kong; C. K. Tham,"Institute for Infocomm Research, Agency for Science, Technology & Research (A&#191;STAR), 1 Fusionopolis Way, #21-01 Connexis South Tower, 138632 Singapore","2009 IEEE 20th International Symposium on Personal, Indoor and Mobile Radio Communications",20100415,2009,,,652,656,"In cooperative retransmissions, nodes with better channel qualities help other nodes in retransmitting a failed packet to its intended destination. In this paper, we propose a cooperative retransmission scheme where each node makes local decision to cooperate or not to cooperate at what transmission power using a Markov decision process with reinforcement learning. With the reinforcement learning, the proposed scheme avoids solving an Markov decision process with a large number of states. Through simulations, we show that the proposed scheme is robust to collisions, is scalable with regard to the network size, and can provide significant cooperative diversity.",2166-9570;21669570,Electronic:978-1-4244-5123-4; POD:978-1-4244-5122-7,10.1109/PIMRC.2009.5450098,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5450098,,Automatic repeat request;Bismuth;Dynamic programming;Learning;Poles and towers;Relays;Robustness;Throughput;Transmitters;Wireless networks,Markov processes;diversity reception;wireless channels,Markov decision process;cooperative diversity;cooperative retransmission;reinforcement learning,,0,,11,,,,13-16 Sept. 2009,,IEEE,IEEE Conferences
Self-optimization of capacity and coverage in LTE networks using a fuzzy reinforcement learning approach,R. Razavi; S. Klein; H. Claussen,"Bell Laboratories, Alcatel-Lucent, Blanchardtown Industrial Park, Dublin 15, Republic of Ireland","21st Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications",20101217,2010,,,1865,1870,This paper introduces a solution to enable self-optimization of coverage and capacity in LTE networks through base stations' downtilt angle adjustment. The proposed method is based on fuzzy reinforcement learning techniques and operates in a fully distributed and autonomous fashion without any need for a priori information or human interventions. The solution is shown to be capable of handling extremely noisy feedback information from mobile users as well as being responsive to the changes in the environment including self-healing properties. The simulation results confirm the convergence of the solution to the global optimal settings and that the proposed scheme provides up to 20% performance improvement when compared with an existing fuzzy logic based reinforcement learning approach.,2166-9570;21669570,Electronic:978-1-4244-8016-6; POD:978-1-4244-8017-3,10.1109/PIMRC.2010.5671622,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5671622,Downtilt Adjustment;Fuzzy Logic;LTE;Reinforcement Learning;Self-x Networks;component,Fuzzy logic;Geophysical measurement techniques;Ground penetrating radar;Learning;Measurement;Mobile communication;Optimization,Long Term Evolution;feedback;fuzzy logic;learning (artificial intelligence);optimisation;telecommunication computing,LTE networks;base station;fuzzy logic;fuzzy reinforcement learning;mobile users;noisy feedback information;self-healing;self-optimization,,30,,14,,,,26-30 Sept. 2010,,IEEE,IEEE Conferences
Reinforcement learning system to mitigate small-cell interference through directionality,A. Paatelma; D. H. Nguyen; H. Saarnisaari; N. Kandasamy; K. R. Dandekar,"CWC, University of Oulu, Finland","2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",20180215,2017,,,1,7,"Beam-steering techniques using directional antennas are expected to play an important role in wireless network capacity expansion through ubiquitous small-cell deployment. However, integrating directional antennas into the existing wireless PHY and MAC stack of small cells has been challenging due to the added protocol overhead and lack of a robust antenna beam selection technique that can adapt well to environmental changes. This paper presents the design, implementation, and evaluation of LinkPursuit, a novel learning protocol for distributed antenna state selection in directional small-cell networks. LinkPursuit relies on reconfigurable antennas and a synchronous TimeDivision Multiple Access (TDMA) MAC to achieve simultaneous directional transmission and reception. Further, the system employs a practical antenna selection protocol based on the well known adaptive pursuit algorithm from the reinforcement learning literature. We implement a realtime prototype of LinkPursuit on the WARP platform and conduct extensive experiments to evaluate its performance. The empirical results show that appropriate use of directionality in LinkPursuit can result in higher network sum rates than omnidirectional transmission under various degrees of cross-link interference.",,Electronic:978-1-5386-3531-5; POD:978-1-5386-3532-2; Paper:978-1-5386-3529-2; USB:978-1-5386-3530-8,10.1109/PIMRC.2017.8292393,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8292393,,Directive antennas;Media Access Protocol;Slot antennas;Time division multiple access;Transmitting antennas;Wireless communication,access protocols;beam steering;cellular radio;directive antennas;learning (artificial intelligence);radiofrequency interference;time division multiple access,LinkPursuit;adaptive pursuit algorithm;beam-steering techniques;directional antennas;directional small-cell networks;distributed antenna state selection;reconfigurable antennas;reinforcement learning system;robust antenna beam selection technique;small-cell interference;synchronous TimeDivision Multiple Access MAC;ubiquitous small-cell deployment;wireless network capacity expansion,,,,,,,,8-13 Oct. 2017,,IEEE,IEEE Conferences
Identification and evaluation of performance parameters for RE-BAR bending training simulator,B. M. Menon; P. Aswathi; S. Deepu; R. R. Bhavani,"AMMACHI Labs, Amrita School of Engineering, Amritapuri, Amrita Vishwa Vidyapeetham, Amrita University, Kerala, India","2017 8th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",20171214,2017,,,1,7,"Construction rebars (steel concrete reinforcing bars) are used to provide structural strength and reinforcement for the concrete structure. This requires the bending and cutting of the rebar to proper size before they can be used for construction. A novel haptic based barbending simulator has been devised which enables the trainees to learn and improve the construction rebar bending skill in a safe and controlled way. With its limited assessment and reporting features, the computerised virtual training simulator proves to be effective in training. Adding the features like personalized skill tracking and predictive performance modeling holds even more potential in supporting the training program. Towards this goal, a user performance modeling needs to be done which includes an initial study on performance parameters, assessment criteria and data collection before remodeling the simulator. This paper presents a study on the performance parameters for the bar bending simulator and also evaluates its effectiveness in modeling expert and novice performances. During this study we also hypothesize the parameters that can distinguish an expert and novice performances which was validated with 2 classification techniques - Support vector machine and J48 Decision tree. While revealing the classification rules J48 algorithm provides 78.94% accuracy where as SVM provides 94.737% accuracy. The study also shows that the 2 performance parameters force applied over time and bend angle accuracy are effective to distinguish expert and novice level of expertize for the construction rebar bending skill.",,Electronic:978-1-5090-3038-5; POD:978-1-5090-3039-2,10.1109/ICCCNT.2017.8204042,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8204042,Decision trees;Intelligent systems;Machine learning algorithms;Modeling;Support vector machines;Vocational training,Bars;Data models;Force;Hidden Markov models;Shape;Training;Vehicles,bending;computer based training;construction industry;decision trees;digital simulation;haptic interfaces;mechanical engineering computing;pattern classification;rebar;steel;support vector machines,RE-BAR bending training simulator;assessment criteria;bar bending simulator;bending cutting;computerised virtual training simulator;concrete structure;construction rebar bending skill;construction rebars;data collection;personalized skill tracking;predictive performance modeling;steel concrete;structural strength;training program;user performance modeling,,,,,,,,3-5 July 2017,,IEEE,IEEE Conferences
Towards an agent-based Approach for Service Emergence in Pervasive Computing,A. Hassnaoui; M. Bakhouya; J. Gaber,"Institut National des Telecommunications, France",Advanced Int'l Conference on Telecommunications and Int'l Conference on Internet and Web Applications and Services (AICT-ICIW'06),20060403,2006,,,15,15,"Pervasive computing is a new paradigm with a goal to provide computing and communication services all the time and everywhere. In this paper, a service emergence model for the implementation of pervasive computing applications is presented. Inspired by natural immune system concepts, the model allows the emergence of ad hoc services on the fly according to dynamically changing context environments such as computing context and user context. In this model, ad hoc or composite services are represented by an organization or group of autonomous agents. Agents establish relationships based on affinities to form groups of agents to provide the composite services. Affinities are parameters that represent priorities between agents. They help to distinguish between agents that can satisfy certain conditions or criteria. Affinity adjustments are based on two level of satisfaction. The first level is a local satisfaction depending on available services offered by neighboring agents together with respect to dynamic changes in network resources. The second level is a global satisfaction determined by the user satisfaction regarding the end service provided.",,POD:0-7695-2522-9,10.1109/AICT-ICIW.2006.196,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1602147,Agent-based system;Anytime/anywhere elearning;Immune system.;Pervasive computing;Reinforcement learning;Service composition and emergence;collaborative mobile learning,Autonomous agents;Computer vision;Context modeling;Context-aware services;Hospitals;Immune system;Personal digital assistants;Pervasive computing;Protocols;Ubiquitous computing,,Agent-based system;Anytime/anywhere elearning;Immune system.;Pervasive computing;Reinforcement learning;Service composition and emergence;collaborative mobile learning,,0,,14,,,,19-25 Feb. 2006,,IEEE,IEEE Conferences
Teaching project management using a real-world group project,L. Collingbourne; W. K. G. Seah,"School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand",2015 IEEE Frontiers in Education Conference (FIE),20151207,2015,,,1,8,"It is well established that an effective pedagogy for project management requires students to get real-world experience. The challenge in providing this when teaching undergraduate engineers is the dichotomy between achieving realism and maintaining sufficient simplicity to make the course tractable. A real-world group technology project at Victoria University of Wellington (VUW) in New Zealand establishes essential non-technical attributes required by the engineering profession while covering key elements of the project management body of knowledge (PMBOK). This paper first shows how the project covers the knowledge required in project management and then presents the results of two years of data collected from students' reflection on their own learning. We have established a pleasing congruence across the years against the specific learning topics of team working skills, communication skills and personal working skills with an improvement in project management skills. A key finding emerging from our analysis is the importance of reinforcement learning and reflective learning. We show a key link between these two learning mechanisms and the project pedagogy. Further analysis shows the link between the project pedagogy and four skill areas acquired. Finally, our research has identified specific areas for us to focus on for subsequent years.",,Paper:978-1-4799-8454-1,10.1109/FIE.2015.7344301,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344301,graduate attributes;project management;real-world project,Cultural differences;Education;Monitoring;Project management;Software;Software engineering,educational courses;engineering education;further education;learning (artificial intelligence);project management;team working,New Zealand;PMBOK;VUW;Victoria University of Wellington;communication skills;dichotomy;educational course;effective pedagogy;personal working skills;project management body of knowledge;real-world group project;reflective learning;reinforcement learning;teaching project management;team working skills;undergraduate engineers teaching,,1,,26,,,,21-24 Oct. 2015,,IEEE,IEEE Conferences
Exploiting Reinforcement Learning to Profile Users and Personalize Web Pages,S. Ferretti; S. Mirri; C. Prandi; P. Salomoni,"Dept. of Comput. Sci. & Eng., Univ. di Bologna, Bologna, Italy",2014 IEEE 38th International Computer Software and Applications Conference Workshops,20140922,2014,,,252,257,"In this paper, we present a Web content adaptation system that is able to automatically adapt textual elements of Web pages, based on the user profile and preferences. The system employs Web intelligence to perform these automatic adaptations on single elements composing a Web page. In particular, a reinforcement learning algorithm, i.e. Q-learning, based on the idea of reward/punishment is utilized as the machine learning system that manages the user profile. Based on it, the user profile is updated, so that automatic adaptations can be effectively performed while surfing the Web. We created a simulation scenario to test our approach over different users with specific preferences and/or different kinds of disabilities. Simulation results confirm the viability of the proposal.",,Electronic:978-1-4799-3578-9; POD:978-1-4799-3579-6,10.1109/COMPSACW.2014.45,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903138,Web personalization;content adaptation;legibility;reinforcement learning;user profiling,Adaptation models;Context;Learning (artificial intelligence);Learning systems;Prototypes;Senior citizens;Web pages,Internet;learning (artificial intelligence);user interfaces,Q-learning;Web content adaptation system;Web intelligence;Web page personalization;machine learning system;reinforcement learning;simulation scenario;textual elements;user preference;user profile,,1,,24,,,,21-25 July 2014,,IEEE,IEEE Conferences
Smart Cable-Driven Camera Robotic Assistant,I. Rivas-Blanco; C. López-Casado; C. J. Pérez-del-Pulgar; F. García-Vacas; J. C. Fraile; V. F. Muñoz,"Department of System Engineering and Automation, University of Malaga, Andaluc&#x00ED;a Tech, M&#x00E1;laga, Spain",IEEE Transactions on Human-Machine Systems,20180313,2018,48,2,183,196,"This paper describes the mechanical design and a cognition system for a novel concept-of-camera robotic assistant. The system combines the advantages of intra-abdominal devices and autonomous camera navigation. The robotic assistant is composed of a magnetic intra-abdominal camera robot with two internal cable-driven degrees of freedom and an external robot that handles an external magnet. The intelligence of the robot is implemented in a cognitive architecture based on a long-term memory that stores the robot's knowledge and learning capabilities to improve the robot's behavior. The navigation strategy combines a reactive control based on instrument tracking and a proactive control based on predefined behaviors, depending on the actual state of the task. The robot's learning capabilities include a semantic customization, to adapt the camera's behavior to the surgeon's preferences, and reinforcement learning, to improve the camera navigation strategy. This paper details both the hardware implementation of the system and the software implementation in a robotic operating system architecture. The cognition system and the performance of the cable-driven mechanism have been validated with a set of in vitro experiments. Moreover, the camera robot has been evaluated through an in-vivo experiment in a pig.",2168-2291;21682291,,10.1109/THMS.2017.2767286,Spanish national projects; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8107576,Cognitive robotics;mechatronics;medical robotics;robot control;robot motion,Cameras;Navigation;Robot vision systems;Surgery;Tools,cameras;control engineering computing;learning (artificial intelligence);medical computing;medical robotics;robot programming;robot vision;surgery,autonomous camera navigation;camera navigation strategy;cognition system;cognitive architecture;concept-of-camera robotic assistant;external magnet;external robot;intra-abdominal devices;learning capabilities;long-term memory;magnetic intra-abdominal camera robot;mechanical design;reinforcement learning;robotic operating system architecture;smart cable,,,,,,,20171114,April 2018,,IEEE,IEEE Journals & Magazines
Multiagent reinforcement learning method with an improved ant colony system,Ruoying Sun; S. Tatsumi; Gang Zhao,"Fac. of Eng., Osaka City Univ., Japan","2001 IEEE International Conference on Systems, Man and Cybernetics. e-Systems and e-Man for Cybernetics in Cyberspace (Cat.No.01CH37236)",20020806,2001,3,,1612,1617 vol.3,"Multiagent reinforcement learning has gained increasing attention in recent years. The authors discuss coordination means for sharing episodes and sharing policies in the field of multiagent reinforcement learning. From the point of the view of reinforcement learning, we analyse the performance of indirect media communication among multi-agents on an ant colony system which is an efficient method that uses pheromones to solve optimization problems. Based on the above, we propose the Q-ACS method, modifying the global updating rule in ACS for learning agents to share better episodes benefited from the exploitation of accumulated knowledge. Meanwhile, taking the visited times into account, we propose T-ACS by presenting a state transition policy for learning agents to share better policies, benefiting from biased exploration. To demonstrate the coordination performance of learning agents in our methods, we conducted experiments on an optimization problem, the traveling salesman problem. Comparison of results with ACS, Q-ACS and T-ACS show that the improved methods are efficient for solving the optimization problem",1062-922X;1062922X,POD:0-7803-7087-2,10.1109/ICSMC.2001.973515,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=973515,,Ant colony optimization;Distributed computing;Explosives;Internet;Learning;Optimization methods;Personal digital assistants;Sun;Telephony;Traveling salesman problems,learning (artificial intelligence);multi-agent systems;travelling salesman problems,ACS;Q-ACS method;T-ACS;accumulated knowledge;ant colony system;biased exploration;coordination means;coordination performance;global updating rule;indirect media communication;learning agents;multiagent reinforcement learning;optimization problems;pheromones;state transition policy,,3,,18,,,,2001,07 Oct 2001-10 Oct 2001,IEEE,IEEE Conferences
A comparison of two algorithms for robot learning from demonstration,H. B. Suay; S. Chernova,"Robotics Engineering Program, Worcester Polytechnic Institute, MA 01609, USA","2011 IEEE International Conference on Systems, Man, and Cybernetics",20111121,2011,,,2495,2500,"Robot learning from demonstration focuses on algorithms that enable a robot to learn a policy from demonstrations performed by a teacher, typically a human expert. This paper presents an experimental evaluation of two learning from demonstration algorithms, Interactive Reinforcement Learning and Behavior Networks. We evaluate the performance of these algorithms using a humanoid robot and discuss the relative advantages and drawbacks of these methods with respect to learning time, number of demonstrations, ease of implementation and other metrics. Our results show that Behavior Networks rely on a greater degree of domain knowledge and programmer expertise, requiring very precise definitions for behavior pre- and post-conditions. By contrast Interactive RL requires a relatively simple implementation based only on the robot's sensor data and actions. However, Behavior Networks leverage the pre-coded knowledge to effectively reduce learning time and the required number of human interactions to learn the task.",1062-922X;1062922X,Electronic:978-1-4577-0653-0; POD:978-1-4577-0652-3,10.1109/ICSMC.2011.6084052,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6084052,Learning and Adaptive Systems;Personal Robots,Actuators;Humans;Knowledge engineering;Learning;Robot sensing systems;Strontium,human-robot interaction;humanoid robots;learning by example,behavior networks;human interactions;humanoid robot;interactive reinforcement learning;robot learning from demonstration,,0,,15,,,,9-12 Oct. 2011,,IEEE,IEEE Conferences
Inter-cluster connection in cognitive wireless mesh networks based on intelligent network coding,X. Chen; Z. Zhao; H. Zhang; T. Jiang; D. Grace,"Key Laboratory of Integrate Information Network Technology, Zhejiang University, Zheda Road 38, Hangzhou, 310027, China","2009 IEEE 20th International Symposium on Personal, Indoor and Mobile Radio Communications",20100415,2009,,,1251,1256,"Cognitive wireless mesh networks have great flexibility to improve the spectrum utilization by opportunistically accessing the authorized frequency bands, within which the secondary users (SUs) should not violate the quality of service (QoS) requirement of the primary users (PUs) while transmitting. In this paper, we consider inter-cluster connection among neighboring clusters under the framework of cognitive wireless mesh networks. Corresponding to the neighboring clusters, all nodes operate in half-duplex mode; hence exchanging control message usually needs four time slots by traditional scheme, which leads to a loss in networking and spectral efficiency especially at the gateway node. A novel scheme based on network coding is proposed, which needs only two time slots. Our simulation experiments reveal the following findings: the performances of traditional inter-cluster connection and network coding based inter-cluster connection are comparable. Next, how to choose optimal signal amplification factor at the gateway node according to the wireless environment is discussed. And we present an intelligent policy based on reinforcement learning to solve the problem. Theoretical analysis and numerical results both show the policy can achieve optimal throughput for the SUs in the long run.",2166-9570;21669570,Electronic:978-1-4244-5123-4; POD:978-1-4244-5122-7,10.1109/PIMRC.2009.5449924,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5449924,CogMesh;cluster;cognitive radio;cognitive wireless mesh networks;network coding;reinforcement learning;relaying,Chromium;Cognitive radio;Frequency;Intelligent networks;Laboratories;Learning;Network coding;Quality of service;Relays;Wireless mesh networks,cognitive radio;network coding;wireless mesh networks,QoS;cognitive wireless mesh networks;gateway node;half-duplex mode;intelligent network coding;intelligent policy;intercluster connection;optimal signal amplification factor;primary users;quality of service;reinforcement learning;secondary users;spectrum utilization,,3,,15,,,,13-16 Sept. 2009,,IEEE,IEEE Conferences
Adaptive hierarchical resource management for satellite channel in hybrid MANET-satellite-Internet network,N. X. Liu; Xiaoming Zhou; J. S. Baras,"Inst. for Syst. Res., Maryland Univ., College Park, MD, USA","IEEE 60th Vehicular Technology Conference, 2004. VTC2004-Fall. 2004",20050425,2004,6,,4027,4031 Vol. 6,"MANETs are often deployed in an infrastructure-less or hostile region where the satellite provides the only link for the MANETs to communicate with the rest of the world. It faces many challenges to support multiple serviced communications between MANETs and Internet through satellite. In this paper, we propose an efficient resource management scheme called AHRM to dynamically allocate bandwidths among multiple MANET users and multiple priority and non-priority services sharing a multi-access satellite channel. It uses a flexible hierarchical structure to exploit the channel utility and resolve contention from two levels. A bandwidth adaptation algorithm is designed to adjust the allocation dynamically in response to traffic and link status changes. The algorithm turns out to be in line with reinforcement learning and is a customized version of it for the practical satellite network setting. Implementation issues are discussed. Simulation results are presented, showing that the scheme can guarantee fast delivery of critical messages in spite of channel contention, and significantly improve the performance of multiple services.",1090-3038;10903038,POD:0-7803-8521-7,10.1109/VETECF.2004.1404834,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1404834,,Algorithm design and analysis;Artificial satellites;Bandwidth;Delay;Educational institutions;Intelligent networks;Mobile ad hoc networks;Resource management;Telecommunication traffic;Web and internet services,IntServ networks;Internet;ad hoc networks;channel allocation;learning (artificial intelligence);mobile satellite communication;multi-access systems,AHRM;adaptive hierarchical resource management;adaptive hierarchical scheduling;bandwidth adaptation;contention resolution;dynamic bandwidth allocation;hybrid MANET-satellite-Internet network;link status;mobile wireless ad hoc network;multiaccess satellite channel;multiple priority services;multiple service communications;reinforcement learning;traffic status,,3,,12,,,,26-29 Sept. 2004,,IEEE,IEEE Conferences
New software for learner-centered circuits instruction,M. D. Ciletti,"Dept. of Electr. & Comput. Eng., Colorado Univ., Colorado Springs, CO, USA","Frontiers in Education Conference, 1998. FIE '98. 28th Annual",20020806,1998,3,,1100 vol.3,,"Summary form only given. Today's classroom and technologies offer solutions to the challenges that face students and instructors in circuits. Software tools now support several areas of the curriculum (e.g. circuits and electromagnetic fields), and many students learn to use them early in their studies. The availability of powerful personal computers linked to classroom video projection systems creates an opportunity for faculty to broaden the scope of their instruction on-the-fly, with a high level of audience interaction and exploration. With software tools, examples can be explored freely, and students can address ""what-if"" questions immediately. Students gain valuable reinforcement for their understanding of abstract concepts by seeing physical, practical effects on the screen, under their control. Here, the authors discuss such software for learner-centered circuits instruction.",0190-5848;01905848,POD:0-7803-4762-5,10.1109/FIE.1998.738572,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=738572,,Circuit analysis;Circuit theory;Electrical engineering education;Fourier series;Fourier transforms;Linear circuits;Software tools;Springs;Time domain analysis;User-generated content,computer aided instruction;educational courses;electronic engineering computing;electronic engineering education;microcomputer applications,classroom video projection systems;curriculum;instructors;learner-centered circuits instruction software;personal computers;software tools;students,,0,,,,,,4-7 Nov. 1998,04 Nov 1998-07 Nov 1998,IEEE,IEEE Conferences
Context-aware unified routing for VANETs based on virtual clustering,Y. Ji; C. Wu; T. Yoshinaga,"National Institute of Informatics, Tokyo, Japan","2016 IEEE 27th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",20161222,2016,,,1,6,"We propose a context-aware routing protocol for vehicular ad hoc networks (VANETs). Two types of context information is considered in this paper specifically communication type (broadcast or unicast) and packet size. The proposed protocol constructs route based on virtual clustering which only exchanges beacon messages in one-hop neighborhood area. The packets are forwarded by the cluster heads, and the last 2-hop route is optimized by using a reinforcement learning algorithm which can attain good performance with low overhead. The advantage of the proposed protocol is shown by using computer simulations.",,Electronic:978-1-5090-3254-9; POD:978-1-5090-3255-6,10.1109/PIMRC.2016.7794599,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7794599,,Conferences;Context;Payloads;Routing protocols;Unicast;Vehicles,learning (artificial intelligence);mobile computing;pattern clustering;routing protocols;telecommunication computing;vehicular ad hoc networks,VANET;computer simulation;context-aware unified routing protocol;one-hop neighborhood area;packet size;reinforcement learning algorithm;vehicular ad hoc network;virtual clustering,,,,,,,,4-8 Sept. 2016,,IEEE,IEEE Conferences
Customised pearlmutter propagation: A hardware architecture for trust region policy optimisation,S. Shao; W. Luk,"Department of Computing, Imperial College London",2017 27th International Conference on Field Programmable Logic and Applications (FPL),20171005,2017,,,1,6,"Reinforcement Learning (RL) is an area of machine learning in which an agent interacts with the environment by making sequential decisions. The agent receives reward from the environment to find an optimal policy that maximises the reward. Trust Region Policy Optimisation (TRPO) is a recent policy optimisation algorithm that achieves superior results in various RL benchmarks, but is computationally expensive. This paper proposes Customised Pearlmutter Propagation (CPP), a novel hardware architecture that accelerates TRPO on FPGA. We use the Pearlmutter Algorithm to address the key computational bottleneck of TRPO in a hardware efficient manner, avoiding symbolic differentiation with change of variables. Experimental evaluation using robotic locomotion benchmarks demonstrates that the proposed CPP architecture implemented on Stratix-V FPGA can achieve up to 20 times speed-up against 6-threaded Keras deep learning library with Theano backend running on a Core i7-5930K CPU.",,Electronic:978-9-0903-0428-1; POD:978-1-5386-2040-3,10.23919/FPL.2017.8056789,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8056789,,Acceleration;Benchmark testing;Computer architecture;Field programmable gate arrays;Hardware;Machine learning;Optimization,computer architecture;field programmable gate arrays;learning (artificial intelligence);multi-agent systems;optimisation,CPP architecture;Customised Pearlmutter Propagation;Customised pearlmutter propagation;Pearlmutter Algorithm;Stratix-V FPGA;TRPO;hardware architecture;machine learning;reinforcement learning;robotic locomotion benchmarks;sequential decision making;trust region policy optimisation,,,,,,,,4-8 Sept. 2017,,IEEE,IEEE Conferences
Adaptive learning based directional MAC protocol for millimeter wave (mmWave) wireless networks,P. Tiwari; D. K. Meena; L. S. Pillutla,"Dhirubhai Ambani Institute of Information and Communication Technology (DA-IICT), Gandhinagar - 382007, Gujarat, India","2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",20180215,2017,,,1,5,"Directional antennas are used to counter the increased path loss at millimeter wave (mmWave) frequencies (beyond 30 GHz). Usage of directional antennas imply lack of channel sensing on the part of various network nodes. Consequently coordination among various network nodes becomes critical for efficient network operation. In this paper we propose the adaptive learning directional medium access control (AL-DMAC) protocol based on reinforcement learning, to facilitate implicit coordination among various nodes. Our simulation results demonstrate that the AL-DMAC protocol along with a suitably chosen neighbor discovery mechanism yields higher network throughput than both the naive directional slotted ALOHA (DSA) protocol and the memory guided directional medium access control (MD-MAC) protocol. Further the AL-DMAC protocol and the MD-MAC protocol have comparable performance in terms of fairness, measured in terms of the Jain's fairness index.",,Electronic:978-1-5386-3531-5; POD:978-1-5386-3532-2; Paper:978-1-5386-3529-2; USB:978-1-5386-3530-8,10.1109/PIMRC.2017.8292296,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8292296,MAC protocols and reinforcement learning;directional antennas;mmWave,Adaptive learning;Directional antennas;Media Access Protocol;Millimeter wave communication;Millimeter wave technology;Throughput,access protocols;directive antennas;learning (artificial intelligence);millimetre wave communication;telecommunication computing;wireless channels,AL-DMAC protocol;MD-MAC protocol;adaptive learning directional medium access control;channel sensing;directional antennas;directional slotted ALOHA protocol;frequency 30.0 GHz;millimeter wave wireless networks;reinforcement learning,,,,,,,,8-13 Oct. 2017,,IEEE,IEEE Conferences
A learning strategy for paging in mobile environments,I. Koukoutsidis; P. Demestichas; M. Theologou,"Sch. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Greece",2003 5th European Personal Mobile Communications Conference (Conf. Publ. No. 492),20041101,2003,,,585,590,"The essence of designing a good paging strategy is to incorporate user mobility characteristics in a predictive mechanism that reduces the average paging cost with as little computational effort as possible. We introduce a novel paging scheme based on the concept of reinforcement learning. Learning endows the paging mechanism with the predictive power necessary to determine a mobile terminal's position, without having to extract a location probability distribution for each specific user. The proposed algorithm is compared against a heuristic randomized learning strategy akin to reinforcement learning, that we invented for this purpose and performs better than the case where no learning is used at all. It is shown that if the user normally moves only among a fraction of cells in the location area, significant savings can be achieved over the randomized strategy, without excessive time to train the network.",0537-9989;05379989,Paper:0-85296-753-5,10.1049/cp:20030322,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1350260,,,cellular radio;learning (artificial intelligence);paging communication;software agents;telecommunication computing,heuristic randomized learning strategy;intelligent agent;learning strategy;location probability distribution;mobile environments;predictive mechanism;reinforcement learning;terminal paging;user mobility characteristics,,0,,,,,,22-25 April 2003,,IET,IET Conferences
Personalizing robot behavior for interruption in social human-robot interaction,Y. S. Chiang; T. S. Chu; C. D. Lim; T. Y. Wu; S. H. Tseng; L. C. Fu,"Department of Computer Science and Information Engineering, National Taiwan University, Taipei 10617, Taiwan",2014 IEEE International Workshop on Advanced Robotics and its Social Impacts,20150126,2014,,,44,49,"People engaging in an activity usually has individual tolerance to be interrupted [1], [2]. Humans subconsciously adapt their behaviors to draw other one's attention and to get into a conversation based on their historical experiences, but robots often fail to be aware of humans' feeling and thus interrupt their users repeatedly. To endow service robots with such socially acceptable ability, we propose an online human-aware interactive learning framework in this paper, under which the robot personalizes its behaviors according to both observed user's attention and its conjecture about user's awareness of itself. To this purpose, the correlation between the robot's theory of awareness, user's attention and robot behavior are explored through reinforcement learning techniques. The conducted experiment shows that the robot can personalize its interruption strategy, and the optimal policies converged for at least 26 episodes.",2162-7568;21627568,Electronic:978-1-4799-6968-5; POD:978-1-4799-6969-2; USB:978-1-4799-6967-8,10.1109/ARSO.2014.7020978,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7020978,,Face;Hidden Markov models;Interrupters;Learning (artificial intelligence);Markov processes;Robot sensing systems,human-robot interaction;learning (artificial intelligence);service robots;social sciences,interruption strategy;online human-aware interactive learning framework;reinforcement learning techniques;robot behavior personalization;robot theory of awareness;service robots;social human-robot interaction;user attention;user awareness,,0,,23,,,,11-13 Sept. 2014,,IEEE,IEEE Conferences
Supervisory output prediction for bilinear systems by reinforcement learning,G. C. Chasparis; T. Natschläger,"Software Competence Center Hagenberg GmbH, Austria",IET Control Theory & Applications,20170608,2017,11,10,1514,1521,"Online output prediction is an indispensable part of any model predictive control implementation. For several application scenarios, operating conditions may change quite often, while designing the data collection process may not be an option. To this end, this study introduces a supervisory output prediction scheme, tailored specifically for input-output stable bilinear systems, that intends on automating the process of selecting the most appropriate prediction model during runtime. The selection process is based upon a reinforcement-learning scheme, where prediction models are selected according to their prior prediction performance. An additional selection process is concerned with appropriately partitioning the control-inputs' domain also to allow for switched-system approximations of the original bilinear dynamics. The authors show analytically that the proposed scheme converges (in probability) to the best model and partition. They also demonstrate these properties through simulations of temperature prediction in residential buildings.",1751-8644;17518644,,10.1049/iet-cta.2016.1400,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7942299,,,bilinear systems;buildings (structures);digital simulation;learning (artificial intelligence);predictive control;switching systems (control);temperature control,control-input domain;data collection process;input-output stable bilinear systems;model predictive control implementation;online output prediction;original bilinear dynamics;prior prediction performance;reinforcement learning;residential buildings;supervisory output prediction;switched-system approximations;temperature prediction simulation,,,,,,,,6 23 2017,,IET,IET Journals & Magazines
A reinforcement self-learning model on an intelligent behavior avatar in a virtual world,Jui-Fa Chen; Wei-Chuan Lin; Hua-Sheng Bai; Hsiao-Chuan Chao,"Dept. of Inf. Eng., Tamkang Univ., Tamsui, Taiwan","IEEE International Conference on Sensor Networks, Ubiquitous, and Trustworthy Computing (SUTC'06)",20060619,2006,1,,7 pp.,,"In this paper, a novel method for personal intelligent behavior avatar (IBA) is proposed to acquire autonomous behavior based on the interactions between user and smart objects in the virtual environment. In this method, the behavior decision model and the self-learning model are integrated by Bayesian networks and reinforcement learning. The Bayesian networks can treat interaction experiences using statistical processes, and the sureness of decision making is represented by certainty factors using stochastic reasoning. The reinforcement learning is implemented by learning experimentation or trial and error mechanisms to improve the performance of IBA through feedback. Therefore, the IBA makes a strategic decision that is approximated and appropriate to the user through the self-learning process by reinforcement learning. Finally, the feasibility of this method is investigated by imitating user's behavior and the results of self-learning process. The results of simulation show that the method is successful in imitating user's behavior and improving the performance of IBA",,POD:0-7695-2553-9,10.1109/SUTC.2006.1636185,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1636185,,Artificial intelligence;Avatars;Bayesian methods;Chaos;Decision making;Educational institutions;Information technology;Intelligent agent;Learning;Virtual environment,approximation theory;avatars;belief networks;decision making;heuristic programming;statistical analysis;unsupervised learning,Bayesian network;IBA;approximation theory;behavior decision model;learning experimentation;personal intelligent behavior avatar;reinforcement self-learning model;statistical process;stochastic reasoning;strategic decision making,,0,,11,,,,5-7 June 2006,,IEEE,IEEE Conferences
Reward Sensitivity of ACC as an Intermediate Phenotype between DRD4-521T and Substance Misuse,T. E. Baker; T. Stockwell; G. Barnes; R. Haesevoets; C. B. Holroyd,1University of Victoria,Journal of Cognitive Neuroscience,20160204,2016,28,3,460,471,"<para>The development and expression of the midbrain dopamine system is determined in part by genetic factors that vary across individuals such that dopamine-related genes are partly responsible for addiction vulnerability. However, a complete account of how dopamine-related genes predispose individuals to drug addiction remains to be developed. Adopting an intermediate phenotype approach, we investigated whether reward-related electrophysiological activity of ACC—a cortical region said to utilize dopamine reward signals to learn the value of extended, context-specific sequences of goal-directed behaviors—mediates the influence of multiple dopamine-related functional polymorphisms over substance use. We used structural equation modeling to examine whether two related electrophysiological phenomena associated with the control and reinforcement learning functions of ACC—theta power and the reward positivity—mediated the relationship between the degree of substance misuse and genetic polymorphisms that regulate dopamine processing in frontal cortex. Substance use data were collected from 812 undergraduate students. One hundred ninety-six returned on a subsequent day to participate in an electrophysiological experiment and to provide saliva samples for DNA analysis. We found that these electrophysiological signals mediated a relationship between the DRD4-521T dopamine receptor genotype and substance misuse. Our results provide a theoretical framework that bridges the gap between genes and behavior in drug addiction and illustrate how future interventions might be individually tailored for specific genetic and neurocognitive profiles.</para>",0898-929X;0898929X,,10.1162/jocn_a_00905,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395964,,,,,,,,,,,,March 2016,,MIT Press,MIT Press Journals
Emotion-driven learning agent for setting rich presence in mobile telephony,S. Saha; R. Quazi,"The Royal Institute of Technology (KTH), Sweden and University of Dhaka, Bangladesh",2008 11th International Conference on Computer and Information Technology,20090321,2008,,,121,126,"Presence or personal status information is going to be an integral part of human life in the near future. With the possibility of personalizing user preferences in a fine grained way, mobile presence will appeal to most users. Among all the advantages, one of the most annoying problems is to set the presence status manually each time. This paper discusses the development of an intelligent agent based presence client that will learn and make decisions on behalf of the user about his or her presence status. The decision is emotion driven and the learning depends on real world experience. The proposed system utilizes a neural network (NN) based emotion-driven agent to learn user preferences. As a NN learning algorithm, two approaches based on Differential Evolution and Reinforcement have been proposed, of which either one can be used. Rich presence status is set using a scripting language named Call Processing Language; and SIP is used for publishing the presence to others.",,CD-ROM:978-1-4244-2136-7; POD:978-1-4244-2135-0,10.1109/ICCITECHN.2008.4803023,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4803023,ANN;CPL;DE;Presence;SIP;context aware;learning agent;neural network;reinforcement,Artificial neural networks;Context awareness;Humans;Information technology;Intelligent agent;Mobile computing;Mobile radio mobility management;Neural networks;Publishing;Telephony,authoring languages;learning (artificial intelligence);mobile computing;neural nets;software agents;telephony,call processing language;differential evolution;emotion-driven agent;emotion-driven learning agent;intelligent agent;mobile telephony;neural network;personalizing user preferences;reinforcement;rich presence;scripting language,,0,,21,,,,24-27 Dec. 2008,,IEEE,IEEE Conferences
Reference signal power control for load balancing in downlink LTE-A self-organizing networks,C. Ma; R. Yin; G. Yu; J. Zhang,"Institute of information and communication engineering, Zhejiang University, Hangzhou, China","2012 IEEE 23rd International Symposium on Personal, Indoor and Mobile Radio Communications - (PIMRC)",20121129,2012,,,460,464,"Self-organizing network (SON) is considered as a driving technology for the deployment of next generation radio access networks. This paper addresses the problem of load balancing (LB) for multi-hop cellular network (MCN) with fixed relays such as LTE-A network in the context of SON. The designed SON algorithm, namely RSPC-RL, is based on two ideas: relay node reference signal power control (RSPC) and multi-agent reinforcement learning (RL). In the proposed RSPC-RL algorithm, the relay node is modeled as an agent that learns an optimal policy of reference signal power control from its interaction with environment to balance the load distribution of the network through dynamically changing its coverage area. Numerical results show the significant performance gain brought about by the proposed algorithm RSPC-RL.",2166-9570;21669570,Electronic:978-1-4673-2569-1; POD:978-1-4673-2566-0; USB:978-1-4673-2568-4,10.1109/PIMRC.2012.6362829,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6362829,,Algorithm design and analysis;Heuristic algorithms;Learning;Load management;Power control;Relays;Throughput,Long Term Evolution;cellular radio;control engineering computing;learning (artificial intelligence);mobile computing;multi-agent systems;next generation networks;power control;radio access networks;resource allocation;telecommunication control,MCN;RSPC-RL;RSPC-RL algorithm;SON;downlink LTE-A self-organizing networks;load balancing;load distribution;multiagent reinforcement learning;multihop cellular network;next generation radio access networks;relay node reference signal power control,,7,,18,,,,9-12 Sept. 2012,,IEEE,IEEE Conferences
Optimal medication dosing from suboptimal clinical examples: A deep reinforcement learning approach,S. Nemati; M. M. Ghassemi; G. D. Clifford,"Dept. of Biomedical Informatics, Emory University, Atlanta, GA 30322",2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20161018,2016,,,2978,2981,"Misdosing medications with sensitive therapeutic windows, such as heparin, can place patients at unnecessary risk, increase length of hospital stay, and lead to wasted hospital resources. In this work, we present a clinician-in-the-loop sequential decision making framework, which provides an individualized dosing policy adapted to each patient's evolving clinical phenotype. We employed retrospective data from the publicly available MIMIC II intensive care unit database, and developed a deep reinforcement learning algorithm that learns an optimal heparin dosing policy from sample dosing trails and their associated outcomes in large electronic medical records. Using separate training and testing datasets, our model was observed to be effective in proposing heparin doses that resulted in better expected outcomes than the clinical guidelines. Our results demonstrate that a sequential modeling approach, learned from retrospective data, could potentially be used at the bedside to derive individualized patient dosing policies.",1557-170X;1557170X,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,10.1109/EMBC.2016.7591355,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7591355,,Cost function;Drugs;Feature extraction;Hospitals;Learning (artificial intelligence);Time series analysis;Training,decision making;drugs;electronic health records;learning (artificial intelligence);patient treatment,MIMIC II intensive care unit database;clinical guidelines;clinical phenotype;clinician-in-the-loop sequential decision making framework;deep reinforcement learning algorithm;electronic medical records;hospital stay length;individualized dosing policy;individualized patient dosing policies;optimal heparin dosing policy;optimal medication dosing;retrospective data;sample dosing trails;sensitive therapeutic windows;sequential modeling approach;suboptimal clinical examples;testing datasets;training datasets,"Algorithms;Databases, Factual;Dose-Response Relationship, Drug;Heparin;Humans;Learning;Length of Stay;Markov Chains;Reinforcement (Psychology);Retrospective Studies",1,1,,,,,16-20 Aug. 2016,,IEEE,IEEE Conferences
Reinforcement learning approach towards effective content recommendation in MOOC environments,V. R. Raghuveer; B. K. Tripathy; T. Singh; S. Khanna,"SCSE, VIT University, Vellore, Tamilnadu, India","2014 IEEE International Conference on MOOC, Innovation and Technology in Education (MITE)",20150126,2014,,,285,289,"Understanding the Learner requirements is an important aspect of any learning environment as it helps to recommend the LOs in a more personalized manner. With the growing demand for MOOCs offered by coursera, edx, etc. the learner information plays a vital role in understanding the extent to which the learners can gain out of such courses. The Learning Management Systems (LMS) across the web uses the explicit (rating, performance, etc.) and implicit feedback (LOs used) obtained through interaction with the learners to derive such information. As the requirements of the learners varies with the individual's interest and learning background, a common approach for recommending LOs may not cater the needs of all the learners. To overcome this issue, this paper proposes reinforcement learning based algorithm to analyze the learner information (derived from both implicit and explicit feedback) and generate the knowledge on the learner's requirements and capabilities inside a specific learning context. The reinforcement learning system (RILS) implemented as a part of this work utilizes the knowledge thus generated in order to recommend the appropriate LOs for the learners. The results have highlighted that the knowledge derived from the learning information analysis proved to effective in generating personalized recommendation policies that can cater the context specific requirements of the learners.",,Electronic:978-1-4799-6876-3; POD:978-1-4799-6877-0,10.1109/MITE.2014.7020289,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7020289,LO recommendation;MOOC;Reinforcement Learning;learning context;learning experience,Collaboration;Conferences;Context;Educational institutions;Electronic learning;Technological innovation,Internet;computer aided instruction;educational courses;human computer interaction;information analysis;learning (artificial intelligence);recommender systems,LMS;MOOC environments;RILS;Web;content recommendation;explicit feedback;generating personalized recommendation policies;implicit feedback;knowledge utilization;learning information analysis;learning management systems;massive open online course;reinforcement learning system,,1,,14,,,,19-20 Dec. 2014,,IEEE,IEEE Conferences
Towards a virtual personal assistant based on a user-defined portfolio of multi-domain vocal applications,T. Ekeinhor-Komi; J. L. Bouraoui; R. Laroche; F. Lefèvre,"Orange Labs, France",2016 IEEE Spoken Language Technology Workshop (SLT),20170209,2016,,,106,113,"This paper proposes a novel approach to defining and simulating a new generation of virtual personal assistants as multi-application multi-domain distributed dialogue systems. The first contribution is the assistant architecture, composed of independent third-party applications handled by a Dispatcher. In this view, applications are black-boxes responding with a self-scored answer to user requests. Next, the Dispatcher distributes the current request to the most relevant application, based on these scores and the context (history of interaction etc.), and conveys its answer to the user. To address variations in the user-defined portfolio of applications, the second contribution, a stochastic model automates the online optimisation of the Dispatcher's behaviour. To evaluate the learnability of the Dispatcher's policy, several parametrisations of the user and application simulators are enabled, in such a way that they cover variations of realistic situations. Results confirm in all considered configurations of interest, that reinforcement learning can learn adapted strategies.",,Electronic:978-1-5090-4903-5; POD:978-1-5090-4904-2; USB:978-1-5090-4902-8,10.1109/SLT.2016.7846252,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846252,dialogue strategy;multi-application spoken dialogue systems;multi-domain;reinforcement learning,Gold;History;Learning (artificial intelligence);Meteorology;Portfolios;Semantics;Symmetric matrices,computational linguistics;human computer interaction;interactive systems;learning (artificial intelligence);optimisation;stochastic processes,Dispatcher policy learnability evaluation;application simulator parametrisation;black-boxes;multiapplication multidomain distributed dialogue systems;multidomain vocal applications;online optimisation;reinforcement learning;stochastic model;third-party applications;user parametrisation;user-defined portfolio;virtual personal assistant architecture,,,,,,,,13-16 Dec. 2016,,IEEE,IEEE Conferences
A learning-based control architecture for an assistive robot providing social engagement during cognitively stimulating activities,J. Chan; G. Nejat,"Autonomous Systems and Biomechatronics Laboratory in the Department of Mechanical and Industrial Engineering at the University of Toronto, 5 King's College Road, ON, M5S 3G8 Canada",2011 IEEE International Conference on Robotics and Automation,20110818,2011,,,3928,3933,"Recent studies have shown that sustained engagement in cognitively stimulating activities has had positive effects on the cognitive functioning of humans. The objective of our work is to develop an intelligent socially assistive robot that can engage individuals in person-centered cognitively stimulating activities. In this paper, we present the design of a novel learning-based control architecture that enables the robot to act as a social motivator by providing assistance, encouragement and celebration during the course of an activity. A hierarchical reinforcement learning (HRL) approach is used to provide the robot with the ability to: (i) learn appropriate assistive behaviors based on the structure of the activity and (ii) personalize the interaction based on the person's affective state during the activity. Preliminary experiments show that the proposed learning-based control architecture is effective in determining the optimal assistive behaviors of the robot during a memory game interaction.",1050-4729;10504729,CD:978-1-61284-380-3; Electronic:978-1-61284-385-8; Paper:978-1-61284-386-5,10.1109/ICRA.2011.5980426,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5980426,,Games;Heart rate;Humans;Robot sensing systems;Speech recognition;Training,cognition;intelligent robots;learning (artificial intelligence);service robots;social sciences,cognitively stimulating activity;hierarchical reinforcement learning;human cognitive function;intelligent socially assistive robot;learning-based control architecture;memory game interaction,,4,,18,,,,9-13 May 2011,,IEEE,IEEE Conferences
Angle of arrival estimation in dynamic indoor THz channels with Bayesian filter and reinforcement learning,B. Peng; Q. Jiao; T. Kürner,"Technische Universit&#x00E4;t Braunschweig Schleinitzstra&#x00DF;e 22, 38106 Braunschweig, Germany",2016 24th European Signal Processing Conference (EUSIPCO),20161201,2016,,,1975,1979,"This paper presents a novel algorithm to estimate the Angle of Arrival (AoA) in a dynamic indoor Terahertz channel. In a realistic application, the user equipment is often moved by the user during the data transmission and the AoA must be estimated periodically, such that the adaptive directional antenna can be adjusted to realize a high antenna gain. The Bayesian filter is applied to exploit continuity and smoothness of the channel dynamics for the AoA estimation. Reinforcement learning is introduced to adapt the prior transition probabilities between system states, in order to fit the variation of application scenarios and personal habits. The algorithm is validated using the ray launching channel simulator and realistic human movement models.",,Electronic:978-0-9928-6265-7; POD:978-1-5090-1891-8,10.1109/EUSIPCO.2016.7760594,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7760594,Bayesian filter;Terahertz communication;angle of arrival estimation;dynamic channel;reinforcement learning,Azimuth;Bayes methods;Directive antennas;Estimation;Gain;Learning (artificial intelligence),belief networks;directive antennas;filtering theory;learning (artificial intelligence);probability,AoA estimation;Bayesian filter;adaptive directional antenna;angle of arrival estimation;dynamic indoor THz channels;high antenna gain;prior transition probabilities;ray launching channel simulator;reinforcement learning,,1,,,,,,Aug. 29 2016-Sept. 2 2016,,IEEE,IEEE Conferences
Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453),,,Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453),20040107,2003,3,,,,The following topics are dealt with: tactile sensing; aerial vehicles; legged robots; motion and path planning; learning systems; simultaneous localization and mapping; visual tracking; cooperative sensing; outdoor vehicles; biped walking; collision avoidance; reinforcement learning; visual servoing; sensor applications; underwater robots; legged locomotion; learning control; sensor-based planning; computational intelligence; mobile robot localization; robot vision; Internet robots; humanoid robots; fuzzy and neural control; sensing for mobile platforms; biologically inspired robots; trajectory planning; architecture and programming; vision-based monitoring; 3D sensing; cellular and modular robots; planning algorithms; mobiligence; multi-robot control; intelligent environment; sensor fusion; micro and nano robotic systems; task allocation; actuator systems; multi-robot systems; manufacturing systems; mechanism design; integrated MEMS sensors and actuators; force-responsive mechatronics in industry; medical robots and haptics; service robots; dexterous hands; sensing and navigation; telerobotics; personal robots; rescue and security robots; spaced robots; human/robot cooperation; compliant motion control; robot assisted surgery; grasping; human-robot interaction; intelligent robots; and virtual reality.,,POD:0-7803-7860-1,10.1109/IROS.2003.1249176,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1249176,,Fuzzy control;Learning control systems;Mechatronics;Motion control;Multisensor systems;Neurocontrollers;Robots;Tactile sensors;Tracking,collision avoidance;control engineering computing;fuzzy control;learning systems;mechatronics;motion control;neurocontrollers;robots;sensor fusion;tactile sensors;tracking,Internet robots;MEMS sensors;actuator systems;aerial vehicles;biped walking;collision avoidance;compliant motion control;computational intelligence;cooperative sensing;force-responsive mechatronics;fuzzy control;human-robot interaction;humanoid robots;intelligent robots;learning control;learning systems;legged locomotion;legged robots;manufacturing systems;medical robots;microrobotic systems;mobile robot localization;motion planning;multirobot control;nanorobotic systems;neural control;outdoor vehicles;path planning;reinforcement learning;robot assisted surgery;robot vision;sensor fusion;sensor-based planning;simultaneous localization;simultaneous mapping;spaced robots;tactile sensing;task allocation;telerobotics;trajectory planning;underwater robots;vision-based monitoring;visual servoing;visual tracking,,0,,,,,,27-31 Oct. 2003,,IEEE,IEEE Conferences
Recipe tuning by reinforcement learning in the SandS ecosystem,B. Fernandez-Gauna; M. Graña,"Computational Intelligence Group, University of the Basque Country, UPV/EHU, San Sebastian, Spain",2014 6th International Conference on Computational Aspects of Social Networks,20141013,2014,,,55,60,"The Social and Smart (SandS) project ecosystem is compounded of household appliance users sharing recipes for the used of appliances, an intermediate control layer, and an intelligent social layer which aims to optimize the appliance recipes maximizing user satisfaction. We consider two aspects of the social intelligence, the innovation producing new recipes for unkown user tasks, and the adaptation to personalize the recipe to an individual user on the basis of his/her specific feedback. The second aspect is proposed to be dealt with by Reinforcement Learning approach, thus user feedback becomes the system reward. In this paper we discuss such an architecture based on the actor-critic approach, providing some experimental results on synthetic datasets that demonstrate the feasibility of the approach, previous to real life implementations.",,Electronic:978-1-4799-5940-2; POD:978-1-4799-5941-9,10.1109/CASoN.2014.6920422,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6920422,Reinforcement Learning;Social computing;Social networks;subconscious social intelligence,Biological system modeling;Computational modeling;Computer architecture;Robots;Service-oriented architecture,domestic appliances;learning (artificial intelligence);social sciences computing;user interfaces,SandS ecosystem;actor-critic approach;household appliance;intelligent social layer;recipe tuning;reinforcement learning;social and smart project ecosystem;social intelligence;user satisfaction,,0,,15,,,,July 30 2014-Aug. 1 2014,,IEEE,IEEE Conferences
Distributed energy cooperation for energy harvesting nodes using reinforcement learning,W. T. Lin; I. W. Lai; C. H. Lee,"Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan","2015 IEEE 26th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",20151203,2015,,,1584,1588,"Wireless communication with nodes capable of harvesting energy emerges as a new technology challenge. In this paper, we investigate the problem of utilizing energy cooperation among energy-harvesting transmitters to maximize the data rate performance. We consider a general framework which can be applied to either cellular networks with base station energy cooperation through wired power grid or sensor networks with transmitting node energy cooperation through wireless power transfer. We model this energy cooperation problem as an infinite horizon Markov decision process (MDP), which can be optimally solved by the value iteration algorithm. Since the optimal value iteration algorithm has high complexity and requires non-causal information, we propose a distributed algorithm by using reinforcement learning and splitting the MDP into several small MDPs, each associated with a transmitter. Simulation results demonstrate the effectiveness of the proposed distributed energy cooperation algorithm.",,Electronic:978-1-4673-6782-0; POD:978-1-4673-6783-7,10.1109/PIMRC.2015.7343551,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7343551,,Batteries;Energy exchange;Power grids;Radio transmitters;Receivers;Wireless communication,Markov processes;cellular radio;computational complexity;energy harvesting;iterative methods;learning (artificial intelligence);power grids;radio transmitters;telecommunication computing;telecommunication power management,MDP;base station energy cooperation;cellular network;distributed energy cooperation;energy harvesting transmitter;infinite horizon Markov decision process;iteration algorithm;reinforcement learning;sensor network;wired power grid;wireless communication;wireless power transfer,,,,16,,,,Aug. 30 2015-Sept. 2 2015,,IEEE,IEEE Conferences
Improving management of Anemia in End Stage Renal Disease using Reinforcement Learning,A. E. Gaweda,"Department of Medicine, Division of Nephrology, University of Louisville, USA",2009 International Joint Conference on Neural Networks,20090731,2009,,,953,958,We present a reinforcement learning approach to elicit individualized dose adjustment policies for patients suffering anemia due to end stage renal disease. Our goal is to achieve stable steady-state anemia management in patients with exhibiting different levels of treatment response. The approach uses Q-learning with parsimonious parametric representation of the state-action value function. We show that this approach achieves stability even in highly responsive patients.,2161-4393;21614393,POD:978-1-4244-3548-7,10.1109/IJCNN.2009.5179004,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5179004,,Automatic control;Cardiac disease;Conference management;Function approximation;Humans;Learning;Medical treatment;Neural networks;Protocols;Steady-state,diseases;kidney;learning (artificial intelligence);medical computing;patient treatment,Q-learning;end stage renal disease;individualized dose adjustment policy;parsimonious parametric representation;patient treatment;reinforcement learning;stable steady-state anemia management;state-action value function,,1,,14,,,,14-19 June 2009,,IEEE,IEEE Conferences
Model Learning and Knowledge Sharing for a Multiagent System With Dyna-Q Learning,K. S. Hwang; W. C. Jiang; Y. J. Chen,"Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan",IEEE Transactions on Cybernetics,20150413,2015,45,5,978,990,"In a multiagent system, if agents' experiences could be accessible and assessed between peers for environmental modeling, they can alleviate the burden of exploration for unvisited states or unseen situations so as to accelerate the learning process. Since how to build up an effective and accurate model within a limited time is an important issue, especially for complex environments, this paper introduces a model-based reinforcement learning method based on a tree structure to achieve efficient modeling and less memory consumption. The proposed algorithm tailored a Dyna-Q architecture to multiagent systems by means of a tree structure for modeling. The tree-model built from real experiences is used to generate virtual experiences such that the elapsed time in learning could be reduced. As well, this model is suitable for knowledge sharing. This paper is inspired by the concept of knowledge sharing methods in multiagent systems where an agent could construct a global model from scattered local models held by individual agents. Consequently, it can increase modeling accuracy so as to provide valid simulated experiences for indirect learning at the early stage of learning. To simplify the sharing process, the proposed method applies resampling techniques to grafting partial branches of trees containing required and useful experiences disseminated from experienced peers, instead of merging the whole trees. The simulation results demonstrate that the proposed sharing method can achieve the objectives of sample efficiency and learning acceleration in multiagent cooperation applications.",2168-2267;21682267,,10.1109/TCYB.2014.2341582,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6871355,Decision tree;Dyna-Q;model sharing;multiagent system,Decision trees;Mathematical model;Multi-agent systems;Planning;Stochastic processes;Support vector machine classification;Vectors,,,,3,,20,,,20140805,May 2015,,IEEE,IEEE Journals & Magazines
Neural networks for incremental dimensionality reduced reinforcement learning,W. Curran; R. Pocius; W. D. Smart,"Oregon State University, Corvallis, United States",2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),20171214,2017,,,1559,1565,"State-of-the-art personal robots must perform complex manipulation tasks to be viable in assistive scenarios. However, many of these robots, like the PR2, use manipulators with high degrees-of-freedom. The complexity of these robots lead to large dimensional state spaces, which are difficult to fully explore. Our previous work introduced the IDRRL algorithm, which compresses the learning space by transforming a high-dimensional learning space onto a lower-dimensional manifold while preserving expressivity. In this work we formally prove that IDRRL maintains PAC-MDP guarantees. We then improve upon our previous formulation of IDRRL by introducing cascading autoencoders (CAE) for dimensionality reduction, producing the new algorithm IDRRL-CAE. We demonstrate the improvement of this extension over our previous formulation, IDRRL-PCA, in the Mountain Car and Swimmers domains.",,Electronic:978-1-5386-2682-5; POD:978-1-5386-2683-2; USB:978-1-5386-2681-8,10.1109/IROS.2017.8205962,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8205962,,Algorithm design and analysis;Computational complexity;Learning (artificial intelligence);Principal component analysis;Robots,learning (artificial intelligence);manipulators;neurocontrollers;principal component analysis,IDRRL algorithm;IDRRL-CAE;IDRRL-PCA;PAC-MDP;PR2;cascading autoencoders;dimensionality reduction;high-dimensional learning space;incremental dimensionality;lower-dimensional manifold;manipulation tasks;manipulators;neural networks;personal robots;reinforcement learning,,,,,,,,24-28 Sept. 2017,,IEEE,IEEE Conferences
<formula><tex>$mathsf{Hap-SliceR}$</tex></formula>: A Radio Resource Slicing Framework for 5G Networks With Haptic Communications,A. Aijaz,"Telecommunications Research Laboratory, Toshiba Research Europe, Ltd., Bristol, BS1 4ND, U.K.&#x00A0;(e-mail: adnan.aijaz@toshiba-trel.com).",IEEE Systems Journal,,2017,Early Access,Early Access,1,12,"It is expected that the emerging 5G networks will not only support diverse use cases, but also enable unprecedented applications such as haptic communications. Therefore, network slicing will provide the required design flexibility. Radio resource slicing would be an indispensable component of any network slicing solution. This paper proposes <formula><tex>$mathsf{Hap-SliceR}$</tex></formula>, which is a novel radio resource slicing framework for 5G networks with haptic communications. First, <formula><tex>$mathsf{Hap-SliceR}$</tex></formula> derives a network-wide radio resource slicing strategy for 5G networks. The optimal slicing strategy, which is based on a reinforcement learning approach, allocates radio resources to different slices while accounting for the dynamics and utility requirements of different slices. Second, <formula><tex>$mathsf{Hap-SliceR}$</tex></formula> provides customization of radio resources for haptic communications over 5G networks. The radio resource allocation requirements of haptic communications have been translated into a unique radio resource allocation problem. A low-complexity heuristic algorithm has been developed for resource allocation. Finally, a comprehensive performance evaluation of <formula><tex>$mathsf{Hap-SliceR}$</tex></formula> has been conducted based on a recently proposed 5G air-interface design.",1932-8184;19328184,,10.1109/JSYST.2017.2647970,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7831356,5G;LTE-A;haptic communications;radio resource allocation;radio resource slicing;virtualization,5G mobile communication;Base stations;Dynamic scheduling;Haptic interfaces;Resource virtualization;Wireless communication,,,,,,,,,20170124,,,IEEE,IEEE Early Access Articles
Contextual multi-armed bandit algorithms for personalized learning action selection,I. Manickam; A. S. Lan; R. G. Baraniuk,"Rice University, United States of America","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",20170619,2017,,,6344,6348,"Optimizing the selection of learning resources and practice questions to address each individual student's needs has the potential to improve students' learning efficiency. In this paper, we study the problem of selecting a personalized learning action for each student (e.g. watching a lecture video, working on a practice question, etc.), based on their prior performance, in order to maximize their learning outcome. We formulate this problem using the contextual multi-armed bandits framework, where students' prior concept knowledge states (estimated from their responses to questions in previous assessments) correspond to contexts, the personalized learning actions correspond to arms, and their performance on future assessments correspond to rewards. We propose three new Bayesian policies to select personalized learning actions for students that each exhibits advantages over prior work, and experimentally validate them using real-world datasets.",,Electronic:978-1-5090-4117-6; POD:978-1-5090-4118-3; USB:978-1-5090-4116-9,10.1109/ICASSP.2017.7953377,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7953377,contextual bandits;personalized learning,Approximation algorithms;Bayes methods;Context;Programmable logic arrays;Random variables;Schedules;Uncertainty,educational institutions,Bayesian policies;contextual multi-armed bandit algorithms;learning resources;personalized learning action selection;prior concept knowledge states;student learning efficiency,,,,,,,,5-9 March 2017,,IEEE,IEEE Conferences
User modeling with limited data: Application to stakeholder-driven watershed design,S. Mukhopadhyay; V. B. Singh; M. Babbar-Sebens,"Computer & Information Science, Indiana University Purdue University Indianapolis, USA","2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",20141204,2014,,,3855,3860,"We have developed a web-based, interactive, watershed planning system called WRESTORE (Watershed Restoration Using Spatio-Temporal Optimization of Resources) (http://wrestore.iupui.edu) that allows stake-holder communities to participate in a democratic, collaborative form of optimization process for designing best management practices (BMPs) on their landscape, while also optimizing based on subjective, qualitative landowners' criteria beyond the usual socio-economic, physical, and ecological criteria. This system utilizes multiple advanced computational approaches including the SWAT (Soil and Water Assessment Tool) hydrologic model for watershed simulations, interactive genetic algorithms and reinforcement-based machine learning algorithms for search and optimization, and deep learning artificial neural networks for user modeling, within an encompassing human-computer interaction framework. A substantial user study of the WRESTORE system was conducted recently involving multiple real stakeholders varying from consultants, government officials, watershed alliance members, etc., with the objective of gaining insight about WRESTORE'S usability and utility. In particular focus was the user modeling component that develops a computational model of a user's preferences and criteria, based on real-time user-provided ratings for a subset of possible designs (similar to the idea of user profiling commonly done for human-computer interaction systems). The user model constructed based on the real user's personalized feedbacks can then be used to influence the automated search and optimization for BMP alternatives in WRESTORE. In this paper, we describe the methods developed for user modeling for interactive optimization, and the experimental set-up as well as results with real user studies. These results clearly demonstrate that development of user models for such personalized, interactive optimization is both feasible and valuable for developing community-based computa- ional water sustainability solutions.",1062-922X;1062922X,Electronic:978-1-4799-3840-7; POD:978-1-4799-3841-4; USB:978-1-4799-3839-1,10.1109/SMC.2014.6974532,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6974532,decision support system;interactive optimization;machine learning;sustainability design;user modeling,Adaptation models;Artificial neural networks;Computational modeling;Data models;Mathematical model;Optimization,Internet;environmental science computing;human computer interaction;hydrology;interactive systems;learning (artificial intelligence);neural nets;socio-economic effects;water resources,BMP;SWAT hydrologic model;WRESTORE;Web-based interactive watershed planning system;best management practices;community-based computational water sustainability solutions;deep learning artificial neural networks;ecological criteria;human-computer interaction framework;interactive genetic algorithms;limited data;physical criteria;reinforcement-based machine learning algorithms;socio-economic criteria;soil and water assessment tool;stakeholder-driven watershed design;user modeling;watershed restoration using spatio-temporal optimization of resources,,0,,21,,,,5-8 Oct. 2014,,IEEE,IEEE Conferences
Learning motor skills with non-rigid materials by reinforcement learning,D. Shinohara; T. Matsubara; M. Kidode,"Graduate School of Information Science, Nara Institute of Science and Technology, Japan",2011 IEEE International Conference on Robotics and Biomimetics,20120412,2011,,,2676,2681,"This paper focuses on learning motor skills for anthropomorphic robots which must interact with non-rigid materials to perform tasks, such as wearing clothes, turning socks inside out, and bandaging. To learn such a motor skill, the task to be performed needs to be quantitatively defined using not only the state of the robot, but also the state of the non-rigid material. However, the non-rigid material is generally represented in a high dimensional configuration space (e.g., [1]) and obtaining such information in a real environment is difficult. In this paper we propose a novel learning framework for learning motor skills interacting with non-rigid materials by reinforcement learning that avoids these difficulties. Our learning framework focuses on the topological relationship between the configuration of the robot and the non-rigid material based on the consideration that most details of the material (e.g., wrinkles) are not important for performing the motor tasks. This focus allows us to define the task performance and provide reward signals based on a low-dimensional variable and to measure task performance in a real environment using reliable sensors. We constructed an experimental setting with an anthropomorphic dual-arm robot and a tailor-made T-shirt for the robot. To demonstrate the feasibility of the proposed method, we applied the method to have the robot perform the motor task of putting on the T-shirt. As a result of our learning framework, through trial and error the robot was able to acquire sequential movements that performed the goal of putting both arms into the corresponding sleeves of the T-shirt.",,DVD:978-1-4577-2137-3; Electronic:978-1-4577-2138-0; POD:978-1-4577-2136-6,10.1109/ROBIO.2011.6181709,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6181709,,Joints;Manipulators;Materials;Robot kinematics;Topology;Trajectory,humanoid robots;learning (artificial intelligence);motion control,anthropomorphic robot;dual-arm robot;high dimensional configuration space;low-dimensional variable;motor skill;nonrigid material;reinforcement learning;sequential movement;tailor-made T-shirt;topological relationship,,2,,18,,,,7-11 Dec. 2011,,IEEE,IEEE Conferences
Genetic reinforcement learning for scheduling heterogeneous machines,G. H. Kim; C. S. G. Lee,"Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA",Proceedings of IEEE International Conference on Robotics and Automation,20020806,1996,3,,2798,2803 vol.3,"Concerns the development of a learning-based heuristic for scheduling heterogeneous machines. List scheduling methods are flexible enough to be used for a large class of problems, including the heterogeneous machine problem. However, designing a priority rule requires insight into the characteristics of the problem. We propose the iterative list scheduling, which refines priority rules while generating a number of schedules. We also show that the iterative list scheduling can be formulated as a reinforcement learning problem, defining states and actions. Due to the large number of possible states, reinforcement learning algorithms which use value functions in constructing an optimal policy may not be suitable for scheduling problems. Encoding the policies of reinforcement learning into genetic algorithms leads to the genetic reinforcement learning (GRL), which directly works with the policies rather than the values of states. A GRL-based scheduler, EVIS (Evolutionary Intracell Scheduler), has been applied to problems such as the heterogeneous machine scheduling, the job-shop scheduling, the flow-shop scheduling, and the open-shop scheduling problems. The proposed model of EVIS, which has the linear order of population-fitness convergence, was verified with computer experiments. Even without fine tuning of EVIS, the quality of solutions found by EVIS was comparable to that of problem-tailored heuristics for most of the problem instances",1050-4729;10504729,POD:0-7803-2988-0,10.1109/ROBOT.1996.506586,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=506586,,Biological cells;Encoding;Genetic algorithms;Genetic engineering;Iterative algorithms;Job shop scheduling;Learning;Processor scheduling;Scheduling algorithm;Space exploration,genetic algorithms;heuristic programming;iterative methods;learning (artificial intelligence);production control,EVIS;Evolutionary Intracell Scheduler;genetic reinforcement learning;heterogeneous machine scheduling;iterative list scheduling;learning-based heuristic;linear-order population-fitness convergence;optimal policy;reinforcement learning problem,,3,,16,,,,22-28 Apr 1996,22 Apr 1996-28 Apr 1996,IEEE,IEEE Conferences
Learning capabilities for improving automatic transmission control,L. Fournier,"Dept. of Comput. Sci., Stanford Univ., CA, USA","Intelligent Vehicles '94 Symposium, Proceedings of the",20020806,1994,,,455,460,"We analyzed the gear-box position selection (GPS) problem on automatic transmission (AT) and proposed an algorithm, based on learning control, to improve vehicle behavior and driver satisfaction. Our approach guarantees optimization of vehicle performance and adaptation to the driver's style with road condition sensitivity. This improvement has been achieved by combining three knowledge acquisition sources: embedded dynamic models of powertrain, inductive inspection of driver actions and AT designer expertise; and by adding learning capabilities in order to significantly increase the system autonomy. Technically, GPS raises the following four problems which this paper addresses: (1) To achieve vehicle performance optimization of multiple antagonistic criteria, locally and globally over time, we considered a parametric disciminant function depending on an evaluation of the driver satisfaction and so called driver-style-state functions, as a reward for the system, and applied a reinforcement learning algorithm, derived from Q-learning method and combined with a mechanism to escape local optima. (2) Learning directly from the driver is performed when he selects AT ratio in manual mode. (3) Each driver's personal style is represented by a Glass creation/selection mechanism. (4) GPS raises a few singularities which are addressed by a set of restriction rules derived from AT control expertise.",,POD:0-7803-2135-9,10.1109/IVS.1994.639561,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=639561,,Algorithm design and analysis;Automatic control;Global Positioning System;Inspection;Knowledge acquisition;Mechanical power transmission;Power system modeling;Road vehicles;Vehicle driving;Vehicle dynamics,intelligent control;learning (artificial intelligence);learning systems;road vehicles,Glass creation/selection mechanism;Q-learning method;automatic transmission control;driver satisfaction;driver-style-state functions;embedded dynamic models;gear-box position selection;inductive inspection;knowledge acquisition;learning control;multiple antagonistic criteria;parametric disciminant function;performance optimization;powertrain;reinforcement learning algorithm;road condition sensitivity;singularities;vehicle behavior,,1,1,5,,,,24-26 Oct. 1994,,IEEE,IEEE Conferences
Computer-aided process control laboratory systems,K. M. Yusof; T. K. Liong; A. K. Baderon,"Dept. of Chem. Eng., Univ. Teknologi Malaysia, Malaysia",Proceedings IEEE 1st International Conference on Multi Media Engineering Education,20020806,1994,,,276,280,"This paper presents the use of personal computers in the process control laboratory at the Department of Chemical Engineering, Universiti Teknologi Malaysia. A description of four control systems interfaced to computers-a flow control system, a level control system, a heated tank control system and a fermenter/batch reactor control system are given. Being versatile controllers and data loggers, the computers also provide a friendly and appealing environment for students to perform experiments on the systems. The different characteristic of each system provides a variety of hands-on experience which incorporates the application of process control and reinforcement of the theories learnt in class. Finally, a sample experiment is included to illustrate the application and experience gained by students",,POD:0-7803-1963-X,10.1109/MMEE.1994.383204,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=383204,,Application software;Chemical engineering;Computer interfaces;Control systems;Inductors;Laboratories;Level control;Microcomputers;Process control;Temperature control,computer aided instruction;control engineering computing;control engineering education;flow control;level control;process control,computer-aided process control laboratory systems;data loggers;fermenter/batch reactor control system;flow control system;heated tank control system;level control system;versatile controllers,,0,,,,,,6-8 Jul 1994,06 Jul 1994-08 Jul 1994,IEEE,IEEE Conferences
Multi-objective reinforcement learning algorithm and its application in drive system,Zhang Huajun; Zhao Jin; Wang Rui; Ma Tan,"Department of Control Science and Engineering, Huazhong University of Science and Technology, China",2008 34th Annual Conference of IEEE Industrial Electronics,20090123,2008,,,274,279,"Generally, reinforcement learning (RL) is used to design neurocontroller for control system with single objective. When facing multi-objective system, it is necessary to design the neurocontroller according to the personal preference. This paper proposed a multi-objective reinforcement learning algorithm (MORLA) to design neurocontroller with the personal preference. It transformed the multi-objective into synthetical objective and applied parallel genetic algorithm (PGA) to evolve the neurocontroller according to the synthetical objective. To establish the synthetical objective, the objective weight which represents the personal preference is calculated by solving the constrained optimization problem (COP) at the end of each generation. The COP requires not only the biggest variance of the synthetical objective in the population, but also requires the weight to fit the designerpsilas preference. After acquiring the weights, the PGA can select the elitists from the population according to the designerpsilas preference and design a satisfying neurocontroller by evolutionary operations. At last, the MORLA is used to design neurocontroller for a speed-controlled induction motor drive with indirect vector control. This paper designed several neurocontrollers with different personal preferences for the drive system. The simulation results show the feasibility and validity of the MORLA.",1553-572X;1553572X,CD-ROM:978-1-4244-1766-7; POD:978-1-4244-1767-4,10.1109/IECON.2008.4757965,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4757965,,Algorithm design and analysis;Constraint optimization;Control systems;Convergence;Design engineering;Design optimization;Electronics packaging;Genetic algorithms;Learning;Neurocontrollers,control system synthesis;genetic algorithms;induction motor drives;learning (artificial intelligence);machine control;neurocontrollers;velocity control,MORLA;constrained optimization problem;control system;drive system;indirect vector control;multiobjective reinforcement learning algorithm;neurocontroller;parallel genetic algorithm;speed-controlled induction motor drive,,1,,32,,,,10-13 Nov. 2008,,IEEE,IEEE Conferences
Visual summary of egocentric photostreams by representative keyframes,M. Bolaños; R. Mestre; E. Talavera; X. Giró-i-Nieto; P. Radeva,"Universitat de Barcelona, Catalonia/Spain",2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),20150730,2015,,,1,6,"Building a visual summary from an egocentric photostream captured by a lifelogging wearable camera is of high interest for different applications (e.g. memory reinforcement). In this paper, we propose a new summarization method based on keyframes selection that uses visual features extracted by means of a convolutional neural network. Our method applies an unsupervised clustering for dividing the photostreams into events, and finally extracts the most relevant keyframe for each event. We assess the results by applying a blind-taste test on a group of 20 people who assessed the quality of the summaries.",,Electronic:978-1-4799-7079-7; POD:978-1-4799-7080-3,10.1109/ICMEW.2015.7169863,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169863,egocentric;keyframes;lifelogging;summarization,Cameras;Feature extraction;Image segmentation;Indexes;Motion segmentation;Videos;Visualization,cameras;convolution;feature extraction;neural nets;pattern clustering;personal computing;unsupervised learning,blind-taste test;convolutional neural network;egocentric photostreams;keyframe selection;lifelogging wearable camera;memory reinforcement;representative keyframes;summarization method;unsupervised clustering;visual feature extraction;visual summary,,2,,17,,,,June 29 2015-July 3 2015,,IEEE,IEEE Conferences
Illustration about a Metacognition-based learning detection system conceived to improve web-based self-regulated learning,J. Liang,"Institute of Educational Technology, Institute of Education, Tsinghua University, Beijing, China",2011 International Conference on E-Business and E-Government (ICEE),20110616,2011,,,1,4,"Many learners have found that it is difficult to complete their web-based learning plan. Once on the Internet, they can't help browsing more interesting Web pages instead of continuing to do their learning tasks. This situation we called Information Trek. To solve this problem, this study proposes an learning detection system which can discover whether the contents of a web page a student viewing is about learning or not. If a student is detected to be in the state of viewing the non-learning pages, then the alert reinforcement window will be shown. If the attentive time in learning has been reached, then encouraging reinforcement feedback is given. We must consider adequately about personalization given the different levels of Metacognition. In this system, preferring the method to let learner go back to the learning state themselves, we design some functions to guide learners regulate themselves based on the essential process of online self-regulated leaning integrating Metacognitive process.",,Electronic:978-1-4244-8694-6; POD:978-1-4244-8691-5,10.1109/ICEBEG.2011.5886854,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5886854,learning detection system;metacognitive strategies;metacogniton;web-based self-regulated learning,Artificial neural networks;Conferences;Educational technology;Filtering;Helium;Text categorization;Web pages,,,,0,,10,,,,6-8 May 2011,,IEEE,IEEE Conferences
Adaptive joint call admission control and access network selection for multimedia wireless systems,E. Alexandri; G. Martinez; D. Zeghlache,"INT, Motorola Labs., Paris, France",The 5th International Symposium on Wireless Personal Multimedia Communications,20021216,2002,3,,1390,1394 vol.3,"Third generation wireless networks and beyond will solicit the cooperation of heterogeneous access networks, so as to provide multimedia traffic to different classes of users, with varying quality requisites over regions and time zones. We address the problem of how to partition the traffic demand efficiently onto the underlying radio access networks. The design objective is a resource allocation strategy, which provides a maximal resource utilization across all access networks. At the same time, the allocation should respect quality levels related to handover dropping performance; these levels can be predefined per service and per region. We propose a solution based on reinforcement learning, which runs independently at each of the cells of every access system, and report results. In the case where network revenue does not depend solely on resource utilization, but on parameters such as the type of service and/or the service duration, the method is readily extensible to include these factors.",1347-6890;13476890,POD:0-7803-7442-8,10.1109/WPMC.2002.1088408,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1088408,,Adaptive control;Call admission control;Communication system traffic control;Learning;Multimedia systems;Programmable control;Radio access networks;Resource management;Telecommunication traffic;Wireless networks,3G mobile communication;cellular radio;learning (artificial intelligence);multimedia communication;optimisation;quality of service;radio access networks;resource allocation;telecommunication congestion control;telecommunication traffic,adaptive access network selection;adaptive call admission control;handover dropping;multimedia traffic;network revenue;quality levels;radio access networks;reinforcement learning;resource allocation strategy;resource utilization optimization;third generation wireless networks,,7,,5,,,,27-30 Oct. 2002,,IEEE,IEEE Conferences
Framework for control and deep reinforcement learning in traffic,C. Wu; K. Parvate; N. Kheterpal; L. Dickstein; A. Mehta; E. Vinitsky; A. M. Bayen,"UC Berkeley, Electrical Engineering and Computer Science",2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC),20180315,2017,,,1,8,"Recent advances in deep reinforcement learning (RL) offer an opportunity to revisit complex traffic control problems at the level of vehicle dynamics, with the aim of learning locally optimal policies (with respect to the policy parameterization) for a variety of objectives such as matching a target velocity or minimizing fuel consumption. In this article, we present a framework called CISTAR (Customized Interface for SUMO, TraCI, and RLLab) that integrates the widely used traffic simulator SUMO with a standard deep reinforcement learning library RLLab. We create an interface allowing for easy customization of SUMO, allowing users to easily implement new controllers, heterogeneous experiments, and user-defined cost functions that depend on arbitrary state variables. We demonstrate the usage of CISTAR with several benchmark control and RL examples.",,Electronic:978-1-5386-1526-3; POD:978-1-5386-1527-0; USB:978-1-5386-1525-6,10.1109/ITSC.2017.8317694,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8317694,Simulation;control;deep reinforcement learning;vehicle dynamics,Acceleration;Automobiles;Learning (artificial intelligence);Libraries;Machine learning;Roads;Vehicle dynamics,control engineering computing;learning (artificial intelligence);optimisation;road traffic control;road vehicles;traffic engineering computing;vehicle dynamics,Customized Interface;SUMO;benchmark control;fuel consumption minimization;locally optimal policies;policy parameterization;standard deep reinforcement learning library RLLab;traffic control;vehicle dynamics,,,,,,,,16-19 Oct. 2017,,IEEE,IEEE Conferences
Towards a learning framework for dancing robots,I. S. Tholley; Q. Meng; P. W. H. Chung,"Computer Science Department and Research School of Informatics, Loughborough University, UK",2009 IEEE International Conference on Control and Automation,20100208,2009,,,1581,1586,"How can we make robots learn how to dance? How do humans learn to dance? An emerging culture of dancing robots is becoming more prominent in the research community with more emphasis on how we can show of our own creativity rather than allowing the robots to develop their own cognitive and psychological behaviours to the music being played. There are many different types of music and indeed, many different robots and many ways, in which they can dance to music however, much of the work carried out in this field concern limiting robots to dance in particular ways to a specific music and no adaptive behaviour implemented in them to be able to respond intuitively to music in general. We propose in this paper, a way in which such a problem can begin to be looked into, by introducing fundamental things that should be learnt that are necessary for dancing. We programmed a virtual robot to learn to dance to the beat as well as recognise the downbeat of any time-signature and tailor its movements to the loudness of music, using the Sarsa and the Sarsa(Â¿) algorithms from reinforcement learning as the learning framework. Experimental results show that it is possible to make robots learn to dance to these fundamental rhythmic features of music.",1948-3449;19483449,POD:978-1-4244-4706-0,10.1109/ICCA.2009.5410324,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5410324,,Automatic control;Cognitive robotics;Computer science;Humans;Learning;Psychology;Rhythm;Robotics and automation;Robots;Timing,learning systems;robots,Sarsa algorithms;dancing robots;reinforcement learning;time-signature;virtual robot,,2,,20,,,,9-11 Dec. 2009,,IEEE,IEEE Conferences
Automated Ingestion Detection for a Health Monitoring System,W. P. Walker; D. K. Bhatia,"Embedded and Adaptive Computing Group, the University of Texas at Dallas, Richardson, TX, USA",IEEE Journal of Biomedical and Health Informatics,20140303,2014,18,2,682,692,"Obesity is a global epidemic that imposes a financial burden and increased risk for a myriad of chronic diseases. Presented here is an overview of a prototype automated ingestion detection (AID) process implemented in a health monitoring system (HMS). The automated detection of ingestion supports personal record keeping which is essential during obesity management. Personal record keeping allows the care provider to monitor the therapeutic progress of a patient. The AID-HMS determines the levels of ingestion activity from sounds captured by an external throat microphone. Features are extracted from the sound recording and presented to machine learning classifiers, where a simple voting procedure is employed to determine instances of ingestion. Using a dataset acquired from seven individuals consisting of consumption of liquid and solid, speech, and miscellaneous sounds, > 94% of ingestion sounds are correctly identified with false positive rates around 9% based on 10-fold cross validation. The detected levels of ingestion activity are transmitted and stored on a remote web server, where information is displayed through a web application operating in a web browser. This information allows remote users (health provider) determine meal lengths and levels of ingestion activity during the meal. The AID-HMS also provides a basis for automated reinforcement for the patient.",2168-2194;21682194,,10.1109/JBHI.2013.2279193,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6594833,Health monitoring;obesity management;patient empowerment;patient monitoring,Detectors;Feature extraction;Liquids;Microphones;Monitoring;Solids,Internet;data acquisition;diseases;epidemics;feature extraction;learning (artificial intelligence);medical signal processing;microphones;online front-ends;patient care;patient monitoring;pattern classification;signal classification;speech processing,AID-HMS;Web application;Web browser;automated ingestion detection;chronic diseases;dataset acquisition;external throat microphone;false positive rates;feature extraction;health monitoring system;ingestion activity;machine learning classifiers;miscellaneous sounds;myriad;obesity management;patient care;patient monitoring;personal record;prototype automated ingestion detection;remote Web server;sound recording;speech;therapeutic progress,0,4,,65,,,20130909,March 2014,,IEEE,IEEE Journals & Magazines
Imitation Learning for Dynamic VFI Control in Large-Scale Manycore Systems,R. G. Kim; W. Choi; Z. Chen; J. R. Doppa; P. P. Pande; D. Marculescu; R. Marculescu,"Carnegie Mellon University, Pittsburgh, PA, USA",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,20170823,2017,25,9,2458,2471,"Manycore chips are widely employed in high-performance computing and large-scale data analysis. However, the design of high-performance manycore chips is dominated by power and thermal constraints. In this respect, voltage-frequency island (VFI) is a promising design paradigm to create scalable energy-efficient platforms. By dynamically tailoring the voltage and frequency of each island, we can further improve the energy savings within given performance constraints. Inspired by the recent success of imitation learning (IL) in many application domains and its significant advantages over reinforcement learning (RL), we propose the first architecture-independent IL-based methodology for dynamic VFI (DVFI) control in manycore systems. Due to its popularity in the EDA community, we consider an RL-based DVFI control methodology as a strong baseline. Our experimental results demonstrate that IL is able to obtain higher quality policies than RL (on average, 5% less energy with the same level of performance) with significantly less computation time and hardware area overheads (3.1X and 8.8X, respectively).",1063-8210;10638210,,10.1109/TVLSI.2017.2700726,10.13039/100000001 - U.S. National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7934357,Dynamic voltage and frequency scaling (DVFS);low power;machine learning (ML);manycore systems;voltage-frequency islands (VFIs),Control systems;Energy consumption;Energy dissipation;Hardware;Resource management;Scalability;Very large scale integration,energy conservation;learning (artificial intelligence);microprocessor chips;multiprocessing systems;parallel processing;performance evaluation;power aware computing,DVFI control;EDA community;RL-based DVFI control;architecture-independent IL-based methodology;dynamic VFI control;dynamic voltage and frequency scaling;energy savings;high-performance computing;high-performance manycore chips;imitation learning;large-scale data analysis;large-scale manycore systems;performance constraints;power constraints;reinforcement learning;scalable energy-efficient platforms;thermal constraints;voltage-frequency island,,1,,,,,20170526,Sept. 2017,,IEEE,IEEE Journals & Magazines
A game-theoretic framework with reinforcement learning for multinode cooperation in wireless networks,M. W. Baidas,"Dept. of Electr. Eng., Kuwait Univ., Kuwait City, Kuwait","2013 IEEE 24th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",20131125,2013,,,981,986,"In this paper, a game-theoretic framework based on the iterated prisoner's dilemma (IPD) is proposed to model the repeated dynamic interactions of multiple source nodes when communicating with multiple destinations in ad-hoc wireless networks. In such networks where nodes are autonomous, selfish, and not familiar with other nodes' strategies, fully cooperative behaviors cannot be assumed. Thus, a Q-learning algorithm is proposed to allow network nodes to adapt to and play the IPD game against opponents with a variety of known and unknown strategies. Simulation results illustrate that the proposed Q-learning algorithm allows network nodes to play optimally and achieve their maximum expected return values.",2166-9570;21669570,Electronic:978-1-4673-6235-1; POD:978-1-4673-6233-7; USB:978-1-4673-6234-4,10.1109/PIMRC.2013.6666280,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6666280,Amplify-and-forward (AF);Q-learning;cooperation;game-theory;prisoner's dilemma;reinforcement learning,Broadcasting;Convergence;Games;Learning (artificial intelligence);Silicon;Thin film transistors;Wireless networks,ad hoc networks;cooperative communication;game theory;learning (artificial intelligence);telecommunication computing,IPD;IPD game;Q-learning algorithm;ad-hoc wireless networks;fully cooperative behaviors;game-theoretic framework;iterated prisoner dilemma;multinode cooperation;multiple source nodes;network nodes;reinforcement learning,,2,,21,,,,8-11 Sept. 2013,,IEEE,IEEE Conferences
Application of reinforcement learning to admission control in CDMA network,B. Makarevitch,"Commun. Lab., Helsinki Univ. of Technol., Espoo, Finland",11th IEEE International Symposium on Personal Indoor and Mobile Radio Communications. PIMRC 2000. Proceedings (Cat. No.00TH8525),20020806,2000,2,,1353,1357 vol.2,The paper describes an admission control algorithm for the CDMA networks which is able to adapt to the operating environment. The algorithm is based on the principle of reinforcement learning and it achieves near-optimal performance for various radio propagation conditions and network operator's objectives. The performance evaluation results for different state space alternatives and algorithm parameters are presented and compared with the conventional admission control based on the power thresholds,,POD:0-7803-6463-5,10.1109/PIMRC.2000.881639,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=881639,,Admission control;Base stations;Communication system control;Degradation;Downlink;Intelligent networks;Interference;Learning;Multiaccess communication;Paper technology,Markov processes;cellular radio;code division multiple access;learning (artificial intelligence);multiuser channels;radio networks;telecommunication congestion control,CDMA network;Markov decision process;admission control algorithm;algorithm parameters;cellular radio;near-optimal performance;network operator objectives;performance evaluation results;power thresholds;radio propagation conditions;reinforcement learning;state space alternatives,,3,1,4,,,,2000,18 Sep 2000-21 Sep 2000,IEEE,IEEE Conferences
Reinforcement learning approach to dynamic activation of base station resources in wireless networks,P. Y. Kong; D. Panaitopol,"Khalifa Univ. of Sci., Technol. & Res. (KUSTAR), Abu Dhabi, United Arab Emirates","2013 IEEE 24th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",20131125,2013,,,3264,3268,"Recently, the issue of energy efficiency in wireless networks has attracted much research attention due to the growing concern on global warming and operator's profitability. We focus on energy efficiency of base stations because they account for 80% of total energy consumed in a wireless network. In this paper, we intend to reduce energy consumption of a base station by dynamically activating and deactivating the modular resources at the base station depending on the instantaneous network traffic. We propose an online reinforcement learning algorithm that will continuously adapt to the changing network traffic in deciding which action to take to maximize energy saving. As an online algorithm, the proposed scheme does not require a separate training phase and can be deployed immediately. Simulation results have confirmed that the proposed algorithm can achieve more than 50% energy saving without compromising network service quality which is measured in terms of user blocking probability.",2166-9570;21669570,Electronic:978-1-4673-6235-1; POD:978-1-4673-6233-7; USB:978-1-4673-6234-4,10.1109/PIMRC.2013.6666710,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6666710,Green wireless networks;energy efficient base station;online Q-Learning;reinforcement learning,Base stations;Dynamic scheduling;Energy consumption;Heuristic algorithms;Learning (artificial intelligence);Q-factor;Wireless networks,learning (artificial intelligence);probability;radio networks;telecommunication computing;telecommunication traffic,base station resource dynamic activation;blocking probability;energy efficiency;energy saving;global warming;network traffic;operator profitability;reinforcement learning approach;wireless networks,,4,,10,,,,8-11 Sept. 2013,,IEEE,IEEE Conferences
Path planning with user route preference - A reward surface approximation approach using orthogonal Legendre polynomials,A. R. Srinivasan; S. Chakraborty,"Department of Mechanical Aerospace and Biomedical Engineering, University of Tennessee, Knoxville, 37996 USA",2016 IEEE International Conference on Automation Science and Engineering (CASE),20161117,2016,,,1100,1105,"As self driving cars become more ubiquitous, users would look for natural ways of informing the car AI about their personal choice of routes. This choice is not always dictated by straightforward logic such as shortest distance or shortest time, and can be influenced by hidden factors, such as comfort and familiarity. This paper presents a path learning algorithm for such applications, where from limited positive demonstrations, an autonomous agent learns the user's path preference and honors that choice in its route planning, but has the capability to adopt alternate routes, if the original choice(s) become impractical. The learning problem is modeled as a Markov decision process. The states (way-points) and actions (to move from one way-point to another) are pre-defined according to the existing network of paths between the origin and destination and the user's demonstration is assumed to be a sample of the preferred path. The underlying reward function which captures the essence of the demonstration is computed using an inverse reinforcement learning algorithm and from that the entire path mirroring the expert's demonstration is extracted. To alleviate the problem of state space explosion when dealing with a large state space, the reward function is approximated using a set of orthogonal polynomial basis functions with a fixed number of coefficients regardless of the size of the state space. A six fold reduction in total learning time is achieved compared to using simple basis functions, that has dimensionality equal to the number of distinct states.",,Electronic:978-1-5090-2409-4; POD:978-1-5090-2410-0; USB:978-1-5090-2408-7,10.1109/COASE.2016.7743527,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7743527,,Automobiles;Databases;Learning (artificial intelligence);Markov processes;Path planning;Planning;Real-time systems,Markov processes;learning (artificial intelligence);mobile robots;path planning;state-space methods,Markov decision process;autonomous agent learns;dimensionality;inverse reinforcement learning algorithm;large state space;orthogonal Legendre polynomials;orthogonal polynomial basis functions;path planning;reward function;reward surface approximation;route planning;self driving cars;six fold reduction;state space explosion;user path preference;user route preference,,,,,,,,21-25 Aug. 2016,,IEEE,IEEE Conferences
An auction-based approach to spectrum allocation using multi-agent reinforcement learning,N. Abji; A. Leon-Garcia,"Dept. of Electrical and Computer Engineering, University of Toronto, Italy","21st Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications",20101217,2010,,,2233,2238,"We present an auction-based approach to spectrum management in a multi-operator context. Service providers compete for customers in real-time through live auctions. To automate the bidding process we implement a multi-agent reinforcement learning solution. We study the effect of real-time competition between service providers by considering the cases where there is a single provider and multiple providers. Furthermore, we demonstrate how users of varying types, based on application-type and willingness to pay, can be accommodated. We utilize a low-complexity bid-proportional allocation mechanism which ensures fairness. Our simulation results show that when there is a single provider, revenue can be maximized by artificially limiting supply and creating contention. However, when there are multiple providers from which the customers can dynamically choose, there is no longer an incentive to restrict supply due to the direct competition between service providers.",2166-9570;21669570,Electronic:978-1-4244-8016-6; POD:978-1-4244-8017-3,10.1109/PIMRC.2010.5671682,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5671682,,Land mobile radio,learning (artificial intelligence);multi-agent systems;radio spectrum management;telecommunication computing,auction-based approach;bidding process;low-complexity bid-proportional allocation mechanism;multiagent reinforcement learning;service providers;spectrum allocation;spectrum management,,4,,10,,,,26-30 Sept. 2010,,IEEE,IEEE Conferences
Risk-sensitivity through multi-objective reinforcement learning,K. Van Moffaert; T. Brys; A. Nowé,"Department of Computer Science, Vrije Universiteit Brussel, Brussels, Belgium",2015 IEEE Congress on Evolutionary Computation (CEC),20150914,2015,,,1746,1753,"Usually in reinforcement learning, the goal of the agent is to maximize the expected return. However, in practical applications, algorithms that solely focus on maximizing the mean return could be inappropriate as they do not account for the variability of their solutions. Thereby, a variability measure could be included to accommodate for a risk-sensitive setting, i.e. where the system engineer can explicitly define the tolerated level of variance. Our approach is based on multi-objectivization where a standard single-objective environment is extended with one (or more) additional objectives. More precisely, we augment the standard feedback signal of an environment with an additional objective that defines the variance of the solution. We highlight that our algorithm, named risk-sensitive Pareto Q-learning, is (1) specifically tailored to learn a set of Pareto non-dominated policies that trade-off these two objectives. Additionally (2), the algorithm can also retrieve every policy that has been learned throughout the state-action space. This in contrast to standard risk-sensitive approaches where only a single trade-off between mean and variance is learned at a time.",1089-778X;1089778X,Electronic:978-1-4799-7492-4; POD:978-1-4799-7493-1,10.1109/CEC.2015.7257098,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7257098,,,feedback;learning (artificial intelligence);risk analysis;sensitivity analysis,multiobjective reinforcement learning;nondominated policies;risk-sensitive Pareto Q-learning;standard feedback signal;standard single-objective environment;state-action space;variability measure,,1,,27,,,,25-28 May 2015,,IEEE,IEEE Conferences
An MARL-Based Distributed Learning Scheme for Capturing User Preferences in a Smart Environment,J. Lim; H. Son; D. Lee; D. Lee,"Sch. of Comput., KAIST, Daejeon, South Korea",2017 IEEE International Conference on Services Computing (SCC),20170914,2017,,,132,139,"Providing a personalized service to a user in a smart environment has been one of the key goals in the area of pervasive computing. The proliferation of individually developed smart devices in the name of Internet of Things opens up a possibility of providing personalized services to a user in an autonomous and distributed manner. As a user's task often involves services supported by multiple devices, capturing a device-specific service preference is not enough to maximize a user's comfort. In this paper, we propose a distributed learning scheme for capturing multiple device service preferences in a smart environment. We exploit multi-agent reinforcement learning (MARL) method where each smart device acts as a reinforcement learning agent to incrementally and cooperatively capture a user specific preference of a task. Experiments confirm that smart devices with the proposed scheme are able to capture multiple device service preferences from a small number of interactions with a user and an environment. Also, the proposed transfer learning method improves learning performance for a new task.",,Electronic:978-1-5386-2005-2; POD:978-1-5386-2006-9; USB:978-1-5386-2004-5,10.1109/SCC.2017.24,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8034977,context aware;distributed learning;personalization;smart device;user preference,Brightness;Learning (artificial intelligence);Ontologies;Performance evaluation;Servers;Smart devices;TV,learning (artificial intelligence);multi-agent systems;ubiquitous computing,Internet of Things;MARL;autonomous distributed manner;capturing user preferences;device-specific service preference;distributed learning scheme;individually developed smart devices;multiagent reinforcement learning method;multiple device service preferences;personalized service;pervasive computing;reinforcement learning agent;smart environment;transfer learning method;user specific preference,,,,,,,,25-30 June 2017,,IEEE,IEEE Conferences
Dynamic Class of Service mapping for Quality of Experience control in future networks,F. D. Priscoli; L. Fogliati; A. Palo; A. Pietrabissa,,WTC 2014; World Telecommunications Congress 2014,20140624,2014,,,1,6,"The present work introduces a novel approach to cope with some key limitations of the present communication networks. In particular, the need for effectively managing heterogeneous resources over heterogeneous networks while guaranteeing personalized Quality of Experience (QoE) requirements to the applications, claims for a full cognitive approach which is realized through the introduction of an appropriate Future Internet architecture. The paper, taking this architecture in mind, introduces the innovative concept of dynamic association between applications and Classes of Services, performed by means of proper Reinforcement Learning algorithms. The resulting procedure guarantees QoE personalization, requires low processing capabilities and entails limited signalling overhead.",,Paper:978-3-8007-3602-7,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6840010,,,,,,0,,,,,,1-3 June 2014,,VDE,VDE Conferences
A Fast Interactive Search System for Healthcare Services,M. Daltayanni; C. Wang; R. Akella,"Technol. & Inf. Manage., Univ. of California Santa Cruz, Santa Cruz, CA, USA",2012 Annual SRII Global Conference,20120924,2012,,,525,534,"In this paper we describe the design, development, and evaluation of a general human-machine interaction search system, and its potential and use in the context of a collaboration project with SAP and Saffron. The objective of a specialized version of the system is to provide medical and healthcare information services to users via interactive search for personalized patient needs. Patients usually have questions regarding healthcare, including those which concern illness symptoms, duration and types of treatment, possible drug effects, and more. Authorized personnel would often be ideal in responding to such needs; however they could potentially be very expensive, and not easy to support and maintain. If patients could have access to information at their home, by means of i-phone or online access, this could save time, doctor office visit expenses, as well as valuable and restricted medical time. What is more, information concerning other anonymized and similar patient cases provides knowledge and perspective on a wide range of patient issues. From the doctors' perspective, they typically need to spend time on differential analysis about new patient cases: study symptoms, research possible causes, rank results by emergency priority and treat them accordingly. A search system that would direct a doctor (or patient/user) to similar patient cases would save significant amount of manual search time. The powerful new feature of this system is the storage and mining of past patient cases knowledge, to create metadata to be used in the subsequent retrieval of relevant documents. Finally, the interactive search system would speed up identification of rare cases; for instance, symptoms that do not appear commonly in past cases may require special treatment or expert referral. We build a model which dynamically learns medical needs of interacting MDs and patients. The model works on free or unstructured text, allowing disambiguation of vague words and flexibility in descri- ing medical needs. In addition, both experts with an advanced knowledge of medical terminology, and beginning users using basic medical terms, can achieve high search relevance. Furthermore, our approach obviates the need for the assignment of tags or labels, such as treatment, symptoms, causes, to documents, to respond effectively to user queries. In particular, we build a temporal difference algorithm to predict user's information needs by incorporating both current and predicted knowledge into learning the user profile. Our source of information about the user consists of submitted queries and feedback on the returned results. We tested our system on publicly available medical data (OhsuMed TREC dataset 2002) and we achieved a significant improvement in retrieval accuracy, compared to the literature. We provide quantitative results as well as demonstration screenshots which illustrate a) the value of interaction (user time spent with system versus results accuracy), b) the value of using medical terminology understanding, when compared with simple general words, and c) the value of allowing the maximum number of feedback submissions to vary.",2166-0778;21660778,Electronic:978-1-5090-5643-9; POD:978-1-4673-2318-5; USB:978-0-7695-4770-1,10.1109/SRII.2012.65,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6311035,healthcare information services;interactive retrieval;medical data retrieval;reinforcement learning;temporal difference,Diseases;Information services;Medical diagnostic imaging;Senior citizens;Terminology;Unified modeling language,data mining;health care;human computer interaction;information needs;information retrieval;interactive systems;medical information systems;text analysis,SAP;Saffron;basic medical terms;collaboration project;data mining;data storage;differential analysis;document retrieval;fast interactive search system;free or unstructured text;healthcare information services;human-machine interaction search system;information needs;medical information services;medical terminology;metadata;patient cases knowledge;patient issues;personalized patient needs;publicly available medical data;rare cases;submitted queries;temporal difference algorithm;unstructured text,,0,,25,,,,24-27 July 2012,,IEEE,IEEE Conferences
Incorporating prior knowledge into Q-learning for drug delivery individualization,A. E. Gaweda; M. K. Muezzinoglu; G. R. Aronoff; A. A. Jacobs; J. M. Zurada; M. E. Brier,"Louisville Univ., KY, USA",Fourth International Conference on Machine Learning and Applications (ICMLA'05),20060320,2005,,,6 pp.,,"Individualization of drug delivery in treatment of chronic ailments is a challenge to the physician. Variability of response across patient population requires tailoring the dosing strategies to individual's needs. We have previously demonstrated the potential of reinforcement learning methods to support the physician in the management of anemia. In this paper, we propose the incorporation of prior knowledge into the learning mechanism to further improve the outcomes of the treatment.",,POD:0-7695-2495-8,10.1109/ICMLA.2005.40,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607452,,Animals;Bones;Decision making;Drug delivery;Humans;Jacobian matrices;Learning systems;Production;Protocols;Red blood cells,diseases;drug delivery systems;drugs;learning (artificial intelligence);medical computing,Q-learning;anemia;chronic ailment treatment;drug delivery individualization;reinforcement learning,,2,,8,,,,15-17 Dec. 2005,,IEEE,IEEE Conferences
"Robot self-preservation and adaptation to user preferences in game play, a preliminary study",Á. Castro-González; F. Amirabdollahian; D. Polani; M. Malfaz; M. A. Salichs,"RoboticsLab at the Carlos III University of Madrid, 28911, Legan&#x00E9;s, Madrid, Spain",2011 IEEE International Conference on Robotics and Biomimetics,20120412,2011,,,2491,2498,"It is expected that in a near future, personal robots will be endowed with enough autonomy to function and live in an individual's home. This is while commercial robots are designed with default configuration and factory settings which may often be different to an individual's operating preferences. This paper presents how reinforcement learning is applied and utilised towards personalisation of a robot's behaviour. Two-level reinforcement learning has been implemented: first level is in charge of energy autonomy, i.e. how to survive, and second level is involved in adapting robot's behaviour to user's preferences. In both levels Q-learning algorithm has been applied. First level actions have been learnt in a simulated environment and then the results have been transferred to the real robot. Second level has been fully implemented in the real robot and learnt by human-robot interaction. Finally, experiments showing the performance of the system are presented.",,DVD:978-1-4577-2137-3; Electronic:978-1-4577-2138-0; POD:978-1-4577-2136-6,10.1109/ROBIO.2011.6181679,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6181679,,Batteries;Games;Humans;Learning;Machine learning;Machine learning algorithms;Robots,human-robot interaction;learning (artificial intelligence),Q-learning algorithm;commercial robots;game play;human-robot interaction;personal robots;reinforcement learning;robot behaviour personalisation;robot self-preservation;user preferences adaptation,,2,,28,,,,7-11 Dec. 2011,,IEEE,IEEE Conferences
A 55nm time-domain mixed-signal neuromorphic accelerator with stochastic synapses and embedded reinforcement learning for autonomous micro-robots,A. Amravati; S. B. Nasir; S. Thangadurai; I. Yoon; A. Raychowdhury,"Georgia Institute of Technology, Atlanta, GA",2018 IEEE International Solid - State Circuits Conference - (ISSCC),20180312,2018,,,124,126,"Even as rapid advances are being made in the areas of deep neural networks (DNNs) and convolutional neural networks (CNNs) with most hardware demonstrations geared towards inference in vision-based platforms [1-5], we recognize that true autonomy in intelligent agents will only emerge when such bio-mimetic systems can perform continuous learning through interactions with the environment. Reinforcement learning (RL) presents one such computational paradigm inspired by behaviorist psychology, where autonomous agents take actions in an environment to maximize a notion of cumulative reward. This concept is deeply rooted in the human brain where dopamine mediated neurotransmitters (in the cortex, striatum and thalamus of the brain) have been shown to encourage reward-motivated behavior in all our social interactions (Fig. 7.4.1). In this paper, we present a 690μW (V<sub>CC</sub>=1.2V) neuromorphic accelerator fabricated in 55nm CMOS, which: (1) inherits unique properties of stochastic neural networks, (2) leverages recent advances in Q-learning as an implementation of RL, and (3) demonstrates energy-efficient time-domain mixed-signal (TD-MS) circuit architectures, to provide autonomy to a mobile, self-driving micro-robot at the edge of the cloud, with possible applications in disaster relief, reconnaissance and personal robotics.",,Electronic:978-1-5090-4940-0; POD:978-1-5386-2227-8,10.1109/ISSCC.2018.8310215,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8310215,,Biological neural networks;Neuromorphics;Neurons;Semiconductor device measurement;Sensors;Stochastic processes;Synapses,biomimetics;brain;feedforward neural nets;learning (artificial intelligence);microrobots;mobile robots;neurocontrollers;robot vision,55nm time-domain mixed-signal neuromorphic accelerator;CMOS;RL;autonomous agents;behaviorist psychology;bio-mimetic systems;computational paradigm;convolutional neural networks;cumulative reward;deep neural networks;dopamine mediated neurotransmitters;human brain;intelligent agents;mobile self-driving microrobot;personal robotics;reinforcement learning;reward-motivated behavior;social interactions;stochastic neural networks;stochastic synapses;striatum;thalamus,,,,,,,,11-15 Feb. 2018,,IEEE,IEEE Conferences
Learning from demonstration: Teaching a myoelectric prosthesis with an intact limb via reinforcement learning,G. Vasan; P. M. Pilarski,"Department of Computing Science and the Department of Medicine, University of Alberta, Edmonton, AB T6G 2E1, Canada",2017 International Conference on Rehabilitation Robotics (ICORR),20170814,2017,,,1457,1464,"Prosthetic arms should restore and extend the capabilities of someone with an amputation. They should move naturally and be able to perform elegant, coordinated movements that approximate those of a biological arm. Despite these objectives, the control of modern-day prostheses is often nonintuitive and taxing. Existing devices and control approaches do not yet give users the ability to effect highly synergistic movements during their daily-life control of a prosthetic device. As a step towards improving the control of prosthetic arms and hands, we introduce an intuitive approach to training a prosthetic control system that helps a user achieve hard-to-engineer control behaviours. Specifically, we present an actor-critic reinforcement learning method that for the first time promises to allow someone with an amputation to use their non-amputated arm to teach their prosthetic arm how to move through a wide range of coordinated motions and grasp patterns. We evaluate our method during the myoelectric control of a multi-joint robot arm by non-amputee users, and demonstrate that by using our approach a user can train their arm to perform simultaneous gestures and movements in all three degrees of freedom in the robot's hand and wrist based only on information sampled from the robot and the user's above-elbow myoelectric signals. Our results indicate that this learning-from-demonstration paradigm may be well suited to use by both patients and clinicians with minimal technical knowledge, as it allows a user to personalize the control of his or her prosthesis without having to know the underlying mechanics of the prosthetic limb. These preliminary results also suggest that our approach may extend in a straightforward way to next-generation prostheses with precise finger and wrist control, such that these devices may someday allow users to perform fluid and intuitive movements like playing the piano, catching a ball, and comfortably shaking hands.",,Electronic:978-1-5386-2296-4; POD:978-1-5386-2297-1; USB:978-1-5386-2295-7,10.1109/ICORR.2017.8009453,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8009453,,Elbow;Manipulators;Muscles;Prosthetics;Training;Wrist,electromyography;gait analysis;medical robotics;medical signal processing;prosthetics;signal sampling;student experiments,actor-critic reinforcement learning method;amputation;biological arm;control approaches;coordinated motions;daily-life control;effect highly synergistic movements;elegant coordinated movements;fluid movements;grasp patterns;hard-to-engineer control behaviours;information sampling;intact limb;intuitive movements;learning-from-demonstration paradigm;minimal technical knowledge;modern-day prostheses;multijoint robot arm;myoelectric prosthesis teaching;next-generation prostheses;nonamputated arm;prosthetic arms;prosthetic control system;prosthetic device;prosthetic limb;reinforcement learning;robot hand;three degrees-of-freedom;user above-elbow myoelectric signals;wrist,,,,,,,,17-20 July 2017,,IEEE,IEEE Conferences
Active learning for personalizing treatment,K. Deng; J. Pineau; S. Murphy,"Department of Statistics, University of Michigan, USA",2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL),20110728,2011,,,32,39,"The personalization of treatment via genetic biomarkers and other risk categories has drawn increasing interest among clinical researchers and scientists. A major challenge here is to construct individualized treatment rules (ITR), which recommend the best treatment for each of the different categories of individuals. In general, ITRs can be constructed using data from clinical trials, however these are generally very costly to run. In order to reduce the cost of learning an ITR, we explore active learning techniques designed to carefully decide whom to recruit, and which treatment to assign, throughout the online conduct of the clinical trial. As an initial investigation, we focus on simple ITRs that utilize a small number of subpopulation categories to personalize treatment. To minimize the maximal uncertainty regarding the treatment effects for each subpopulation, we propose the use of a minimax bandit model and provide an active learning policy for solving it. We evaluate our active learning policy using simulated data and data modeled after a clinical trial involving treatments for depressed individuals. We contrast this policy with other plausible active learning policies. The techniques presented in the paper may be generalized to tackle problems of efficient exploration in other domains.",2325-1824;23251824,Electronic:978-1-4244-9888-8; POD:978-1-4244-9887-1,10.1109/ADPRL.2011.5967348,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967348,,Clinical trials;Learning systems;Loss measurement;Machine learning;Recruitment;Resource management;Uncertainty,learning (artificial intelligence);medical computing;minimax techniques;patient treatment,active learning;clinical research;genetic biomarkers;individualized treatment rules;minimax bandit model;risk category;treatment personalization,,1,,32,,,,11-15 April 2011,,IEEE,IEEE Conferences
Coordinating SON instances: Reinforcement learning with distributed value function,O. Iacoboaiea; B. Sayrac; S. Ben Jemaa; P. Bianchi,"Orange Labs, 38-40 rue du General Leclerc 92130, Issy les Moulineaux, France","2014 IEEE 25th Annual International Symposium on Personal, Indoor, and Mobile Radio Communication (PIMRC)",20150629,2014,,,1642,1646,"With the emergence of Self-Organizing Network (SON) functions network operators are faced with a practical problem: coordination of SON instances. The SON functions are usually designed in a standalone manner, i.e. they do not take into account the possibility that other instances of the same or different SON functions may be running in the network. This creates the risk of conflicts and network instability. Therefore a SON COordinator (SONCO) is needed. In this paper we design an operator centric SONCO that sees the SON instances as black-boxes, i.e. it does not know the algorithm inside the SON functions. Our aim is to improve the network stability (i.e. number of parameter changes) for SON instances of the same SON function. We employ Reinforcement Learning (RL) in order to profit from the information on the past SONCO decisions. We simplify the expression of the action-value function and we use state aggregation to further reduce the required state space, making it scale linearly with the number of coordinated cells. We provide a study case with the Mobility Load Balancing (MLB) function independently instantiated on every cell. The results show that the proposed SONCO improves the network stability.",2166-9570;21669570,Electronic:978-1-4799-4912-0; POD:978-1-4799-4911-3,10.1109/PIMRC.2014.7136431,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7136431,Coordination;LTE;MLB;SON;SON instances;reinforcement learning;state aggregation,Algorithm design and analysis;Conferences;Heuristic algorithms;Kernel;Learning (artificial intelligence);Markov processes;Optimization,Long Term Evolution;cellular radio;learning (artificial intelligence);resource allocation;telecommunication computing,LTE;MLB;RL;SON instance coordination;SONCO operator centric design;action-value function;black-boxes;mobility load balancing coordinated cells;network stability;reinforcement learning;self-organizing network functions;state aggregation;state space,,2,,16,,,,2-5 Sept. 2014,,IEEE,IEEE Conferences
Offline and online adaptation in prosocial games,K. C. Apostolakis; K. Stefanidis; A. Psaltis; K. Dimitropoulos; P. Daras,"Information Technologies Institute - ITI, Centre for Research and Technology Hellas - CERTH, Thessaloniki, Greece",2017 9th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games),20171005,2017,,,201,208,"Personalization and maintenance of high levels of engagement still remain two of the main challenges in the design of serious games. Towards this end, in this paper we propose a novel adaptation approach for both online and offline adaptation in prosocial games. In this paper, we describe the implementation of an artificial intelligence driven adaptation manager, whose purpose is to direct players towards game content the players are most likely to enjoy (measured in their engagement responses). As a consequence, we demonstrate how the adaptation manager can be used to increase the chances of players attaining the game's specific prosocial learning objectives. Each mechanism (offline and online) processes different information about the player and concerns different types of factors affecting engagement and prosocial behavior. More specifically, the online mechanism maintains a player engagement profile for game elements related to the provision of Corrective Feedback and Positive Reinforcement, in order to adapt existing game content in real time. On the other hand, off-line adaptation matches players to game scenarios according to the players' prosocial ability and the game scenarios' ranking. The efficiency of the proposed adaptation manger as a tool for enhancing students' prosocial skills development is demonstrated through a small scale experiment, under real-conditions in a school environment, using the prosocial game of Path of Trust.",,Electronic:978-1-5090-5812-9; POD:978-1-5386-1203-3; USB:978-1-5090-5811-2,10.1109/VS-GAMES.2017.8056602,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8056602,adaptation;engagement;prosociality;serious games,,artificial intelligence;computer aided instruction;serious games (computing),Path of Trust;artificial intelligence driven adaptation manager;corrective feedback;direct players;engagement responses;existing game content;game elements;game scenarios;maintenance;off-line adaptation;offline adaptation;online adaptation;online mechanism;player engagement profile;positive reinforcement;prosocial behavior;prosocial games;prosocial skills development;school environment;serious games;students,,,,,,,,6-8 Sept. 2017,,IEEE,IEEE Conferences
Genetic reinforcement learning approach to the heterogeneous machine scheduling problem,Gyoung Hwan Kim; C. S. G. Lee,"Dept. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA",IEEE Transactions on Robotics and Automation,20020806,1998,14,6,879,893,"Focuses on the development of a learning-based heuristic for scheduling heterogeneous machines. Although list scheduling methods have been widely used for a large class of scheduling problems, including the heterogeneous machine scheduling problem, they involve designing priority rules, which usually require a fair amount of insights on the characteristics of the problem to be solved. Instead of elaborate design of priority rules in a single step, we propose an iterative list scheduling process, which refines priority rules while generating a number of schedules. The proposed iterative list scheduling is formulated as a reinforcement learning problem, with states and actions defined in list scheduling. Due to the large number of possible states, reinforcement learning algorithms which use value functions in constructing an optimal policy may not be suitable for scheduling problems. Thus, to directly work with policies rather than the values of states, we propose genetic reinforcement learning (GRL), in which the policies of reinforcement learning are encoded into the chromosomes of genetic algorithms and a near-optimal policy is searched for by genetic algorithms. A GRL-based scheduler, called evolutionary intracell scheduler (EVIS), has been developed and applied to various scheduling problems such as the heterogeneous machine scheduling, the processor scheduling, the job-shop scheduling, the flow-shop scheduling, and the open-shop scheduling problems. The proposed model of EVIS, which has a linear order of population-fitness convergence, is verified by computer experiments. Even without fine tuning EVIS, the quality of solutions achieved by EVIS is comparable to that of problem-tailored heuristics for most of the problem instances",1042-296X;1042296X,,10.1109/70.736772,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=736772,,Approximation algorithms;Biological cells;Cost function;Genetic algorithms;Helium;Iterative algorithms;Job shop scheduling;Learning;Processor scheduling;Scheduling algorithm,genetic algorithms;iterative methods;learning (artificial intelligence);minimisation;production control;scheduling,evolutionary intracell scheduler;flow-shop scheduling;genetic reinforcement learning;heterogeneous machine scheduling problem;iterative list scheduling process;job-shop scheduling;learning-based heuristic;near-optimal policy;open-shop scheduling problems,,6,,67,,,,Dec 1998,,IEEE,IEEE Journals & Magazines
Self-learning system for personalized e-learning,V. Pant; S. Bhasin; S. Jain,"Dept. of Computer Science & Engineering, Graphic Era University, Dehradun, India",2017 International Conference on Emerging Trends in Computing and Communication Technologies (ICETCCT),20180205,2017,,,1,6,"Knowledge is a basic requirement in the era of globalization. The act of acquiring knowledge is learning and with the advent of technology, learning has become easier. E-Learning is a popular learning method which uses the web or other technologies to promote learning. A very traditional learning method is discussed and some new methods with the integration of modern technology have been discussed. Although technologies like artificial intelligence, cloud computing, ontology, fuzzy logic etc. have been used in learning systems they are still less automated, not personalized and do not pose the capability to self-learn. A method is proposed to increase the automation and self-learning in the e-learning management systems. Further, the personalization of learning for the user so that every type of learner can learn from the type and level of the content the learner seems fit is the main aim. The proposed system is feasible, economical and scalable which makes it suitable for every level.",,CD:978-1-5386-1146-3; Electronic:978-1-5386-1147-0; POD:978-1-5386-1148-7,10.1109/ICETCCT.2017.8280344,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8280344,Machine learning;e-learning system;learning management system;reinforcement learning,Crawlers;Electronic learning;Learning management systems;Semantics;Task analysis;Videos,cloud computing;computer aided instruction;fuzzy logic;ontologies (artificial intelligence),artificial intelligence;cloud computing;e-learning management systems;fuzzy logic;learning systems;ontology;personalized e-learning;self-learning system,,,,,,,,17-18 Nov. 2017,,IEEE,IEEE Conferences
Towards scalable and privacy preserving commercial content dissemination in social wireless networks,F. Hajiaghajani; S. Biswas,"Electrical and Computer Engineering, Michigan State University, East Lansing, USA","2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",20180215,2017,,,1,7,"This paper proposes a Q-learning based Device-to-Device multicast routing framework for Social Wireless Networks. The goal of the proposed Scalable Q-learning based Gain-aware Routing (SQGR) content dissemination algorithm is to maximize a predefined economic gain for commercial content generators. This economic gain is defined as the revenue from delivery of a coupon minus the forwarding cost associated with that delivery. SQGR, with its embedding learning abilities, is expected to be robust in dynamic mobility environments. It also preserves scalability and privacy since it does not require storage of per-individual consuming interest and interaction profiles within the network. Using the DTN simulator software ONE, we evaluate functional validity and compare gain performance of SQGR with few existing protocols under various commercial, network and protocol parameters.",,Electronic:978-1-5386-3531-5; POD:978-1-5386-3532-2; Paper:978-1-5386-3529-2; USB:978-1-5386-3530-8,10.1109/PIMRC.2017.8292416,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8292416,Economic Gain of Dissemination;Q-Learning;Reinforcement Learning;Social Wireless Networks,Biological system modeling;Economics;Peer-to-peer computing;Privacy;Probabilistic logic;Routing;Scalability,data privacy;delay tolerant networks;learning (artificial intelligence);mobile computing;mobile radio;multicast communication;routing protocols,Device-to-Device multicast;Gain-aware Routing content dissemination algorithm;SQGR;Scalable Q-learning;commercial content dissemination;dynamic mobility environments;social wireless networks,,,,,,,,8-13 Oct. 2017,,IEEE,IEEE Conferences
A Q-learning-based multi-rate transmission control scheme for RRC in WCDMA systems,Fang-Ching Ren; Chung-Ju Chang; Yih-Shen Chen,,"The 13th IEEE International Symposium on Personal, Indoor and Mobile Radio Communications",20021210,2002,3,,1422,1426 vol.3,"A Q-learning-based multirate transmission control scheme (Q-MRTC) for radio resource control (RRC) in WCDMA systems is proposed. The RRC problem is modelled as a semi-Markov decision process (SMDP). We successfully apply a real-time reinforcement learning algorithm, named Q-learning, to accurately estimate the transmission cost for the multi-rate transmission control. For the cost function approximation, we apply the feature extraction method to map the original state space into a more compact set which represents the resultant interference profile. Simulation results show that the Q-MRTC can achieve higher system throughput and better users' satisfaction index, by an amount of 87% and 50%, respectively, than the interference-based multi-rate transmission control scheme, while keeping the QoS requirement.",,POD:0-7803-7589-0,10.1109/PIMRC.2002.1045263,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1045263,,Control systems;Cost function;Feature extraction;Function approximation;Interference;Learning;Multiaccess communication;Radio control;State-space methods;Throughput,Markov processes;broadband networks;code division multiple access;costing;feature extraction;function approximation;learning (artificial intelligence);multiuser channels;radio networks;radiofrequency interference;telecommunication control,Q-learning-based multirate transmission control;QoS;RRC;WCDMA systems;feature extraction method;function approximation;interference profile;real-time reinforcement learning algorithm;semi-Markov decision process;simulation results;state space mapping;system throughput;transmission cost estimation;users satisfaction index,,0,,12,,,,15-18 Sept. 2002,,IEEE,IEEE Conferences
Deep Reinforcement Learning for Dynamic Treatment Regimes on Medical Registry Data,Y. Liu; B. Logan; N. Liu; Z. Xu; J. Tang; Y. Wang,"Div. of Biostat., Med. Coll. of Wisconsin, Milwaukee, WI, USA",2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914,2017,,,380,385,"In this paper, we propose the first deep reinforce-ment learning framework to estimate the optimal Dynamic Treat-ment Regimes from observational medical data. This framework is more flexible and adaptive for high dimensional action and state spaces than existing reinforcement learning methods to model real life complexity in heterogeneous disease progression and treatment choices, with the goal to provide doctor and patients the data-driven personalized decision recommendations. The proposed deep reinforcement learning framework contains a supervised learning step to predict the most possible expert actions; and a deep reinforcement learning step to estimate the long term value function of Dynamic Treatment Regimes. We motivated and implemented the proposed framework on a data set from the Center for International Bone Marrow Transplant Research (CIBMTR) registry database, focusing on the sequence of prevention and treatments for acute and chronic graft versus host disease. We showed results of the initial implementation that demonstrates promising accuracy in predicting human expert decisions and initial implementation for the reinforcement learning step.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.45,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031178,,Biomedical imaging;Decision making;Diseases;Games;Learning (artificial intelligence);Machine learning,bone;data analysis;diseases;learning (artificial intelligence);patient treatment,CIBMTR;Dynamic Treatment Regimes;International Bone Marrow Transplant Research registry database;chronic graft versus host disease;deep reinforcement learning framework;deep reinforcement learning step;heterogeneous disease progression;medical registry data;observational medical data;personalized decision recommendations;reinforcement learning framework;supervised learning step,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conferences
Consistent Goal-Directed User Model for Realisitc Man-Machine Task-Oriented Spoken Dialogue Simulation,O. Pietquin,"&#201;cole Sup&#233;rieure d'&#201;lectricit&#233; - SUPELEC, Metz Campus - STS Team, 2 rue &#201;douard Belin - F-57070 Metz - FRANCE. email: olivier.pietquin@supelec.fr",2006 IEEE International Conference on Multimedia and Expo,20061226,2006,,,425,428,"Because of the great variability of factors to take into account, designing a spoken dialogue system is still a tailoring task. Rapid design and reusability of previous work is made very difficult. For these reasons, the application of machine learning methods to dialogue strategy optimization has become a leading subject of researches this last decade. Yet, techniques such as reinforcement learning are very demanding in training data while obtaining a substantial amount of data in the particular case of spoken dialogues is time-consuming and therefore expansive. In order to expand existing data sets, dialogue simulation techniques are becoming a standard solution. In this paper we describe a user modeling technique for realistic simulation of man-machine goal-directed spoken dialogues. This model, based on a stochastic description of man-machine communication, unlike previously proposed models, is consistent along the interaction according to its history and a predefined user goal",1945-7871;19457871,CD-ROM:1-4244-0367-7; POD:1-4244-0366-7,10.1109/ICME.2006.262563,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4036627,,Acoustic noise;Automatic speech recognition;History;Learning systems;Man machine systems;Optimization methods;Sociotechnical systems;Speech processing;Stochastic processes;Training data,interactive systems;learning (artificial intelligence);man-machine systems;optimisation;speech processing;stochastic processes;user modelling,machine learning method;optimization;realistic man-machine simulation;spoken dialogue system;stochastic description;task-oriented dialogue simulation technique;user modeling technique,,4,,14,,,,9-12 July 2006,,IEEE,IEEE Conferences
A tailored Q- Learning for routing in wireless sensor networks,V. K. Sharma; S. S. P. Shukla; V. Singh,"Dept. of Comput. Sci. & Eng., Jaypee Polytech. & Training Centre, Rewa, India","2012 2nd IEEE International Conference on Parallel, Distributed and Grid Computing",20130207,2012,,,663,668,Wireless sensor networks (WSNs) have major importance in distributed sensing applications. The important concern in the intend of wireless sensor networks is battery consumption which usually rely on non-renewable sources of energy. In this paper we have proposed a tailored Q-Learning algorithm for routing scheme in wireless sensor network. Our primary goal is to make an efficient routing algorithm with help of modified Q-Learning approach to minimize the energy consumption utilized by sensor nodes. This approach is a modified version of existing Q-Learning method for WSN that leads to the convergence problem.,,Electronic:978-1-4673-2925-5; POD:978-1-4673-2922-4,10.1109/PDGC.2012.6449899,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6449899,Convergence Problem;Q-Learning;Reinforcement learning;WSN Flooding Routing Protocol,Artificial neural networks;Lead;Wireless sensor networks,learning (artificial intelligence);telecommunication computing;telecommunication network routing;wireless sensor networks,Q- learning;Q-learning algorithm;WSN;battery consumption;distributed sensing applications;energy consumption;modified Q-learning approach;nonrenewable energy sources;routing algorithm;routing scheme;sensor nodes;wireless sensor network routing,,1,,13,,,,6-8 Dec. 2012,,IEEE,IEEE Conferences
A Service Recommendation Using Reinforcement Learning for Network-based Robots in Ubiquitous Computing Environments,A. Moon; T. Kang; H. Kim; H. Kim,"Electronice and Telecommunication Research Institute, 161 Gajeong-dong, Yuseong-gu, Daejeon, 305-350, Korea. Email: akmoon@etri.re.kr",RO-MAN 2007 - The 16th IEEE International Symposium on Robot and Human Interactive Communication,20080116,2007,,,821,826,"Ubiquitous robotic companion (URC ) is a new concept for the network-based robot platform which can enable to be following its master wherever or whenever he/she be in order to provide necessary services. The robot platforms in present normally interest in providing services through the direct interaction in responding to the user's demands. On the other hand, URC services are required to be provided by the means of recognizing the circumstances and taking a user's preference into account. In this paper, we propose a service recommendation scheme for URC robots. The proposed service recommendation, developed based on the reinforcement learning, can be used to provide personalized services by learning users' preferences or tasks through the interaction with users. Using simulation for rapid testing, we evaluate of the proposed scheme under a variety of user modeling types and discount factors.",1944-9445;19449445,CD-ROM:978-1-4244-1635-6; POD:978-1-4244-1634-9,10.1109/ROMAN.2007.4415198,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4415198,,Control systems;Human robot interaction;Intelligent robots;Inventory management;Machine learning;Moon;Robot control;Robot sensing systems;Service robots;Ubiquitous computing,computer networks;control engineering computing;learning (artificial intelligence);man-machine systems;robots;ubiquitous computing;user modelling,human-robot interaction;network-based robot;network-based robot platform;rapid testing;reinforcement learning;service recommendation scheme;ubiquitous computing environment;ubiquitous robotic companion;user modeling,,2,,15,,,,26-29 Aug. 2007,,IEEE,IEEE Conferences
A task-oriented service personalization scheme for smart environments using reinforcement learning,B. Tegelund; H. Son; D. Lee,"School of Computer Science, KAIST, Daejeon, S. Korea",2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops),20160421,2016,,,1,6,"Users want IoT environments to provide them with personalized support. These environments therefore need to be able to learn user preferences, such as what temperature the room should be or which lights should be turned on. We propose a novel system that separates the tasks of learning a user's preferences and realizing them within the environment. The system is able to capture user preferences by reinterpreting the problem as an optimization problem and applying inverse reinforcement learning to it. The system is shown to be able to accurately extract simple preferences given a small number of user demonstrations. These preferences are then realized by actuates devices running reinforcement learning-based agents to provide an environment consistent with the learnt preferences, also in situations not included in any user demonstration.",,Electronic:978-1-5090-1941-0; POD:978-1-5090-1942-7,10.1109/PERCOMW.2016.7457110,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457110,,Brightness;Context;Context modeling;Learning (artificial intelligence);Motion pictures;Performance evaluation;Sensors,Internet of Things;ambient intelligence;learning (artificial intelligence);multi-agent systems,IoT environments;personalized support;reinforcement learning-based agents;smart environments;task-oriented service personalization scheme;user preference learning,,2,,10,,,,14-18 March 2016,,IEEE,IEEE Conferences
Adaptive Learning Based on Exercises Fitness Degree,A. M. Mirea; M. C. Preda,,2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology,20091009,2009,3,,215,218,"The paper considers the e-learning systems that provide personalized content to their users and that permanently adapt to the evolution of the users during their learning stages. Such systems help the students to consolidate their knowledge faster than other methods. The main contribution is the proposal of a mathematical model of an adaptive learning system with the mentioned characteristics. The model involves a multi step process where, at each stage, the performances of the student are measured and the system is adapting accordingly.",,POD:978-0-7695-3801-3,10.1109/WI-IAT.2009.266,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5284965,adaptive control;adaptive learning environments;learning systems;personalized learning;reinforcement learning,Adaptive systems;Computer science;Electronic learning;Intelligent agent;Learning systems;Mathematical model;Paper technology;Performance evaluation;Proposals;Testing,,,,0,,5,,,,15-18 Sept. 2009,,IEEE,IEEE Conferences
Budgeted Learning for Developing Personalized Treatment,K. Deng; R. Greiner; S. Murphy,"Dept. of Stat., Univ. of Michigan, Ann Arbor, MI, USA",2014 13th International Conference on Machine Learning and Applications,20150209,2014,,,7,14,"There is increased interest in using patient-specific information to personalize treatment. Personalized treatment decision rules can be learned using data from standard clinical trials, but such trials are very costly to run. This paper explores the use of budgeted learning techniques to design more efficient clinical trials, by effectively determining which type of patients to recruit, at each time, throughout the duration of the trial. We propose a Bayesian bandit model and discuss the computational challenges and issues pertaining to this approach. We compare our budgeted learning algorithm, which approximately minimizes the Bayes risk, using both simulated data and data modeled after a clinical trial for treating depressed individuals, with other plausible algorithms. We show that our budgeted learning algorithm demonstrated excellent performance across a wide variety of situations.",,Electronic:978-1-4799-7415-3; POD:978-1-4799-7416-0; USB:978-1-4799-7414-6,10.1109/ICMLA.2014.8,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7033084,Active Learning;Bayesian;Budgeted Learning;Personalized Treatment;Reinforcement Learning,Algorithm design and analysis;Approximation algorithms;Approximation methods;Bayes methods;Clinical trials;Fasteners;Resource management,Bayes methods;learning (artificial intelligence);medical information systems;minimisation;patient treatment,Bayes risk;Bayesian bandit model;budgeted learning;clinical trial;patient-specific information;personalized treatment decision rule,,0,,24,,,,3-6 Dec. 2014,,IEEE,IEEE Conferences
A dialogue game framework with personalized training using reinforcement learning for computer-assisted language learning,P. h. Su; Y. B. Wang; T. h. Yu; L. s. Lee,"Graduate Institute of Communication Engineering, National Taiwan University, Taiwan","2013 IEEE International Conference on Acoustics, Speech and Signal Processing",20131021,2013,,,8213,8217,"We propose a framework for computer-assisted language learning as a pedagogical dialogue game. The goal is to offer personalized learning sentences on-line for each individual learner considering the learner's learning status, in order to strike a balance between more practice on poorly-pronounced units and complete practice on the whole set of pronunciation units. This objective is achieved using a Markov decision process (MDP) trained with reinforcement learning using simulated learners generated from real learner data. Preliminary experimental results on a subset of the example dialogue script show the effectiveness of the framework.",1520-6149;15206149,Electronic:978-1-4799-0356-6; POD:978-1-4799-0357-3; USB:978-1-4799-0355-9,10.1109/ICASSP.2013.6639266,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6639266,Computer-Assisted Language Learning;Dialogue Game;Markov Decision Process;Reinforcement Learning,Educational institutions;Games;Hidden Markov models;Learning (artificial intelligence);Markov processes;Speech;Training,computer aided instruction;computer games;learning (artificial intelligence);natural languages;speech recognition,Markov decision process;computer assisted language learning;dialogue game framework;pedagogical dialogue game;personalized sentence learning;personalized training;reinforcement learning;simulated learner,,6,,35,,,,26-31 May 2013,,IEEE,IEEE Conferences
Impedance Control of Robot Manipulator in Contact Task Using Machine Learning,B. Kim; S. Park,"Department of Mechanical Engineering, Korea University, Seoul, Korea, Tel : +82-2-3290-3868; E-mail: biomimetic@korea.ac.kr",2006 SICE-ICASE International Joint Conference,20070226,2006,,,2590,2594,"In performing contact tasks using robot manipulators, force control is essential. One approach is to select appropriate stiffness ellipse at the endpoint of the manipulator, where stiffness ellipse is a geometrical shape of force element represented in the principal axis of task space. In this study, we introduce a novel method to tailor stiffness ellipse required to perform contact tasks by using associative search network. Using appropriate performance indexes in the open-door task experiment, we acquired stiffness ellipse trajectory which optimizes dynamic movement of manipulator. Derived stiffness ellipse (or impedance in general) through learning process can be used for the similar task of learning process",,CD-ROM:89-950038-5-5; POD:89-950038-4-7,10.1109/SICE.2006.314795,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4108082,Associative Search Network;Contact task;impedance control;reinforcement learning;stiffness ellipse,Force control;Humans;Impedance;Machine learning;Manipulators;Mechanical engineering;Orbital robotics;Performance analysis;Robot control;Shape,force control;learning (artificial intelligence);manipulators,associative search network;force control;impedance control;machine learning;robot manipulator;stiffness ellipse,,2,,14,,,,18-21 Oct. 2006,,IEEE,IEEE Conferences
Implementation of a learning fuzzy controller,S. Shenoi; K. Ashenayi; M. Timmerman,"Center for Intelligent Syst., Tulsa Univ., OK, USA",IEEE Control Systems,20020806,1995,15,3,73,80,"This article describes our efforts at designing and implementing a practical learning fuzzy controller using inexpensive hardware. The controller engages basic control concepts and system-independent learning rules to enable it to adapt in real time to unknown plants even when it starts with a vacuous initial control policy. The controller remains dormant when the plant is operating satisfactorily and autonomously initiates online adaptation in real time when adverse performance is observed. The Intel-8031-based hardware implementation is geared for extensibility, robustness, and fault tolerance. Limited plant-dependent information is incorporated to tailor the hardware to applications. The design produces learning rates exceeding 200 reinforcements per second. The controller thus is able to learn to control unknown plants in real time even while it is controlling them. Physical experiments indicate that the learning fuzzy controller can rapidly and effectively deal with variations in plant characteristics, compensate for wear and tear, and handle disturbances and noise.<<ETX>>",1066-033X;1066033X,,10.1109/37.387620,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=387620,,Control systems;Delay;Error correction;Fault tolerance;Fuzzy control;Hardware;Real time systems;Robustness;Shape control;Table lookup,control system synthesis;fuzzy control;learning systems;microcontrollers;robust control,Intel-8031-based hardware;basic control concepts;extensibility;fault tolerance;learning fuzzy controller;limited plant-dependent information;robustness;system-independent learning rules;vacuous initial control policy,,11,,24,,,,June 1995,,IEEE,IEEE Journals & Magazines
Reinforcement learning for game personalization on edge devices,A. Bodas; B. Upadhyay; C. Nadiger; S. Abdelhak,Intel Corporation,2018 International Conference on Information and Computer Technologies (ICICT),20180510,2018,,,119,122,"Good progress has been shown recently in the area of active learning, specifically, Reinforcement learning (RL). In this paper, the authors show how RL can be used to personalize games based on user-interaction with the game. The work uses Deep Q network models (DQN) and the open source framework OpenAI to build an RL model that is able to optimize the gamer's engagement level in a game. The authors define an example quantitative measure of gamer engagement and incorporate that into the DQN learning reward function. The gamer experience optimization is empirically demonstrated using a game of Pong. Simulation testing and analysis of results indicate adapted RL models increase engagement reward values, thus enhancing gamer experience. The contribution of this paper is twofold: (1) using RL, it paves the path for wider adaptation to user-behavior, starting with gaming, and (2) it shows analysis and feasibility of an RL algorithm on an edge device (Personal Computer) in real-time.",,Electronic:978-1-5386-5384-5; POD:978-1-5386-5385-2; Paper:978-1-5386-5382-1; USB:978-1-5386-5383-8,10.1109/INFOCT.2018.8356853,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8356853,artificial intelligence;computer games;edge computing;game personalization;reinforcement learning,Adaptation models;Buildings;Games;Learning (artificial intelligence);Mathematical model;Training,computer games;learning (artificial intelligence);mobile computing,DQN learning reward function;RL algorithm;RL model;active learning;deep Q network models;edge device;engagement reward values;game personalization;gamer engagement;gamer experience;open source framework OpenAI;personal computer;reinforcement learning,,,,,,,,23-25 March 2018,,IEEE,IEEE Conferences
Learning to dribble on a real robot by success and failure,M. Riedmiller; R. Hafner; S. Lange; M. Lauer,"Dept. of Mathematics and Informatics, Institute of Computer Science and Institute of Cognitive Science, University of Osnabr&#252;ck, Germany",2008 IEEE International Conference on Robotics and Automation,20080613,2008,,,2207,2208,"Learning directly on real world systems such as autonomous robots is a challenging task, especially if the training signal is given only in terms of success or failure (reinforcement learning). However, if successful, the controller has the advantage of being tailored exactly to the system it eventually has to control. Here we describe, how a neural network based RL controller learns the challenging task of ball dribbling directly on our middle-size robot. The learned behaviour was actively used throughout the RoboCup world championship tournament 2007 in Atlanta, where we won the first place. This constitutes another important step within our Brainstormers project. The goal of this project is to develop an intelligent control architecture for a soccer playing robot, that is able to learn more and more complex behaviours from scratch.",1050-4729;10504729,POD:978-1-4244-1646-2,10.1109/ROBOT.2008.4543536,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4543536,,Biological neural networks;Cognitive robotics;Control systems;Informatics;Intelligent robots;Learning;Mathematics;Robotics and automation;System testing;USA Councils,control engineering computing;learning (artificial intelligence);learning systems;mobile robots;multi-robot systems;neurocontrollers,Brainstormers project;RoboCup;intelligent control;middle-size robot;neural network;reinforcement learning;robot learning;soccer playing robot,,2,,7,,,,19-23 May 2008,,IEEE,IEEE Conferences
A Neural Network Model of Multisensory Representation of Peripersonal Space: Effect of tool use,M. Ursino; M. Zavaglia; E. Magosso; A. Serino; G. di Pellegrino,"Department of Electronics, Computer Science and Systems, University of Bologna, Bologna, Italy",2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,20071022,2007,,,2735,2739,"This work describes an original neural network to simulate representation of the peripersonal space around one hand, in basal conditions and after training with a tool used to reach the far space. The model is composed of two unimodal areas (visual and tactile) connected to a third bimodal area (visual-tactile). Neurons in the bimodal area integrate visual and tactile information and are activated only when a stimulus falls inside the peripersonal space. Moreover, the model assumes that synapses linking unimodal to bimodal neurons can be reinforced by an Hebbian rule during training, but this reinforcement is also under the influence of attention mechanisms. Results show that the peripersonal space, which includes just a small visual space around the hand in normal conditions, becomes elongated in the direction of the tool after training. This expansion of the peripersonal space depends on an expansion of the visual receptive field of bimodal neurons, due to a reinforcement of visual synapses, which were just latent before training. The model may be of value to analyze the neural mechanisms responsible for representing and plastically shaping peripersonal space, and in perspective, for interpretation of psychophysical data on patients with brain damage.",1094-687X;1094687X,CD-ROM:978-1-4244-0788-0; POD:978-1-4244-0787-3,10.1109/IEMBS.2007.4352894,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4352894,,Animals;Biological neural networks;Computer science;Computer simulation;Humans;Neural networks;Neurons;Plastics;Psychology;Radio frequency,Hebbian learning;biology computing;neural nets;neurophysiology;touch (physiological);vision,Hebbian rule;attention mechanism;bimodal neurons;bimodal visual-tactile perception;hand peripersonal space;neural network model;peripersonal space multisensory representation;peripersonal space plastic shaping;synapses;unimodal neurons;unimodal tactile perception;unimodal visual perception;visual receptive field expansion;visual synapse reinforcement,"Animals;Hand;Haplorhini;Humans;Neural Networks (Computer);Neurons, Afferent;Personal Space;Physical Stimulation;Synapses;Tool Use Behavior;Touch;Visual Perception",2,,19,,,,22-26 Aug. 2007,,IEEE,IEEE Conferences
Distributed Q-learning based dynamic spectrum access in high capacity density cognitive cellular systems using secondary LTE spectrum sharing,N. Morozs; D. Grace; T. Clarke,"Department of Electronics, University of York, Heslington, YO10 5DD, United Kingdom",2014 International Symposium on Wireless Personal Multimedia Communications (WPMC),20150122,2014,,,462,467,"In this paper a distributed Q-learning based dynamic spectrum access (DSA) algorithm is applied to a cognitive cellular system designed for providing ultra high capacity density with only secondary access to an LTE channel. Large scale simulations of a stadium temporary event scenario show that the distributed Q-learning based DSA scheme provides robust quality of service (QoS) and extremely high system throughput densities to the users of the stadium network, whilst successfully coexisting with a primary network of macro eNodeBs on the same LTE channel. It is also shown that incorporating spectrum awareness or spectrum sensing based admission control into the DSA algorithm in this scenario does not improve its performance. Therefore, distributed Q-learning based DSA is a viable and easily implementable solution for facilitating secondary LTE spectrum sharing in high capacity density cognitive cellular systems.",1347-6890;13476890,Electronic:978-9860-3-3407-4,10.1109/WPMC.2014.7014863,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014863,Cognitive Cellular Systems;Dynamic Spectrum Access;Reinforcement Learning;Small Cells;Spectrum Sharing,Cognitive radio;Data models;Interference;Quality of service;Sensors;Throughput,Long Term Evolution;cellular radio;channel allocation;cognitive radio;learning (artificial intelligence);quality of service;radio spectrum management;signal detection;telecommunication computing;telecommunication congestion control,LTE channel secondary access;admission control;distributed Q-learning;dynamic spectrum access;high capacity density cognitive cellular systems;macro eNodeBs;quality of service;secondary LTE spectrum sharing;spectrum awareness;spectrum sensing,,4,,23,,,,7-10 Sept. 2014,,IEEE,IEEE Conferences
Learning rate control for downlink shared channel in WCDMA,B. Makarevitch,,"14th IEEE Proceedings on Personal, Indoor and Mobile Radio Communications, 2003. PIMRC 2003.",20040114,2003,3,,2919,2922 vol.3,"The paper considers bit rate control algorithms for the downlink shared channel in WCDMA (wideband code division multiple access) network. The goal of the rate control is to maximise the data throughput while satisfying quality constraints for the voice users. A learning algorithm, based on Reinforcement learning, is proposed. The algorithm facilitates frame acknowledgments to make decisions on the rate change. The control channel power is used as the state variable and a probabilistic decision policy is applied. Performance of the algorithm is evaluated and compared with performance of a simple algorithm without learning. Data throughput and voice quality cost function are used as performance measures. The learning algorithm achieves higher throughput and lower voice quality cost than the reference algorithm.",,POD:0-7803-7822-9,10.1109/PIMRC.2003.1259284,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1259284,,Control systems;Cost function;Downlink;Intelligent networks;Interference;Learning;Multiaccess communication;Propagation losses;Throughput;Wideband,broadband networks;code division multiple access;learning (artificial intelligence);probability;telecommunication channels;telecommunication links,Reinforcement learning;WCDMA;bit rate control algorithm;downlink shared channel;learning algorithm;probabilistic decision policy;state variable;voice user;wideband code division multiple access,,1,,5,,,,7-10 Sept. 2003,,IEEE,IEEE Conferences
Design of semi-decentralized control laws for distributed-air-jet micromanipulators by reinforcement learning,L. Matignon; G. J. Laurent; N. Le Fort-Piat,"FEMTOST/UFC-ENSMM-UTBM-CNRS, Universit&#233; de Franche-Comt&#233;, Besan&#231;on, France",2009 IEEE/RSJ International Conference on Intelligent Robots and Systems,20091215,2009,,,3277,3283,"Recently, a great deal of interest has been developed in learning in multi-agent systems to achieve decentralized control. Machine learning is a popular approach to find controllers that are tailored exactly to the system without any prior model. In this paper, we propose a semi-decentralized reinforcement learning control approach in order to position and convey an object on a contact-free MEMS-based distributed-manipulation system. The experimental results validate the semi-decentralized reinforcement learning method as a way to design control laws for such distributed systems.",2153-0858;21530858,POD:978-1-4244-3803-7,10.1109/IROS.2009.5353902,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5353902,,Actuators;Control systems;Distributed control;Electrodes;Machine learning;Micromanipulators;Microvalves;Multiagent systems;Open loop systems;Sorting,distributed control;jets;learning (artificial intelligence);micromanipulators;micromechanical devices;multi-agent systems,MEMS based distributed manipulation system;distributed air jet micromanipulators;distributed systems;machine learning;multi agent systems;reinforcement learning;semi decentralized control laws;semi decentralized reinforcement learning control,,1,,21,,,,10-15 Oct. 2009,,IEEE,IEEE Conferences
Reinforcement Learning for Active Queue Management in Mobile All-IP Networks,N. Vucevic; J. Perez-Romero; O. Sallent; R. Agusti,"Dept. TSC, Universitat Polit&#232;cnica de Catalunya (UPC), Barcelona, Spain","2007 IEEE 18th International Symposium on Personal, Indoor and Mobile Radio Communications",20071204,2007,,,1,5,"In future all-IP based wireless networks, like the envisaged in the long term evolution (LTE) architectures for future systems, network providers will have to deal with large traffic volumes with different QoS requirements. In order to increase exploitation of network resources wisely, intelligent adaptive solutions for class based traffic regulation are needed. In particular, active queue management (AQM) is regarded as one of these solutions to provide low queuing delay and high throughput to flows by smart packet discarding. In this paper, we propose a novel AQM solution for future all-IP networks based on a reinforcement learning scheme that allows controlling both the queuing delay and the packet loss of the different service classes. The proposed approach is evaluated through simulations and compared against other algorithms used in the literature, like the random early detection (RED) and the drop from tail (DFT), confirming the benefits of the proposed algorithm.",2166-9570;21669570,CD-ROM:978-1-4244-1144-3; POD:978-1-4244-1143-6,10.1109/PIMRC.2007.4394713,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4394713,,Bandwidth;Delay;Diffserv networks;Intelligent networks;Learning;Mobile communication;Quality of service;Tail;Telecommunication traffic;Traffic control,IP networks;learning (artificial intelligence);mobile computing;mobility management (mobile radio);queueing theory;telecommunication traffic,AQM;QoS;active queue management;all-IP based wireless networks;class based traffic regulation;drop from tail;intelligent adaptive solutions;mobile all-IP networks;network resources exploitation;queuing delay;random early detection;reinforcement learning,,2,,15,,,,3-7 Sept. 2007,,IEEE,IEEE Conferences
Emergency Navigation in Confined Spaces Using Dynamic Grouping,H. Bi; O. J. Akinwande; E. Gelenbe,"Dept. of Electr. & Electron. Eng., Imperial Coll. London, London, UK","2015 9th International Conference on Next Generation Mobile Applications, Services and Technologies",20160107,2015,,,120,125,"The performance of Emergency Management Systems (EMS) in confined spaces is highly dependent on the decision algorithm employed for the safe navigation of the evacuees to the available exits. In the algorithm proposed in this paper, we have considered evacuees under two groups, based on their age and physical condition, and we tailor two routing metrics, one for each group, in finding suitable paths for the evacuees. A dynamic grouping mechanism that can adjust an evacuee's group, and therefore routing metric, according to its on-going health condition is employed during the evacuation. To implement the routing metrics, we have used the Cognitive Packet Network (CPN) with random neural networks (RNN) and reinforcement learning. The CPN is an adaptive routing protocol that is loop-free at all times and easily handles multiple quality of service (QoS) metrics. Simulation results show that allowing the navigation system to be sensitive to the on-going health conditions and mobility of the evacuees, using our proposed dynamic grouping, can achieve higher survival rates.",,CD-ROM:978-1-4799-8659-0; Electronic:978-1-4799-8660-6; POD:978-1-4799-8661-3,10.1109/NGMAST.2015.12,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7373229,Cognitive Packet Network;Dynamic Grouping;Emergency navigation;QoS driven protocol,Buildings;Hazards;Heuristic algorithms;Measurement;Navigation;Quality of service;Routing,cognitive radio;emergency management;learning (artificial intelligence);navigation;neural nets;packet radio networks;quality of service;routing protocols;safety,CPN;EMS;QoS metrics;RNN;adaptive routing protocol;cognitive packet network;confined spaces;decision algorithm;dynamic grouping mechanism;emergency management systems;emergency navigation;health conditions;navigation safety;navigation system;physical condition;quality of service metrics;random neural networks;reinforcement learning;routing metrics,,,,22,,,,9-11 Sept. 2015,,IEEE,IEEE Conferences
MPRL: Multiple-Periodic Reinforcement Learning for difficulty adjustment in rehabilitation games,Y. A. Sekhavat,"Faculty of Multimedia, Tabriz Islamic Art University, Iran",2017 IEEE 5th International Conference on Serious Games and Applications for Health (SeGAH),20170608,2017,,,1,7,"Generally, the difficulty level of a therapeutic game is regulated manually by a therapist. However, home-based rehabilitation games require a technique for automatic difficulty adjustment. This paper proposes a personalized difficulty adjustment technique for a rehabilitation game that automatically regulates difficulty settings based on a patient's skills in real-time. To this end, ideas from reinforcement learning are used to dynamically adjust the difficulty of a game. We show that difficulty adjustment is a multiple-objective problem, in which some objectives might be evaluated at different periods. To address this problem, we propose and use Multiple-Periodic Reinforcement Learning (MPRL) that makes it possible to evaluate different objectives of difficulty adjustment in separate periods. The results of experiments show that MPRL outperforms traditional Multiple-Objective Reinforcement Learning (MORL) in terms of user satisfaction parameters as well as improving the movement skills of patients.",,Electronic:978-1-5090-5482-4; POD:978-1-5090-5483-1,10.1109/SeGAH.2017.7939260,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7939260,,Biological cells;Silicon,learning (artificial intelligence);medical computing;patient rehabilitation;serious games (computing),home-based rehabilitation games;multiple-objective problem;multiple-periodic reinforcement learning;patient movement skills;therapeutic game,,,,,,,,2-4 April 2017,,IEEE,IEEE Conferences
Personalized tuning of a reinforcement learning control algorithm for glucose regulation,E. Daskalaki; P. Diem; S. G. Mougiakakou,"Diabetes Technology Research Group, ARTORG Center for Biomedical Engineering Research, University of Bern, 3010, Switzerland",2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20130926,2013,,,3487,3490,"Artificial pancreas is in the forefront of research towards the automatic insulin infusion for patients with type 1 diabetes. Due to the high inter- and intra-variability of the diabetic population, the need for personalized approaches has been raised. This study presents an adaptive, patient-specific control strategy for glucose regulation based on reinforcement learning and more specifically on the Actor-Critic (AC) learning approach. The control algorithm provides daily updates of the basal rate and insulin-to-carbohydrate (IC) ratio in order to optimize glucose regulation. A method for the automatic and personalized initialization of the control algorithm is designed based on the estimation of the transfer entropy (TE) between insulin and glucose signals. The algorithm has been evaluated in silico in adults, adolescents and children for 10 days. Three scenarios of initialization to i) zero values, ii) random values and iii) TE-based values have been comparatively assessed. The results have shown that when the TE-based initialization is used, the algorithm achieves faster learning with 98%, 90% and 73% in the A+B zones of the Control Variability Grid Analysis for adults, adolescents and children respectively after five days compared to 95%, 78%, 41% for random initialization and 93%, 88%, 41% for zero initial values. Furthermore, in the case of children, the daily Low Blood Glucose Index reduces much faster when the TE-based tuning is applied. The results imply that automatic and personalized tuning based on TE reduces the learning period and improves the overall performance of the AC algorithm.",1094-687X;1094687X,Electronic:978-1-4577-0216-7; POD:978-1-4577-0215-0; USB:978-1-4577-0214-3,10.1109/EMBC.2013.6610293,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6610293,,Algorithm design and analysis;Diabetes;Insulin;Integrated circuits;Learning (artificial intelligence);Sugar;Vectors,blood;entropy;learning (artificial intelligence);medical computing;paediatrics;sugar,Actor-Critic learning approach;TE-based initialization;adolescents;artificial pancreas;automatic insulin infusion;basal rate;control variability grid analysis;glucose regulation;glucose signals;insulin signals;insulin-to-carbohydrate ratio;low-blood glucose index;patient-specific control strategy;personalized tuning;reinforcement learning control algorithm;transfer entropy;type 1 diabetes,"Adolescent;Adult;Algorithms;Blood Glucose;Child;Humans;Insulin;Pancreas, Artificial;Precision Medicine;Signal Processing, Computer-Assisted",3,,27,,,,3-7 July 2013,,IEEE,IEEE Conferences
A Three-Layered Mutually Reinforced Model for Personalized Citation Recommendation,X. Cai; J. Han; W. Li; R. Zhang; S. Pan; L. Yang,"School of Automation, Northwestern Polytechnical University, Xi'an 710072, China.",IEEE Transactions on Neural Networks and Learning Systems,,2018,Early Access,Early Access,1,12,"Fast-growing scientific papers pose the problem of rapidly and accurately finding a list of reference papers for a given manuscript. Citation recommendation is an indispensable technique to overcome this obstacle. In this paper, we propose a citation recommendation approach via mutual reinforcement on a three-layered graph, in which each paper, author or venue is represented as a vertex in the paper layer, author layer, and venue layer, respectively. For personalized recommendation, we initiate the random walk separately for each query researcher. However, this has a high computational complexity due to the large graph size. To solve this problem, we apply a three-layered interactive clustering approach to cluster related vertices in the graph. Personalized citation recommendations are then made on the subgraph, generated by the clusters associated with each researcher's needs. When evaluated on the ACL anthology network, DBLP, and CiteSeer ML data sets, the performance of our proposed model-based citation recommendation approach is comparable with that of other state-of-the-art citation recommendation approaches. The results also demonstrate that the personalized recommendation approach is more effective than the nonpersonalized recommendation approach.",2162-237X;2162237X,,10.1109/TNNLS.2018.2817245,China Postdoctoral Science Foundation; National Natural Science Foundation of China; Research Grants Council of Hong Kong; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8337085,Mutually reinforced model;personalized citation recommendation;three-layered interactive clustering.,Clustering algorithms;Computational complexity;Context modeling;Data models;Hybrid power systems;Indexes;Learning systems,,,,,,,,,20180412,,,IEEE,IEEE Early Access Articles
Evaluation of trust in robots: A cognitive approach,B. Kumar; A. D. Dubey,"Department of CS/IS SMCS-CEST, Fiji National University, Fiji",2017 International Conference on Computer Communication and Informatics (ICCCI),20171123,2017,,,1,6,"The study of Human-Robot interaction faces one of the biggest challenges in measuring the trustworthiness of the robots. The enhancement and the augmentation of the human capabilities using the human robot integration are dependent on the reliability and dependability of the robots. These factors become more significant when the participation of the robot is the human robot integration is active and the cohesion between humans and robots is high. In order to measure the trust and other cognitive parameters of the robot, we have designed trust model in this research paper. This paper evaluates the trust of a customized robot while performing a task using three different algorithms. The algorithms used for the path planning task in this paper are simple artificial neural network; reinforcement based artificial neural network and Situation-Operator Model. The trust model proposed in this paper has been simulated using the results obtained while the robot performed its tasks using the three algorithms. The results show that the trust of the robot increases with each learning cycle thereby indicating that the training of the robot enhances the trust parameter of the robot.",,CD:978-1-4673-8853-5; Electronic:978-1-4673-8855-9; POD:978-1-4673-8856-6,10.1109/ICCCI.2017.8117701,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8117701,Capability;Cognitive;Human Computer Interaction;Robot;Trust Model,Artificial neural networks;Indexes;Informatics;Path planning;Robots,human-robot interaction;learning (artificial intelligence);man-machine systems;neural nets;path planning;robots,Situation-Operator Model;customized robot;human capabilities;human robot integration;robot increases;robot trustworthiness;trust model;trust parameter,,,,,,,,5-7 Jan. 2017,,IEEE,IEEE Conferences
Recurrent neural-network training by a learning automaton approach for trajectory learning and control system design,M. K. Sudareshan; T. A. Condarcure,"Dept. of Electr. & Comput. Eng., Arizona Univ., Tucson, AZ, USA",IEEE Transactions on Neural Networks,20020806,1998,9,3,354,368,"We present a training approach using concepts from the theory of stochastic learning automata that eliminates the need for computation of gradients. This approach also offers the flexibility of tailoring a number of specific training algorithms based on the selection of linear and nonlinear reinforcement rules for updating automaton action probabilities. The training efficiency is demonstrated by application to two complex temporal learning scenarios, viz, learning of time-dependent continuous trajectories and feedback controller designs for continuous dynamical plants. For the first problem, it is shown that training algorithms can be tailored following the present approach for a recurrent neural net to learn to generate a benchmark circular trajectory more accurately than possible with existing gradient-based training procedures. For the second problem, it is shown that recurrent neural-network-based feedback controllers can be trained for different control objectives",1045-9227;10459227,,10.1109/72.668879,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=668879,,Adaptive control;Application software;Automatic control;Backpropagation;Computational complexity;Control systems;Learning automata;Microcomputers;Recurrent neural networks;Stochastic processes,control system synthesis;learning automata;learning systems;neurocontrollers;recurrent neural nets,control system design;feedback control;learning systems;recurrent neural-network;reinforcement rules;stochastic learning automaton;temporal learning;trajectory learning,,22,,45,,,,May 1998,,IEEE,IEEE Journals & Magazines
A distributed reinforcement learning approach to maximize resource utilization and control handover dropping in multimedia wireless networks,E. Alexandri; G. Martinez; D. Zeghlache,"Motorola Labs, Gif-sur-Yvette, France","The 13th IEEE International Symposium on Personal, Indoor and Mobile Radio Communications",20021210,2002,5,,2249,2253 vol.5,"A new scheme to maximize resource utilization in a cellular network while respecting constraints on handover dropping probability is proposed and analyzed. The constraints are set for each traffic class separately and have to be respected by the network independently of the area in a localized manner. The problem is formulated as a Markov Decision Process (MDP) and solved by making use of the model-free simulation-based Q-learning algorithm that runs at each cell. Integration of the handover limit in the model is achieved by observing which of the new call arrivals, at a particular state of the system, are mostly responsible for violation of the handover dropping limit. Through trial and error, the algorithm proceeds to the statistical elimination of new admissions in the system, those causing excessive dropping. Results obtained via the proposed Reinforcement Learning (RL) based approach are compared with a resource allocation that takes into consideration heterogeneous and unevenly distributed traffic over the geographical area under consideration. For the scenarios examined, comparable results and performance are observed with an advantage for RL in blocking and utilization.",,POD:0-7803-7589-0,10.1109/PIMRC.2002.1046544,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1046544,,Bandwidth;Communication system traffic control;Intelligent networks;Land mobile radio cellular systems;Learning;Quality of service;Resource management;Telecommunication traffic;Traffic control;Wireless networks,Markov processes;cellular radio;learning (artificial intelligence);multimedia communication;probability,Markov decision process;call arrivals;cellular network;distributed reinforcement learning approach;handover dropping;handover dropping probability;handover limit;model-free simulation-based Q-learning algorithm;multimedia wireless networks;resource utilization;statistical elimination;traffic class;unevenly distributed traffic,,3,,4,,,,15-18 Sept. 2002,,IEEE,IEEE Conferences
A Location Management Scheme with Reinforcement Learning for PCS Networks,Y. h. Zhu; G. Xiao; Z. y. Li; F. Zhu,"College of Information Engineering, Zhejiang University of Technology, Hangzhou, China 310032, phone: +86-571-88320163; fax: +86-571-88320163; e-mail: yhzhu@zjut.edu.cn","2007 IEEE International Conference on Networking, Sensing and Control",20070625,2007,,,224,227,"Mobility management, consisting of location management and handoff management, is a challenge for personal communication services (PCS) networks. Location management mainly involves two operations: location update and paging. A two-stage paging strategy with reinforcement learning is proposed. Under this scheme, each cell in a location area is given a preference. Besides, a small real number is used to decide whether to choose and page cells with higher preferences or with lower ones during the first paging stage so that the scheme can quickly adapt to the moving pattern of mobiles. Simulation results show that the proposed paging strategy can reduce the cost as much as 25%, compared with the paging strategy of the basic location management scheme used in the existing PCS networks.",,CD-ROM:1-4244-1076-2; POD:1-4244-1075-4,10.1109/ICNSC.2007.372781,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4238994,Personal communication service (PCS) networks;location management;mobility management,Communication switching;Conference management;Databases;Learning;Mobile communication;Mobile radio mobility management;Paging strategies;Personal communication networks;Personal digital assistants;Wireless sensor networks,learning (artificial intelligence);mobile computing;mobility management (mobile radio);personal communication networks,handoff management;location management scheme;mobility management;paging strategy;personal communication services;reinforcement learning,,0,,22,,,,15-17 April 2007,,IEEE,IEEE Conferences
Learning to Ground in Spoken Dialogue Systems,O. Pietquin,"&#201;cole Sup&#233;rieure d'&#201;lectricit&#233; - SUP&#201;LEC, Metz Campus - IMS Team, 2 rue &#201;douard Belin - F-57070 Metz - FRANCE. e-mail: olivier.pietquin@supelec.fr","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07",20070604,2007,4,,IV-165,IV-168,"Machine learning methods such as reinforcement learning applied to dialogue strategy optimization has become a leading subject of researches since the mid 90's. Indeed, the great variability of factors to take into account makes the design of a spoken dialogue system a tailoring task and reusability of previous work is very difficult. Yet, techniques such as reinforcement learning are very demanding in training data while obtaining a substantial amount of data in the particular case of spoken dialogues is time-consuming and therefore expansive. In order to expand existing data sets, dialogue simulation techniques are becoming a standard solution. In this paper, we present a user model for realistic spoken dialogue simulation and a method for using this model so as to simulate the grounding process. This allows including grounding subdialogues as actions in the reinforcement learning process and learning adapted strategy.",1520-6149;15206149,CD-ROM:1-4244-0728-1; POD:1-4244-0727-3,10.1109/ICASSP.2007.367189,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218063,Speech Communication;Unsupervised Learning;User Modelling,Automatic speech recognition;Grounding;Learning systems;Machine learning;Man machine systems;Optimization methods;Space exploration;Speech processing;Speech synthesis;Stochastic processes,interactive systems;speech-based user interfaces;unsupervised learning,dialogue simulation techniques;grounding process;realistic spoken dialogue simulation;reinforcement learning;spoken dialogue systems,,2,,15,,,,15-20 April 2007,,IEEE,IEEE Conferences
Design and Evaluation of a Self-Learning HTTP Adaptive Video Streaming Client,M. Claeys; S. Latre; J. Famaey; F. De Turck,"Dept. of Inf. Tech., Ghent Univ., Ghent, Belgium",IEEE Communications Letters,20140416,2014,18,4,716,719,"HTTP Adaptive Streaming (HAS) is becoming the de facto standard for Over-The-Top (OTT)-based video streaming services such as YouTube and Netflix. By splitting a video into multiple segments of a couple of seconds and encoding each of these at multiple quality levels, HAS allows a video client to dynamically adapt the requested quality during the playout to react to network changes. However, state-of-the-art quality selection heuristics are deterministic and tailored to specific network configurations. Therefore, they are unable to cope with a vast range of highly dynamic network settings. In this letter, a novel Reinforcement Learning (RL)-based HAS client is presented and evaluated. The self-learning HAS client dynamically adapts its behaviour by interacting with the environment to optimize the Quality of Experience (QoE), the quality as perceived by the end-user. The proposed client has been thoroughly evaluated using a network-based simulator and is shown to outperform traditional HAS clients by up to 13% in a mobile network environment.",1089-7798;10897798,,10.1109/LCOMM.2014.020414.132649,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6746772,Streaming media;intelligent agent;learning systems;quality of service,Adaptive systems;Bandwidth;Bit rate;Convergence;Standards;Streaming media;Video sequences,hypermedia;learning (artificial intelligence);quality of experience;transport protocols;video streaming,OTT media streaming;QoE;mobile network environment;network-based simulator;novel reinforcement learning based HAS client;over-the-top based video streaming services;quality of experience;self-learning HAS client;self-learning HTTP adaptive video streaming client,,27,,11,,,20140221,April 2014,,IEEE,IEEE Journals & Magazines
Self-organized femto-to-macro interference coordination with partial information,A. Galindo-Serrano; L. Giupponi,"Centre Tecnol. de Telecomunicacions de Catalunya (CTTC), Castelldefels, Spain","2013 IEEE 24th International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC Workshops)",20140109,2013,,,111,116,"In this paper we propose a self-organized method for Intercell Interference Coordination (ICIC) between femto and macro layers. We consider the challenging situation where femtocells are completely autonomous, i.e. they do not receive feedback from the macro network. The absence of a macro to femto interface is compliant with 3GPP Releases 10. We propose a distributed learning approach, based on Reinforcement Learning (RL), for environments characterized by partial information, due to the lack of communication between femtos and macros. The theory behind this approach is funded in the Partially Observable Markov Decision Process (POMDP). The POMDP requires to construct a set of beliefs about the environment. These beliefs are constructed following the spatial Interpolation theory, which allows to estimate the interference perceived by the macrousers. Simulation results show that, through the proposed methodology, femtocells can autonomously learn a transmission power policy to manage the aggregated interference at macrousers. Performances are compared to the complete information situation, which is compliant with the status of Release 11.",,Electronic:978-1-4799-0122-7; POD:978-1-4799-0121-0; USB:978-1-4799-0120-3,10.1109/PIMRCW.2013.6707847,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6707847,Femtocell network;POMDP;interference management;multiagent system;spatial interpolation,Estimation;Femtocells;Interference;Interpolation;Macrocell networks;OFDM;Signal to noise ratio,Markov processes;femtocellular radio;learning (artificial intelligence);telecommunication computing,3GPP Releases 10;POMDP;RL;distributed learning approach;femtocell;intercell interference coordination;macrouser;partially observable Markov decision process;reinforcement learning;self-organized femto-to-macro interference coordination;spatial interpolation theory;transmission power policy,,0,1,16,,,,8-9 Sept. 2013,,IEEE,IEEE Conferences
Extraction of a reward expectation signal from cortical units following ballistic movements generated by a brain machine interface,D. McNiel; M. Akanda; A. Tarigoppula; P. Y. Chhatbar; J. Francis,"Dept. of Phys. & Pharm., State University of New York Downstate Med. Center, Brooklyn, NY",2014 IEEE Signal Processing in Medicine and Biology Symposium (SPMB),20150108,2014,,,1,1,"Movement decoding algorithms used in today's brain-machine interface (BMI) technologies require movement-related neural activity in large quantities as training data to decode with sufficient accuracy the intended movements of the user. Because of physical disability the end users of BMI systems may be unable to readily provide such training data. Moreover, variability in the neural control of movements across patients with disability may result in individually unique training data. These issues limit the generalizability of movement decoding algorithms across BMI users. One potential method of circumventing this generalizability limitation and individualizing BMI technology is the use of reinforcement learning, a group of techniques that require minimal feedback in order to find solutions to an arbitrary problem. One promising means of providing feedback to a reinforcement learning-based BMI is via a neural reward signal found in multiple cortical and subcortical areas. Particularly attractive is the idea of parallel extraction of both the movement control signal and the reward signal from the same electrode array. We examined the neural signal underlying the expectation of reward depending on the probability of successfully reaching a target given the initial ballistic movement generated by a BMI. The real-time extraction of such signal could be used to determine if the user expects a movement generated by a BMI to succeed or fail. This information could then be used to update the control architecture of the BMI to generate an output more in line with the user's intention.",,Electronic:978-1-4799-8184-7; POD:978-1-4799-8185-4,10.1109/SPMB.2014.7002960,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7002960,,Accuracy;Biomedical engineering;Decoding;Educational institutions;Nervous system;Neurosurgery;Training data,biomechanics;biomedical electrodes;brain;brain-computer interfaces;decoding;feature extraction;feedback;learning (artificial intelligence);medical control systems;motion control;neurophysiology,arbitrary problem;brain-machine interface;cortical units;electrode array;generalizability limitation;individualizing BMI technology;individually unique training data;initial ballistic movement;intended user movements;minimal feedback;movement decoding algorithms;movement-related neural activity;multiple cortical areas;neural control variability;parallel extraction;physical disability;real-time extraction;reinforcement learning;reinforcement learning-based BMI;reward expectation signal extraction;subcortical areas;training data decoding,,0,,,,,,13-13 Dec. 2014,,IEEE,IEEE Conferences
"Towards efficient, personalized anesthesia using continuous reinforcement learning for propofol infusion control",C. Lowery; A. A. Faisal,"Dept. of Comput., Imperial Coll. London, London, UK",2013 6th International IEEE/EMBS Conference on Neural Engineering (NER),20140102,2013,,,1414,1417,"We demonstrate the use of reinforcement learning algorithms for efficient and personalized control of patients' depth of general anesthesia during surgical procedures - an important aspect for Neurotechnology. We used the continuous actor-critic learning automaton technique, which was trained and tested in silico using published patient data, physiological simulation and the bispectral index (BIS) of patient EEG. Our two-stage technique learns first a generic effective control strategy based on average patient data (factory stage) and can then fine-tune itself to individual patients (personalization stage). The results showed that the reinforcement learner as compared to a bang-bang controller reduced the dose of the anesthetic agent administered by 9.4% and kept the patient closer to the target state, as measured by RMSE (4.90 compared to 8.47). It also kept the BIS error within a narrow, clinically acceptable range 93.9% of the time. Moreover, the policy was trained using only 50 simulated operations. Being able to learn a control strategy this quickly indicates that the reinforcement learner could also adapt regularly to a patient's changing responses throughout a live operation and facilitate the task of anesthesiologists by prompting them with recommended actions.",1948-3546;19483546,CD-ROM:978-1-4673-1968-3; Electronic:978-1-4673-1969-0; POD:978-1-4673-1967-6,10.1109/NER.2013.6696208,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6696208,,Algorithm design and analysis;Anesthesia;Brain modeling;Indexes;Learning (artificial intelligence);Monitoring;Surgery,bang-bang control;drug delivery systems;drugs;electroencephalography;learning (artificial intelligence);medical control systems;neurophysiology;surgery,BIS error;EEG;RMSE;anesthesiology;anesthetic agent dose reduction;bang-bang controller;bispectral index;continuous actor-critic learning automaton technique;continuous reinforcement learning algorithm;control fine tuning;depth of general anesthesia control;efficient personalized anesthesia control;generic effective control strategy learning;neurotechnology;physiological simulation;propofol infusion control;surgical procedure,,1,1,20,,,,6-8 Nov. 2013,,IEEE,IEEE Conferences
A learning multi-agent system for personalized information filtering,Junhua Chen; Zhonghua Yang,"Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore, Singapore","Fourth International Conference on Information, Communications and Signal Processing, 2003 and the Fourth Pacific Rim Conference on Multimedia. Proceedings of the 2003 Joint",20040504,2003,3,,1864,1868 vol.3,"A multi-agent hybrid learning approach to the problem of personalized information filtering is proposed in this paper. There are four agents in the multi-agent model. The problem is modeled as Monte Carlo reinforcement learning. Our proposed algorithm is modified Monte Carlo method combined with features of unsupervised suffix tree clustering and supervised backpropagation network. We argue that this proposed approach could precisely capture the user's interest without repeatedly asking for his/her explicit rates and converge to the user's interest quickly. A conclusion is drawn that our approach is efficient, precise and converges more quickly compared with existing approaches. A prototype system is being developed.",,POD:0-7803-8185-8,10.1109/ICICS.2003.1292790,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1292790,,Backpropagation algorithms;Clustering algorithms;Contacts;Information filtering;Information filters;Information retrieval;Monte Carlo methods;Multiagent systems;Search engines;Supervised learning,Internet;Monte Carlo methods;backpropagation;information filters;multi-agent systems,Monte Carlo reinforcement learning;backpropagation network;learning multiagent system;personalized information filtering;suffix tree clustering,,1,,9,,,,15-18 Dec. 2003,,IEEE,IEEE Conferences
A Novel Scheduling Algorithm for Video Traffic in High-Rate WPANs,S. Moradi; A. H. Mohsenian Rad; V. W. S. Wong,"Univ. of British Columbia, Vancouver",IEEE GLOBECOM 2007 - IEEE Global Telecommunications Conference,20071226,2007,,,742,747,"The emerging high-rate wireless personal area network (WPAN) technology is capable of supporting high-speed and high-quality real-time multimedia applications. In particular, MPEG-4 video streams are deemed to be a widespread traffic type. However, in the current IEEE 802.15.3 standard for media access control (MAC) of high-rate WPANs, the implementation details of some key issues such as scheduling and quality of service (QoS) provisioning have not been addressed. In this paper, we first propose a mathematical model for the optimal scheduling scheme for MPEG-4 flows in high-rate WPANs. We also propose an RL scheduler based on the reinforcement learning (RL) technique. Simulation results show that our proposed RL scheduler achieves nearly optimal performance and performs better than F-SRPT (Mangharam et al., 2004), EDD+SRPT (Torok et al., 2005), and PAP (Kim and Cho, 2005) scheduling algorithms in terms of a lower decoding failure rate.",1930-529X;1930529X,CD-ROM:978-1-4244-1043-9; POD:978-1-4244-1042-2,10.1109/GLOCOM.2007.144,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4411054,,Communication system traffic control;MPEG 4 Standard;Mathematical model;Media Access Protocol;Optimal scheduling;Quality of service;Scheduling algorithm;Streaming media;Traffic control;Wireless personal area networks,personal area networks;scheduling;telecommunication traffic;video coding;video streaming;wireless sensor networks,MPEG-4 video streams;RL scheduler;high-rate wireless personal area network;real-time multimedia;reinforcement learning;scheduling algorithm;video traffic,,3,,22,,,,26-30 Nov. 2007,,IEEE,IEEE Conferences
An Effective Approach to Continuous User Authentication for Touch Screen Smart Devices,A. B. Buduru; S. S. Yau,"Inf. Assurance Center, Sch. of Comput., Inf., & Decision Syst. Eng., Arizona State Univ., Tempe, AZ, USA","2015 IEEE International Conference on Software Quality, Reliability and Security",20150924,2015,,,219,226,"Due to the rapid increase in the use of personal smart devices, more sensitive data is stored and viewed on these smart devices. This trend makes it easier for attackers to access confidential data by physically compromising (including stealing) these smart devices. Currently, most personal smart devices employ one of the one-time user authentication schemes, such as four-to-six digits, fingerprint or pattern-based schemes. These authentication schemes are often not good enough for securing personal smart devices because the attackers can easily extract all the confidential data from the smart device by breaking such schemes, or by keeping the authenticated session open on a physically compromised smart device. In addition, existing re-authentication or continuous authentication techniques for protecting personal smart devices use centralized architecture and require servers at a centralized location to train and update the learning model used for continuous authentication, which impose additional communication overhead. In this paper, an approach is presented to generating and updating the authentication model on the user's smart device with user's gestures, instead of a centralized server. There are two major advantages in this approach. One is that this approach continuously learns and authenticates finger gestures of the user in the background without requiring the user to provide specific gesture inputs. The other major advantage is to have better authentication accuracy by treating uninterrupted user finger gestures over a short time interval as a single gesture for continuous user authentication.",,Electronic:978-1-4673-7989-2; POD:978-1-4673-7990-8,10.1109/QRS.2015.40,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272936,Touch-screen smart devices;adaptive continuous user authentication;and user re-authentication;reinforcement learning,Accuracy;Authentication;Context;Fingers;Object recognition;Performance evaluation,authorisation;computer crime;gesture recognition;message authentication;smart phones;touch sensitive screens,attackers;authentication accuracy;authentication model;centralized server;confidential data access;continuous user authentication;finger gestures authentication;personal smart devices;sensitive data;touch screen smart devices;uninterrupted user finger gestures,,2,,19,,,,3-5 Aug. 2015,,IEEE,IEEE Conferences
Learning social relations for culture aware interaction,P. Patompak; S. Jeong; I. Nilkhamhang; N. Y. Chong,"School of Information Science, Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Nomi, Ishikawa 923-1292, Japan",2017 14th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI),20170727,2017,,,26,31,"Each person has their private physical and/or psychological area where they do not want to share with others during social interactions. This area gives them comfort about interactions and its size usually depends on various factors such as culture, personal traits, and acquaintanceship. This issue may also arise in case of human-robot interaction, especially when the robot is required to generate a socially competent interaction strategy toward people they are interacting with. Here, we propose a new robot exploration strategy to socially interact with people by considering the social relationship between the robot and each person. To that end, two definitions of interaction area are made: (1) Acceptable area allowed to be shared with other people and robots, and (2) Private area where a human does not want to be interfered by others. Based on these definitions, the robot can optimize the path to maximize the frequency/degree of visiting the acceptable area of each person and to minimize the frequency/degree of trespassing into the private area of them at the same time in an iterative way. In this paper, the social force model (SFM) of each person, based on the potential field concept, is designed by a fuzzy inference system and its parameter is optimized by the reinforcement learning model during interactions. We have shown that the proposed model can generate a suitable SFM of each person, which was quite similar to a ground truth model, allowing to plan a path to simultaneously optimize the two factors of interaction area, respectively.",,Electronic:978-1-5090-3056-9; POD:978-1-5090-3057-6,10.1109/URAI.2017.7992879,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7992879,,Adaptation models;Force;Human-robot interaction;Iron;Mathematical model;Navigation;Robots,fuzzy reasoning;human-robot interaction;learning (artificial intelligence);social aspects of automation,SFM;culture aware interaction;fuzzy inference system;human-robot interaction;potential field concept;reinforcement learning model;robot exploration strategy;social force model;social interactions;social relations;socially competent interaction strategy,,,,,,,,June 28 2017-July 1 2017,,IEEE,IEEE Conferences
Joint radio resource management for LTE-UMTS coexistence scenarios,N. Vučević; J. Pérez-Romero; O. Sallent; R. Agustí,"Dept. TSC, Universitat Polit&#232;cnica de Catalunya (UPC), c/ Jordi Girona 1-3, 08034, Barcelona, Spain","2009 IEEE 20th International Symposium on Personal, Indoor and Mobile Radio Communications",20100415,2009,,,12,16,"Constantly increasing demand for throughput and quality in wireless communication systems leads to continuous research of wise radio resource management, because of the scarce availability of frequency bands and the consequent capacity limitations. In addition, technology evolution is addressed towards spectral efficient techniques that can offer higher data rates. This is the case of OFDMA (Orthogonal Frequency-Division Multiple Access), introduced by 3GPP as the technology for future Long Term Evolution (LTE). However, given the current penetration of legacy technologies such as UMTS (Universal Mobile Telecommunications System), operators will have to deal with the coexistence of multiple Radio Access Technologies (RATs), so that the exploitation of the complementarities between technologies through Joint Radio Resource Management (JRRM) mechanisms will be needed. In this paper we propose a novel dynamic JRRM algorithm for LTE-UMTS coexistence scenarios. The proposed mechanism is based on Reinforcement Learning (RL) which is considered to be a good candidate to achieve cognition in future reconfigurable networks. The proposed solution implements autonomous RL agents in each base station which decide on the allocation of the most suitable RAT to each user. We give a detailed description of the solution and analyze the behavior under various load conditions. We also demonstrate the capability of the algorithm to adjust in dynamic scenarios.",2166-9570;21669570,Electronic:978-1-4244-5123-4; POD:978-1-4244-5122-7,10.1109/PIMRC.2009.5450181,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5450181,LTE;WCDMA;joint radio resource management;reinforcement learning,3G mobile communication;Availability;Cognition;Heuristic algorithms;Learning;Long Term Evolution;Rats;Resource management;Throughput;Wireless communication,3G mobile communication;OFDM modulation;frequency division multiple access,3GPP;LTE-UMTS coexistence;OFDMA;joint radio resource management;multiple radio access technology;orthogonal frequency-division multiple access;reinforcement learning;spectral efficient technique;universal mobile telecommunications system;wireless communication system,,3,1,18,,,,13-16 Sept. 2009,,IEEE,IEEE Conferences
A Comparative Study of Parallel Reinforcement Learning Methods with a PC Cluster System,M. Kushida; K. Takahashi; H. Ueda; T. Miyahara,"Hiroshima City University, Japan",2006 IEEE/WIC/ACM International Conference on Intelligent Agent Technology,20070108,2006,,,416,419,"This paper presents a comparative study of three parallel implementation models for reinforcement learning. Two of them utilize Q-learning, and the other one utilizes fuzzy Q-learning for agent learning. In order to evaluate performance and validity of the three method, a PC (personal computer) cluster system consisting of 16 PCs connected via Gigabit ethernet has been built. For communications to deliver data among PCs, MPI (Message Passing Interface) is employed. Experimental results are compared with one another to show the performance and characteristics of the three methods.",,POD:0-7695-2748-5,10.1109/IAT.2006.3,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4052954,,Computer simulation;Ethernet networks;Learning systems;Master-slave;Message passing;Personal communication networks;Process control,fuzzy set theory;learning (artificial intelligence);message passing;parallel algorithms;workstation clusters,PC cluster system;agent learning;fuzzy Q-learning;gigabit Ethernet;message passing interface;parallel implementation model;parallel reinforcement learning,,2,,7,,,,18-22 Dec. 2006,,IEEE,IEEE Conferences
Comparing generic parameter controllers for EAs,G. Karafotias; M. Hoogendoorn; B. Weel,"Computational Intelligence Group, VU University Amsterdam, The Netherlands",2014 IEEE Symposium on Foundations of Computational Intelligence (FOCI),20150115,2014,,,46,53,"Parameter controllers for Evolutionary Algorithms (EAs) deal with adjusting parameter values during an evolutionary run. Many ad hoc approaches have been presented for parameter control, but few generic parameter controllers exist and, additionally, no comparisons or in depth analyses of these generic controllers are available in literature. This paper presents an extensive comparison of such generic parameter control methods, including a number of novel controllers based on reinforcement learning which are introduced here. We conducted experiments with different EAs and test problems in an one-off setting, i.e. relatively long runs with controllers used out-of-the-box with no tailoring to the problem at hand. Results reveal several interesting insights regarding the effectiveness of parameter control, the niche applications/EAs, the effect of continuous treatment of parameters and the influence of noise and randomness on control.",,Electronic:978-1-4799-4491-0; POD:978-1-4799-4490-3,10.1109/FOCI.2014.7007806,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7007806,,Aerospace electronics;Computational intelligence;Estimation;Interpolation;Learning (artificial intelligence);Phase change random access memory;Silicon,control engineering computing;evolutionary computation;learning (artificial intelligence),ad hoc approaches;depth analyses;evolutionary algorithms;generic parameter control methods;niche applications;reinforcement learning,,0,,24,,,,9-12 Dec. 2014,,IEEE,IEEE Conferences
Latent state models of primary user behavior for opportunistic spectrum access,J. Pajarinen; J. Peltonen; M. A. Uusitalo; A. Hottinen,"Department of Information and Computer Science, Helsinki University of Technology, P.O. Box 5400, FI-02015 TKK, Finland","2009 IEEE 20th International Symposium on Personal, Indoor and Mobile Radio Communications",20100415,2009,,,1267,1271,"Opportunistic spectrum access, where cognitive radio devices detect available unused radio channels and exploit them for communication, avoiding collisions with existing users of the channels, is a central topic of research for future wireless communication. When each device has limited resources to sense which channels are available, the task becomes a reinforcement learning problem that has been studied with partially observable Markov decision processes (POMDPs). However, current POMDP solutions are based on simplistic representations where channels are simply on/off (transmitting or idle). We show that more complicated Markov models where on/off states are part of complicated behavior of the channel owner (primary user) yield better POMDPs achieving more successful transmissions and less collisions.",2166-9570;21669570,Electronic:978-1-4244-5123-4; POD:978-1-4244-5122-7,10.1109/PIMRC.2009.5450037,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5450037,,Chromium;Cognitive radio;Computer science;Interference;Internet telephony;Learning;Road accidents;Telecommunication traffic;Wireless communication;Wireless sensor networks,Markov processes;cognitive radio;learning (artificial intelligence);wireless channels,cognitive radio devices;latent state models;opportunistic spectrum access;partially observable Markov decision processes;primary user behavior;radio channels;reinforcement learning;wireless communication,,3,4,16,,,,13-16 Sept. 2009,,IEEE,IEEE Conferences
Spectrum markets for service provider spectrum trading with reinforcement learning,N. Abji; A. Leon-Garcia,"University of Toronto, Canada","2011 IEEE 22nd International Symposium on Personal, Indoor and Mobile Radio Communications",20120126,2011,,,650,655,We present an auction-based spectrum market approach to service provider spectrum trading. Service providers buy and sell spectrum amongst one another in a spectrum market and simultaneously compete for customers from a common pool. Multi-agent reinforcement learning solutions are applied in both customer nodes and service providers to dynamically manage participation in the market. We outline four possible regulatory scenarios with varying degrees of flexibility and competition. Simulations demonstrate that the allocation of spectrum is efficient and fair. Customers and service providers of varying size are shown to benefit from this approach while the system spectrum efficiency is also significantly improved.,2166-9570;21669570,Electronic:978-1-4577-1348-4; POD:978-1-4577-1346-0; USB:978-1-4577-1347-7,10.1109/PIMRC.2011.6140043,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140043,,Base stations;FCC;Learning;Radio spectrum management;Real time systems;Resource management;Wireless communication,cognitive radio;multi-agent systems;telecommunication services,customer providers;multiagent reinforcement learning solutions;reinforcement learning;service provider spectrum trading,,2,,11,,,,11-14 Sept. 2011,,IEEE,IEEE Conferences
Planning and learning algorithms for routing in Disruption-Tolerant Networks,M. O. Stehr; C. Talcott,"SRI International, Computer Science Laboratory, Menlo Park, California 94025, USA",MILCOM 2008 - 2008 IEEE Military Communications Conference,20090119,2008,,,1,8,"We give an overview of algorithms that we have been developing in the DARPA disruption-tolerant networking program, which aims at improving communication in networks with intermittent and episodic connectivity. Thanks to the use of network caching, this can be accomplished without the need for a simultaneous end-to-end path that is required by traditional Internet and mobile ad-hoc network (MANET) protocols. We employ a disciplined two-level approach that clearly distinguishes the dissemination of application content from the dissemination of network-related knowledge, each of which can be supported by different algorithms. Specifically, we present probabilisitc reflection, a single-message protocol enabling the dissemination of knowledge in strongly disrupted networks. For content dissemination, we present two approaches, namely a symbolic planning algorithm that exploits partially predictable temporal behavior, and a distributed and disruption-tolerant reinforcement learning algorithm that takes into account feedback about past performance.",2155-7578;21557578,POD:978-1-4244-2676-8,10.1109/MILCOM.2008.4753336,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4753336,,Ad hoc networks;Costs;Disruption tolerant networking;IP networks;Mobile ad hoc networks;Personal digital assistants;Protocols;Routing;Satellites;Unmanned aerial vehicles,learning (artificial intelligence);protocols;telecommunication computing;telecommunication network planning;telecommunication network routing,DARPA;disruption-tolerant networks;probabilisitc reflection;reinforcement learning algorithm;routing;single-message protocol;symbolic planning algorithm,,10,,21,,,,16-19 Nov. 2008,,IEEE,IEEE Conferences
Personalized automatic image annotation based on reinforcement learning,Yabo Ni; Miao Zheng; Jiajun Bu; Chun Chen; Dazhou Wang,"Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science, Zhejiang University, Hangzhou 310027, China",2013 IEEE International Conference on Multimedia and Expo (ICME),20130926,2013,,,1,6,"With the rapidly increasing number of personal image collections on the web, it is of great importance to annotate these user-uploaded images in personalized manner. But personalized image annotation is largely ignored by the mainstream of image annotation research. In this paper, we focus on personalizing the automatic image annotation by proposing a general framework which jointly exploits the generic content-based image annotation, personal image tagging history and the content of personal history images. In our framework, two sets of candidate annotations are extracted for each image based on content-based annotation and personal image tagging history. Considering that the user's interest may not stay the same, when exploiting the personal image tagging history, we also take the content of personal history images into account to avoid the noise. To get the final annotations, we propose an unsupervised algorithm based on reinforcement learning to combine the above two candidate annotation sets. Encouraging results show that the proposed framework is effective and promising for personalizing automatic image annotation.",1945-7871;19457871,Electronic:978-1-4799-0015-2; POD:978-1-4799-0014-5; USB:978-1-4799-0013-8,10.1109/ICME.2013.6607456,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607456,Automatic image annotation;Personal image tagging history;Personalization;Reinforcement learning,History;Learning (artificial intelligence);Noise;Semantics;Tagging;Unsupervised learning;Vocabulary,Internet;image classification;image retrieval;learning (artificial intelligence),Web;generic content-based image annotation;personal image collections;personal image tagging history;personalized automatic image annotation;reinforcement learning;user-uploaded images,,0,,14,,,,15-19 July 2013,,IEEE,IEEE Conferences
Context-aware reinforcement learning-based mobile cloud computing for telemonitoring,X. Wang; W. Wang; Z. Jin,"Department of Electrical and Computer Engineering, Binghamton University, State University of New York, Binghamton, NY, 13902 USA",2018 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI),20180409,2018,,,426,429,"Mobile cloud computing (MCC) has been extensively studied to provide pervasive healthcare services in a more affordable manner. Through offloading computation-intensive tasks from mobile to cloud, a significant portion of energy can be saved to extend the mobile battery life, which is critical to maintaining continuous and uninterrupted healthcare services. However, given the ever-changing clinical severity, personal demands, and environmental conditions, it is essential to explore context-aware approach capable of dynamically determining the optimal task offloading strategies and algorithmic settings, with the goal of achieving a balanced trade-off among energy efficiency, diagnostic accuracy, and processing latency. To this aim, we propose a model-free reinforcement learning based task scheduling approach to adapt to the changing requirements.",,Electronic:978-1-5386-2405-0; POD:978-1-5386-2406-7,10.1109/BHI.2018.8333459,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8333459,,Batteries;Cloud computing;Electrocardiography;Hidden Markov models;Medical services;Monitoring;Task analysis,cloud computing;health care;learning (artificial intelligence);mobile computing;scheduling,affordable manner;context-aware approach capable;context-aware reinforcement learning;continuous healthcare services;mobile battery life;mobile cloud;model-free reinforcement learning;offloading computation-intensive tasks;optimal task offloading strategies;pervasive healthcare services;task scheduling approach;uninterrupted healthcare services,,,,,,,,4-7 March 2018,,IEEE,IEEE Conferences
Automatically Learning User Preferences for Personalized Service Composition,Y. Zhao; S. Wang; Y. Zou; J. Ng; T. Ng,"Queen's Univ., Kingston, ON, Canada",2017 IEEE International Conference on Web Services (ICWS),20170911,2017,,,776,783,"With the rapid growth of the web services technologies, users often leverage various web services to perform their daily activities, such as on-line shopping. Due to the massive amount of web services available, a user faces numerous choices to meet their personal preferences when selecting the desired services from the web services with the similar functionality. Therefore, it becomes tedious and cumbersome tasks for users to discover and compose services. To reduce user's cognitive burden, it is critical to support automated service composition and make efficient recommendation of personalized services to achieve user's overall goals. However, existing approaches only offer users with limited options designed for the interest of a group of users without considering individual users' interests. To allow users to compose personalized services without much manual specification, we propose a machine learning approach that applies a learning-to-rank algorithm, RankBoost, to automatically learn user preferences and the prioritization of the preferences from users' historical data. Moreover, our approach uses the multi-objective reinforcement learning (MORL) algorithm to make trade-offs among user preferences and recommends a collection of services to achieve the highest objective. We conduct an empirical study to evaluate our approach by collecting the historical data from 12 subjects. The results demonstrate that our approach outperforms the two well-established baseline approaches by 100%-200% in terms of precision on recommending services.",,Electronic:978-1-5386-0752-7; POD:978-1-5386-0753-4,10.1109/ICWS.2017.93,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029835,,Data mining;Feature extraction;History;Learning (artificial intelligence);Machine learning algorithms;Time-frequency analysis;Web services,Internet;Web services;learning (artificial intelligence);recommender systems;retail data processing,MORL algorithm;RankBoost;Web service technologies;automated service composition;automatic user preference learning;learning-to-rank algorithm;machine learning approach;multiobjective reinforcement learning algorithm;online shopping;personalized service composition;service recommendation,,,,,,,,25-30 June 2017,,IEEE,IEEE Conferences
Hello! Mr. Sage,O. E. Lancaster,,IEEE Transactions on Education,20071112,1976,19,2,54,58,"Mr. Sage, an ideal Professor, can motivate, evoke involvement, and supply rewards and reinforcement, for each individual student all within the same class period. Although his minimum standards require mastery of concepts at all levels of learning, from simple memory to creativity, he does not expect the same performance of all students. His grades are directly related to the breadth of subject matter and depth of mastery. He personalizes his instruction within the classroom and in homework assignments by a range in the difficulty of illustration, thereby challenging each student to his capacity. He uses tests and examinations as learning experiences. To achieve this he combines the excellent feature of the conventional system and the current views on personalized system of instruction.",0018-9359;00189359,,10.1109/TE.1976.4321037,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4321037,,Engineering education;Eyes;History;Testing;Vehicle dynamics;Vehicles,,,,0,,15,,,,May 1976,,IEEE,IEEE Journals & Magazines
"Introduce the ""Flipped Classroom"" into the Basic Medical Experimental Teaching by the Medical Virtual Simulation Platform",G. Zhenying; B. Huiling; W. Guoying,"Sch. of Med., Henan Univ., Kaifeng, China",2015 7th International Conference on Information Technology in Medicine and Education (ITME),20160310,2015,,,484,487,"Based on drawing ""flipped classroom"" experience of practitioners domestic and foreign, together with the full use of modern information tools and the reinforcement of the construction of discipline support system, we build the medical virtual simulation platform. On this basis, we perform reasonable instructional design and good application of ""flipped classroom"" in medical experimental teaching. These efforts adapt the learning requirements of students at different levels, and can enhance students' self-learning ability. These efforts can also train students' creative and independent thinking ability, achieve personalized learning, and improve the quality of medical students' experimental teaching. Construction of medical virtual simulation platform in the school of medicine not only provides a new method for basic medical experimental teaching to enable introduction of ""flipped classroom"" into basic medical experimental teaching, but also promotes implementation of information education and improves the teaching quality of professional disciplines.",,CD-ROM:978-1-4673-8301-1; Electronic:978-1-4673-8302-8; POD:978-1-4673-8303-5,10.1109/ITME.2015.172,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429195,"""flipped classroom"";basic medicine;experimental teaching",Adaptation models;Computational modeling;Computers;Education;Online services;Solid modeling;Videos,computer aided instruction;educational institutions;human factors;learning (artificial intelligence);medical computing;teaching,"""Flipped Classroom"";information education;medical experimental teaching;medical virtual simulation platform;modern information tool",,,,9,,,,13-15 Nov. 2015,,IEEE,IEEE Conferences
Work in progress — Tools and technology to implement a students personal laboratory,M. Walters,"Academic Product Manager, National Instruments",2011 Frontiers in Education Conference (FIE),20120202,2011,,,F1G-1,F1G-2,"Today's students want to solve problems and experience engineering regardless of where they are - in lecture, in the laboratory, or the dorm room. Professors want to provide a hands-on learning experience to empower students who want to tinker, experiment, and explore concepts while improving the comprehension through reinforcement. Student access to affordable, low-cost technology enables educators to address limitations in the laboratory, including access to equipment, time on task, and cost. With a portable laboratory, a student can learn concepts in their preferred environments and provides a supplement to the traditional lecture and laboratory based courses.",0190-5848;01905848,Electronic:978-1-61284-469-5; POD:978-1-61284-468-8; USB:978-1-61284-467-1,10.1109/FIE.2011.6143112,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143112,hands-on;myDAQ;student-laboratory;student-ownership,Data acquisition;Educational institutions;Hardware;Instruments;Laboratories;Nickel;Signal generators,data acquisition;educational technology;engineering education;laboratory techniques;portable instruments;virtual instrumentation,equipment access;hands-on learning experience;low-cost educational technology;myDAQ;portable laboratory;students personal laboratory;work in progress,,1,,3,,,,12-15 Oct. 2011,,IEEE,IEEE Conferences
Implications of decentralized Q-learning resource allocation in wireless networks,F. Wilhelmi; B. Bellalta; C. Cano; A. Jonsson,"Wireless Networking (WN-UPF), Universitat Pompeu Fabra, Barcelona, Spain","2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",20180215,2017,,,1,5,"Reinforcement Learning is gaining attention by the wireless networking community due to its potential to learn good-performing configurations only from the observed results. In this work we propose a stateless variation of Q-learning, which we apply to exploit spatial reuse in a wireless network. In particular, we allow networks to modify both their transmission power and the channel used solely based on the experienced throughput. We concentrate in a completely decentralized scenario in which no information about neighbouring nodes is available to the learners. Our results show that although the algorithm is able to find the best-performing actions to enhance aggregate throughput, there is high variability in the throughput experienced by the individual networks. We identify the cause of this variability as the adversarial setting of our setup, in which the most played actions provide intermittent good/poor performance depending on the neighbouring decisions. We also evaluate the effect of the intrinsic learning parameters of the algorithm on this variability.",,Electronic:978-1-5386-3531-5; POD:978-1-5386-3532-2; Paper:978-1-5386-3529-2; USB:978-1-5386-3530-8,10.1109/PIMRC.2017.8292321,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8292321,,Aggregates;Interference;Meters;Resource management;Signal to noise ratio;Throughput;Wireless networks,learning (artificial intelligence);radio networks;resource allocation,Reinforcement Learning;decentralized Q-learning resource allocation;wireless network;wireless networking community,,,,,,,,8-13 Oct. 2017,,IEEE,IEEE Conferences
Dynamic power control in Wireless Body Area Networks using reinforcement learning with approximation,R. Kazemi; R. Vesilo; E. Dutkiewicz; Ren Liu,"Department of Electronic Engineering, Macquarie University, Sydney, Australia","2011 IEEE 22nd International Symposium on Personal, Indoor and Mobile Radio Communications",20120126,2011,,,2203,2208,"A Wireless Body Area Network (WBAN) is made up of multiple tiny physiological sensors implanted in/on the human body with each sensor equipped with a wireless transceiver that communicates to a coordinator in a star topology. Energy is the scarcest resource in WBANs. Power control mechanisms to achieve a certain level of utility while using as little power for transmission as possible can play an important role in reducing energy consumption in such very energy-constrained networks. In this paper, we propose a novel power controller to mitigate internetwork interference in WBANs and increase the maximum achievable throughput with the minimum energy consumption. The proposed power controller employs reinforcement learning with approximation to learn from the environment and improve its performance. We compare the performance of the proposed controller to two other power controllers, one based on game theory and the other one based on fuzzy logic. Simulation results show that compared to the other two approaches, RLPC provides a substantial saving in energy consumption per bit, with a substantial increase in network lifetime.",2166-9570;21669570,Electronic:978-1-4577-1348-4; POD:978-1-4577-1346-0; USB:978-1-4577-1347-7,10.1109/PIMRC.2011.6139908,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139908,Dynamic Power Control;Fuzzy Logic;Game Theory;Interference;Reinforcement Learning;WBAN,Approximation methods;Energy consumption;Interference;Power control;Sensors;Throughput;Wireless sensor networks,approximation theory;body area networks;control engineering computing;fuzzy control;game theory;interference suppression;learning (artificial intelligence);power control;radio transceivers;telecommunication control;telecommunication network reliability,WBAN;approximation;dynamic power control;energy consumption reduction;energy-constrained networks;fuzzy logic;game theory;interference mitigation;network lifetime;physiological sensors;reinforcement learning;wireless body area networks;wireless transceiver,,6,,17,,,,11-14 Sept. 2011,,IEEE,IEEE Conferences
User centered and context dependent personalization through experiential transcoding,S. Ferretti; S. Mirri; C. Prandi; P. Salomoni,"Department of Computer Science and Engineering University of Bologna Bologna, Italy",2014 IEEE 11th Consumer Communications and Networking Conference (CCNC),20141103,2014,,,486,491,"The pervasive presence of devices exploited to use and deliver entertainment text-based content can make reading easier to get but more difficult to enjoy, in particular for people with reading-related disabilities. The main solutions that allow overcoming some of the difficulties experienced by users with specific and special needs are based on content adaptation and user profiling. This paper presents a system that aims to improve content legibility by exploiting experiential transcoding techniques. The system we propose tracks users' behaviors so as to provide a tailored adaptation of textual content, in order to fit the needs of a specific user on the several different devices she/he actually uses.",2331-9852;23319852,Electronic:978-1-4799-2355-7; POD:978-1-4799-2357-1,10.1109/CCNC.2014.6940520,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6940520,content adaptation;device capabilities;legibility;reinforcement learning;user profiling,Adaptation models;Context;Entertainment industry;Multimedia communication;Prototypes;Transcoding;Web pages,Web sites;assisted living;human factors;text analysis;transcoding;ubiquitous computing,content adaptation;content legibility improvement;context dependent personalization;entertainment text-based content;transcoding techniques;user centered personalization;user profiling,,4,,23,,,,10-13 Jan. 2014,,IEEE,IEEE Conferences
Concept maps and learning objects,L. I. Navarro; M. M. Such; D. M. Martin; C. P. Sancho; P. P. Peco,"Dept. of Comput. Sci., Agrarian Univ. of Havana, Cuba",Fifth IEEE International Conference on Advanced Learning Technologies (ICALT'05),20050919,2005,,,263,265,"Concept maps constitute one of the tools frequently used in learning management as they offer the possibility to personalize learning, share knowledge and reinforce learning to learn skills. At the same time, many initiatives or standards are being developed rapidly to make the contents in different learning management systems and learning environments compatible. This paper states the need to combine the technique of Concept Maps with initiatives that package contents developed by IMS to produce more portable and powerful content. A model to create tools for learning management is proposed.",2161-3761;21613761,POD:0-7695-2338-2,10.1109/ICALT.2005.90,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1508670,,Computer science;Content management;Education;Environmental management;Knowledge management;Packaging;Power system management;Power system modeling;Psychology;Standards development,computer aided instruction;educational administrative data processing,concept maps;knowledge sharing;learning environments;learning management systems;learning objects;learning personalization;learning reinforcement;skills learning,,1,,8,,,,5-8 July 2005,,IEEE,IEEE Conferences
GongBroker: A Broker Model for Power Trading in Smart Grid Markets,X. Wang; M. Zhang; F. Ren; T. Ito,"Sch. of Comput. & Inf. Technol., Univ. of Wollongong, Wollongong, NSW, Australia",2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT),20160204,2015,2,,21,24,"The Smart Grid market is dynamic and complex, and brokers are widely introduced to better manage this market. This paper proposes an intelligent broker model - GongBroker, with smart trading strategies to cope with the dynamics and complexity. GongBroker first predicts the short-term demands of various consumers, and then buys energy from the wholesale market through auctions, and sells energy to various consumers in the retail market. In order to predict the customer demand, a data-driven method is proposed. All the consumers are hierarchically clustered according to their historical energy consumptions, and different prediction methods are tailored for different customer clusters to predict one-day-ahead hourly energy demand. Based on the predicted demand, the GongBroker employs a Markov Decision Process for the one-day-ahead auction in the wholesale market. To compete with other brokers, GongBroker uses independent reinforcement learning processes to optimize prices for different types of consumers. Experimental results demonstrate that GongBroker not only is competitive in making profit, but also keeps a good supply-demand balance.",,Electronic:978-1-4673-9618-9; POD:978-1-4673-9619-6,10.1109/WI-IAT.2015.108,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7397309,Broker Model;Data-driven;Reinforcement Learning;Smart Grid Market,Adaptation models;Electronic mail;Energy consumption;Mathematical model;Prediction algorithms;Prediction methods;Smart grids,Markov processes;demand side management;learning (artificial intelligence);power markets;smart power grids;supply and demand,GongBroker;Markov decision process;consumers short-term demands;data-driven method;day-ahead hourly energy demand;independent reinforcement learning processes;intelligent broker model;power trading;prediction methods;smart grid markets;smart trading strategies;supply-demand balance,,1,,6,,,,6-9 Dec. 2015,,IEEE,IEEE Conferences
Attention-Aware Face Hallucination via Deep Reinforcement Learning,Q. Cao; L. Lin; Y. Shi; X. Liang; G. Li,"Sch. of Data & Comput. Sci., Sun Yat-sen Univ., Guangzhou, China",2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),20171109,2017,,,1656,1664,"Face hallucination is a domain-specific super-resolution problem with the goal to generate high-resolution (HR) faces from low-resolution (LR) input images. In contrast to existing methods that often learn a single patch-to-patch mapping from LR to HR images and are regardless of the contextual interdependency between patches, we propose a novel Attention-aware Face Hallucination (Attention-FH) framework which resorts to deep reinforcement learning for sequentially discovering attended patches and then performing the facial part enhancement by fully exploiting the global interdependency of the image. Specifically, in each time step, the recurrent policy network is proposed to dynamically specify a new attended region by incorporating what happened in the past. The state (i.e., face hallucination result for the whole image) can thus be exploited and updated by the local enhancement network on the selected region. The Attention-FH approach jointly learns the recurrent policy network and local enhancement network through maximizing the long-term reward that reflects the hallucination performance over the whole image. Therefore, our proposed Attention-FH is capable of adaptively personalizing an optimal searching path for each face image according to its own characteristic. Extensive experiments show our approach significantly surpasses the state-of-the-arts on in-the-wild faces with large pose and illumination variations. The state (i.e., face hallucination result for the whole image) can thus be exploited and updated by the local enhancement network on the selected region. The Attention-FH approach jointly learns the recurrent policy network and local enhancement network through maximizing the long-term reward that reflects the hallucination performance over the whole image. Therefore, our proposed Attention-FH is capable of adaptively personalizing an optimal searching path for each face image according to its own characteristic. Extensive experiments show our appro- ch significantly surpasses the state-of-the-arts on in-the-wild faces with large pose and illumination variations.",1063-6919;10636919,Electronic:978-1-5386-0457-1; POD:978-1-5386-0458-8,10.1109/CVPR.2017.180,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8099663,,Computer vision;Face;History;Image resolution;Learning (artificial intelligence);Neural networks,face recognition;image enhancement;image resolution;learning (artificial intelligence),attention-FH approach;attention-aware face hallucination;deep reinforcement learning;domain-specific super-resolution problem;facial part enhancement;hallucination performance;high-resolution face generation;in-the-wild faces;local enhancement network;low-resolution input images;optimal searching path;patch-to-patch mapping;recurrent policy network,,1,,,,,,21-26 July 2017,,IEEE,IEEE Conferences
Data-driven inverse learning of passenger preferences in urban public transits,G. Wu; Y. Ding; Y. Li; J. Luo; F. Zhang; J. Fu,"Worcester Polytechnic Institute (WPI), 100 Institute Road, Worcester, MA 01609, USA",2017 IEEE 56th Annual Conference on Decision and Control (CDC),20180122,2017,,,5068,5073,"Urban public transit planning is crucial in reducing traffic congestion and enabling green transportation. However, there is no systematic way to integrate passengers' personal preferences in planning public transit routes and schedules so as to achieve high occupancy rates and efficiency gain of ride-sharing. In this paper, we take the first step tp exact passengers' preferences in planning from history public transit data. We propose a data-driven method to construct a Markov decision process model that characterizes the process of passengers making sequential public transit choices, in bus routes, subway lines, and transfer stops/stations. Using the model, we integrate softmax policy iteration into maximum entropy inverse reinforcement learning to infer the passenger's reward function from observed trajectory data. The inferred reward function will enable an urban planner to predict passengers' route planning decisions given some proposed transit plans, for example, opening a new bus route or subway line. Finally, we demonstrate the correctness and accuracy of our modeling and inference methods in a large-scale (three months) passenger-level public transit trajectory data from Shenzhen, China. Our method contributes to smart transportation design and human-centric urban planning.",,Electronic:978-1-5090-2873-3; POD:978-1-5090-2874-0; USB:978-1-5090-2872-6,10.1109/CDC.2017.8264410,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8264410,,Data models;Entropy;Markov processes;Planning;Public transportation;Roads;Trajectory,Markov processes;data analysis;entropy;learning (artificial intelligence);town and country planning;traffic engineering computing;transportation,China;Markov decision process model;Shenzhen;bus route;bus routes;data-driven inverse learning;data-driven method;enabling green transportation;green transportation;high occupancy rates;history public transit data;human-centric urban planning;inference methods;inferred reward function;large-scale passenger-level public transit trajectory data;maximum entropy inverse reinforcement;observed trajectory data;public transit routes;sequential public transit choices;smart transportation design;traffic congestion;transit plans;urban planner;urban public transit planning,,,,,,,,12-15 Dec. 2017,,IEEE,IEEE Conferences
Personalized Course Sequence Recommendations,J. Xu; T. Xing; M. van der Schaar,"Department of Electrical and Computer Engineering, University of Miami, Coral Gables, FL, USA",IEEE Transactions on Signal Processing,20160824,2016,64,20,5340,5352,"Given the variability in student learning, it is becoming increasingly important to tailor courses as well as course sequences to student needs. This paper presents a systematic methodology for offering personalized course sequence recommendations to students. First, a forward-search backward-induction algorithm is developed that can optimally select course sequences to decrease the time required for a student to graduate. The algorithm accounts for prerequisite requirements (typically present in higher level education) and course availability. Second, using the tools of multiarmed bandits, an algorithm is developed that can optimally recommend a course sequence that both reduces the time to graduate while also increasing the overall GPA of the student. The algorithm dynamically learns how students with different contextual backgrounds perform for given course sequences and, then, recommends an optimal course sequence for new students. Using real-world student data from the UCLA Mechanical and Aerospace Engineering Department, we illustrate how the proposed algorithms outperform other methods that do not include student contextual information when making course sequence recommendations.",1053-587X;1053587X,,10.1109/TSP.2016.2595495,10.13039/100000001 - National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7524023,Personalized education;contextual bandits;course sequence recommendation;dynamic programming,Adaptive systems;Aerospace engineering;Complexity theory;Education;Electronic mail;Heuristic algorithms;Signal processing algorithms,aerospace computing;computer aided instruction;educational courses;engineering education;inference mechanisms;mechanical engineering computing;recommender systems,GPA;UCLA Mechanical and Aerospace Engineering Department;contextual backgrounds;course availability;forward-search backward-induction algorithm;multiarmed bandit tool;personalized course sequence recommendations;prerequisite requirements;student learning;systematic methodology,,2,,,,,20160727,"Oct.15, 15 2016",,IEEE,IEEE Journals & Magazines
Policy search for learning robot control using sparse data,B. Bischoff; D. Nguyen-Tuong; H. van Hoof; A. McHutchon; C. E. Rasmussen; A. Knoll; J. Peters; M. P. Deisenroth,"Cognitive Systems, Bosch Corporate Research, Germany",2014 IEEE International Conference on Robotics and Automation (ICRA),20140929,2014,,,3882,3887,"In many complex robot applications, such as grasping and manipulation, it is difficult to program desired task solutions beforehand, as robots are within an uncertain and dynamic environment. In such cases, learning tasks from experience can be a useful alternative. To obtain a sound learning and generalization performance, machine learning, especially, reinforcement learning, usually requires sufficient data. However, in cases where only little data is available for learning, due to system constraints and practical issues, reinforcement learning can act suboptimally. In this paper, we investigate how model-based reinforcement learning, in particular the probabilistic inference for learning control method (Pilco), can be tailored to cope with the case of sparse data to speed up learning. The basic idea is to include further prior knowledge into the learning process. As Pilco is built on the probabilistic Gaussian processes framework, additional system knowledge can be incorporated by defining appropriate prior distributions, e.g. a linear mean Gaussian prior. The resulting Pilco formulation remains in closed form and analytically tractable. The proposed approach is evaluated in simulation as well as on a physical robot, the Festo Robotino XT. For the robot evaluation, we employ the approach for learning an object pick-up task. The results show that by including prior knowledge, policy learning can be sped up in presence of sparse data.",1050-4729;10504729,Electronic:978-1-4799-3685-4; POD:978-1-4799-3686-1; USB:978-1-4799-3684-7,10.1109/ICRA.2014.6907422,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6907422,,Computational modeling;Data models;Grasping;Heuristic algorithms;Pneumatic systems;Robots;Valves,Gaussian distribution;Gaussian processes;learning (artificial intelligence);learning systems;probability;robots,Festo Robotino XT;PILCO;dynamic environment;generalization performance;grasping;learning robot control;linear mean Gaussian prior distribution;machine learning;manipulation;model-based reinforcement learning;object pick-up task;physical robot;policy learning;policy search;probabilistic Gaussian processes framework;probabilistic inference for learning control method;sound learning;sparse data;system constraints;uncertain environment,,3,,14,,,,May 31 2014-June 7 2014,,IEEE,IEEE Conferences
Teaching mentoring program for the application of active methodologies and ICT tools,N. E. Salazar,"Educational Innovation Deparment, Tecsup, Trujillo, Per&#x00FA;",2017 IEEE Frontiers in Education Conference (FIE),20171214,2017,,,1,6,"This research is characterized by the effectiveness of the Teaching Mentoring Program for the application of active methodologies and ICT tools in Tecsup Norte teachers, Trujillo, Peru. A goal was set for the three locations of Tecsup Norte (Trujillo, City), Tecsup Centro (Lima, City), Tecsup Sur (Arequipa, City), to achieve that in 2016, 80%, apply at least one active methodology per semester and make use of ICT tools. The result for Tecsup Norte after completing the first semester was that 28% of teachers used an active methodology and 16% used ICT tools. At the beginning of the 2016 II semester, the Teaching Mentoring Program was designed, consisting of five stages: 1. first contact and interview with the teacher, 2. personalized training, 3. programming and revision of class material, 4. accompaniment in classroom and evaluation by rubrics, and 5. feedforward and recognition. For this, psychological strategies such as rapport, positive reinforcement and behavioral modeling were adapted, as well as the tool of coaching feedforward and evaluation rubrics were designed for the application of active methodologies. To guarantee the effectiveness of the program was a team of two specialists in active methodologies and ICT tools. Work schedule and control of man hours was approached using Gantt diagram. The results of the implementation of the program are relevant, since 95% of the teachers of the semester 2016 II applied active methodologies and ICT tools; in comparison to the result of the 2016 I semester. Tecsup Norte becomes the first site that exceeds the established goal of 80% in application of active methodologies and ICT tools, through the Teaching Mentoring program, the other venues that did not apply the Teaching Mentoring Program were: Tecsup Centro 62% and Tecsup Sur 50%. This educational innovation contributes a program for the training, accompaniment and evaluation of the teaching performance through active methodologies, such as: Flipped le- rning, case-based learning, problem-based learning, guided-learning project and the use of ICT tools.",,Electronic:978-1-5090-5920-1; POD:978-1-5090-4920-2; USB:978-1-5090-5919-5,10.1109/FIE.2017.8190607,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8190607,ICT tools;Mentoring Teaching;active methodologies;evaluation rubrics;feedforward;personalized training,Google;Interviews;Mentoring;Technological innovation;Tools;Urban areas,computer aided instruction;teaching,Arequipa City;Gantt diagram;ICT tools;Lima City;Peru;Teaching Mentoring Program;Teaching Mentoring program;Teaching mentoring program;Tecsup Centro;Tecsup Norte teachers;Tecsup Sur;Trujillo,,,,,,,,18-21 Oct. 2017,,IEEE,IEEE Conferences
Q-learning based power control algorithm for D2D communication,S. Nie; Z. Fan; M. Zhao; X. Gu; L. Zhang,"Key Lab of Universal Wireless Communication, Beijing University of Posts and Telecommunications, Beijing, China 100876","2016 IEEE 27th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",20161222,2016,,,1,6,"In this paper, reinforcement learning (RL) based power control algorithm in underlay D2D communication is studied. The approach we use regards D2D communication as a multi-agents system, and power control is achieved by maximizing system capacity while maintaining the requirement of quality of service(QoS) from cellular users. We propose two RL based power control methods for D2D users, i.e., team-Q learning and distributed-Q learning. The former is a centralized method in which only one Q-value table needs to be maintained, while the latter enables D2D users to learn independently and reduces the complexity of Q-value table. Simulation results show the difference of the two Q-learning algorithm in terms of convergence and reward function. In addition, it is shown that through our distributed-Q learning, D2D users not only are able to learn their power in a self-organized way, but also achieve better system performance than that using traditional method in LTE(Long Term Evolution).",,Electronic:978-1-5090-3254-9; POD:978-1-5090-3255-6,10.1109/PIMRC.2016.7794793,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7794793,D2D communication;Q-learning;Reinforcement learning;multi-agent system;power control,Algorithm design and analysis;Device-to-device communication;Interference;Power control;Quality of service;Radio transmitters;Signal to noise ratio,Long Term Evolution;learning (artificial intelligence);multi-agent systems;power control;quality of service;telecommunication control,LTE;Long Term Evolution;Q-learning based power control algorithm;QoS;cellular users;distributed-Q learning;multiagents system;quality of service;reinforcement learning;system capacity;underlay D2D communication,,,,,,,,4-8 Sept. 2016,,IEEE,IEEE Conferences
MOSAIC for Multiple-Reward Environments,N. Sugimoto; M. Haruno; K. Doya; M. Kawato,"Center for Information and Neural Networks, National Institute of Information and Communications Technology, Kyoto 619-0288, Japan, and Department of Brain Robot Interface, Brain Information Communication Research Laboratory Group, ATR, Kyoto 619-0288, Japan xsugi@nict.go.jp",Neural Computation,20140519,2012,24,3,577,606,"Reinforcement learning (RL) can provide a basic framework for autonomous robots to learn to control and maximize future cumulative rewards in complex environments. To achieve high performance, RL controllers must consider the complex external dynamics for movements and task (reward function) and optimize control commands. For example, a robot playing tennis and squash needs to cope with the different dynamics of a tennis or squash racket and such dynamic environmental factors as the wind. In addition, this robot has to tailor its tactics simultaneously under the rules of either game. This double complexity of the external dynamics and reward function sometimes becomes more complex when both the multiple dynamics and multiple reward functions switch implicitly, as in the situation of a real (multi-agent) game of tennis where one player cannot observe the intention of her opponents or her partner. The robot must consider its opponent's and its partner's unobservable behavioral goals (reward function). In this article, we address how an RL agent should be designed to handle such double complexity of dynamics and reward. We have previously proposed modular selection and identification for control (MOSAIC) to cope with nonstationary dynamics where appropriate controllers are selected and learned among many candidates based on the error of its paired dynamics predictor: the forward model. Here we extend this framework for RL and propose MOSAIC-MR architecture. It resembles MOSAIC in spirit and selects and learns an appropriate RL controller based on the RL controller's TD error using the errors of the dynamics (the forward model) and the reward predictors. Furthermore, unlike other MOSAIC variants for RL, RL controllers are not a priori paired with the fixed predictors of dynamics and rewards. The simulation results demonstrate that MOSAIC-MR outperforms other counterparts because of this flexible association ability among RL controllers, forward models, and reward pr- dictors.",0899-7667;08997667,,10.1162/NECO_a_00246,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6797532,,,,,,2,,,,,,March 2012,,MIT Press,MIT Press Journals
A Recursive Dialogue Game for Personalized Computer-Aided Pronunciation Training,P. h. Su; C. h. Wu; L. s. Lee,"Department of Engineering, University of Cambridge, Cambridge, United Kingdom","IEEE/ACM Transactions on Audio, Speech, and Language Processing",20150114,2015,23,1,127,141,"Learning languages in addition to the native language is very important for all people in the globalized world today, and computer-aided pronunciation training (CAPT) is attractive since the software can be used anywhere at any time, and repeated as many times as desired. In this paper, we introduce the immersive interaction scenario offered by spoken dialogues to CAPT by proposing a recursive dialogue game to make CAPT personalized. A number of tree-structured sub-dialogues are linked sequentially and recursively as the script for the game. The system policy at each dialogue turn is to select in real-time along the dialogue the best training sentence for each specific individual learner within the dialogue script, considering the learner's learning status and the future possible dialogue paths in the script, such that the learner can have the scores for all pronunciation units considered reaching a predefined standard in a minimum number of turns. The purpose here is that those pronunciation units poorly produced by the specific learner can be offered with more practice opportunities in the future sentences along the dialogue, which enables the learner to improve the pronunciation without having to repeat the same training sentences many times. This makes the learning process for each learner completely personalized. The dialogue policy is modeled by Markov decision process (MDP) with high-dimensional continuous state space, and trained with fitted value iteration using a huge number of simulated learners. These simulated leaners have the behavior similar to real learners, and were generated from a corpus of real learner data. The experiments demonstrated very promising results and a real cloud-based system is also successfully implemented.",2329-9290;23299290,,10.1109/TASLP.2014.2375572,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6971195,Computer-aided pronunciation training (CAPT);Markov decision process;computer-assisted language learning;dialogue game;reinforcement learning,Computers;Games;Markov processes;Software;Speech;Speech processing;Training,Markov processes;cloud computing;computer based training;computer games;interactive systems;linguistics;real-time systems,MDP;Markov decision process;dialogue paths;dialogue policy;dialogue script;fitted value iteration;game script;high-dimensional continuous state space;immersive interaction scenario;language learning;learner learning status;native language;personalized CAPT;personalized computer-aided pronunciation training;pronunciation improvement;pronunciation unit scores;real cloud-based system;real learners;recursive dialogue game;sequentially-recursively linked tree-structured subdialogues;simulated leaners;spoken dialogues;system policy;training sentence,,2,,52,,,20141202,Jan. 2015,,IEEE,IEEE Journals & Magazines
Video Annotation Through Search and Graph Reinforcement Mining,E. Moxley; T. Mei; B. S. Manjunath,"University of California at Santa Barbara, California, U.S.A.",IEEE Transactions on Multimedia,20100315,2010,12,3,184,193,"Unlimited vocabulary annotation of multimedia documents remains elusive despite progress solving the problem in the case of a small, fixed lexicon. Taking advantage of the repetitive nature of modern information and online media databases with independent annotation instances, we present an approach to automatically annotate multimedia documents that uses mining techniques to discover new annotations from similar documents and to filter existing incorrect annotations. The annotation set is not limited to words that have training data or for which models have been created. It is limited only by the words in the collective annotation vocabulary of all the database documents. A graph reinforcement method driven by a particular modality (e.g., visual) is used to determine the contribution of a similar document to the annotation target. The graph supplies possible annotations of a different modality (e.g., text) that can be mined for annotations of the target. Experiments are performed using videos crawled from YouTube. A customized precision-recall metric shows that the annotations obtained using the proposed method are superior to those originally existing for the document. These extended, filtered tags are also superior to a state-of-the-art semi-supervised technique for graph reinforcement learning on the initial user-supplied annotations.",1520-9210;15209210,,10.1109/TMM.2010.2041101,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5398917,Data mining;graph theory;video annotation;video content analysis,,data mining;document handling;learning (artificial intelligence);video retrieval,annotations discovery;collective annotation vocabulary;customized precision recall metric;database documents;graph reinforcement mining;multimedia documents annotation;online media database;video annotation,,28,,36,,,20100126,April 2010,,IEEE,IEEE Journals & Magazines
Performance improvements in schools with Adaptive Learning and Assessment,R. Raman; P. Nedungadi,"School of Engineering, Amrita Vishwa Vidyapeetham, India",2010 4th International Conference on Distance Learning and Education,20101021,2010,,,10,14,"This paper presents Amrita Learning, a web-based, multimedia-enabled, Adaptive Assessment and Learning System for schools. Computer-based adaptive assessments aim to use an optimal and individualized assessment path to determine the knowledge level of students. The new goal for adaptive assessment is based on educational outcomes, which describe what learners must be able to do as a result of items studied. Assessment based on outcomes creates the initial roadmap for the educational model, ensuring that students are not learning items that are already mastered. Learners and instructors can accurately determine their areas of strengths and weaknesses, and use this to determine future instruction. This paper explains the underlying principles used in the initial adaptive assessment followed by evaluation that is closely interwoven with learning. An expert module continuously adjusts the content and method of presentation based on the sequence of learner's recent responses and prior knowledge. The system maintains and updates both the individual learner profile and group profiles. Amrita Learning, targeted to school students, is built upon the principles of spiral learning with mixed presentation from multiple skill areas, thus providing continuous reinforcement in all skill-areas. The proposed competency model has been pilot tested in both city and rural area schools. In the majority of cases where students used it consistently, there were quantifiable improvements in learning levels and performance in schools. Summaries of the results and recommendations are included in this paper.",2169-1428;21691428,Electronic:978-1-4244-8752-3; POD:978-1-4244-8751-6,10.1109/ICDLE.2010.5606052,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5606052,Adaptive Learning;Assessment;Continuous Evaluation;Flash Animations;ICT;Intelligent Tutoring Systems Spiral Learning;Interactive;Mastery Learning;Mixed Presentation;individualised instruction;special needs,Adaptation model;Browsers;Equations;Fires;Geometry;Mathematical model;XML,Internet;computer aided instruction;educational institutions;learning (artificial intelligence);multimedia systems,Amrita learning;Schools;Web based multimedia enabled adaptive assessment;adaptive learning;city area school;learner response;performance improvement;rural area school;spiral learning;student knowledge level,,1,,10,,,,3-5 Oct. 2010,,IEEE,IEEE Conferences
Hardware implementation of FAST-based reinforcement learning algorithm,Kao-Shing Hwang; Yuan-Pao Hsu; His-Wen Hsieh; Hsin-Yi Lin,"Dept. of Electr., Nat. Chung-Cheng Univ., Taiwan","Proceedings of 2005 IEEE International Workshop on VLSI Design and Video Technology, 2005.",20050906,2005,,,435,438,"A FAST-based (flexible adaptable-size topology) reinforcement learning chip is implemented in this article. Basically, the FAST is an ART-like (adaptive resonance theory) mechanism. The ART is characterized as one of unsupervised learning neural network models, facilitated to solve stability-plasticity dilemma. The chip is a self organizing architecture that consists of three main structures including similarity, learning, and pruning. Dynamically adjusting the size of sensitivity regions of each neuron and adaptively pruning one of the neurons when an input pattern activates more than one neuron, the chip can preserve hardware resources (available neurons) to accommodate more categories. The clustered result by the implemented chip is then sent to an AHC (adaptive heuristic critic) architecture (emulated by a personal computer) to learn to balance an inverted pendulum system, which is also emulated by the personal computer for verifying the implemented architecture.",,POD:0-7803-9005-9,10.1109/IWVDVT.2005.1504643,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1504643,,Computer architecture;Hardware;Microcomputers;Network topology;Neural networks;Neurons;Resonance;Stability;Subspace constraints;Unsupervised learning,field programmable gate arrays;learning (artificial intelligence);microcomputers;neural net architecture;pendulums;self-adjusting systems,adaptive heuristic critic architecture;adaptive resonance theory;flexible adaptable-size topology;hardware implementation;inverted pendulum system;reinforcement learning algorithm;self organizing architecture,,4,,5,,,,28-30 May 2005,,IEEE,IEEE Conferences
Video summarization using reinforcement learning in eigenspace,K. Masumitsu; T. Echigo,"IBM Tokyo Res. Lab., Kanagawa, Japan",Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101),20020806,2000,2,,267,270 vol.2,"We propose video summarization using reinforcement learning. The importance score of each frame in a video is calculated from the user's actions in handling similar previous frames; if such frames were watched rather than skipped, a high score is assigned. To calculate the score, instead of using raw feature vectors extracted from images, we use feature vectors projected on eigenspace: as a result, we can deal with the features comprehensively. We also give an algorithm that uses the reinforcement learning method to create a personalized video summary. The summarization algorithm is applied to a soccer video to confirm its effectiveness.",1522-4880;15224880,POD:0-7803-6297-7,10.1109/ICIP.2000.899351,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=899351,,Data mining;Feature extraction;Information retrieval;Joining processes;Laboratories;Layout;Learning;Multimedia communication;TV broadcasting;Watches,feature extraction;image sequences;learning (artificial intelligence);video signal processing,algorithm;eigenspace;feature vectors extraction;personalized video summary;reinforcement learning;soccer video;summarization algorithm;video frame;video summarization,,4,3,7,,,,10-13 Sept. 2000,10 Sep 2000-13 Sep 2000,IEEE,IEEE Conferences
Anti-lock braking systems data-driven control using Q-learning,M. B. Radac; R. E. Precup; R. C. Roman,"Department of Automation and Applied Informatics, Politehnica University of Timisoara, Timisoara, Romania",2017 IEEE 26th International Symposium on Industrial Electronics (ISIE),20170807,2017,,,418,423,"A model-free tire slip control solution for a fast, highly nonlinear Anti-lock Braking System (ABS) is proposed in this work via a reinforcement Q-learning optimal control approach. The solution is tailored around a batch neural fitted scheme using two neural networks to approximate the value function and the controller, respectively. The transition samples are collected from the process through interaction by online exploiting the current iteration controller (or policy) under an ε-greedy exploration strategy. The ABS process fits this type of learning-by-interaction since it does not need an initial stabilizing controller. The validation case studies carried out on a real laboratory setup reveal that high control system performance can be achieved after several tens of interaction episodes with the controlled process. Insightful comments on the observed control behavior in a set of real-time experiments are offered along with performance comparisons with several other controllers.",,Electronic:978-1-5090-1412-5; POD:978-1-5090-1413-2,10.1109/ISIE.2017.8001283,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8001283,Actor critic control;Anti-lock Braking System;Q-learning;model-free control;neural networks,,automotive components;braking;greedy algorithms;learning (artificial intelligence);motion control;neurocontrollers;optimal control,ε-greedy exploration strategy;anti-lock braking system;batch neural fitted scheme;data-driven control;iteration controller;learning-by-interaction;model-free tire slip control;neural networks;nonlinear ABS;optimal control;reinforcement Q-learning;value function,,,,,,,,19-21 June 2017,,IEEE,IEEE Conferences
A Reinforcement Learning Approach to Emotion-based Automatic Playlist Generation,C. Y. Chi; R. T. H. Tsai; J. Y. Lai; J. Y. j. Hsu,"Dept. of CSIE, Nat. Taiwan Univ., Taipei, Taiwan",2010 International Conference on Technologies and Applications of Artificial Intelligence,20110120,2010,,,60,65,"A novel trend emerged in music exploration is to organize and search songs according to their emotions. However, research on automatic playlist generation (APG) primarily focuses on metadata and audio similarity. Mainstream solutions view APG as a static problem. This paper argues that the APG problem is better modeled as a continuous optimization problem, and proposes an adaptive preference model for personalized APG based on emotions. The main idea is to collect a user's behavior in music playing, e.g., rating, skipping and replaying, as immediate feedback in learning the user's preferences for music emotion within a playlist. Reinforcement learning is adopted to learn the user's current preferences, which are used to generate personalized playlists. Learning parameters are tuned by simulation of two hypothetical users. A two-month user study is conducted to evaluate the APG solutions. The results show that the proposed approach reduces the Miss Ratio by 10% in comparison with the baseline approach.",2376-6816;23766816,Electronic:978-0-7695-4253-9; POD:978-1-4244-8668-7,10.1109/TAAI.2010.21,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5695433,automatic playlist generation;reinforcement learning;song emotion,,learning (artificial intelligence);music;optimisation;user interfaces,adaptive preference model;continuous optimization problem;emotion-based automatic playlist generation;music emotion;reinforcement learning;user preference,,1,,20,,,,18-20 Nov. 2010,,IEEE,IEEE Conferences
Dynamic Game Difficulty Scaling Using Adaptive Behavior-Based AI,C. H. Tan; K. C. Tan; A. Tay,"Institute for Infocomm Research, Agency for Science Technology and Research (A*STAR), Singapore, Singapore",IEEE Transactions on Computational Intelligence and AI in Games,20111212,2011,3,4,289,301,"Games are played by a wide variety of audiences. Different individuals will play with different gaming styles and employ different strategic approaches. This often involves interacting with nonplayer characters that are controlled by the game AI. From a developer's standpoint, it is important to design a game AI that is able to satisfy the variety of players that will interact with the game. Thus, an adaptive game AI that can scale the difficulty of the game according to the proficiency of the player has greater potential to customize a personalized and entertaining game experience compared to a static game AI. In particular, dynamic game difficulty scaling refers to the use of an adaptive game AI that performs game adaptations in real time during the game session. This paper presents two adaptive algorithms that use ideas from reinforcement learning and evolutionary computation to improve player satisfaction by scaling the difficulty of the game AI while the game is being played. The effects of varying the learning and mutation rates are examined and a general rule of thumb for the parameters is proposed. The proposed algorithms are demonstrated to be capable of matching its opponents in terms of mean scores and winning percentages. Both algorithms are able to generalize well to a variety of opponents.",1943-068X;1943068X,,10.1109/TCIAIG.2011.2158434,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783334,Artificial intelligence;behavior based;car racing simulation;game AI;player satisfaction;real-time adaptation,Adaptation model;Artificial intelligence;Games;Humans;Pixel;Real time systems;Vehicles,computer games;evolutionary computation;learning (artificial intelligence),adaptive behavior-based artificial intelligence;dynamic game difficulty scaling;evolutionary computation;game adaptation;gaming styles;mutation rates;reinforcement learning,,15,1,37,,,20110602,Dec. 2011,,IEEE,IEEE Journals & Magazines
Towards a meta motion planner B: algorithm and applications,A. Adam; E. Rivlin; I. Shimshoni,"Dept. of Math., Technion-Israel Inst. of Technol., Haifa, Israel",Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164),20060418,2001,1,,291,298 vol.1,"In a companion paper (see Proceedings of ICRA 2001) we developed a framework for rating or comparing navigation packages. For a given environment a navigation package consists of a motion planner and a sensor to be used during navigation. The ability to rate or measure a navigation package is important in order to address issues like sensor customization for an environment and choice of a motion planner in an environment. In this paper we present the algorithm which we use in order to rate a given navigation package. Under the framework which was presented previously, a partially observable Markov decision process is defined. The algorithm searches for an optimal policy to be employed in this decision process. We briefly review the problem and framework, develop the algorithm and present experimental results.",1050-4729;10504729,POD:0-7803-6576-3,10.1109/ROBOT.2001.932568,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=932568,,Application software;Computer science;Industrial engineering;Mathematics;Motion measurement;Navigation;Packaging;Paper technology;Path planning;Robot sensing systems,Markov processes;computerised navigation;decision theory;learning (artificial intelligence);path planning;robots;search problems,Markov decision process;meta motion planner;navigation;reinforcement learning;robots;search problem;sensor customization,,2,,7,,,,2001,,IEEE,IEEE Conferences
Increasing Retrieval Quality in Conversational Recommenders,M. S. Llorente; S. E. Guerrero,"Universitat de Barcelona, Barcelona",IEEE Transactions on Knowledge and Data Engineering,20120817,2012,24,10,1876,1888,"A major task of research in conversational recommender systems is personalization. Critiquing is a common and powerful form of feedback, where a user can express her feature preferences by applying a series of directional critiques over the recommendations instead of providing specific preference values. Incremental Critiquing (IC) is a conversational recommender system that uses critiquing as a feedback to efficiently personalize products. The expectation is that in each cycle the system retrieves the products that best satisfy the user's soft product preferences from a minimal information input. In this paper, we present a novel technique that increases retrieval quality based on a combination of compatibility and similarity scores. Under the hypothesis that a user learns during the recommendation process, we propose two novel exponential Reinforcement Learning (RL) approaches for compatibility that take into account both the instant at which the user makes a critique and the number of satisfied critiques. Moreover, we consider that the impact of features on the similarity differs according to the preferences manifested by the user. We propose a Global Weighting (GW) approach that uses a common weight for nearest cases in order to focus on groups of relevant products. We show that our methodology significantly improves recommendation efficiency in four data sets of different sizes in terms of session length in comparison with state-of-the-art approaches. Moreover, our recommender shows higher robustness against noisy user data when compared to classical approaches.",1041-4347;10414347,,10.1109/TKDE.2011.116,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5871618,Conversational recommender systems;case-based reasoning;critiquing elicitation;personalization.,Cognition;Current measurement;Learning systems;Monte Carlo methods;Recommender systems;Space exploration,,,,3,,38,,,20110609,Oct. 2012,,IEEE,IEEE Journals & Magazines
Using Contextual Learning to Improve Diagnostic Accuracy: Application in Breast Cancer Screening,L. Song; W. Hsu; J. Xu; M. van der Schaar,"Department of Electrical Engineering, University of California, Los Angeles, Los Angeles, CA, USA",IEEE Journal of Biomedical and Health Informatics,20170520,2016,20,3,902,914,"Clinicians need to routinely make management decisions about patients who are at risk for a disease such as breast cancer. This paper presents a novel clinical decision support tool that is capable of helping physicians make diagnostic decisions. We apply this support system to improve the specificity of breast cancer screening and diagnosis. The system utilizes clinical context (e.g., demographics, medical history) to minimize the false positive rates while avoiding false negatives. An online contextual learning algorithm is used to update the diagnostic strategy presented to the physicians over time. We analytically evaluate the diagnostic performance loss of the proposed algorithm, in which the true patient distribution is not known and needs to be learned, as compared with the optimal strategy where all information is assumed known, and prove that the false positive rate of the proposed learning algorithm asymptotically converges to the optimum. In addition, our algorithm also has the important merit that it can provide individualized confidence estimates about the accuracy of the diagnosis recommendation. Moreover, the relevancy of contextual features is assessed, enabling the approach to identify specific contextual features that provide the most value of information in reducing diagnostic errors. Experiments were conducted using patient data collected at a large academic medical center. Our proposed approach outperforms the current clinical practice by 36% in terms of false positive rate given a 2% false negative rate.",2168-2194;21682194,,10.1109/JBHI.2015.2414934,10.13039/100000181 - U.S. Air Force Office of Scientific Research; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7064753,Breast cancer;computer-aided diagnosis system;contextual learning;multiarmed bandit (MAB);online learning,Biopsy;Breast cancer;Clustering algorithms;Context;Imaging,cancer;learning (artificial intelligence);medical diagnostic computing;patient diagnosis,breast cancer screening;clinical decision support tool;contextual features;diagnostic accuracy;disease;false positive rates;online contextual learning algorithm,"Aged;Algorithms;Breast Neoplasms;Diagnosis, Computer-Assisted;Early Detection of Cancer;Electronic Health Records;Female;Humans;Mammography;Middle Aged;Signal Processing, Computer-Assisted",2,,42,,,20150320,May 2016,,IEEE,IEEE Journals & Magazines
Customized learning algorithms for episodic tasks with acyclic state spaces,T. Bountourelis; S. Reveliotis,"School of Industrial & Systems Engineering, Georgia Institute of Technology, USA",2009 IEEE International Conference on Automation Science and Engineering,20090909,2009,,,627,634,"The work presented in this paper provides a practical, customized learning algorithm for reinforcement learning tasks that evolve episodically over acyclic state spaces. The presented results are motivated by the optimal disassembly planning (ODP) problem described in, and they complement and enhance some earlier developments on this problem that were presented in. In particular, the proposed algorithm is shown to be a substantial improvement of the original algorithm developed in, in terms of, both, the involved computational effort and the attained performance, where the latter is measured by the accumulated reward. The new algorithm also leads to a robust performance gain over the typical Q-learning implementations for the considered problem context.",2161-8070;21618070,POD:978-1-4244-4578-3,10.1109/COASE.2009.5234189,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5234189,,Aerospace industry;Algorithm design and analysis;Automation;Convergence;Data mining;Learning;Q factor;Space technology;State-space methods;Systems engineering and theory,learning (artificial intelligence),Q-learning implementation;acyclic state space;customized learning algorithm;episodic task;optimal disassembly planning;reinforcement learning;robust performance gain,,0,,18,,,,22-25 Aug. 2009,,IEEE,IEEE Conferences
Multi-objective reinforcement learning algorithm and its improved convergency method,Z. Jin; Z. Huajun,"Department of Control Science and Engineering, Huazhong University of Science and Technology, China",2011 6th IEEE Conference on Industrial Electronics and Applications,20110804,2011,,,2438,2445,"This paper proposes a multi-objective reinforcement learning algorithm (MORLA) and uses simultaneous perturbation stochastic approximation (SPSA) to improve the convergence of it. Usually, reinforcement learning (RL) is used to design neurocontroller for control system with single objective. When facing multi-objective system, it is necessary to design the neurocontroller according to the personal preference. The MORLA can transform the multi-objective into synthetical objective and applies parallel genetic algorithm (PGA) to evolve the neurocontroller according to the synthetical objective. To establish the synthetical objective, the objective weight which represents the personal preference is calculated by solving the constrained optimization problem (COP) at the end of each generation. The COP requires not only the biggest variance of the synthetical objective in the population, but also requires the weight to fit the designer's preference. After acquiring the weights, the PGA can select the elitists from the population according to the designer's preference and design a satisfying neurocontroller by evolutionary operations. In addition, although GA has good global search ability, it descends slowly at local area. This paper applies SPSA algorithm to search optimal solution when GA is vibrating at local area. The SPSA converges fast by efficient gradient approximation that relies on measurements of the objective function. The hybrid algorithm accelerates the learning speed of reinforcement learning. At last, the MORLA is used to design neurocontroller for a speed-controlled induction motor drive with indirect vector control. With different personal preferences for the drive system, the simulation results show the feasibility and validity of the MORLA.",2156-2318;21562318,Electronic:978-1-4244-8756-1; POD:978-1-4244-8754-7,10.1109/ICIEA.2011.5976002,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976002,SPSA;multi-objective reinforcement learning;speed-controlled,Algorithm design and analysis;Approximation methods;Control systems;Convergence;Genetic algorithms;Learning;Neurocontrollers,genetic algorithms;learning (artificial intelligence);neurocontrollers,constrained optimization problem;indirect vector control;multiobjective reinforcement learning;multiobjective system;neurocontroller;objective function;parallel genetic algorithm;simultaneous perturbation stochastic approximation;speed-controlled induction motor drive,,0,,47,,,,21-23 June 2011,,IEEE,IEEE Conferences
Knowledge sharing approaches for distributed agents system,K. S. Hwang; C. W. Hsieh; W. C. Jiang,"Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan",2017 International Conference on Advanced Robotics and Intelligent Systems (ARIS),20180222,2017,,,59,61,"Considering situations in a multi-agent system, if there are tremendous number of agents sharing knowledge with each other, it is complicated activities hard to be solved. This thesis proposed a method that all agents just connect with a server to alleviate the complexity of the experiences exchange activities. The server collects learning knowledge loaded from all the agents, merges the knowledge, and shares the knowledge to all agents which lack akin experiences. The agents utilized the proposed Pheromone Mechanism in Ant Colony Algorithm to evaluate whether an experience is worthy to upload to the server. Meanwhile, to deal with the problem of massive data processing, this thesis used the open source software, Apache Hadoop, along with the MapReduce programming model. The agents can take shared experiences integrated with their own knowledge to achieve knowledge sharing and increase the efficiency significantly. The proposed approach in this thesis was implemented by a homemade server and personal computers.",,Electronic:978-1-5386-2419-7; POD:978-1-5386-2420-3; USB:978-1-5386-2418-0,10.1109/ARIS.2017.8297184,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8297184,Ant colony algorithm;Distributed computing;Knowledge merging;Knowledge sharing;Reinforcement learning,Computational modeling;Data models;Multi-agent systems;Programming;Servers;Training;Trajectory,ant colony optimisation;data handling;learning (artificial intelligence);multi-agent systems;parallel programming;public domain software;software agents,Ant Colony Algorithm;Apache Hadoop;MapReduce programming model;Pheromone Mechanism;distributed agents system;experience exchange;experience sharing;homemade server;knowledge merging;knowledge sharing;learning knowledge;massive data processing;multiagent system;open source software,,,,,,,,6-8 Sept. 2017,,IEEE,IEEE Conferences
Constructing an intelligent behavior avatar in a virtual world: a self-learning model based on reinforcement,Jui-Fa Chen; Wei-Chuan Lin; Hua-Sheng Bai; Chia-Che Yang; Hsiao-Chuan Chao,"Dept. of Inf. Eng., Tamkang Univ., Tamsui, Taiwan","IRI -2005 IEEE International Conference on Information Reuse and Integration, Conf, 2005.",20050912,2005,,,421,426,"In this paper, a novel method for personal intelligent behavior avatar (IBA) is proposed to acquire autonomous behavior based on the interactions between user and smart objects in the virtual environment. In this method, the behavior decision model and the self-learning model are integrated by Bayesian networks and reinforcement learning. The Bayesian networks can treat interaction experiences using statistical processes, and the sureness of decision making is represented by certainty factors using stochastic reasoning. The reinforcement learning is implemented by learning experimentation or trial and error mechanisms to improve the performance of IBA through feedback. Therefore, the IBA makes a strategic decision that is approximated and appropriate to the user through the self-learning process by reinforcement learning. Finally, the feasibility of this method is investigated by imitating user's behavior and the results of self-learning process. The results of simulation show that the method is successful in imitating user's behavior and improving the performance of IBA.",,POD:0-7803-9093-8,10.1109/IRI-05.2005.1506510,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1506510,,Artificial intelligence;Avatars;Bayesian methods;Chaos;Decision making;Educational institutions;Information technology;Intelligent agent;Learning;Virtual environment,avatars;belief networks;decision making;human computer interaction;inference mechanisms;stochastic processes;unsupervised learning,Bayesian networks;behavior decision model;decision making;personal intelligent behavior avatar;reinforcement learning;self-learning model;statistical process;stochastic reasoning;user behavior;virtual world,,1,,9,,,,15-17 Aug. 2005,,IEEE,IEEE Conferences
Learning to give route directions from human demonstrations,S. Oßwald; H. Kretzschmar; W. Burgard; C. Stachniss,"Department of Computer Science, University of Freiburg, 79110 Freiburg, Germany",2014 IEEE International Conference on Robotics and Automation (ICRA),20140929,2014,,,3303,3308,"For several applications, robots and other computer systems must provide route descriptions to humans. These descriptions should be natural and intuitive for the human users. In this paper, we present an algorithm that learns how to provide good route descriptions from a corpus of human-written directions. Using inverse reinforcement learning, our algorithm learns how to select the information for the description depending on the context of the route segment. The algorithm then uses the learned policy to generate directions that imitate the style of the descriptions provided by humans, thus taking into account personal as well as cultural preferences and special requirements of the particular user group providing the learning demonstrations. We evaluate our approach in a user study and show that the directions generated by our policy sound similar to human-given directions and substantially more natural than directions provided by commercial web services.",1050-4729;10504729,Electronic:978-1-4799-3685-4; POD:978-1-4799-3686-1; USB:978-1-4799-3684-7,10.1109/ICRA.2014.6907334,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6907334,,Computers;Context;Cultural differences;Entropy;Learning (artificial intelligence);Measurement;Web services,control engineering computing;learning (artificial intelligence);mobile robots;path planning,computer systems;cultural preferences;human demonstrations;human-given directions;human-written directions;inverse reinforcement learning;learning demonstrations;personal preferences;robots;route descriptions;route directions;route segment,,3,,25,,,,May 31 2014-June 7 2014,,IEEE,IEEE Conferences
Dynamic Information Retrieval Modeling,G. H. Yang; M. Sloan; J. Wang,,Dynamic Information Retrieval Modeling,20160706,2016,,,,,"<p> Big data and human-computer information retrieval (HCIR) are changing IR. They capture the dynamic changes in the data and dynamic interactions of users with IR systems. A dynamic system is one which changes or adapts over time or a sequence of events. Many modern IR systems and data exhibit these characteristics which are largely ignored by conventional techniques. What is missing is an ability for the model to change over time and be responsive to stimulus. Documents, relevance, users and tasks all exhibit dynamic behavior that is captured in data sets typically collected over long time spans and models need to respond to these changes. Additionally, the size of modern datasets enforces limits on the amount of learning a system can achieve. Further to this, advances in IR interface, personalization and ad display demand models that can react to users in real time and in an intelligent, contextual way. </p> <p>In this book we provide a comprehensive and up to-date introduction to Dynamic Information Retrieval Modeling, the statistical modeling of IR systems that can adapt to change. We define <i>dynamics</i>, what it means within the context of IR and highlight examples of problems where dynamics play an important role. We cover techniques ranging from classic relevance feedback to the latest applications of partially observable Markov decision processes (POMDPs) and a handful of useful algorithms and tools for solving IR problems incorporating dynamics. </p> <p>The theoretical component is based around the Markov Decision Process (MDP), a mathematical framework taken from the field of Artificial Intelligence (AI) that enables us to construct models that change according to sequential inputs. We define the framework and the algorithms commonly used to optimize over it and generalize it to the case where the inputs aren't reliable. We explore the topic of reinforcement learning more broadly and introduce nother tool known as a Multi-Armed Bandit which is useful for cases where exploring model parameters is beneficial. Following this we introduce theories and algorithms which can be used to incorporate dynamics into an IR model before presenting an array of state-of-the-art research that already does, such as in the areas of session search and online advertising. </p> <p>Change is at the heart of modern Information Retrieval systems and this book will help equip the reader with the tools and knowledge needed to understand <i>Dynamic Information Retrieval Modeling</i>. </p>",,97816270552,10.2200/S00718ED1V01Y201605ICR049,,https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=7503460.pdf&bkn=7503459&pdfType=book,Markov decision process;dynamic information retrieval;information retrieval;information retrieval evaluation;information retrieval models;recommender systems;reinforcement learning,,,,,,,,2016,,,,,Morgan & Claypool,Morgan and Claypool eBooks
Expert Level Control of Ramp Metering Based on Multi-Task Deep Reinforcement Learning,F. Belletti; D. Haziza; G. Gomes; A. M. Bayen,"Computer Science Deptartment, University of California at Berkeley, Berkeley, CA, USA",IEEE Transactions on Intelligent Transportation Systems,20180328,2018,19,4,1198,1207,"This paper shows how the recent breakthroughs in reinforcement learning (RL) that have enabled robots to learn to play arcade video games, walk, or assemble colored bricks, can be used to perform other tasks that are currently at the core of engineering cyberphysical systems. We present the first use of RL for the control of systems modeled by discretized non-linear partial differential equations (PDEs) and devise a novel algorithm to use non-parametric control techniques for large multi-agent systems. Cyberphysical systems (e.g., hydraulic channels, transportation systems, the energy grid, and electromagnetic systems) are commonly modeled by PDEs, which historically have been a reliable way to enable engineering applications in these domains. However, it is known that the control of these PDE models is notoriously difficult. We show how neural network-based RL enables the control of discretized PDEs whose parameters are unknown, random, and time-varying. We introduce an algorithm of mutual weight regularization (MWR), which alleviates the curse of dimensionality of multi-agent control schemes by sharing experience between agents while giving each agent the opportunity to specialize its action policy so as to tailor it to the local parameters of the part of the system it is located in. A discretized PDE, such as the scalar Lighthill-Whitham-Richards PDE can indeed be considered as a macroscopic freeway traffic simulator and which presents the most salient challenges for learning to control large cyberphysical system with multiple agents. We consider two different discretization procedures and show the opportunities offered by applying deep reinforcement for continuous control on both. Using a neural RL PDE controller on a traffic flow simulation based on a Godunov discretization of the San Francisco Bay Bridge, we are able to achieve precise adaptive metering without model calibration thereby beating the state of the art in traffic metering. Furthermore, with the m- re accurate BeATS simulator, we manage to achieve a control performance on par with ALINEA, a state-of-the-art parametric control scheme, and show how using MWR improves the learning procedure.",1524-9050;15249050,,10.1109/TITS.2017.2725912,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8011495,Deep learning;continuous control;deep reinforcement learning;macroscopic traffic models;partial differential equations;reinforcement learning;transportation systems,Biological system modeling;Control systems;Cyber-physical systems;Learning (artificial intelligence);Mathematical model;Neural networks;Transportation,computer games;digital simulation;learning (artificial intelligence);multi-agent systems;partial differential equations;road traffic;road traffic control;traffic control;traffic engineering computing,Godunov discretization;MWR;PDE models;arcade video games;assemble colored bricks;continuous control;control performance;cyberphysical system;different discretization procedures;discretized PDE;electromagnetic systems;engineering applications;engineering cyberphysical systems;expert level control;hydraulic channels;learning procedure;macroscopic freeway traffic simulator;model calibration;multiagent control schemes;multiagent systems;multiple agents;multitask deep reinforcement learning;mutual weight regularization;neural RL PDE controller;nonlinear partial differential equations;nonparametric control techniques;parametric control scheme;precise adaptive metering;ramp metering;scalar Lighthill-Whitham-Richards PDE;traffic metering;transportation systems,,,,,,,20170816,April 2018,,IEEE,IEEE Journals & Magazines
Reinforcement learning-based cooperative sensing in cognitive radio ad hoc networks,B. F. Lo; I. F. Akyildiz,"Broadband Wireless Networking Laboratory, School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, 30332, USA","21st Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications",20101217,2010,,,2244,2249,"In cognitive radio networks, spectrum sensing is a fundamental function for detecting the presence of primary users in licensed frequency bands. Due to multipath fading and shadowing, the performance of detection may be considerably compromised. To improve the detection probability, cooperative sensing is an effective approach for secondary users to cooperate and combat channel impairments. This approach, however, incurs overhead such as sensing delay for reporting local decisions and the increase of control traffic in the network. In this paper, a reinforcement learning-based cooperative sensing method is proposed to address the cooperation overhead problem. By using the proposed cooperative sensing model, the secondary user learns to (i) find the optimal set of cooperating neighbors with minimum control traffic, (ii) minimize the overall cooperative sensing delay, (iii) select independent users for cooperation under correlated shadowing, and (iv) improve the energy efficiency for cooperative sensing. The simulation results show that the proposed reinforcement learning-based cooperative sensing method reduces the overhead of cooperative sensing while effectively improving the detection performance to combat correlated shadowing.",2166-9570;21669570,Electronic:978-1-4244-8016-6; POD:978-1-4244-8017-3,10.1109/PIMRC.2010.5671686,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5671686,,Correlation;Delay;Fading;Learning;Markov processes;Sensors;Shadow mapping,ad hoc networks;cognitive radio;cooperative systems;learning (artificial intelligence);telecommunication computing,ad hoc networks;cognitive radio networks;control traffic;cooperative sensing delay;detection performance;multipath fading;reinforcement learning-based cooperative sensing;reinforcement learning-based cooperative sensing method;shadowing;spectrum sensing,,11,,12,,,,26-30 Sept. 2010,,IEEE,IEEE Conferences
A learning model for personalized adaptive cruise control,X. Chen; Y. Zhai; C. Lu; J. Gong; G. Wang,"Intelligent Vehicle Research Center, Beijing Institute of Technology, Beijing 100081 China",2017 IEEE Intelligent Vehicles Symposium (IV),20170731,2017,,,379,384,"This paper develops a learning model for personalized adaptive cruise control that can learn from human demonstration online and mimic a human driver's driving strategies in the dynamic traffic environment. Under the framework of the proposed model, reinforcement learning is used to capture the human-desired driving strategy, and the proportion-integration-differentiation controller is adopted to convert the learning strategy to low-level control commands. The performance of the learning model is tested in the simulation environment built in a driving simulator using PreScan. Experimental results show that the learning model can duplicate human driving strategies with acceptable errors. Moreover, compared with the traditional adaptive cruise control, the proposed model can provide better driving comfort and smoothness in the dynamic situation.",,Electronic:978-1-5090-4804-5; POD:978-1-5090-4805-2; USB:978-1-5090-4803-8,10.1109/IVS.2017.7995748,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7995748,,Acceleration;Adaptation models;Cruise control;Data models;Learning (artificial intelligence);Vehicle dynamics;Vehicles,adaptive control;intelligent transportation systems;learning (artificial intelligence);three-term control,PreScan;driving simulator;human demonstration;human driver;human-desired driving strategy;intelligent driving systems;learning model;personalized adaptive cruise control;proportion-integration-differentiation controller;reinforcement learning,,,,,,,,11-14 June 2017,,IEEE,IEEE Conferences
Past Makes Future: Role of pFC in Prediction,J. M. Fuster; S. L. Bressler,"1University of California, Los Angeles",Journal of Cognitive Neuroscience,20150305,2015,27,4,639,654,"<para>The pFC enables the essential human capacities for predicting future events and preadapting to them. These capacities rest on both the structure and dynamics of the human pFC. Structurally, pFC, together with posterior association cortex, is at the highest hierarchical level of cortical organization, harboring neural networks that represent complex goal-directed actions. Dynamically, pFC is at the highest level of the perception–action cycle, the circular processing loop through the cortex that interfaces the organism with the environment in the pursuit of goals. In its predictive and preadaptive roles, pFC supports cognitive functions that are critical for the temporal organization of future behavior, including planning, attentional set, working memory, decision-making, and error monitoring. These functions have a common future perspective and are dynamically intertwined in goal-directed action. They all utilize the same neural infrastructure: a vast array of widely distributed, overlapping, and interactive cortical networks of personal memory and semantic knowledge, named cognits, which are formed by synaptic reinforcement in learning and memory acquisition. From this cortex-wide reservoir of memory and knowledge, pFC generates purposeful, goal-directed actions that are preadapted to predicted future events.</para>",0898-929X;0898929X,,10.1162/jocn_a_00746,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7052276,,,,,,0,,,,,,April 2015,,MIT Press,MIT Press Journals
Autonomous resource allocation for dense LTE networks: A Multi Armed Bandit formulation,A. Feki; V. Capdevielle,"Alcatel-Lucent Bell Labs, France","2011 IEEE 22nd International Symposium on Personal, Indoor and Mobile Radio Communications",20120126,2011,,,66,70,"Resource allocation is an important prerequisite for the effective deployment of Pico Cells (PCs). This topic becomes even more challenging in the case of heterogeneous networks, where autonomous interference management mechanisms are necessary. In this article, we propose a resource sharing method inspired from the reinforcement learning theory and particularly the methods used to solve the Multi Armed Bandit (MAB) problem. The main goal resides in giving the ability for each cell to make its decision autonomously while dynamically taking into account the resource occupation of each surrounding cell. We set up the global framework for the MAB based resource allocation strategies in the case of total frequency overlapping PCs. The performances of the proposed method are evaluated in the case of Long Term Evolution (LTE) Pico Cells deployment and compared to static allocation schemes. The results demonstrate the efficiency of our method.",2166-9570;21669570,Electronic:978-1-4577-1348-4; POD:978-1-4577-1346-0; USB:978-1-4577-1347-7,10.1109/PIMRC.2011.6140047,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140047,Inter-Cell Interference (ICI);LTE;MAB;Pico Cell (PC);Reinforcement Learning theory;Resource allocation,Indexes;Interference;OFDM;Resource management;Signal to noise ratio;Throughput;Time frequency analysis,Long Term Evolution;interference suppression;learning (artificial intelligence);picocellular radio;telecommunication computing;telecommunication network management,Long Term Evolution;MAB based resource allocation;autonomous interference management mechanisms;autonomous resource allocation;dense LTE networks;heterogeneous networks;multiarmed bandit formulation;pico cells;reinforcement learning theory;resource sharing method,,6,1,12,,,,11-14 Sept. 2011,,IEEE,IEEE Conferences
A unified learning paradigm for large-scale personalized information management,E. Y. Chang; S. C. H. Hop; Xinjing Wang; Wei-Ying Max; M. R. Lyu,"Electr. & Comput. Eng., California Univ., Santa Barbara, CA, USA","Conference, Emerging Information Technology 2005.",20051205,2005,,,4 pp.,,"Statistical-learning approaches such as unsupervised learning, supervised learning, active learning, and reinforcement learning have generally been separately studied and applied to solve application problems. In this paper, we provide an overview of our newly proposed unified learning paradigm (ULP), which combines these approaches into one synergistic framework. We outline the architecture and the algorithm of ULP, and explain benefits of employing this unified learning paradigm on personalizing information management.",,POD:0-7803-9328-7,10.1109/EITC.2005.1544372,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544372,,Clustering algorithms;Convergence;Humans;Information management;Kernel;Large-scale systems;Machine learning;Stability;Supervised learning;Unsupervised learning,information management;learning (artificial intelligence),large-scale personalized information management;statistical-learning approach;unified learning paradigm,,1,,9,,,,15-16 Aug. 2005,,IEEE,IEEE Conferences
QoS-Oriented Wireless Routing for Smart Meter Data Collection: Stochastic Learning on Graph,Y. Cao; D. Duan; X. Cheng; L. Yang; J. Wei,"Dept. of Electron. & Inf. Eng., Huazhong Univ. of Sci. & Technol., Wuhan, China",IEEE Transactions on Wireless Communications,20140808,2014,13,8,4470,4482,"To ensure resilient and reliable meter data collection that is essential for the smart grid operation, we propose a QoS-oriented wireless routing scheme. Specifically tailored for the heterogeneity of the meter data traffic in the smart grid, we first design a novel utility function that not only jointly accounts for system throughput and transmission latency, but also allows for flexible tradeoff between the two with a strict transmission latency constraint, as desired by various smart meter applications. Then, we model the interactions among smart meter data concentrators as a mixed-strategy network formation game. To avoid potential information exchange which is not always practical in meter data collection scenario, a stochastic reinforcement learning algorithm with only private and incomplete information is proposed to solve the network formation problem. Such a problem formulation, together with our proposed stochastic learning algorithm on graph, results in a steady probabilistic route. Both contributions are novel and unique in comparison with existing work on this topic. Another distinct feature of our approach is its capability of effectively maintaining the QoS of smart meter data collection, even when the network is under fault or attack, as verified by simulations.",1536-1276;15361276,,10.1109/TWC.2014.2314121,Science Foundation for the Youth Scholar of Ministry of Education of China; 10.13039/100000001 - National Science Foundation; 10.13039/501100001809 - National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6779690,Network formation;smart grid;stochastic learning;wireless routing,Data collection;Games;Quality of service;Routing;Smart grids;Throughput;Wireless communication,graph theory;learning (artificial intelligence);power engineering computing;quality of service;smart meters;stochastic processes;telecommunication network routing;wireless channels,QoS-oriented wireless routing;mixed-strategy network formation game;network formation problem;smart grid operation;smart meter data collection;stochastic learning;stochastic reinforcement learning algorithm;transmission latency constraint;wireless channels,,5,,40,,,20140327,Aug. 2014,,IEEE,IEEE Journals & Magazines
Towards a General Supporting Framework for Self-Adaptive Software Systems,L. Wang; Y. Gao; C. Cao; L. Wang,"State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China",2012 IEEE 36th Annual Computer Software and Applications Conference Workshops,20121110,2012,,,158,163,"When confronted with their internal and environmental dynamics, various software systems increasingly require self-adaptation capabilities. The vision above requires the self-adaptation approach to tackle these challenges and some software systems have their own specific implementations. However, in this paper a general supporting framework is proposed for software systems to self-adapt with running environmental dynamics and meanwhile fulfill various user requirements. Three key issues are covered in the framework: 1) the overall control architecture, which adopts the double closed-loop style and respectively includes the self-adaptation loop and the self-learning loop; 2) a general descriptive language, which is a application-independent and unified language to represent self-adaptation knowledge about target systems; 3) three implementation mechanisms, including forward reasoning, planning and reinforcement learning using feedback, which are supported by the above descriptive language and executed at runtime in different modules. Finally, one scenario of on-demand services of massive data mining tasks is selected and the case study demonstrates how the framework is customized as required and how the approach works.",,Electronic:978-0-7695-4758-9; POD:978-1-4673-2714-5,10.1109/COMPSACW.2012.38,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341568,Double Closed-loop Control Arthitecture;General Descriptive Language;Hierarchical Task Network;Rete Algorithm;Self-Adaptive Supporting Framework;Self-Adaptive System,Adaptation models;Cognition;Computer architecture;Learning;Monitoring;Planning;Software systems,data mining;inference mechanisms;software engineering;unsupervised learning,data mining;environmental dynamics;forward reasoning;general descriptive language;general supporting framework;on-demand service;overall control architecture;reinforcement learning;self-adaptation knowledge;self-adaptation loop;self-adaptive software system;self-learning loop,,2,,18,,,,16-20 July 2012,,IEEE,IEEE Conferences
Personalized Web recommendations: supporting epistemic information about end-users,M. Preda; D. Popescu,"Dept. of Comput. Sci., Craiova Univ., Romania",The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05),20051017,2005,,,692,695,The online recommendations are a popular presence in the Web sites world due to their potential to increase the customers' satisfaction. The ability to represent epistemic information about the clients' beliefs is important to understand their needs. This paper presents a recommender system based on reinforcement learning. The system represents concepts presented on a Web site by epistemic logical programs and uses a similarity measure between programs in order to facilitate generalization. A prototype of this system and experiments are presented.,,POD:0-7695-2415-X,10.1109/WI.2005.115,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1517935,,Automation;Computer science;Feedback;Function approximation;Humans;Learning;Logic;Ontologies;Prototypes;Recommender systems,Web sites;customer satisfaction;information filters;learning (artificial intelligence);logic programming,Web site;customer satisfaction;end-user;epistemic logical program;online recommendation;personalized Web recommendation;program similarity measure;reinforcement learning,,0,,8,,,,19-22 Sept. 2005,,IEEE,IEEE Conferences
Self-configuring Switched Multi-Element Antenna system for interference mitigation in femtocell networks,R. Razavi; H. Claussen,"Bell Laboratories, Alcatel-Lucent, Blanchardtown Industrial Park, Dublin 15, Republic of Ireland","2011 IEEE 22nd International Symposium on Personal, Indoor and Mobile Radio Communications",20120126,2011,,,237,242,"This paper introduces a Switched Multi-Element Antenna (SMEA) solution for interference mitigation in femtocell networks. While the main objective is to protect the femtocell users against uplink interference, the downlink interference from femtocell base stations to the other users is simultaneously reduced as a by-product of this technique. A tailored form of reinforcement learning is used to allow for self-configuration of the femtocell base station and to adaptively select the optimal antenna configuration in a time varying environment. Compared to the traditional Omni-directional antenna systems, the results show an average of 2.5dB gain in uplink direction in terms of reduced transmission power and approximately 1dB of gain in the downlink channel.",2166-9570;21669570,Electronic:978-1-4577-1348-4; POD:978-1-4577-1346-0; USB:978-1-4577-1347-7,10.1109/PIMRC.2011.6139947,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139947,Femtocell networks;Interference management;Multi-element antenna;Q-learning;Reinforcement learning;Self-configuration;WCDMA Femtocells,Antenna measurements;Base stations;Interference;Macrocell networks;Signal to noise ratio;Transmitting antennas,antenna arrays;femtocellular radio;radiofrequency interference,SMEA;femtocell networks;interference mitigation;omnidirectional antenna systems;reinforcement learning;self-configuring switched multielement antenna system;transmission power reduction,,3,,19,,,,11-14 Sept. 2011,,IEEE,IEEE Conferences
A unified control framework of HVAC system for thermal and acoustic comforts in office building,Y. Zhao; Q. Zhao; L. Xia; Z. Cheng; F. Wang; F. Song,"Dept. of Autom., Tsinghua Univ., Beijing, China",2013 IEEE International Conference on Automation Science and Engineering (CASE),20131107,2013,,,416,421,"Intelligent building system attracts more and more attention in both academic and industrial communities. Learning human comfort requirements and incorporating it into building control system is one of the important issues. In the traditional HVAC control system, the thermal comfort and the acoustic comfort are often conflicted and we lack of a scheme to trade off them well. In this paper, we propose a unified control framework based on reinforcement learning to balance the multiple dimension comforts, including the thermal and acoustic comforts. We utilize the user's complaints in thermal and acoustic sensations as feedback and combine the current environment and devices information to learn the personalized optimal control policy using online Q-learning. The challenge caused by the complaints is coped with an incorporated perception estimation scheme in the Q-learning reward design. Both simulation results and the field experimental results demonstrate the effectiveness of the algorithm, especially in the adaptivity to the individual tradeoff between thermal and acoustic comfort.",2161-8070;21618070,Electronic:978-1-4799-1515-6; POD:978-1-4799-1513-2; USB:978-1-4799-1514-9,10.1109/CoASE.2013.6653964,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6653964,,Acoustics;Buildings;Estimation;Humidity;Noise;Temperature sensors;Upper bound,HVAC;architectural acoustics;building management systems;control engineering computing;learning (artificial intelligence);optimal control,HVAC system;Q-learning reward design;acoustic comforts;acoustic sensations;office building;online Q-learning;perception estimation scheme;personalized optimal control policy;reinforcement learning;thermal comforts;thermal sensations;unified control framework,,1,,17,,,,17-20 Aug. 2013,,IEEE,IEEE Conferences
Learning Bayesian networks probabilities from longitudinal data,R. Bellazzi; A. Riva,"Lab. di Inf. Medica, Pavia Univ., Italy","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans",20020806,1998,28,5,629,636,"Many real applications of Bayesian networks (BN) concern problems in which several observations are collected over time on a certain number of similar plants. This situation is typical of the context of medical monitoring, in which several measurements of the relevant physiological quantities are available over time on a population of patients under treatment, and the conditional probabilities that describe the model are usually obtained from the available data through a suitable learning algorithm. In situations with small data sets for each plant, it is useful to reinforce the parameter estimation process of the BN by taking into account the observations obtained from other similar plants. On the other hand, a desirable feature to be preserved is the ability to learn individualized conditional probability tables, rather than pooling together all the available data. In this work we apply a Bayesian hierarchical model able to preserve individual parameterization, and, at the same time, to allow the conditionals of each plant to borrow strength from all the experience contained in the data-base. A testing example and an application in the context of diabetes monitoring will be shown",1083-4427;10834427,,10.1109/3468.709608,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=709608,,Bayesian methods;Biomedical monitoring;Condition monitoring;Context modeling;Diabetes;Medical treatment;Parameter estimation;Patient monitoring;Testing;Time measurement,Bayes methods;learning (artificial intelligence);observers;parameter estimation;patient diagnosis;probability,BN;Bayesian hierarchical model;Bayesian networks probability learning;conditional probabilities;diabetes monitoring;individualized conditional probability tables;longitudinal data;medical monitoring;parameter estimation process reinforcement,,2,,19,,,,Sep 1998,,IEEE,IEEE Journals & Magazines
"Utilization of machine learning methods for assembling, training and understanding autonomous robots",P. Hartono,"Department of Mechanics and Information Technology, Chukyo University, Toyota, Japan","2011 4th International Conference on Human System Interactions, HSI 2011",20110704,2011,,,398,402,"For decades human society has been supported by the proliferation of complex artifacts such as electronic appliances, personal vehicles and mass transportation systems, electrical and communications grids, and in the past few decades, Internet. In the very near future, robots will play increasingly important roles in our daily life. The increase in complexity of the tasks and sometimes physical forms or morphologies of the artifacts consequently requires complex assembling and controlling procedures of them, which soon will be unmanageable by the traditional manufacturing process. The aim of this paper is to give a brief review on the potentials of the non-traditional assembling of complex artifacts, which in this study is symbolized by the creation of autonomous robots. Methods in self-assembling modular robots, real time learning of autonomous robots and a method for giving the comprehensive understanding, albeit intuitively, to human will be explained through some physical experiments.",2158-2246;21582246,Electronic:978-1-4244-9639-6; Electronic:978-1-4244-9640-2; POD:978-1-4244-9638-9,10.1109/HSI.2011.5937399,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5937399,Autonomous Robots;Modular Robots;Perceptual Understanding;Reinforcement Learning;Self Assembling;Self-Organizing Map,Learning systems;Morphology;Robot kinematics;Robot sensing systems;Topology;Training,learning (artificial intelligence);mobile robots;robotic assembly,autonomous robots assembling;autonomous robots training;autonomous robots undertanding;machine learning methods;selfassembling modular robots,,2,,15,,,,19-21 May 2011,,IEEE,IEEE Conferences
Reducing Delay during Vertical Handover,N. Bagdure; B. Ambudkar,"Dept. of Electron. Eng., Pad. Dr. D.Y.P.I.E.T., Pune, India",2015 International Conference on Computing Communication Control and Automation,20150716,2015,,,200,204,"The next generation of wireless networks will provide heterogeneous wireless technologies to a mobile user, in which they can move using terminals with multiple access interfaces and non-real-time or real-time services. The most important issue in such environment is the Always Best Connected (ABC) concept allowing the best connectivity to applications anywhere at any time. With the growing of communication world lot of research is carried out in the field of wireless networks specifically next generation networks. The integration and interoperability of various wireless networks will lead to number of challenges. One of the challenges is the handovers across various/heterogeneous wireless networks as well as reduction of call drop probability. Number of researches has proposed solutions for this challenge. This paper proposes a solution to this problem for the improvement of end-to-end Quality of Service supporting high data rates, real time transmission over wide area and enabling users to specify their personal preferences using Markov Decision Process (MDP). The proposed method uses delay as its basic parameter to select a network during handovers. The selection of the network amongst the existing wireless network considers the ongoing application of the user during the handover. Through simulations we show that our algorithm reduces call drop probability and satisfies user's requirements.",,Electronic:978-1-4799-6892-3; POD:978-1-4799-6893-0,10.1109/ICCUBEA.2015.44,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155834,Markov Decision process;Reinforcement Learning;Reward;Transition Probability;Vertical Handover,Bandwidth;Delays;Handover;Mobile communication;Quality of service;Wireless networks,delays;mobility management (mobile radio);next generation networks;probability;quality of service,call drop probability reduction;delay reduction;end-to-end quality of service;handover;heterogeneous wireless networks;mobile user;next generation networks;wireless network integration;wireless network interoperability,,0,,13,,,,26-27 Feb. 2015,,IEEE,IEEE Conferences
Design considerations of reinforcement learning power controllers in Wireless Body Area Networks,R. Kazemi; R. Vesilo; E. Dutkiewicz; R. P. Liu,"Department of Engineering, Macquarie University, Sydney, Australia","2012 IEEE 23rd International Symposium on Personal, Indoor and Mobile Radio Communications - (PIMRC)",20121129,2012,,,2030,2036,"A Wireless Body Area Network (WBAN) comprises a number of tiny devices implanted in/on the body that sample physiological signals of the human body and send them to a coordinator node for medical or other purposes. As these miniature devices run on built-in batteries, energy is the most valuable resource in WBANs. This makes signal interference between neighboring WBANs a serious threat because it causes energy waste in these systems. To mitigate this internetwork interference, we propose a dynamic power control mechanism in WBANs which employs reinforcement learning (RL) to learn from experience and improve its performance. This paper presents guidelines in designing efficient RL power controllers in WBANs and provides an analysis of the effect of the reward function, discount factor, learning rate and eligibility trace parameter where the main performance criteria used are convergence and solution optimality in terms of throughput and energy consumption per bit.",2166-9570;21669570,Electronic:978-1-4673-2569-1; POD:978-1-4673-2566-0; USB:978-1-4673-2568-4,10.1109/PIMRC.2012.6362688,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6362688,Game;Interference;Power Control;Reinforcement Learning;WBAN;convergence;multi agent;optimality,Convergence;Energy consumption;Games;Interference;Power control;Signal to noise ratio;Wireless communication,biomedical communication;body area networks;control engineering computing;control system synthesis;interference suppression;learning (artificial intelligence);medical computing;medical control systems;power control;prosthetics,RL;WBAN;eligibility trace parameter;energy consumption;energy waste;human body implantation;internetwork interference mitigation;medical coordinator node;physiological signal;power control mechanism;reinforcement learning;signal interference;wireless body area network,,1,,14,,,,9-12 Sept. 2012,,IEEE,IEEE Conferences
Learning algorithms For intelligent agents based e-learning system,N. Pandey; R. K. Tyagi; S. Sahu; A. Dwivedi,"Department of Computer Science & Engineering, Ajay Kumar Garg Engineering College, Ghaziabad. India",2013 3rd IEEE International Advance Computing Conference (IACC),20130513,2013,,,1034,1039,"Requirements are the basic entity which makes the project to be successfully implemented. In the development of the software, theses requirements must be measured accurately and correctly. The requirements of the end users may be changed with respect to different individual personality. As different requirements are given by different customers, all of them cannot be processed by single software system. In such a case, it is essential to make the changes in the software systems automatically which fulfills all the requirements of the user. Such condition is met by the systems with intelligent agents. In this paper, algorithms of intelligent agents adviser agent, personalization agent and content managing agent are proposed to automatically gather the requirements of the students and fulfill them. The algorithms are structured using reinforcement learning. Theses algorithm makes the agents to sense the needs of the students and evolve in the course of their operation. These intelligent agents are applied after the deployment of the e-learning software. All the algorithms have been implemented successfully.",,Electronic:978-1-4673-4529-3; POD:978-1-4673-4527-9,10.1109/IAdCC.2013.6514369,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514369,E-Learning;Intelligent Agent;Reinforcement Learnin;Requirement Engineering,Databases;Electronic learning;Intelligent agents;Negative feedback;Software algorithms;Standards,computer aided instruction;learning (artificial intelligence);multi-agent systems,adviser agent;content managing agent;e-learning system;electronic learning;intelligent agent;learning algorithm;personalization agent;reinforcement learning;software development;student requirement;user requirement,,1,,8,,,,22-23 Feb. 2013,,IEEE,IEEE Conferences
Augmenting practical cross-layer MAC schedulers via offline reinforcement learning,F. Pianese; P. J. Danielsen,Nokia Bell Labs,"2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)",20180215,2017,,,1,6,"An automated offline design process for optimized cross-layer schedulers can produce augmented scheduling algorithms tailored to a target deployment scenario. We discuss the application of ODDS, a reinforcement learning technique we introduced, for augmenting LTE MAC scheduler algorithms of practical significance. ODDS observes the correlation between the value of a utility function and the cross-layer state parameters seen by an instrumented baseline scheduler, determining the best actions via an offline Monte Carlo exploration of the problem space. The result of the ODDS process is a compact definition of a scheduling policy that has been optimized for a target scenario and utility function. In this paper we instrument a production scheduler definition to evaluate the potential of augmented schedulers in practical use, and experiment with awareness to application traffic properties by using a multi-class utility function, yielding scheduling policies that behave differently depending on the features of individual traffic classes.",,Electronic:978-1-5386-3531-5; POD:978-1-5386-3532-2; Paper:978-1-5386-3529-2; USB:978-1-5386-3530-8,10.1109/PIMRC.2017.8292409,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8292409,,Algorithm design and analysis;Measurement;Optimization;Production;Scheduling algorithms;Throughput;Training,Long Term Evolution;Monte Carlo methods;access protocols;learning (artificial intelligence);telecommunication scheduling;telecommunication traffic,LTE MAC scheduler algorithms;ODDS process;augmented schedulers;augmented scheduling algorithms;automated offline design process;cross-layer state parameters;instrumented baseline scheduler;multiclass utility function;offline Monte Carlo exploration;offline reinforcement learning;optimized cross-layer schedulers;practical cross-layer MAC schedulers;production scheduler definition;reinforcement learning technique;scheduling policy;target deployment scenario,,,,,,,,8-13 Oct. 2017,,IEEE,IEEE Conferences
Reinforcement learning for resource provisioning in the vehicular cloud,M. A. Salahuddin; A. Al-Fuqaha; M. Guizani,Universite du Quebec a Montreal,IEEE Wireless Communications,20160826,2016,23,4,128,135,"This article presents a concise view of vehicular clouds that incorporates various vehicular cloud models that have been proposed to date. Essentially, they all extend the traditional cloud and its utility computing functionalities across the entities in the vehicular ad hoc network. These entities include fixed roadside units, onboard units embedded in the vehicle, and personal smart devices of drivers and passengers. Cumulatively, these entities yield abundant processing, storage, sensing, and communication resources. However, vehicular clouds require novel resource provisioning techniques that can address the intrinsic challenges of dynamic demands for the resources and stringent QoS requirements. In this article, we show the benefits of reinforcement-learning-based techniques for resource provisioning in the vehicular cloud. The learning techniques can perceive long-term benefits and are ideal for minimizing the overhead of resource provisioning for vehicular clouds.",1536-1284;15361284,,10.1109/MWC.2016.7553036,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7553036,,Cloud computing;Dynamic scheduling;Intelligent vehicles;Quality of service;Resource management;Software as a service;Vehicle dynamics,cloud computing;learning (artificial intelligence);minimisation;quality of service;resource allocation;traffic engineering computing;vehicular ad hoc networks,QoS requirements;communication resources;dynamic resource demands;fixed roadside units;onboard units;overhead minimization;personal smart devices;processing resources;reinforcement learning;resource provisioning;sensing resources;storage resources;utility computing functionalities;vehicular ad hoc network;vehicular cloud models,,1,,,,,,August 2016,,IEEE,IEEE Journals & Magazines
